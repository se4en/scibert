{"sent_id": "0860b08831b01e7e98c66ced63b256-C001-27", "intents": ["@MOT@"], "paper_id": "ABC_0860b08831b01e7e98c66ced63b256_0", "text": "The dependency-based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings [12] ."}
{"sent_id": "ad62ec914bb7b002952f22afdca15f-C001-32", "intents": ["@USE@"], "paper_id": "ABC_ad62ec914bb7b002952f22afdca15f_0", "text": "Second, we extract word-to-word associations (or called word dependency, a dependency implies its close association with other words in either syntactic or semantic perspective) from large amounts of auto-parsed data and adopt word2vecf [13] to train dependency-based word embeddings."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-11", "intents": ["@MOT@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "Accordingly, there is a growing incentive for businesses to solicit and manufacture DECEPTIVE OPINION SPAMfictitious reviews that have been deliberately written to sound authentic and deceive the reader (Ott et al., 2011) ."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-51", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "Following Ott et al. (2011) , we asked three volunteer undergraduate university students to read and make assessments on a subset of the negative review dataset described in Section 2."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-78", "intents": ["@SIM@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "Likewise, our fake negative reviews had more verbs relative to nouns than truthful, suggesting a more narrative style that is indicative of imaginative writing (Biber et al., 1999; Rayson et al., 2001 ), a pattern also observed by Ott et al. (2011) ."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-85", "intents": ["@BACK@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "In the positive reviews reported by Ott et al. (2011) , the rate of first person singular in fake reviews (M=4.36%, SD=2.96%) was twice the rate observed in truthful reviews (M=2.18%, SD=2.04%)."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-75", "intents": ["@UNSURE@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "An important question is how language features operate in our fake negative reviews compared with the fake positive reviews of Ott et al. (2011) ."}
{"sent_id": "5f62958d0cdd32b15067c1afe458a5-C001-3", "intents": ["@MOT@", "@EXT@"], "paper_id": "ABC_5f62958d0cdd32b15067c1afe458a5_0", "text": "In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models."}
{"sent_id": "5f62958d0cdd32b15067c1afe458a5-C001-49", "intents": ["@EXT@", "@BACK@"], "paper_id": "ABC_5f62958d0cdd32b15067c1afe458a5_0", "text": "e toolkit includes implementations of state-of-the-art representation learning models that were applied to expert nding [9] and product search [8] ."}
{"sent_id": "5f62958d0cdd32b15067c1afe458a5-C001-62", "intents": ["@UNSURE@"], "paper_id": "ABC_5f62958d0cdd32b15067c1afe458a5_0", "text": "However, in the special case when the model is interpreted as a metric vector space [2, 8] , SERT casts entity ranking as a k-nearest neighbor problem and uses specialized data structures for retrieval [5] ."}
{"sent_id": "5f62958d0cdd32b15067c1afe458a5-C001-65", "intents": ["@USE@"], "paper_id": "ABC_5f62958d0cdd32b15067c1afe458a5_0", "text": "e toolkit contains implementations of state-of-the-art entity representations algorithms [8, 9] and consists of three components: text processing, representation learning and inference."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "In their first game, Lazaridou's Sender and Receiver are exposed to the same pair of images, one of them being randomly marked as the \"target\"."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-41", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "Following Lazaridou, the images are passed through a pre-trained VGG ConvNet (Simonyan and Zisserman, 2015) ."}
{"sent_id": "737a452057be3e254b35bd8df492be-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_737a452057be3e254b35bd8df492be_0", "text": "[10] released one of the initial data sets from Twitter with the goal of identifying what constitutes racism and sexism."}
{"sent_id": "737a452057be3e254b35bd8df492be-C001-17", "intents": ["@USE@"], "paper_id": "ABC_737a452057be3e254b35bd8df492be_0", "text": "Stop saying dumb blondes with pretty faces as you need a pretty face to pull them off !!! #mkr In Islam women must be locked in their houses and Muslims claim this is treating them well Table 1 : Tweets from [10] data set demonstrating online abuse They find that racist and homophobic tweets are more likely to be classified as hate speech but sexist tweets are generally classified as offensive."}
{"sent_id": "737a452057be3e254b35bd8df492be-C001-66", "intents": ["@USE@"], "paper_id": "ABC_737a452057be3e254b35bd8df492be_0", "text": "We call [10] data set as D1 , [9] data set as D2 and [4] as D3 For tweet tokenization, we use Ekphrasis which is a text processing tool built specially from social platforms such as Twitter."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-19", "intents": ["@MOT@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "al. [18] proposed a dynamic memory based neural network for implicitly modeling the state of entities present in the text for question answering."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "Recent successful approaches use memory based neural networks for question answering [24, 19, 25, 20, 18] ."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-97", "intents": ["@DIF@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model [18] ."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-95", "intents": ["@SIM@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "The baseline EntNet model was run for 10 times for each task [18] ."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-88", "intents": ["@UNSURE@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "We Task EntNet [18] compare the performance with the Recurrent Entity Networks model (EntNet) [18] ."}
{"sent_id": "b5097b3d901d073bfe06bcd88318ac-C001-105", "intents": ["@DIF@"], "paper_id": "ABC_b5097b3d901d073bfe06bcd88318ac_0", "text": "Besides considering quality metrics, it can be observed from table 4 that comparative ratio of values between the models is not commensurate with the results in intrinsic or extrinsic values, especially when we consider the amount of time and energy spent, since more training time results in more energy consumption Information on the length of training time for the released Mikolov model is not readily available."}
{"sent_id": "b5097b3d901d073bfe06bcd88318ac-C001-14", "intents": ["@MOT@"], "paper_id": "ABC_b5097b3d901d073bfe06bcd88318ac_0", "text": "There have been many implementations of the word2vec model in either of the two architectures it provides: continuous skipgram and continuous bag of words (CBoW) (Mikolov et al. (2013a) )."}
{"sent_id": "b5097b3d901d073bfe06bcd88318ac-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_b5097b3d901d073bfe06bcd88318ac_0", "text": "Breaking away from the non-distributed (high-dimensional, sparse) representations of words, typical of traditional bag-of-words or one-hot-encoding (Turian et al. (2010) ), Mikolov et al. (2013a) created word2vec."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Bod, 2007) ."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "As an extension, Zhang et al. (2008a) proposed two more categories: Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR)."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "And DPR refers to the rules having at least one non-terminal leaf node between two terminal leaf nodes."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "Based on these descriptions, R 7 , R 8 in Figure 2 belong to the category of SRR and R 6 , R 7 fall into the category of DPR."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-91", "intents": ["@DIF@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "Given an abstract rule r =< Note that the intersection of SRR NT 2 and SRR NT-T is not necessary an empty set, i.e. a rule can be both SRR NT 2 and SRR NT-T rule."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-108", "intents": ["@FUT@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "Taking such a system as the experimental platform, we can perform comprehensive statistics about distributions of different rule categories."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-65", "intents": ["@BACK@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "They do not, however, provide a much simpler averaging-based baseline."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-110", "intents": ["@DIF@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "This result corroborates our findings on the tuning data, and clearly contradicts the (implicit) claim made by Gong et al. (2018) regarding the infeasibility of document-level matching for documents of different lengths."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-113", "intents": ["@DIF@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "8 This is the more important as we exclusively employ off-the-shelf, general-purpose embeddings, while Gong et al. (2018) reach their best results with a much more sophisticated system and with embeddings that were custom-trained for the science domain."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-2", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "We explore blindfold (question-only) baselines for Embodied Question Answering."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "To foster and measure progress in such virtual environments, new tasks have been introduced, one of them being Embodied Question Answering (EmbodiedQA) [5] ."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "EmbodiedQA Methods: Das et al. [5] introduced the PACMAN-RL+Q model which is bootstrapped with expert shortest-path demonstrations and later fine-tuned with REINFORCE [24] ."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-70", "intents": ["@USE@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "For completeness, we also include a question only baseline derived directly from the EmbodiedQA codebase, which uses only the Question LSTM in the PACMAN model, termed as PACMAN Q-only (LSTM)."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-88", "intents": ["@USE@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "(*) indicates our reproduction of the model described in [5] Error Analysis: To better understand the shortcomings and limitations, we perform an error analysis of the one of the runs of the BoW model on different question types: Here, the color category Preposition Location Color 9.09 51.72 53.31 Table 2 : Accuracy of the BoW model on different question types subsumes color and color_room both."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-24", "intents": ["@UNSURE@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "We acknowledge the active effort of Das et al. [5] in removing some biases via entropy-pruning but note that further efforts might be necessary to fully correct these biases."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-80", "intents": ["@UNSURE@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "We observe the accuracy is improved over text baselines in this unrealistic setting, but the use of this model with navigation in PACMAN reduces performance to below the text baselines."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-82", "intents": ["@SIM@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "As noted in [5] , we re-iterate that these oracles are far from perfect, as they may not contain the best vantage or context to answer the question."}
{"sent_id": "684a637d08e8dbabddc1f1982f5393-C001-9", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_684a637d08e8dbabddc1f1982f5393_1", "text": "Finally, we hypothesize that UMSR is more effective because it uses linguistic devices to highlight relations (e.g., trade-offs) between options and attributes."}
{"sent_id": "684a637d08e8dbabddc1f1982f5393-C001-10", "intents": ["@USE@"], "paper_id": "ABC_684a637d08e8dbabddc1f1982f5393_1", "text": "We report the results of two studies which show that the discourse cues in UMSR summaries help users compare different options and choose between options, even though they do not improve verbatim recall."}
{"sent_id": "418016e4df12f80205cadc41119244-C001-25", "intents": ["@MOT@"], "paper_id": "ABC_418016e4df12f80205cadc41119244_1", "text": "Actually we find Inspired by [2] , we formulate BioCS as a sequence tagging problem."}
{"sent_id": "53bc4206427e95d600f787e0531df1-C001-13", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_53bc4206427e95d600f787e0531df1_1", "text": "To solve this problem, we consider the Pointwise Mutual Information (PMI) [9] ."}
{"sent_id": "cf2cc67035107f5bdaab85a760e56e-C001-27", "intents": ["@USE@"], "paper_id": "ABC_cf2cc67035107f5bdaab85a760e56e_1", "text": "To train the word embeddings, we experimented with three algorithms: word2vec (Mikolov et al., 2013b) , fastText (Bojanowski et al., 2017) , and GloVe (Pennington et al., 2014) ."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "This was first proposed by Miceli Barone (2016) , who initially used an adversarial network similar to Conneau et al. (2018) , and found that the mapper (which is also the encoder) translates everything to a single embedding, known commonly as the mode collapse issue (Goodfellow, 2017) ."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-153", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Our training setting is similar to Conneau et al. (2018) , and we apply the same pre-and postprocessing steps."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-155", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "For selecting the best model, we use the unsupervised validation criterion proposed by Conneau et al. (2018) , which correlates highly with the mapping quality."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-190", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "We present our results on European languages on the datasets of Conneau et al. (2018) and Dinu et al. (2015) in Tables 1 and 3 , while the results on non-European languages are shown in Table 2 ."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-199", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "For a fair comparison with respect to the quality of the learned mappings (or induced seed dictionary), here we only consider the results of our approach that use the refinement procedure of Conneau et al. (2018) ."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-104", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "This is in contrast with most existing methods (e.g., Conneau et al. (2018) ; Artetxe et al. (2017) ) that directly map the distribution of the source word embeddings p(x) to the distribution of the target p(y)."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-206", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Compared to that, our method is more robust and converged most of the time we ran."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-210", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Compared to that, our method was more robust, converging 4 times out of 10 attempts."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-259", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "If we compare the results of ---Cycle with Conneau-18, we see sizeable gains for En-Es in both directions."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-100", "intents": ["@DIF@", "@USE@", "@EXT@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "We discover that ELMo's performance improvements over the baseline are robust across varying levels of ambiguity, whereas the advantage of Hu et al. (2016) is reversed in sentences of low ambiguity (restricting to A-but-B style sentences)."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-117", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "Across all thresholds, we notice trends similar to previous sections: 1) ELMo performs the best among all models on A-but-B style sentences, and projection results in only a slight improvement; 2) models in Hu et al. (2016) (with and without distillation) benefit considerably from projection; but 3) distillation offers little improvement (with or without projection)."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-20", "intents": ["@USE@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "We focus on a logic rule for sentences containing an \"A-but-B\" structure (the only rule for which Hu et al. (2016) provide experimental results)."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-36", "intents": ["@USE@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "All of our experiments (as well as those in Hu et al. (2016) ) use the SST2 dataset, a binarized subset of the popular Stanford Sentiment Treebank (SST) (Socher et al., 2013) ."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-59", "intents": ["@USE@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "We keep our hyper-parameters identical to Hu et al. (2016) ."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "Here we briefly review background from Hu et al. (2016) to provide a foundation for our reanalysis in the next section."}
{"sent_id": "b2cb08afadadeddc0f8e7267163c0e-C001-9", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_b2cb08afadadeddc0f8e7267163c0e_1", "text": "In Romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve (Banea et al., 2008) ."}
{"sent_id": "3b0a82129333203eca96a7473095f3-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_3b0a82129333203eca96a7473095f3_1", "text": "However, other approaches are possible, such as the use of probabilities to guide parse selection, if not grammar induction (e.g., Black et al. 1992 )."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-32", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "Ma and Hovy (2016) do not use preprocessing."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-120", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "Table 3 shows the hyperparameters used in our experiments, which mostly follow Ma and Hovy (2016) , including the learning rate η = 0.015 for word LSTM models."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-205", "intents": ["@USE@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "The OOV entities and chunks are categorized following Ma and Hovy (2016) ."}
{"sent_id": "f78b11352d01b6567392f8cb3c7642-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_f78b11352d01b6567392f8cb3c7642_1", "text": "Despite its outward simplicity, co-occurrence networks have proven useful in many applications, such as in authorship recognition [9] , extractive summarization [10, 11, 12] , stylistic identification [13] and part-of-speech tagging [14] ."}
{"sent_id": "2b6dd9388c43df4416c738b2d1ed5f-C001-43", "intents": ["@USE@"], "paper_id": "ABC_2b6dd9388c43df4416c738b2d1ed5f_1", "text": "The embeddings were trained on both the datasets provided by (Davidson et al. 2017 ) and HEOT."}
{"sent_id": "2b6dd9388c43df4416c738b2d1ed5f-C001-48", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_2b6dd9388c43df4416c738b2d1ed5f_1", "text": "Both the HEOT and (Davidson et al. 2017 ) datasets contain tweets which are annotated in three categories: offensive, abusive and none (or benign)."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-93", "intents": ["@DIF@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "We call the baseline model as Baseline CE CE where encoder and decoder submodels are trained with CE loss."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-128", "intents": ["@DIF@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "Max3 CE MP model also performs better than the baseline but not as good as Max4."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "In [1] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-103", "intents": ["@USE@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "Max3 CE SMP used baseline CE loss for encoder."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-39", "intents": ["@SIM@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "Both the baseline and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 )."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-87", "intents": ["@UNSURE@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "The front-end extracts and stacks a 40-d feature vector of log-mel filter-bank energies at each frame and stacks them to generate input feature vector Xt. Refer to [1] for further details."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-92", "intents": ["@UNSURE@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "For detailed architectural parameters, please refer to [1] ."}
{"sent_id": "12c5d72fad925c8ec025cda87a0fd9-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_12c5d72fad925c8ec025cda87a0fd9_2", "text": "Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005) , including English VNCs (e.g., Fazly et al., 2009; Salton et al., 2016) , although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017) ."}
{"sent_id": "12c5d72fad925c8ec025cda87a0fd9-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_12c5d72fad925c8ec025cda87a0fd9_2", "text": "Fazly et al. (2009) formed a set of eleven lexicosyntactic patterns for VNC instances capturing the voice of the verb (active or passive), determiner (e.g., a, the), and number of the noun (singular or plural)."}
{"sent_id": "12c5d72fad925c8ec025cda87a0fd9-C001-66", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_12c5d72fad925c8ec025cda87a0fd9_2", "text": "We use the VNC-Tokens dataset (Cook et al., 2008) -the same dataset used by Fazly et al. (2009) and Salton et al. (2016) -to train and evaluate our models."}
{"sent_id": "12c5d72fad925c8ec025cda87a0fd9-C001-80", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_12c5d72fad925c8ec025cda87a0fd9_2", "text": "We therefore use accuracy to evaluate our models following Fazly et al. (2009) because the classes are roughly balanced."}
{"sent_id": "12c5d72fad925c8ec025cda87a0fd9-C001-96", "intents": ["@DIF@"], "paper_id": "ABC_12c5d72fad925c8ec025cda87a0fd9_2", "text": "In Table 2 we report results on DEV and TEST for each model, as well as the unsupervised CForm model of Fazly et al. (2009) , which simply labels a VNC as idiomatic if it occurs in its canonical form, and as literal otherwise."}
{"sent_id": "15dd59368074f3473b57d86568807f-C001-81", "intents": ["@BACK@"], "paper_id": "ABC_15dd59368074f3473b57d86568807f_2", "text": "We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al., 2015; Dong and Zhang, 2016) ."}
{"sent_id": "15dd59368074f3473b57d86568807f-C001-107", "intents": ["@DIF@"], "paper_id": "ABC_15dd59368074f3473b57d86568807f_2", "text": "For each and every source→target pair, we report better results than both state-of-theart methods (Phandi et al., 2015; Dong and Zhang, 2016) ."}
{"sent_id": "15dd59368074f3473b57d86568807f-C001-119", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_15dd59368074f3473b57d86568807f_2", "text": "Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (Phandi et al., 2015; Dong and Zhang, 2016) ."}
{"sent_id": "15dd59368074f3473b57d86568807f-C001-101", "intents": ["@SIM@"], "paper_id": "ABC_15dd59368074f3473b57d86568807f_2", "text": "Although the BOSWE model can be regarded as a shallow approach, its overall results are comparable to those of deep learning approaches (Dong and Zhang, 2016; Tay et al., 2018) ."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "DAQUAR contains 1088 different nouns in the question, 803 in the answers, and 1586 altogether (we use the Stanford POS Tagger [59] to extract the nouns from the questions)."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-95", "intents": ["@BACK@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "Some authors [23, 24, 27] treat the grounding (understood here as the logical representation of the meaning of the question) as a latent variable in the question answering task."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-22", "intents": ["@MOT@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "For instance [27] reports that for a question answering task on real-world images even human answers are inconsistent."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-85", "intents": ["@MOT@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "DAQUAR includes various challenges related to natural language understanding."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-131", "intents": ["@FUT@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "We identify particular challenges that holistic tasks should exhibit and exemplify how they are manifested in a recent question answering challenge [27] ."}
{"sent_id": "4821d5a283c1d4ba162b58e5fac8bc-C001-109", "intents": ["@DIF@"], "paper_id": "ABC_4821d5a283c1d4ba162b58e5fac8bc_2", "text": "We particularly notice that the difference from (Phandi et al., 2015) when n t = 0 is always higher than 10%."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "Also, when using the approach of Tu and Gimpel (2018) , there is a mismatch between the training and test-time uses of the trained inference network."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "Tu and Gimpel (2018) alternatively optimized Θ and Φ, which is similar to training in generative adversarial networks (Goodfellow et al., 2014) ."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-55", "intents": ["@USE@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "The energy functions we use for our sequence labeling tasks are taken from Tu and Gimpel (2018) and are described in detail in the appendix."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-219", "intents": ["@SIM@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "For inference networks, we use architectures similar to those used by (Tu and Gimpel, 2018) ."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-139", "intents": ["@DIF@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "Tu and Gimpel (2018) pretrained their tag language model (TLM) on a large, automatically-tagged corpus and fixed its parameters when optimizing Θ. We instead do not pretrain the TLM and learn its parameters when training Θ."}
{"sent_id": "03b7c2e050957dcff336183823e6f1-C001-11", "intents": ["@USE@"], "paper_id": "ABC_03b7c2e050957dcff336183823e6f1_2", "text": "We use a recently proposed dependency parser (Titov and Henderson, 2007b ) 1 which has demonstrated state-of-theart performance on a selection of languages from the CoNLL-X shared task (Buchholz and Marsi, 2006) ."}
{"sent_id": "03b7c2e050957dcff336183823e6f1-C001-43", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_03b7c2e050957dcff336183823e6f1_2", "text": "In our experiments we use the same definition of structural locality as was proposed for the ISBN dependency parser in (Titov and Henderson, 2007b) ."}
{"sent_id": "03b7c2e050957dcff336183823e6f1-C001-87", "intents": ["@UNSURE@"], "paper_id": "ABC_03b7c2e050957dcff336183823e6f1_2", "text": "5 We also refer the reader to (Titov and Henderson, 2007b) for more detailed analysis of the ISBN dependency parser results, where, among other things, it was shown that the ISBN model is especially accurate at modeling long dependencies."}
{"sent_id": "9a8b29b10539be9e6c65172a97b16f-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_9a8b29b10539be9e6c65172a97b16f_2", "text": "[9] in their work pointed out that hate speech is different from offensive language and released a data set of 25k tweets with the goal of distinguishing hate speech from offensive language."}
{"sent_id": "9a8b29b10539be9e6c65172a97b16f-C001-64", "intents": ["@USE@"], "paper_id": "ABC_9a8b29b10539be9e6c65172a97b16f_2", "text": "The [9] data set had a total of 25,112 tweets out of which 1498 were labelled as hate speech, 19,326 as offensive language and 4,288 as neither."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-27", "intents": ["@EXT@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "The original approach was developed on English."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-40", "intents": ["@EXT@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "To verify that expectation, we apply their approach to German, for which all resources required to reproduce their experiments are available."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-254", "intents": ["@EXT@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "To increase the size of our lexicon, we bootstrap additional shifters following their approach."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Once available, they can be used to improve the aforementioned tasks, as has already been shown for the case of English polarity classification (Schulder et al., 2017) ."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Schulder et al. (2017) automatically bootstrap a lexicon which covers 980 verbal shifters at the lemma level, while Schulder et al. (2018) manually annotate word senses of verbs, creating a lexicon of 2131 shifter senses across 1220 verbs."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-39", "intents": ["@MOT@", "@FUT@", "@DIF@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "They limited their work to English verbs, but expressed the expectation that their methods should also work for other languages."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-117", "intents": ["@MOT@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Therefore, they collect particle verbs containing relevant English particles, such as away, down and out."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-249", "intents": ["@DIF@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Figure 2 shows the performance of the differently sized dictionaries as stand-alone classifiers, while Figure 3 shows how much they can improve the best classifier of Schulder et al. (2017) , i.e. SVM data+resource ."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-120", "intents": ["@SIM@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Schulder et al. (2017) showed that the English NPI any co-occurs with shifters, so its presence in a verb phrase can indicate the presence of a verbal shifter."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-218", "intents": ["@SIM@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "All features in data and resource were also used in Schulder et al. (2017) ."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "In fact, most approaches simply use a one-layer CNN embedding [7, 8] ."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "In the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross-domain search [1, 2, 3, 4, 5] ."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-105", "intents": ["@DIF@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "The original setup [1] limited the word embedding to the 300 most frequent words, while using 300 GRUs."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-144", "intents": ["@DIF@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "For the multimodal pipeline of Kiros et al. [1] , using the Full-Network image embedding results in consistently higher performances than using a one-layer image embedding."}
{"sent_id": "95883b369c4b019fa98493a728c1a0-C001-38", "intents": ["@DIF@"], "paper_id": "ABC_95883b369c4b019fa98493a728c1a0_2", "text": "Next, we evaluated the stability of the direct bias measure proposed in (Bolukbasi et al. 2016) ."}
{"sent_id": "95883b369c4b019fa98493a728c1a0-C001-40", "intents": ["@DIF@"], "paper_id": "ABC_95883b369c4b019fa98493a728c1a0_2", "text": "For instance, for word embeddings trained on Google News articles, they reported that direct gender bias on 327 profession names is 0.08 and thus they concluded that this embedding is biased."}
{"sent_id": "95883b369c4b019fa98493a728c1a0-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_95883b369c4b019fa98493a728c1a0_2", "text": "Initial attempts have been made to develop metrics which seek to describe the geometric properties of the embedding space with respect to various axes of interest which are empirically determined to correspond to our intuitions of the hypothetical biases under study to quantify the degrees to which various biases exist within the embedding space, and presumably, the underlying text corpus (Bolukbasi et al. 2016; Caliskan, Bryson, and Narayanan 2017; Garg et al. 2018) ."}
{"sent_id": "95883b369c4b019fa98493a728c1a0-C001-50", "intents": ["@UNSURE@"], "paper_id": "ABC_95883b369c4b019fa98493a728c1a0_2", "text": "While it is not difficult to hypothesize various words which are supposed to be either ideally neutral terms W , or ideally bias-axis-aligned G 1 , G 2 , or as in (Bolukbasi et al. 2016) , have these term sets evaluated by a crowd, it is much more difficult to argue the canonicity of a given term set W , G 1 or G 2 . If it is not possible to defend the term set selections used to define the metric as being canonical, it is better for them to be mathematically regarded as a sample of the canonical term set."}
{"sent_id": "822b2010b07d3e1103f904fa45388a-C001-3", "intents": ["@MOT@"], "paper_id": "ABC_822b2010b07d3e1103f904fa45388a_2", "text": "Transfer Learning (Zoph et al., 2016 ) is a simple approach in which we can simply initialize an NMT model (child model) for a resource poor language pair using a previously trained model (parent model) for a resource rich language pair where the target languages are the same."}
{"sent_id": "822b2010b07d3e1103f904fa45388a-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_822b2010b07d3e1103f904fa45388a_2", "text": "However, it is possible to use a previously trained X-Y model (parent model; X-Y being the resource rich language pair where X and Y represent the source and target languages respectively) to initialize the parameters of a Z-Y model (child model; Z-Y being the resource poor language pair) leading to significant improvements (Zoph et al., 2016) for the latter."}
{"sent_id": "822b2010b07d3e1103f904fa45388a-C001-32", "intents": ["@USE@"], "paper_id": "ABC_822b2010b07d3e1103f904fa45388a_2", "text": "To ensure replicability we use the same NMT model design as in the original work (Zoph et al., 2016) ."}
{"sent_id": "6b147afca676882878e67bc10abd58-C001-121", "intents": ["@SIM@"], "paper_id": "ABC_6b147afca676882878e67bc10abd58_2", "text": "The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from Barzilay and Lapata (2008)."}
{"sent_id": "6b147afca676882878e67bc10abd58-C001-40", "intents": ["@UNSURE@"], "paper_id": "ABC_6b147afca676882878e67bc10abd58_2", "text": "Barzilay and Lapata (2008) )."}
{"sent_id": "6b147afca676882878e67bc10abd58-C001-51", "intents": ["@UNSURE@"], "paper_id": "ABC_6b147afca676882878e67bc10abd58_2", "text": "Barzilay and Lapata (2008) ) suffices, or whether performance is gained when allowing the whole noun group in order to determine similarity."}
{"sent_id": "6b147afca676882878e67bc10abd58-C001-139", "intents": ["@UNSURE@"], "paper_id": "ABC_6b147afca676882878e67bc10abd58_2", "text": "Taking into account the performance of content models and the baseline of the Barzilay and Lapata (2008) model, the most convincing explanation is that the sentence ordering in the earthquake datasets is based on some sort of topic notion, providing a variety of possible antecedents between which our model is thus far unable to distinguish without resorting to the original (correct) ordering."}
{"sent_id": "6b147afca676882878e67bc10abd58-C001-114", "intents": ["@DIF@"], "paper_id": "ABC_6b147afca676882878e67bc10abd58_2", "text": "(Barzilay and Lapata (2008) only perform well when using their coreference module, which determines antecedents based on the identified coreferences in the original sentence ordering, thereby biasing their orderings towards the correct ordering.) Longer range and WordNet relations together (Chunk+Temp-WN+LongRange+) achieve the best performance."}
{"sent_id": "6b147afca676882878e67bc10abd58-C001-68", "intents": ["@USE@"], "paper_id": "ABC_6b147afca676882878e67bc10abd58_2", "text": "The first two are the earthquake and accident datasets used by Barzilay and Lapata (2008) ."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "The MMD dataset (Saha et al., 2017) consists of 100/11/11k train/validation/test chat sessions comprising 3.5M context-response pairs for the model."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-85", "intents": ["@DIF@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of Saha et al. (2017) by modelling the full multimodal context."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-30", "intents": ["@DIF@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach (Wang et al., 2014a) ."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "Wang et al. (2014a) combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-93", "intents": ["@USE@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "As to the methods, \"Separately\" denotes the method of separately embedding knowledge bases and text. \"Jointly(anchor)\" and \"Jointly(name)\" denote the jointly embedding methods based on Alignment by Wikipedia Anchors and Alignment by Entity Names in (Wang et al., 2014a) respectively."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-100", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "Following the settings in (Wang et al., 2014a) , we apply the same preprocessing steps, including sentence segmentation, tokenization, and named entity recognition."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-139", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "We use the original dataset released by (Mikolov et al., 2013b) and follow the same evaluation protocol of (Wang et al., 2014a) ."}
{"sent_id": "c60a1131c6b1639b772b0e5c59588e-C001-26", "intents": ["@DIF@"], "paper_id": "ABC_c60a1131c6b1639b772b0e5c59588e_2", "text": "This is different from the scheme in (Gardner et al., 2013) and (Gardner et al., 2014) , which adds edges between KB nodes by mining surface relations from an external corpus."}
{"sent_id": "7f78697390e28cc7798f8cb183cb59-C001-60", "intents": ["@USE@"], "paper_id": "ABC_7f78697390e28cc7798f8cb183cb59_2", "text": "Note that this approach is our re-implementation of the skipthoughts based method of Salton et al. (2016) , and we use it as a strong baseline for comparison."}
{"sent_id": "7f78697390e28cc7798f8cb183cb59-C001-131", "intents": ["@USE@"], "paper_id": "ABC_7f78697390e28cc7798f8cb183cb59_2", "text": "We compared these approaches against a linguistically-informed unsupervised baseline, and a model based on skip-thoughts previously applied to this task (Salton et al., 2016) ."}
{"sent_id": "7f78697390e28cc7798f8cb183cb59-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_7f78697390e28cc7798f8cb183cb59_2", "text": "In the most closely related work to ours, Salton et al. (2016) represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) -an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov et al., 2013 ) skip-gram model."}
{"sent_id": "7f78697390e28cc7798f8cb183cb59-C001-29", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_7f78697390e28cc7798f8cb183cb59_2", "text": "Salton et al. then use these sentence embeddings, representing VNC token instances, as features in a supervised classifier."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-145", "intents": ["@DIF@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "Our method achieved better accuracy in ranking integration than previous methods (Specia et al., 2012; Kajiwara and Yamamoto, 2015) and is similar to the results from De Belder and Moens (2012) ."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-76", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "It follows the data creation procedure of Kajiwara and Yamamoto's (2015) dataset with improvements to resolve the problems described in Section 3."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-40", "intents": ["@MOT@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "Namely, they split the data creation process into two steps: substitute extraction and simplification ranking."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-20", "intents": ["@EXT@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "Hence, we propose a new dataset addressing the problems in the dataset of Kajiwara and Yamamoto (2015) ."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-125", "intents": ["@SIM@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "It is about the same size as previous work (Specia et al., 2012; Kajiwara and Yamamoto, 2015) ."}
{"sent_id": "6330c615d4c62b30933dac3057c9d6-C001-40", "intents": ["@USE@"], "paper_id": "ABC_6330c615d4c62b30933dac3057c9d6_3", "text": "We use the same pre-processing as in earlier work [20, 24] ."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-29", "intents": ["@DIF@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "In this work we first propose a supervised approach to identifying VNIC token instances based on word embeddings that outperforms the supervised method of Fazly et al. (2009) ."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-107", "intents": ["@DIF@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "Analyzing the results on individual expressions indicates that the unsupervised approach gives especially low accuracy for hit roof -which is in DEV-as compared to the CFORM method of Fazly et al., which could contribute to the overall lower accuracy of the unsupervised approach on this dataset."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "Fazly et al. (2009) exploit this property in their unsupervised approach, referred to as CFORM."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "They propose a statistical method for automatically determining a given VNIC type's canonical idiomatic form, based on the frequency of its usage in these patterns in a corpus."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-53", "intents": ["@USE@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "Our unsupervised approach combines the word embedding-based representation used in the supervised approach (without relying on training a supervised classifier, of course) with the unsupervised CFORM method of Fazly et al. (2009) ."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-3", "intents": ["@BACK@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "(Herbelot and Vecchi, 2015) explored word embeddings and their utility for modeling language semantics."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-67", "intents": ["@USE@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "We compare (Herbelot and Vecchi, 2015) 's model (PLSR + word2vec) against three baselines: random vectors, mode, and nearest neighbor."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-89", "intents": ["@USE@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "We could reproduce the results for the QMR dataset using PLSR and word2vec embeddings (0.346 in (Herbelot and Vecchi, 2015) vs. 0.332 in our experiments, but we could not ex-3 https://github.com/Franck-Dernoncourt/ model-theoretic actly reproduce the results for the AD dataset (0.634 in (Herbelot and Vecchi, 2015) vs. 0.572 in our experiments): this discrepancy most likely results from the choice of the training set."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-107", "intents": ["@DIF@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "The mode baseline significantly outperforms (Herbelot and Vecchi, 2015) 's model on the QMR dataset, and yields competitive results on the AD dataset."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-90", "intents": ["@SIM@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "Our experiments' results are averaged over 1000 runs, and for each run the training/test split is randomly chosen, the only constraint being having the same number of training samples as in (Herbelot and Vecchi, 2015) ."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-20", "intents": ["@USE@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "We explore two different strategies: (1) a training-time data augmentation technique (Zhao et al., 2018a) , where we augment the corpus for training the coreference system with its genderswapped variant (female entities are swapped to male entities and vice versa) and, afterwards, retrain the coreference system; and (2)"}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-75", "intents": ["@USE@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "We evaluate bias with respect to the WinoBias dataset (Zhao et al., 2018a) , a benchmark of paired male and female coreference resolution examples following the Winograd format (Hirst, 1981; Rahman and Ng, 2012; Peng et al., 2015) ."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-79", "intents": ["@USE@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "ELMo improves the performance on the OntoNotes dataset by 5% but shows stronger bias on the WinoBias dataset."}
{"sent_id": "311b238406da4891c09cb9c3c0334d-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_311b238406da4891c09cb9c3c0334d_3", "text": "This makes the task more difficult, compared to the sentiment analysis, but it can often bring complementary information [3] ."}
{"sent_id": "311b238406da4891c09cb9c3c0334d-C001-31", "intents": ["@USE@"], "paper_id": "ABC_311b238406da4891c09cb9c3c0334d_3", "text": "We preprocessed the Czech commentaries by the same rules as in the original system [3] (for example: all urls were replaced by keyword URL, links to images are replaced by IMGURL, only letters are preserved, the rest of the characters is removed, …)."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "Conversely, Ambati et al. (2013) showed that a Hindi dependency parser (Malt) could be improved by using CCG categories."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-85", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "In contrast, for Malt, Ambati et al. (2013) had shown that coarse-grained supertags gave larger improvements of 0.3% and 0.4% in UAS and LAS respectively."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-13", "intents": ["@EXT@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "In this paper, we extend this work and show that CCG categories are useful even in the case of English, a typologically different language, where parsing accuracy of dependency parsers is already extremely high."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-65", "intents": ["@USE@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "Following Ambati et al. (2013) , we used supertags which occurred at least K times in the training data, and backed off to coarse POS-tags otherwise."}
{"sent_id": "f1a800c7cd47ac2edf2172cedb5889-C001-11", "intents": ["@MOT@"], "paper_id": "ABC_f1a800c7cd47ac2edf2172cedb5889_3", "text": "Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by Tsarfaty et al. (2010 Tsarfaty et al. ( , 2013 :"}
{"sent_id": "f1a800c7cd47ac2edf2172cedb5889-C001-14", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_f1a800c7cd47ac2edf2172cedb5889_3", "text": "For the second, Tsarfaty et al. (2010) and Seeker and Kuhn (2013) reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But these studies focus on vintage parsers; do neural parsers with character-level representations also solve this second problem?"}
{"sent_id": "4dbf6e2bfa30e96816b7f6d9c6e069-C001-53", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_4dbf6e2bfa30e96816b7f6d9c6e069_3", "text": "We model the problem as an integer linear programming (ILP) formulation, similar to the dependency graph fusion as proposed by Fillipova and Strube [1] ."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-3", "intents": ["@EXT@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "The method is designed for intrinsic evaluation and extends the approach proposed in (Schnabel et al., 2015) ."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-166", "intents": ["@EXT@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "In this paper, a crowdsourcing-based word embedding evaluation technique of (Schnabel et al., 2015) was extended to provide data-driven treatment of word sense ambiguity."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "In (Schnabel et al., 2015) , crowdsourcingbased evaluation was proposed for synonyms or a word relatedness task where six word embedding techniques were evaluated."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-26", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "In this paper, we show what are the consequences of the lack of the word context in (Schnabel et al., 2015) , and we discuss how to address the resulting challenge."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-56", "intents": ["@USE@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "Before we introduce our extensions in the next section, we investigate how (Schnabel et al., 2015) accommodates word sense ambiguity."}
{"sent_id": "fe539365c7bb4555280fd1a5478aba-C001-158", "intents": ["@USE@"], "paper_id": "ABC_fe539365c7bb4555280fd1a5478aba_3", "text": "For the experimental study we have used the positive and negative polarities reviews corresponding to the corpora proposed by (Ott et al., 2011; Ott et al., 2013) with 800 reviews each one (400 true and 400 false opinions)."}
{"sent_id": "fe539365c7bb4555280fd1a5478aba-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_fe539365c7bb4555280fd1a5478aba_3", "text": "The Opinion Spam corpus presented in (Ott et al., 2011; Ott et al., 2013 ) is composed of 1600 positive and negative opinions for hotels with the corresponding gold-standard."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "The unbounded dependency corpus of Rimell et al. (2009) includes seven grammatical constructions: object extraction from a relative clause (ObRC), object extraction from a reduced relative clause (ObRed), subject extraction from a relative clause (SbRC), free relatives (Free), object questions (ObQ), right node raising (RNR), and subject extraction from an embedded clause (SbEm), all chosen for being relatively frequent and easy to identify in PTB trees."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "In Rimell et al. (2009) , five state-of-the-art parsers were evaluated for their recall on the goldstandard dependencies."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-16", "intents": ["@DIF@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "Unlike the best performing grammar-based parsers studied in Rimell et al. (2009) , neither MSTParser nor MaltParser was developed specifically as a parser for English, and neither has any special mechanism for dealing with unbounded dependencies."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-19", "intents": ["@DIF@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "Nevertheless, the two dependency parsers are found to perform only slightly worse than the best grammar-based parsers evaluated in Rimell et al. (2009) and considerably better than the other statistical parsers in that evaluation."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-123", "intents": ["@USE@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "Table 2 shows the results for MSTParser and MaltParser in the context of the other parsers evaluated in Rimell et al. (2009) ."}
{"sent_id": "9f1d2be80dbfd726a24fb2a05e130b-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_9f1d2be80dbfd726a24fb2a05e130b_3", "text": "Recent studies pay a great attention to the task of Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014) ."}
{"sent_id": "e7f972baa73e7ababa28eded3adad9-C001-67", "intents": ["@BACK@"], "paper_id": "ABC_e7f972baa73e7ababa28eded3adad9_3", "text": "The work releases a dataset of 119,000 sentences in Sanskrit along with the lexical and morphological analysis from a shallow parser."}
{"sent_id": "e7f972baa73e7ababa28eded3adad9-C001-104", "intents": ["@USE@"], "paper_id": "ABC_e7f972baa73e7ababa28eded3adad9_3", "text": "We used a dataset of 107,000 sentences from the Sanskrit Word Segmentation Dataset (Krishna et al., 2017) ."}
{"sent_id": "1540b0b172971ac75771b414765f1d-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_1540b0b172971ac75771b414765f1d_3", "text": "Bollmann and Søgaard (2016) and Bollmann et al. (2017) recently showed that we can obtain more robust historical text normalization models by exploiting synergies across historical text normalization datasets and with related tasks."}
{"sent_id": "1540b0b172971ac75771b414765f1d-C001-100", "intents": ["@USE@"], "paper_id": "ABC_1540b0b172971ac75771b414765f1d_3", "text": "There is a wide range of design questions and sharing strategies that we ignore here, focusing instead on under what circumstances the approach advocated in (Bollmann et al., 2017) works."}
{"sent_id": "81499fd759b958a0c02d9ed9d72a46-C001-48", "intents": ["@EXT@", "@BACK@", "@USE@"], "paper_id": "ABC_81499fd759b958a0c02d9ed9d72a46_3", "text": "Duong et al. (2016) constructed bilingual word embeddings based on monolingual data and PanLex."}
{"sent_id": "91e4fd2556d4a04e477ea97208b218-C001-5", "intents": ["@DIF@"], "paper_id": "ABC_91e4fd2556d4a04e477ea97208b218_4", "text": "Evaluated by human graders on 500 random-selected triples from Freebase, questions generated by our system are judged to be more fluent than those of Serban et al. (2016) by human graders."}
{"sent_id": "91e4fd2556d4a04e477ea97208b218-C001-45", "intents": ["@DIF@"], "paper_id": "ABC_91e4fd2556d4a04e477ea97208b218_4", "text": "For System grammatical naturalness Serban et al. (2016) 3.36 3.14 Ours 3.53 3.31 Table 1 : Comparing generated questions domain relevance, we take the seed question set as the in-domain data D in , the domain relevance of expanded question q is defined as:"}
{"sent_id": "91e4fd2556d4a04e477ea97208b218-C001-26", "intents": ["@USE@"], "paper_id": "ABC_91e4fd2556d4a04e477ea97208b218_4", "text": "In our experiment, we compare with Serban et al. (2016) on 500 random selected triples from Freebase (Bollacker et al., 2008) ."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-20", "intents": ["@MOT@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "However, it is not flexible with regards to (C3), or the sense granularity problem, as it requires the users to specify the number of senses: Current systems (Wang et al. 2015; Chang, Pei, and Chen 2014) required to set the number of senses to a small number (set to 3 or 5 in the literature) to get a good accuracy, however many words may have a large number of senses, e.g. play in Figure 1 ."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-72", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "LDA extensions (Wang et al. 2015; Chang, Pei, and Chen 2014) mitigated this problem by setting S to a small number (e.g. 3 or 5)."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "Previous works also attempted to introduce a separate sense latent variable to generate all the words (Chang, Pei, and Chen 2014) , or to generate only the neighboring words within a local context, decided by a strict user-specified window (Wang et al. 2015) ."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-119", "intents": ["@USE@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "Following the convention of previous works (Lau et al. 2012; Goyal and Hovy 2014; Wang et al. 2015) , we assume convergence when the number of iterations is high."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-147", "intents": ["@USE@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "For completeness, we also report STM with additional contexts, STM+actual and STM+ukWac (Wang et al. 2015) , where they used the actual additional contexts from the original data and semantically similar contexts from ukWac, respectively, as additional global context."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-196", "intents": ["@USE@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "We use LDA (Blei, Ng, and Jordan 2003) , HC (Chang, Pei, and Chen 2014) and STM (Wang et al. 2015) as baselines."}
{"sent_id": "55160a7ab2df9a86e677bcc72d9842-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_55160a7ab2df9a86e677bcc72d9842_4", "text": "State-of-the-art semantic parsers are neural encoder-decoder models, where decoding is guided by the grammar of the target programming language (Yin and Neubig, 2017; Rabinovich et al., 2017; Iyer et al., 2018) to ensure syntactically valid programs."}
{"sent_id": "55160a7ab2df9a86e677bcc72d9842-C001-93", "intents": ["@USE@"], "paper_id": "ABC_55160a7ab2df9a86e677bcc72d9842_4", "text": "The decoder of Iyer et al. (2018) is left unchanged."}
{"sent_id": "55160a7ab2df9a86e677bcc72d9842-C001-70", "intents": ["@SIM@"], "paper_id": "ABC_55160a7ab2df9a86e677bcc72d9842_4", "text": "The decoder is then trained similar to previous approaches (Yin and Neubig, 2017; Iyer et al., 2018) using the compressed set of rules."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-17", "intents": ["@EXT@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "This paper proposes a new method on Japanese word reordering based on concurrent execution with dependency parsing by extending the probablistic model proposed by Yoshida et al. (2014) , and describes an evaluation experiment using our 1 Bunsetsu is a linguistic unit in Japanese that roughly corresponds to a basic phrase in English."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-46", "intents": ["@EXT@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "We extend the above model and calculate P (S|B) as follows:"}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-38", "intents": ["@USE@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "We use the same search algorithm as one proposed by Yoshida et al. (2014) , which can efficiently find the approximate solution from a huge number of candidates of the pattern by extending CYK algorithm used in conventional dependency parsing."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-55", "intents": ["@DIF@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "Therefore, we mix Formulas (3) and (4) by adjusting the weight α depending on the adequacy of word order in an input sentence, instead of using the constant 0.5 in the previous model proposed by Yoshida et al. (2014) ."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "To this end, Hayashi (2016) framed Evocation prediction as a supervised regression task."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. In the original formula, s and t are nodes representing a single synset."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-57", "intents": ["@EXT@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "First, we modify Hayashi (2016)'s lexVector."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-62", "intents": ["@DIF@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "Second, instead of computing Wu-Palmer similarity (WUP, Wu and Palmer, 1994 ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-76", "intents": ["@DIF@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "Finally, our 300 topic LDA model (Blei et al., 2003) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in Hayashi (2016) ."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-124", "intents": ["@DIF@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "Although we report a lower performance than that in Hayashi (2016) , potentially indicating that predicting association strengths in word-sense ambiguous contexts is a harder task, we believe our results are a promising start."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-79", "intents": ["@USE@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "Following the setup used in Hayashi (2016) , all neural networks are trained using the Chainer 7 Python library with rectified linear units, dropout, and two hidden layers, each with 50% of the number of units in the input layer."}
{"sent_id": "87a87855d67d90c691ab5bedb4d460-C001-138", "intents": ["@EXT@"], "paper_id": "ABC_87a87855d67d90c691ab5bedb4d460_4", "text": "We present baseline results on End-to-End Automatic Speech Translation on a new speech translation corpus of audiobooks, and on a synthetic corpus extracted from BTEC (follow-up to [2] )."}
{"sent_id": "87a87855d67d90c691ab5bedb4d460-C001-83", "intents": ["@USE@"], "paper_id": "ABC_87a87855d67d90c691ab5bedb4d460_4", "text": "Speech files were preprocessed using Yaafe [13] , to extract 40 MFCC features and frame energy for each frame with a step size of 10 ms and window size of 40 ms, following [14, 2] ."}
{"sent_id": "87a87855d67d90c691ab5bedb4d460-C001-59", "intents": ["@SIM@"], "paper_id": "ABC_87a87855d67d90c691ab5bedb4d460_4", "text": "Like [2] , these features are given as input to two non-linear (tanh) layers, which output new features of size n ."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-106", "intents": ["@DIF@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in (Ng and Lee, 1996) ."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "Many different learning approaches have been used, including neural networks (Leacock et al., 1993) , probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al., 1992a; Gale et al., 1995; Leacock et al., 1993; Yarowsky, 1992) , decision lists (Yarowsky, 1994) , exemplar-based learning algorithms (Cardie, 1993; Ng and Lee, 1996) , etc."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-74", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "This corpus was first reported in (Ng and Lee, 1996) , and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-108", "intents": ["@BACK@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "The feature value pruning method of (Ng and Lee, 1996) is intended to keep only feature values deemed important for classification."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-28", "intents": ["@USE@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "Section 4 presents the disambiguation accuracy of PEBLS and Naive-Bayes on the large corpus of (Ng and Lee, 1996) ."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "For example, (Devlin et al., 2018) has shown that across natural language understanding tasks, using larger hidden layer size, more hidden layers, and more attention heads always leads to better performance."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-19", "intents": ["@MOT@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "However, they stop at a hidden layer size of 1024."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-93", "intents": ["@USE@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "We also use the next sentence prediction loss as introduced in (Devlin et al., 2018) to train our models."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-143", "intents": ["@USE@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "Table 2 shows the BERT base models, including the original BERT-base model in (Devlin et al., 2018) and our implementation, and the bidirectional LSTM model accuracy over SQuAD 1.1 development dataset."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-180", "intents": ["@USE@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "Following the BERT setting (Devlin et al., 2018) , we exclude the problematic WNLI set."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-178", "intents": ["@SIM@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "Additionally similar to (Devlin et al., 2018) , for large BERT and TRANS-BLSTM models, we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the development set."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-22", "intents": ["@MOT@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "However, even with our diversified adversarial training data, the model is still not fully resilient to AddSent-style attacks, e.g., its antonymy-style semantic perturbations."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "Thus during training, the model is rarely punished for answering questions based on syntactic similarity, and learns it as a reliable approach to Q&A. This correlation between syntactic similarity and correctness is of course not true in general: the adversaries generated by AddSent (Jia and Liang, 2017) are syntactically similar to the question but do not answer them."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "In the field of Q&A, Jia and Liang (2017) attempted to retrain the BiDAF (Seo et al., 2017) model with data generated with AddSent algorithm."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-60", "intents": ["@DIF@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "To prevent the model from superficially deciding what is a distractor based on certain specific words, we dynamically generate the fake answers instead of using AddSent's pre-defined set."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-62", "intents": ["@DIF@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "For each answer a, we generate the fake answer dynamically by randomly selecting another answer a = a from S that has the same type as a, as opposed to AddSent (Jia and Liang, 2017) , which uses a pre-defined fake answer for each type (e.g., \"Chicago\" for any location)."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-87", "intents": ["@USE@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "First, as shown, the AddSent-trained model is not able to perform well on test sets where the distractors are not inserted at the end, e.g., the AddSentRandom adversarial test set."}
{"sent_id": "15c8ca572430c214d9c571fbe0db95-C001-32", "intents": ["@USE@"], "paper_id": "ABC_15c8ca572430c214d9c571fbe0db95_4", "text": "Our baseline model is the unigram monotone model described in (Tillmann and Xia, 2003) ."}
{"sent_id": "91c1a4ab0347fb8b11ff213a97e864-C001-59", "intents": ["@USE@"], "paper_id": "ABC_91c1a4ab0347fb8b11ff213a97e864_4", "text": "The feature templates we used are a superset of Zhang and McDonald (2012) ."}
{"sent_id": "bdd7a4dabf8a8d7c0a2b638eb6eb72-C001-120", "intents": ["@BACK@"], "paper_id": "ABC_bdd7a4dabf8a8d7c0a2b638eb6eb72_4", "text": "In addition to the time taken for the full SVO augmentation, PRA-VS takes additional time to generate embeddings (13 minutes) from the added verbs."}
{"sent_id": "bdd7a4dabf8a8d7c0a2b638eb6eb72-C001-111", "intents": ["@DIF@"], "paper_id": "ABC_bdd7a4dabf8a8d7c0a2b638eb6eb72_4", "text": "Between the two top performing systems, i.e., PRA-ODA and PRA-VS, PRA-ODA is faster by a factor of 1.8."}
{"sent_id": "6092234b23f2620c356c2e417c2ce8-C001-2", "intents": ["@EXT@"], "paper_id": "ABC_6092234b23f2620c356c2e417c2ce8_4", "text": "We extend our previous work on constituency parsing (Kitaev and Klein, 2018) by incorporating pre-training for ten additional languages, and compare the benefits of no pre-training, ELMo , and BERT (Devlin et al., 2018)."}
{"sent_id": "6092234b23f2620c356c2e417c2ce8-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_6092234b23f2620c356c2e417c2ce8_4", "text": "Our parser as described in Kitaev and Klein (2018) accepts as input a sequence of vectors corresponding to words in a sentence, transforms these repre-1 https://github.com/nikitakit/self-attentive-parser sentations using one or more self-attention layers, and finally uses these representations to output a parse tree."}
{"sent_id": "6092234b23f2620c356c2e417c2ce8-C001-9", "intents": ["@MOT@"], "paper_id": "ABC_6092234b23f2620c356c2e417c2ce8_4", "text": "However, these results only considered the LSTM-based ELMo representations , and only for the English language."}
{"sent_id": "6092234b23f2620c356c2e417c2ce8-C001-22", "intents": ["@USE@"], "paper_id": "ABC_6092234b23f2620c356c2e417c2ce8_4", "text": "The extra layers on top of BERT use word-based tokenization instead of sub-words, apply the factored version of self-attention proposed in Kitaev and Klein (2018) , and are randomly-initialized instead of being pre-trained."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "There have been a few studies on transformers for end-to-end speech recognition, particularly for sequence-to-sequence with attention model [10, 11, 12] , as well as transducer [13] and CTC models [14] ."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "Furthermore, lower sampling rates are usually used for transformer-based acoustic models, which makes it easier for sequence-level self-attention as the input sequences are much shorter."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "Another key idea from the transformer paper [8] is the multihead attention mechanism, which performs multiple attention operations in parallel using different model parameters."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-24", "intents": ["@MOT@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "Transformer based transducer [13] and CTC model [14] do not have the issue for online speech recognition, however, the results presented in the two studies are not competitive compared the hybrid baseline system from Kaldi [15] ."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-122", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "However, this is very challenging for sequenceto-sequence model based on transformer as the boundary for each output token is unclear."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-4", "intents": ["@EXT@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "In this paper, we revisit the transformer-based hybrid acoustic model, and propose a model structure with interleaved self-attention and 1D convolution, which is proven to have faster convergence and higher recognition accuracy."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-5", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "We also study several aspects of the transformer model, including the impact of the positional encoding feature, dropout regularization, as well as training with and without time restriction."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-75", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "To constrain our research scope, we fixed the depth of the transformer models to be 6 layers, and the dimension of the model d k in Eq (1) to be 512."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-94", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "Unlike the observations in the area of machine translation, positional encoding did not make a big difference in terms of recognition accuracies for our transformer models."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-131", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "For faster convergence, we used the transformer models with one more layer normalization as in section 4.2, although the offline results on the dev-other and test-other evaluation sets are slightly worse."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-67", "intents": ["@SIM@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "Same as the standard transformer [8] , we also insert the feedforward layer after the multi-head attention."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-83", "intents": ["@FUT@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "As the memory cost of self-attention is in the order of O(T 2 ), where T is the length of the acoustic sequence, lower frame rate would significantly cut down the memory cost, and enable the training of much deeper transformer models that will studied in our future work."}
{"sent_id": "1dd3adcb79c8bc4b5187b85d836ceb-C001-18", "intents": ["@USE@"], "paper_id": "ABC_1dd3adcb79c8bc4b5187b85d836ceb_4", "text": "Beam-search parsing using an unnormalized discriminative model, as in Collins and Roark (2004) , requires a slightly different search strategy than the original generative model described in Roark (2001; 2004) ."}
{"sent_id": "3251c6cd1afccf6ad8d5391a4360b0-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_3251c6cd1afccf6ad8d5391a4360b0_4", "text": "Sperber et al. (2017) proposes a lattice-tosequence model which, in theory, can address both problems above."}
{"sent_id": "3251c6cd1afccf6ad8d5391a4360b0-C001-16", "intents": ["@MOT@"], "paper_id": "ABC_3251c6cd1afccf6ad8d5391a4360b0_4", "text": "However, their model suffers from training speed performance due to the lack of efficient batching procedures and they rely on transcriptions for pretraining."}
{"sent_id": "3251c6cd1afccf6ad8d5391a4360b0-C001-82", "intents": ["@DIF@"], "paper_id": "ABC_3251c6cd1afccf6ad8d5391a4360b0_4", "text": "The reason is because their model relies on the CPU while our GGNN-based model can be easily batched and computed in a GPU."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "The creation of question answering (QA) benchmarks for these knowledge bases (KB) has a significant impact on the domain, as shown by the number of QA systems recently proposed in the literature (Berant and Liang, 2014; Berant et al., 2013; Bordes et al., 2014a; Bordes et al., 2014b; Fader et al., 2013; Fader et al., 2014; Yao and Van Durme, 2014; Yih et al., 2014; Dong et al., 2015) ."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "Bordes et al. (2014b) introduce a linguistically leaner IR-based approach which identifies the KB triple most similar to the input NL question."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-107", "intents": ["@BACK@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "In (Bordes et al., 2014b) , the embedding of the semantics is then calculated as e + r for this very simple case."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-19", "intents": ["@USE@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "We follow the approach of Bordes et .al (2014b) which learns the embeddings of words and KB elements."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-145", "intents": ["@USE@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "This is the \"reranking\" setting used in (Bordes et al., 2014b) ."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-114", "intents": ["@DIF@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "What do cassava come from ? (cassava.e be-source-of.r security.e) (cassava.e be-grow-in.r africa.e) Table 1 : Some examples for which our system differs from ( (Bordes et al., 2014b) )."}
{"sent_id": "9272b3f7e628a156caed328d475d0c-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_9272b3f7e628a156caed328d475d0c_5", "text": "They further showed that English readability indices were inadequate for Bengali, and built their own readability model on 16 texts."}
{"sent_id": "55f67c918001335974608200a87cfc-C001-20", "intents": ["@BACK@", "@MOT@", "@EXT@"], "paper_id": "ABC_55f67c918001335974608200a87cfc_5", "text": "These high-level knowledge graph representations are particularly important for question answering task [22, 10, 17] ."}
{"sent_id": "55f67c918001335974608200a87cfc-C001-83", "intents": ["@DIF@"], "paper_id": "ABC_55f67c918001335974608200a87cfc_5", "text": "These include Key-Value Memory Networks (KVMN) [3] and GRAFT-Net [17] ."}
{"sent_id": "bdcd8b0f3a56606427ee298d454b52-C001-59", "intents": ["@USE@"], "paper_id": "ABC_bdcd8b0f3a56606427ee298d454b52_5", "text": "The first two data sets evaluate ADS for unsupervised information extraction, and were taken from (Downey et al., 2007) ."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-167", "intents": ["@USE@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "Table 5 : Comparison between our Embedded CNN+LSTM approach that computes the similarity between input and candidate answers in the embedding space, and the plain CNN+LSTM original approach from [32] ."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-25", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "Although related ideas have been explored for visual question answering [22] , and even have been used in Visual Madlibs [32] , we are first to show a significant improvement of such representation by using object proposals."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-63", "intents": ["@DIF@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "This puts CNN+LSTM approach closer to nCCA with a tighter integration with the multi-choice Visual Madlibs task."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-175", "intents": ["@DIF@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "Our \"Embedded CNN+LSTM\" outperforms other methods on both tasks confirming our hypothesis. \"Ask Your Neurons\" [17] is also slightly better than the original CNN+LSTM [32] (on the 10 categories that the results for CNN+LSTM are available it achieves 49.8% accuracy on the easy task, which is 2.1 percentage points higher than CNN+LSTM)."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "It asks machines to fill the blank prompted with a natural language description with a phrase chosen from four candidate completions (Figure 4 )."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-134", "intents": ["@BACK@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "Finally, Visual Madlibs considers an easy and difficult tasks that differ in how the negative 3 candidate completions (distractors) are chosen."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-14", "intents": ["@UNSURE@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "The copyright of this document resides with its authors."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-15", "intents": ["@UNSURE@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "It may be distributed unchanged freely in print or electronic forms."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-88", "intents": ["@UNSURE@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "Note that, we do not encode prompts as they follow the same pattern for each Visual Madlibs category."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-107", "intents": ["@UNSURE@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "which is a cosine similarity between the representation of the available during the training correct completionŝ i , and an output embedding vector of the i-th image-prompt training Table 4 : BLEU-1 and BLEU-2 computed on Madlibs testing dataset for different approaches."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-118", "intents": ["@UNSURE@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "We have introduced a new fill-in-the blank strategy for targeted natural language descriptions and used this to collect a Visual Madlibs dataset."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-32", "intents": ["@EXT@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "Such an approach integrates more tightly with the multi-choice filling the blanks task, and significantly outperforms the prior CNN+LSTM method [32] ."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-98", "intents": ["@SIM@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "The improvement is consistent with the findings of [32] , where nCCA performs better than CCA by about five percentage points in average on the hard task."}
{"sent_id": "247bbc4eb671895222065ed425f968-C001-133", "intents": ["@SIM@"], "paper_id": "ABC_247bbc4eb671895222065ed425f968_5", "text": "In comparison with the results obtained on the CTS evaluation with similar acoustic models [2] , the LSTM and ResNet operate at similar WERs."}
{"sent_id": "1056d36c5ed22c7a34f6fe82b4962f-C001-35", "intents": ["@USE@"], "paper_id": "ABC_1056d36c5ed22c7a34f6fe82b4962f_5", "text": "Since our systems do not generate novel sequences, we follow Bryant and Briscoe (2018) and use simple heuristics to generate a confusion set of sentences that our language models score."}
{"sent_id": "f6694f359ae948b6e4563b927a672c-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_f6694f359ae948b6e4563b927a672c_5", "text": "Xing et al. (2015) incorporate length normalization in the training of word embeddings and try to maximize the cosine similarity instead, introducing an orthogonality constraint to preserve the length normalization after the projection."}
{"sent_id": "f6694f359ae948b6e4563b927a672c-C001-40", "intents": ["@DIF@"], "paper_id": "ABC_f6694f359ae948b6e4563b927a672c_5", "text": "However, the fact is that, as long as W is orthogonal, optimizing the squared Euclidean distance of length-normalized embeddings is equivalent to optimizing the cosine, and therefore, the mapping objective proposed by Xing et al. (2015) is equivalent to that used by Mikolov et al. (2013b) with orthogonality constraint and unit vectors."}
{"sent_id": "f6694f359ae948b6e4563b927a672c-C001-70", "intents": ["@USE@"], "paper_id": "ABC_f6694f359ae948b6e4563b927a672c_5", "text": "The code for Mikolov et al. (2013b) and Xing et al. (2015) is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of Xing et al. (2015) (postprocessing instead of constrained training)."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-19", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "On the other hand, Miwa and Bansal (2016) 's model is trained locally, without considering structural correspondences between incremental decisions."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "In particular, they used bidirectional LSTM (Graves et al., 2013) to learn hidden word representations under a sentential context, and further leveraged treestructured LSTM (Tai et al., 2015) to encode syntactic information, given the output of a parser."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-102", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; Miwa and Bansal, 2016) ."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-113", "intents": ["@DIF@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "Our exploration of syntactic features has two main advantages over the method of Miwa and Bansal (2016) , where dependency path LSTMs are used for relation classification."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-42", "intents": ["@SIM@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "We follow Miwa and Sasaki (2014) and , treating relation extraction as a tablefilling problem, performing entity detection and relation classification using a single incremental model, which is similar in spirit to Miwa and Bansal (2016) by performing the task end-to-end."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-45", "intents": ["@USE@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "We map entity words into labels under the BILOU (Begin, Inside, Last, Outside, Unit) scheme, assuming that there are no overlapping entities in one sentence (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016) ."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-179", "intents": ["@USE@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "Compared with Miwa and Bansal (2016) features for entity detection."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-69", "intents": ["@EXT@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "The above two components have also been used by Miwa and Bansal (2016) ."}
{"sent_id": "3fd7a249a8fa7a71a4c6aa2e79fecf-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_3fd7a249a8fa7a71a4c6aa2e79fecf_5", "text": "A typical data preprocessing step is to normize digit characters (Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Strubell et al., 2017) ."}
{"sent_id": "3fd7a249a8fa7a71a4c6aa2e79fecf-C001-89", "intents": ["@USE@"], "paper_id": "ABC_3fd7a249a8fa7a71a4c6aa2e79fecf_5", "text": "We choose Lample et al. (2016) 's structure as its character LSTMs can be calculated in parallel, making the system more efficient."}
{"sent_id": "13091dd4d06e11957a5cb7785b92d4-C001-110", "intents": ["@MOT@"], "paper_id": "ABC_13091dd4d06e11957a5cb7785b92d4_5", "text": "These models are particularly suited for specific types of swapping and scrambling noises, but are not robust to natural noise."}
{"sent_id": "13091dd4d06e11957a5cb7785b92d4-C001-86", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_13091dd4d06e11957a5cb7785b92d4_5", "text": "The fact that we use deletion and insertion also explains why our model was able to regain a significant portion of its original performance when confronted with natural noise at test time, while previous work that trained only on substitutions and swaps was not able to do so (Belinkov and Bisk, 2018) ."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou' et al., 2013; Bansal et al., 2014; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015) ."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-17", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "Another advantage of this approach (versus previous work on feature embeddings or special neural networks for parsing) is that these link embeddings can be imported as off-the-shelf, dense, syntactic features into various other NLP tasks, similar to word embedding features, but now with richer, structured information, and in tasks where plain word embeddings have not proven useful ."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-40", "intents": ["@DIF@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "Also note the difference of our unary bucket features from the binary bucket features of Bansal et al. (2014) , who had to work with pairwise, conjoined features of the head and the argument."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-64", "intents": ["@SIM@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "Therefore, the main contribution of these link embeddings is that their significantly simpler, smaller, and faster set of unary features can match the performance of complex, template-based BROWN features (and of the dependency-based word embedding features of Bansal et al. (2014) ), and also stack over them."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "They have used unsupervised clustering techniques to induce a sense of a word and then compared the induced senses of two time periods to get the new sense for a particular target word."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-79", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "Baseline 1: Mitra et al. [19] The authors proposed an unsupervised method to identify word sense changes automatically for nouns."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-135", "intents": ["@USE@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "From both the time point pairs (T 1 and T 2 ), we take 100 random samples from the birth cases reported by Mitra et al. [19] and get these manually evaluated."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-212", "intents": ["@USE@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "We now compare the sense clusters extracted across two different time points to obtain the suitable signals of sense change following the approach proposed in Mitra et al. Mitra et al. [19] ."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-81", "intents": ["@MOT@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "The authors constructed distributional thesauri (DT) networks from the Google books syntactic n-grams data [9] ."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-187", "intents": ["@MOT@", "@DIF@", "@FUT@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "We find that in most of such cases, the sense cluster reported as 'birth' contained many new terms (and therefore, the network properties have undergone change) but the implied sense was already present in one of the previous clusters with very few common words (and therefore, the new cluster contained > 80% new words, and is being reported as 'birth' in Mitra et al. [19] )."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-189", "intents": ["@MOT@", "@FUT@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "The split-join algorithm proposed in Mitra et al. [19] needs to be adapted for such cases."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-153", "intents": ["@DIF@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "Since the reported novel sense cluster can in principle be different from the 'birth' sense reported by the method of Mitra et al. [19] for the same word, we get the novel sense cases manually evaluated by 3 annotators (42 and 28 cases for the two time periods, respectively)."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-57", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "By allowing pattern X, we allow certain semantic units with no natural language word counter- part to exist in the joint relaxed hybrid tree representation."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-73", "intents": ["@MOT@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "However, as mentioned above, doing so will lead to latent structures (relaxed hybrid tree representations) of infinite heights."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-55", "intents": ["@DIF@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "This ensured relaxed hybrid trees with an infinite number of nodes were not considered (Lu, 2014) when computing the denominator term of Equation 1."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-113", "intents": ["@DIF@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "We summarized in Table 3 the number of features used in both the previous RHT system and our system across four different languages."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-116", "intents": ["@DIF@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "We also note that the training time for our model is longer than that of the relaxed hybrid tree model since the space for H (n, m ) is now much larger than the space for H(n, m )."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Examples of such models include the WASP system (Wong and Mooney, 2006) which regards the semantic parsing problem as a statistical machine translation problem, the UBL system (Kwiatkowski et al., 2010) which performs CCG-based semantic parsing using a log-linear model, as well as the relaxed hybrid tree model (Lu, 2014) which extends the generative hybrid tree model."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Let us use m to denote a complete semantic representation, n to denote a complete natural language sentence, and h to denote a complete latent structure that jointly represents both m and n. The model defines the conditional probability for observing a (m, h) pair for a given natural language sentence n using a log-linear approach:"}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "The relaxed hybrid trees are analogous to the hybrid trees, which was earlier introduced as a generative framework."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Both the hybrid tree and relaxed hybrid tree models define patterns at each level of their latent structure which specify how the words and child semantic units are organized at each level."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-58", "intents": ["@UNSURE@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "This may lead to possible relaxed hybrid tree representations consisting of an infinite number of internal nodes (semantic units), as seen in Figure 3 (b) ."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-64", "intents": ["@UNSURE@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Specifically, for certain semanticssentence pairs, it is not possible to find relaxed hybrid trees that jointly represent them."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-65", "intents": ["@UNSURE@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "In the example semantics-sentence pair given in Figure 3 (a) , it is not possible to find any relaxed hybrid tree that contains both the sentence and the semantics since each semantic unit which takes one argument must be associated with at least one word."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-68", "intents": ["@UNSURE@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "For example, we can append the special beginning-of-sentence symbol s and end-of-sentence symbol /s to all sentences to increase their lengths, allowing the relaxed hybrid trees to be constructed for certain sentence-semantics pairs with short sentences."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-69", "intents": ["@UNSURE@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "However, such an approach does not resolve the theoretical limitation of the model."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-93", "intents": ["@USE@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "To make our system directly comparable to previous works, we used the same train/test split used in those works (Jones et al., 2012; Lu, 2014) for evaluation."}
{"sent_id": "4b0aab00a99547791bff0597aabc06-C001-17", "intents": ["@USE@"], "paper_id": "ABC_4b0aab00a99547791bff0597aabc06_5", "text": "Our method leverages Cotterell et al. (2017) 's formulation of Mikolov et al. (2013) 's popular skip-gram model as exponential family principal component analysis (EPCA) and tensor factorization."}
{"sent_id": "4b0aab00a99547791bff0597aabc06-C001-19", "intents": ["@EXT@"], "paper_id": "ABC_4b0aab00a99547791bff0597aabc06_5", "text": "In so doing, we also generalize Cotterell et al.'s method to arbitrary tensor dimensions."}
{"sent_id": "4b0aab00a99547791bff0597aabc06-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_4b0aab00a99547791bff0597aabc06_5", "text": "Specifically, Cotterell et al. recast higher-order SG as maximizing the log-likelihood"}
{"sent_id": "d5144370a9361ff870dd3cb2e064ff-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_d5144370a9361ff870dd3cb2e064ff_5", "text": "The evaluation is then performed similarly to the LM setup of Linzen et al. (2016) : the sentence is fed into a pretraiend LSTM LM up to the focus verb, and the model is considered correct if the probability assigned to the correct inflection of the original verb form given the prefix is larger than that assigned to the incorrect inflection."}
{"sent_id": "e5886e138ce8d84a48e44db3f3d6a1-C001-21", "intents": ["@MOT@", "@EXT@"], "paper_id": "ABC_e5886e138ce8d84a48e44db3f3d6a1_5", "text": "Thus, a natural question arises as to whether network embeddings should be more effective than the handcrafted network features used by Jana and Goyal (2018b) for cohyponymy detection."}
{"sent_id": "e5886e138ce8d84a48e44db3f3d6a1-C001-61", "intents": ["@USE@"], "paper_id": "ABC_e5886e138ce8d84a48e44db3f3d6a1_5", "text": "We perform experiments using three benchmark datasets for co-hyponymy detection (Weeds et al., 2014; Santus et al., 2016; Jana and Goyal, 2018b) ."}
{"sent_id": "e5886e138ce8d84a48e44db3f3d6a1-C001-90", "intents": ["@USE@"], "paper_id": "ABC_e5886e138ce8d84a48e44db3f3d6a1_5", "text": "Table 5 : Accuracy scores on a ten-fold cross validation of models (svmSS, rfALL) proposed by Jana and Goyal (2018b) and our models for the dataset prepared by Jana and Goyal (2018b) ."}
{"sent_id": "bebcad79900e9a4a25020ed0d886b5-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_bebcad79900e9a4a25020ed0d886b5_5", "text": "This results in \"coloreless green ideas\" nonce sentences."}
{"sent_id": "bebcad79900e9a4a25020ed0d886b5-C001-82", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_bebcad79900e9a4a25020ed0d886b5_5", "text": "The Gulordava et al. (2018) and Marvin and Linzen (2018) conditions rule out the possibility of overly relying on selectional preference cues or memorizing the wikipedia training data, and suggest real syntactic generalization is taking place."}
{"sent_id": "bebcad79900e9a4a25020ed0d886b5-C001-65", "intents": ["@USE@"], "paper_id": "ABC_bebcad79900e9a4a25020ed0d886b5_5", "text": "While not strictly comparable, the numbers reported by Gulordava et al. (2018) for the LSTM in this condition (on All) is 74.1 ± 1.6."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "However, the RL models have drawbacks of their own, the most notable of which are computational cost and predictive accuracy."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-91", "intents": ["@SIM@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Similar to [3, 13, 30] , we formulate this problem as a Markov Decision Process (MDP), in which the goal is to train a policy gradient agent (using REIN- FORCE [28] ) to learn an optimal reasoning path to answer a given query (e s , r, ?)."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-187", "intents": ["@USE@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "For the two Amazon datasets, we perform a grid search for our method and all baselines and report the best performance for each."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-218", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "For this analysis, we compare our ablation models (Ours (-N) and Ours (-T)) with the best performing RL baseline by Lin et al. [13] ."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-257", "intents": ["@USE@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Therefore, we focus on the diversity of the relations used in our method and the best performing baseline [13] for the discovered paths in the development set."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-199", "intents": ["@DIF@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "We can see that in all three datasets, our results outperform both RL baselines (Lin et al. [13] and MINERVA [3] )."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-256", "intents": ["@UNSURE@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "As a result, we observe many frequent patterns that all RL baselines are able to discover."}
{"sent_id": "0c233d68fb2ccdf033fc6a08c8f4bf-C001-3", "intents": ["@BACK@"], "paper_id": "ABC_0c233d68fb2ccdf033fc6a08c8f4bf_6", "text": "Currently, the primary application of the PDTB annotation has been to news articles."}
{"sent_id": "0c233d68fb2ccdf033fc6a08c8f4bf-C001-40", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_0c233d68fb2ccdf033fc6a08c8f4bf_6", "text": "The PDTB also reported a higher level of agreement in annotating Arg2 than in annotating Arg1 (Miltsakaki et al. 2004) ."}
{"sent_id": "0c233d68fb2ccdf033fc6a08c8f4bf-C001-4", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_0c233d68fb2ccdf033fc6a08c8f4bf_6", "text": "In this study, we tested whether the PDTB guidelines can be adapted to a different genre."}
{"sent_id": "0c233d68fb2ccdf033fc6a08c8f4bf-C001-15", "intents": ["@MOT@"], "paper_id": "ABC_0c233d68fb2ccdf033fc6a08c8f4bf_6", "text": "This work examines whether the PDTB annotation guidelines can be adapted to a different genre, the biomedical literature."}
{"sent_id": "0c233d68fb2ccdf033fc6a08c8f4bf-C001-29", "intents": ["@USE@"], "paper_id": "ABC_0c233d68fb2ccdf033fc6a08c8f4bf_6", "text": "When the annotation work was completed, we measured the inter-annotator agreement, following the PDTB exact match criterion (Miltsakaki et al. 2004 )."}
{"sent_id": "0c233d68fb2ccdf033fc6a08c8f4bf-C001-9", "intents": ["@EXT@"], "paper_id": "ABC_0c233d68fb2ccdf033fc6a08c8f4bf_6", "text": "Thus our experiments suggest that the PDTB annotation can be adapted to new domains by minimally adjusting the guidelines and by adding some further domain-specific linguistic cues."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-19", "intents": ["@MOT@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "The dataset has favorable properties: in addition to being a benchmark for much previous work, it provides with human annotations at all nodes of the trees, enabling us to comprehensively explore the properties of S-LSTM."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-134", "intents": ["@DIF@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "RNTN is different from RvNN in that when merging two nodes to obtain the hidden vector of their parent, tensor is used to obtain the second-degree polynomial interactions."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-110", "intents": ["@BACK@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "The Stanford Sentiment Tree Bank (Socher et al., 2013) contains about 11,800 sentences from the movie reviews that were originally discussed in (Pang & Lee, 2005) ."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-106", "intents": ["@USE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "We used the same dataset, the Stanford Sentiment Tree Bank, to evaluate the performances of the models."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-107", "intents": ["@USE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "In addition to being a benchmark for much previous work, the data provide with human annotations at all nodes of the trees, facilitating a more comprehensive exploration of the properties of S-LSTM."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-123", "intents": ["@USE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "We tuned our model against the development data set as split in (Socher et al., 2013) ."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-105", "intents": ["@EXT@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "In this paper, we put the proposed LSTM memory blocks at tree nodes-we replaced the tensorenhanced composition layer at each tree node presented in (Socher et al., 2013 ) with a S-LSTM memory block."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-130", "intents": ["@UNSURE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "Table 1 shows the accuracies of different models on the test set of the Stanford Sentiment Tree Bank."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-132", "intents": ["@UNSURE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "In Table 1 , NB and SVM are naive Bayes and support vector machine classifiers, respectively; RvNN corresponds to RNN in (Socher et al., 2013) ."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-147", "intents": ["@UNSURE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "However, these should not be regarded as comments on the value of the Sentiment Tree Bank."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "By comparing the accuracy of the very same algorithm according to whether or not it takes into account complementary semantic knowledge, they were able to show the benefit derived from such knowledge."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-41", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "Choi et al. (2001) have shown that using this procedure to compute the inter-sentence similarities results in the previous version of the algorithm (based solely on word repetition) being outperformed."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "One Within semantic space, which corresponds to the one used by Choi et al., was built using the entire Brown corpus as the LSA corpus."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-20", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "The first experiment is based on the original materials from Choi et al., which consisted of a small corpus (1,000,000 words)."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-70", "intents": ["@USE@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "The C99 algorithm, which does not employ LSA to estimate the similarities between the sentences, produces a Pk of 0.13 (Choi et al. 2001, Table 3, line 3: No stemming) ."}
{"sent_id": "9567cb276162a6e9d445f13f06f5a2-C001-47", "intents": ["@BACK@", "@EXT@", "@DIF@"], "paper_id": "ABC_9567cb276162a6e9d445f13f06f5a2_6", "text": "In our previous work (Zapirain et al., 2009 ), we modelled SPs for pairs of predicates (verbs) and arguments, independently of the fact that the argument is a core argument (typically a noun) or an adjunct argument (typically a prepositional phrase)."}
{"sent_id": "c856f5ce5d2cdcfc71027d6fa4c6b3-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_c856f5ce5d2cdcfc71027d6fa4c6b3_6", "text": "In a first evaluation based on the procedure described below, Choi showed that its algorithm outperforms several other approaches such as TextTiling (Hearst 1997) and Segmenter (Kan, Klavans, and McKeown 1998) ."}
{"sent_id": "c856f5ce5d2cdcfc71027d6fa4c6b3-C001-93", "intents": ["@USE@"], "paper_id": "ABC_c856f5ce5d2cdcfc71027d6fa4c6b3_6", "text": "The test materials were extracted from the 1997-1998 corpus following the guidelines given in Choi (2000) ."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-5", "intents": ["@DIF@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "Cumulatively up to 20% error reduction is achieved relative to the standard Boulis and Ostendorf (2005) algorithm for classifying individual conversations on Switchboard, and accuracy for gender detection on the Switchboard corpus (aggregate) and Gulf Arabic corpus exceeds 95%."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-104", "intents": ["@EXT@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "Thus, on top of the standard Boulis and Ostendorf (2005) model, we also investigated the following features motivated by the sociolinguistic literature on gender differences in discourse (Macaulay, 2005) :"}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-36", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "While Boulis and Ostendorf (2005) observe that the gender of the partner can have a substantial effect on their classifier accuracy, given that same-gender conversations are easier to classify than mixed-gender classifications, they don't utilize this observation in their work."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-54", "intents": ["@USE@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "As our reference algorithm, we used the current state-of-the-art system developed by Boulis and Ostendorf (2005) using unigram and bigram features in a SVM framework."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-70", "intents": ["@USE@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "The \"Boulis and Ostendorf, 05\" rows in Table 3 show the performance of this reimplemented algorithm on both the Fisher (90.84%) and Switchboard (90.22%) corpora, under the identical training and test conditions used elsewhere in our paper for direct comparison with subsequent results 2 ."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-6", "intents": ["@USE@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016)."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-66", "intents": ["@USE@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Gender Following the indication that gender can positively influence classification scores (Waseem and Hovy, 2016) , we compute the gender of the users in our data set."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-28", "intents": ["@SIM@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "In fact, we see that the amateur majority voted labels is the only distribution that tends towards a label distribution similar to Waseem and Hovy (2016) Our annotation effort deviates from Waseem and Hovy (2016) ."}
{"sent_id": "04b525b91b48e31258287a015d0401-C001-19", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_04b525b91b48e31258287a015d0401_6", "text": "One type regards words which refer specifically to the category name's meaning, such as pitcher for the category Baseball."}
{"sent_id": "04b525b91b48e31258287a015d0401-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_04b525b91b48e31258287a015d0401_6", "text": "As described in Section 1, the keyword list in (Gliozzo et al., 2005) consisted of the category name alone."}
{"sent_id": "04b525b91b48e31258287a015d0401-C001-85", "intents": ["@DIF@"], "paper_id": "ABC_04b525b91b48e31258287a015d0401_6", "text": "As we hypothesized, the Reference model achieves much better precision than the Context model from (Gliozzo et al., 2005) resources, yielding a lower F1."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "MULTIR uses features which are based on Mintz et al. (2009) and consist of conjunctions of named entity tags, syntactic dependency paths between arguments, and lexical information."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-72", "intents": ["@EXT@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "Because the two types of lexical features used in our passage retrieval models are not used in MUL-TIR, we created another baseline MULTIRLEX by adding these features into MULTIR in order to rule out the improvement from additional information."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-59", "intents": ["@USE@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "We use a state-of-the-art open-source system, MULTIR (Hoffmann et al., 2011) , as the relation extraction component."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-65", "intents": ["@USE@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "Our complete system, which we call IRMIE, combines our passage retrieval component with MULTIR."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-23", "intents": ["@USE@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation -namely WordSim-353 (Finkelstein et al., 2001) , MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015) ."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-96", "intents": ["@USE@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by Hill et al. (2015) , who used the code (or directly the embeddings) shared by the original authors."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-130", "intents": ["@USE@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "For the sake of comparison, we also report the results of the state-of-the-art DSMs mentioned in Hill et al. (2015) (see Section 2.5)."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-170", "intents": ["@USE@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "Concerning the discrimination between similarity and association, the good performance of APSyn on SimLex-999 (which was built with a specific attention to genuine similarity) and the large difference in performance between the two subsets of WordSim-353 described in Table  3 and Table 4 make us conclude that APSyn is indeed efficient in quantifying genuine similarity."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-89", "intents": ["@BACK@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "According to Hill et al. (2015) , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-97", "intents": ["@SIM@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "As we trained our models on almost the same corpora used by Hill and colleagues, the results are perfectly comparable."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-138", "intents": ["@DIF@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "The former appears to perform better on SimLex-999, while the latter seems to have some advantages on the other datasets."}
{"sent_id": "14529822630fb469f5fc8f37aaf473-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_14529822630fb469f5fc8f37aaf473_6", "text": "Reddy et al. (2011) introduced a dataset consisting of 90 compound nouns along with human judgments of their literality or compositionally at both the constituent and the phrase level."}
{"sent_id": "14529822630fb469f5fc8f37aaf473-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_14529822630fb469f5fc8f37aaf473_6", "text": "With their optimal settings, they achieved a Spearman's rank correlation coefficient of 0.714 with the human judgments, which remains the state-of-the-art on this dataset 1 ."}
{"sent_id": "4176674f83dec5389a23d9d45654c7-C001-39", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_4176674f83dec5389a23d9d45654c7_6", "text": "In Marton et al. (2013) , we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA)."}
{"sent_id": "4176674f83dec5389a23d9d45654c7-C001-85", "intents": ["@USE@"], "paper_id": "ABC_4176674f83dec5389a23d9d45654c7_6", "text": "For gold tokenization and predicted morphology (Table 2) , we also give the performance reported in our previous work (Marton et al., 2013) ."}
{"sent_id": "4176674f83dec5389a23d9d45654c7-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_4176674f83dec5389a23d9d45654c7_6", "text": "In Marton et al. (2013) , we showed that by modeling the functional morphology rather than the form-based morphology, we obtain a further increase in parsing performance (2013) test (old split) 81.0 84.0 92.7 Table 2 : Results of our system on Shared Task test data, Gold Tokenization, Predicted Morphological Tags; and for reference also on the data splits used in our previous work (Marton et al., 2013) ; \"≤ 70\" refers to the test sentences with 70 or fewer words."}
{"sent_id": "4176674f83dec5389a23d9d45654c7-C001-86", "intents": ["@DIF@"], "paper_id": "ABC_4176674f83dec5389a23d9d45654c7_6", "text": "The increase over the previously reported work may simply be due to the different split for training and test, but it may also be due to improvements to the functional feature prediction (Alkuhlani and Habash, 2012) , and the predicted features provided by the Shared Task organizers."}
{"sent_id": "a6564c4b215e6c5ad4f53eeb5dd69c-C001-23", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_a6564c4b215e6c5ad4f53eeb5dd69c_6", "text": "LSTM-based turn-taking models that operate in a similar predictive fashion were proposed by Skantze in [13] ."}
{"sent_id": "a6564c4b215e6c5ad4f53eeb5dd69c-C001-25", "intents": ["@MOT@"], "paper_id": "ABC_a6564c4b215e6c5ad4f53eeb5dd69c_6", "text": "The networks are trained to predict a vector of probability scores for speech activity in each individual frame within a set future window."}
{"sent_id": "a6564c4b215e6c5ad4f53eeb5dd69c-C001-27", "intents": ["@MOT@"], "paper_id": "ABC_a6564c4b215e6c5ad4f53eeb5dd69c_6", "text": "They can therefore be applied to a wide variety of turn-taking prediction tasks and have been shown to outperform traditional classifiers when applied to HOLD/SHIFT predictions."}
{"sent_id": "780b96afa8d417aa241e01ad594ce9-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_780b96afa8d417aa241e01ad594ce9_6", "text": "In a recent work (Yang et al., 2015) , a new corpus was constructed from the Pun of the Day website."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-3", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "We use pretrained BERT (Devlin et al., 2018) architecture and investigate the effect of different fine tuning regimes on the final classification task."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-39", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "In this study, we use an open source PyTorch implementation 3 of BERT architecture."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-60", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "We use the same tokenization method and embeddings as Devlin et al. (2018) to represent the words."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-65", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "We set 16 as our batch size and 2e-5 as our learning rate as recommended by Devlin et al. (2018) Table 1 : Classification results on our portal-wise data splits with fine-tuned BERT."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-109", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "We presented a BERT baseline for the Hyperpartisan News Detection task."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "Devlin et al. (2018) introduced two unsupervised tasks to pretrain this architecture, Next Sentence Prediction and Masked Language Modeling."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-41", "intents": ["@SIM@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "Similar to Devlin et al. (2018) , we use the representation obtained from the last layer for the first token (i.e. \"[CLS]\") for the sentence representation and a softmax classifier on top of it for predicting hyperpartisanship."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-86", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "Results show that pretraining BERT further with data from an unseen domain greatly increases its representational power."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-95", "intents": ["@EXT@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "In our first attempt, we fine-tuned BERT with portal-wise train split using development set to get the best model."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "The reason why the local context model of Ganea and Hofmann (2017) couldn't capture such apparent cue is two folds."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-122", "intents": ["@BACK@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "The local context model of Ganea and Hofmann (2017) mainly captures the topic level entity relatedness information based on a long range bag-of-words context."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-178", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Moreover, BERT+G&Hs embeddings performs significantly worse than the baseline (Ganea and Hofmann 2017) and our proposed BERT-Entity-Sim model."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-67", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Local Model Following Ganea and Hofmann (2017) , we instantiate the local model as an attention model based on pre-trained word and entity embeddings."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-141", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Besides, our candidate generation strategy follows that of Ganea and Hofmann (2017) to make our results comparable."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-149", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "To do so, we introduce a 768 × 300 dimensional matrix W which projects BERT-based context representation c into Ganea and Hofmann (2017)'s entity embeddings space when calculating the similarity score."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-166", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Note that all the hyper-parameters used in the local context and global model of Ganea and Hofmann (2017) were set to the same values as theirs for direct comparison purpose."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-255", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "As we can see, we query STEVE JOBS, the nearest entity in Ganea and Hofmann (2017) is APPLE INC."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-119", "intents": ["@DIF@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "As will be shown in the analysis section, the entity embeddings from BERT better capture entity type information than those from Ganea and Hofmann (2017) ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-174", "intents": ["@DIF@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Equipped with the global modeling method of Ganea and Hofmann (2017) , the performance of our model further increase to 93.54 with an average 1.32 improvement in terms of F1 over Ganea and Hofmann (2017) ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-258", "intents": ["@DIF@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Another example is when we query NA-TIONAL BASKETBALL ASSOCIATION, the most similar entities in Ganea and Hofmann (2017) are NBA teams which are topically related, while the entities retrieved by our approach are all basketball leagues."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-189", "intents": ["@UNSURE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "• Do the entity embeddings from BERT better capture latent entity type information than that of Ganea and Hofmann (2017) ?"}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-190", "intents": ["@UNSURE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "• Does the proposed model correct the type errors in the baseline (Ganea and Hofmann 2017)?"}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-235", "intents": ["@UNSURE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Compared to BERT-Entity-Sim equipped with Ganea and Hofmann (2017)'s global model in Table 8 Ganea and Hofmann (2017) )."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-16", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Contributions We focus on the recent stateof-the-art unsupervised model of Conneau et al. (2018) ."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-18", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "(b) In §3, we identify circumstances under which the unsupervised bilingual dictionary induction (BDI) algorithm proposed in Conneau et al. (2018) does not lead to good performance."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-20", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Our main finding is that the performance of unsupervised BDI depends heavily on all three factors: the language pair, the comparability of the monolingual corpora, and the parameters of the word embedding algorithms."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-53", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "The algorithms are not intended to be limited to learning scenarios where these assumptions hold, but since they do in the reported experiments, it is important to see to what extent these assumptions are necessary for the algorithms to produce useful embeddings or dictionaries."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-63", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "We evaluate Conneau et al. (2018) on pairs of embeddings induced with different hyper-parameters in §4.4."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-99", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Our default experimental setup closely follows the setup of Conneau et al. (2018) ."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-213", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "We investigated when unsupervised BDI (Conneau et al., 2018 ) is possible and found that differences in morphology, domains or word embedding algorithms may challenge this approach."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Unsupervised approaches to learning crosslingual word embeddings are based on the assumption that monolingual word embedding graphs are approximately isomorphic, that is, after removing a small set of vertices (words) (Mikolov et al., 2013b; Barone, 2016; Zhang et al., 2017; Conneau et al., 2018) ."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-22", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "As mentioned, recent work focused on unsupervised BDI assumes that monolingual word embedding spaces (or at least the subgraphs formed by the most frequent words) are approximately isomorphic."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-139", "intents": ["@BACK@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Monolingual word embeddings used in Conneau et al. (2018) are induced from Wikipedia, a nearparallel corpus."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-65", "intents": ["@MOT@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "We also investigate the sensitivity of unsupervised BDI to the dimensionality of the monolingual word embeddings in §4.5."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-129", "intents": ["@UNSURE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "We note that while languages with mixed marking may be harder to align, it seems unsupervised BDI is possible between similar, mixed marking languages."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-162", "intents": ["@UNSURE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Our main finding here is that unsupervised BDI fails (even) for EN-ES when the two monolingual embedding spaces are induced by two different algorithms (see the results of the entire Spanish cbow column)."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-197", "intents": ["@UNSURE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "The correlation between BDI performance and graph similarity is strong (ρ ∼ 0.89)."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-214", "intents": ["@FUT@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Further, we found eigenvector similarity of sampled nearest neighbor subgraphs to be predictive of unsupervised BDI performance."}
{"sent_id": "c3f71bea55f85633568c7ba57f6fd5-C001-106", "intents": ["@EXT@"], "paper_id": "ABC_c3f71bea55f85633568c7ba57f6fd5_6", "text": "We also propose a paradigm for data generation that is an extension of the CLEVR paradigm: The acoustic scenes are generated by combining a number of elementary sounds, and the corresponding questions and answers are generated based on the properties of those sounds and their mutual relationships."}
{"sent_id": "c3f71bea55f85633568c7ba57f6fd5-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_c3f71bea55f85633568c7ba57f6fd5_6", "text": "This motivated research [23, 8, 11] on how to reduce the bias in VQA datasets."}
{"sent_id": "c3f71bea55f85633568c7ba57f6fd5-C001-72", "intents": ["@USE@"], "paper_id": "ABC_c3f71bea55f85633568c7ba57f6fd5_6", "text": "Questions are structured in a logical tree introduced in CLEVR [11] as a functional program."}
{"sent_id": "b7278824bdae498021b899fbc6c638-C001-46", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_b7278824bdae498021b899fbc6c638_6", "text": "This analysis is inspired by (Zhang and Iria, 2009 ), but performed on the entire abstract which is clearly dis- (Zhang and Iria, 2009) , where this is applied only to the first sentence (as WIKIPEDIA does not directly provide a concept of \"abstract\")."}
{"sent_id": "b7278824bdae498021b899fbc6c638-C001-74", "intents": ["@DIF@"], "paper_id": "ABC_b7278824bdae498021b899fbc6c638_6", "text": "The experiment domains and (Zhang and Iria, 2009 ), which we outperform for all entity types, in some cases up to 5% in F 1 score."}
{"sent_id": "b7278824bdae498021b899fbc6c638-C001-41", "intents": ["@SIM@"], "paper_id": "ABC_b7278824bdae498021b899fbc6c638_6", "text": "As (Zhang and Iria, 2009 ), we reject redirection entries in this step as ambiguous."}
{"sent_id": "b7278824bdae498021b899fbc6c638-C001-87", "intents": ["@USE@"], "paper_id": "ABC_b7278824bdae498021b899fbc6c638_6", "text": "Finally, we also include the performance numbers report in (Zhang and Iria, 2009 ) for comparison (since we share their evaluation settings)."}
{"sent_id": "6ed955baf28ad1c7fd6d590e660c20-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_6ed955baf28ad1c7fd6d590e660c20_7", "text": "The word vectors Tian et al. (2014) use to calculate similarities are reported able to capture semantic compositions by simple additions and subtractions (Mikolov et al., 2013) ."}
{"sent_id": "6ed955baf28ad1c7fd6d590e660c20-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_6ed955baf28ad1c7fd6d590e660c20_7", "text": "The system of Tian et al. (2014) generated onthe-fly knowledge to join several fragments in T and wrongly proved H. In examples of such complexity, distributional similarity is no longer reliable."}
{"sent_id": "926e7df3c367ae29da574ba465504f-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_926e7df3c367ae29da574ba465504f_7", "text": "Thus the scope of this method gets limited to solving a local problem of re-ordering a small set of candidate images for a given topic."}
{"sent_id": "926e7df3c367ae29da574ba465504f-C001-70", "intents": ["@BACK@"], "paper_id": "ABC_926e7df3c367ae29da574ba465504f_7", "text": "The 20 candidate image labels per topic are collected by Aletras and Stevenson (2013) using an information retrieval engine (Google)."}
{"sent_id": "926e7df3c367ae29da574ba465504f-C001-88", "intents": ["@USE@"], "paper_id": "ABC_926e7df3c367ae29da574ba465504f_7", "text": "We compare our approach to the state-of-the-art method that uses Personalized PageRank (Aletras and Stevenson, 2013) to re-rank image candidates."}
{"sent_id": "926e7df3c367ae29da574ba465504f-C001-116", "intents": ["@DIF@"], "paper_id": "ABC_926e7df3c367ae29da574ba465504f_7", "text": "Our evaluation results show that our proposed approach significantly outperforms the state-of-the-art method of Aletras and Stevenson (2013) and a relevant method originally utilized for image annotation proposed by Weston et al. (2010) ."}
{"sent_id": "2eeffe385539b28c7d31eeb176e926-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_2eeffe385539b28c7d31eeb176e926_7", "text": "The benefit of these models has also been demonstrated in specialized NLP domains."}
{"sent_id": "2eeffe385539b28c7d31eeb176e926-C001-44", "intents": ["@USE@"], "paper_id": "ABC_2eeffe385539b28c7d31eeb176e926_7", "text": "Best results for these tasks were achieved with the following parameters: two learning epochs, maximal sequence length of 300 (BERT-base, EDUBERT) and 512 for the distilled models, all other parameter values were equal to the ones used for pre-training."}
{"sent_id": "c7c9266b5063ec85494fde45d1dce1-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_c7c9266b5063ec85494fde45d1dce1_7", "text": "(1) Char n-grams: It is the state-ofthe-art method [6] which uses character n-grams for hate speech detection."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-73", "intents": ["@USE@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "In order to compare our results with those reported in [5] , we also used accuracy, precision, and recall."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-81", "intents": ["@USE@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "The last two rows show the results obtained by applying the system from [5] 6 to our cleaned dataset (Section 3)."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-28", "intents": ["@EXT@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "However, perhaps surprisingly, we are able to achieve the overall best performance by simply using higher-length n-grams than those used in the original work from [5] : this seems to indicate a strong lexical overlap between different sources with the same orientation, which, in turn, calls for more challenging datasets and task formulations to encourage the development of models covering more subtle, i.e., implicit, forms of bias."}
{"sent_id": "ec0ae4e56c069e3efb4a2dc12199cd-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_ec0ae4e56c069e3efb4a2dc12199cd_7", "text": "AL has been successfully applied to speed up the annotation process for many NLP tasks without sacrificing annotation quality (Engelson and Dagan, 1996; Ngai and Yarowsky, 2000; Hwa, 2001; Tomanek et al., 2007a) ."}
{"sent_id": "74e7b114ae968e196ea87f529f5eff-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_74e7b114ae968e196ea87f529f5eff_7", "text": "The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including automatic thesauri creation, paraphrasing, textual entailment, sentiment analysis and so on (Weeds et al., 2014) ."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "They consider a setting with streaming tasks, where each task consists of a number of distinct relations, and proposed to align the representation of relation instances in the embedding space to enable continual learning of new relations without forgetting knowledge from past relations."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-87", "intents": ["@BACK@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "ACC whole measures accuracy on the test set of all tasks and gives a balanced measure of model performance on both observed (seen) and unobserved (unseen) tasks, and is the primary metric we report for all experiments."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-96", "intents": ["@USE@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "The aim of our limited supervision experiments is to compare the use of an alignment module as proposed by Wang et al. (2019) to using our approach when only limited supervision is available for all tasks."}
{"sent_id": "6fbfc9f887e736472510bce30c9228-C001-18", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_6fbfc9f887e736472510bce30c9228_7", "text": "In previous work, we collected a small corpus of task-oriented dialogues between customers and support representatives from the MSN Shopping online support service (Ivanovic, 2005b) ."}
{"sent_id": "6fbfc9f887e736472510bce30c9228-C001-30", "intents": ["@USE@"], "paper_id": "ABC_6fbfc9f887e736472510bce30c9228_7", "text": "The tags were derived in Ivanovic (2005b) by manually labelling the MSN Shopping corpus using the tags that seemed appropriate from a list of 42 tags in Stolcke et al. (2000) ."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-103", "intents": ["@USE@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "Finally, we give the results of Zitouni et al. (2006) on the last line, which we understand to be the best published results currently."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-77", "intents": ["@BACK@", "@SIM@", "@DIF@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "Zitouni et al. (2006) use a maximum entropy classifier to assign a set of diacritics to the letters of each word."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-78", "intents": ["@BACK@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "They use the output of a tokenizer (segmenter) and a part-of-speech tagger (which presumably tags the output of the tokenizer)."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-80", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "Thus, while many of the same elements are used in their and our work (word n-grams, features related to morphological analysis), the basic approach is quite different: while we have one procedure that chooses a correct analysis (including to- Figure 1: Diacritization Results (all followed by single-choice-diac model); our best results are shown in boldface; Only-DLM-1 is the baseline; \"Zitouni\" is (Zitouni et al., 2006) kenization, morphological tag, and diacritization), they have a pipeline of processors."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-106", "intents": ["@DIF@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "This is because we are choosing among complete diacritization options for white space-tokenized words, while Zitouni et al. (2006) make choices for each diacritic."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-114", "intents": ["@UNSURE@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "It is possible to construct a combined system which uses a lexicon, but backs off to a Zitouni-style system for unknown words."}
{"sent_id": "fd5a6307b398f37d8729c21cfce6c1-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_fd5a6307b398f37d8729c21cfce6c1_7", "text": "Chiu and Nichols (2016) search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "However, such a model also needs to be able to identify when no answer is found in the text, and to achieve this they trained a slightly modified version of BiDAF (Seo et al., 2016) both positive examples, containing answers, and negative examples, without answers."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-40", "intents": ["@EXT@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "We also construct a series of datasets that combine increasing quantities of the UWRE entity split training set into the SQuAD training set, to evaluate the benefits of SQuAD when dedicated relation extraction data is limited."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-47", "intents": ["@EXT@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "As can be seen, BIDAF is actually more effective at answering the questions for the unseen relation types in the UWRE test set when it is trained on a standard QA dataset, rather than a dedicated relation extraction dataset."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-64", "intents": ["@EXT@", "@USE@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "Models We re-use the UWRE and SQuAD trained models in addition to training on the UWRE+ datasets described in the previous section."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-67", "intents": ["@USE@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "Table 3 : Zero-shot Precision, Recall and F1 on the UWRE relation split test set."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-73", "intents": ["@USE@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "We use the target corpus built by Tyers et al. (2018) which comprises 28,862 sentences which were extracted from Faroese Wikipedia dumps 6 using the WikiExtractor script 7 and further pre-processed to remove any non-Faroese texts and other forms of unsuitable sentences."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-132", "intents": ["@USE@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "In this section, we describe our experiments, which include a replication of the main findings of Tyers et al. (2018) , using AllenNLP for POS tagging and parsing instead of UDPipe (Straka and Straková, 2017 Figure 3 : Multi-source projection."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-173", "intents": ["@SIM@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "Note that they use predicted tokenization and segmentation whereas our experiments and Tyers et al.'s use gold tokenization and segmentation, which provides a small artificial boost."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-166", "intents": ["@DIF@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "Another reason why the multi-source model does not work as well in our experiments as it does in those of Tyers et al. (2018) might be that we use pre-trained embeddings whereas Tyers et al. (2018) do not."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "Tyers et al. (2018) describe a method for creating synthetic treebanks for Faroese based on previous work which uses machine translation and word alignments to transfer trees from source language(s) to the target language."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-114", "intents": ["@BACK@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "In some cases, not all tokens are aligned and Tyers et al. (2018) work around this by falling back to a 1:1 mapping between the target index and the source index."}
{"sent_id": "05fe3e9c1598f5b36b6efa79216309-C001-57", "intents": ["@DIF@"], "paper_id": "ABC_05fe3e9c1598f5b36b6efa79216309_7", "text": "The model differs from recent works [1, 6] , due to the use of attention layer combined with character-level LSTM."}
{"sent_id": "008d5261ee7385a2b7e39772938f51-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_008d5261ee7385a2b7e39772938f51_7", "text": "This setup has been shown to produce good results earlier as well (Pang et al., 2002; Athar, 2011) ."}
{"sent_id": "d06a49ad232f73328874282d91cde0-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_d06a49ad232f73328874282d91cde0_7", "text": "In (Zhang et al., 2007a) , we have pointed out that most applications are only interested in certain aspects of parsing results."}
{"sent_id": "d06a49ad232f73328874282d91cde0-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_d06a49ad232f73328874282d91cde0_7", "text": "While there is no gold-standard corpus for the purpose of partial parse evaluation, Zhang et al. (2007a) manually compared the parser's partial derivation trees with the Penn Treebank annotation for syntactic similarity."}
{"sent_id": "d06a49ad232f73328874282d91cde0-C001-158", "intents": ["@BACK@"], "paper_id": "ABC_d06a49ad232f73328874282d91cde0_7", "text": "For the purpose of evaluation, Zhang et al. (2007a) compared the partial derivation tree to the Penn Treebank bracketing, and partial RMRS fragments to the RASP RMRS outputs."}
{"sent_id": "9731b4cea1405b7cbf3792aed5b1e4-C001-14", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_9731b4cea1405b7cbf3792aed5b1e4_7", "text": "Therefore, our setting resembles the established task of entity recognition (Finkel et al., 2005; Ratinov and Roth, 2009) , with the difference being that we focus on un-named entities."}
{"sent_id": "9731b4cea1405b7cbf3792aed5b1e4-C001-114", "intents": ["@USE@"], "paper_id": "ABC_9731b4cea1405b7cbf3792aed5b1e4_7", "text": "For the CRF, we use the commonly used features for named entity recognition: words, prefix/suffices, and part-of-speech tag (Ratinov and Roth, 2009 )."}
{"sent_id": "a2b945e18ab6b73b4021a2db8bda4f-C001-86", "intents": ["@USE@"], "paper_id": "ABC_a2b945e18ab6b73b4021a2db8bda4f_7", "text": "Since our goal is to compare with the manual annotation setup, we use the first setting, where we train on the S&G dataset and test on our T OEF L arg dataset."}
{"sent_id": "78afdf391c70d7992200b4071e4ac2-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_78afdf391c70d7992200b4071e4ac2_7", "text": "As Bonial et al. (2014) stated 'PB has previously treated language as if it were purely compositional, and has therefore lumped the majority of MWEs in with lexical verb usages'."}
{"sent_id": "78afdf391c70d7992200b4071e4ac2-C001-38", "intents": ["@DIF@"], "paper_id": "ABC_78afdf391c70d7992200b4071e4ac2_7", "text": "In addition, we set up an annotation effort to gather a frequency-balanced, data-driven evaluation set that is larger and more diverse than the annotated set provided by Bonial et al. (2014) ."}
{"sent_id": "78afdf391c70d7992200b4071e4ac2-C001-65", "intents": ["@MOT@"], "paper_id": "ABC_78afdf391c70d7992200b4071e4ac2_7", "text": "Roleset id: care.01, to be concerned Arg0: carer, agent Arg1: thing cared for/about Encouraged by the high proportion of CPs that could successfully be aliased in the pilot study by Bonial et al. (2014) , we created a method to automatically find aliases for CPs in order to decrease the amount of human intervention, thereby scaling up the coverage of CPs in PB."}
{"sent_id": "78afdf391c70d7992200b4071e4ac2-C001-96", "intents": ["@EXT@"], "paper_id": "ABC_78afdf391c70d7992200b4071e4ac2_7", "text": "In order to evaluate our system, we set up an annotation effort loosely following the guidelines provided by Bonial et al. (2014) ."}
{"sent_id": "78afdf391c70d7992200b4071e4ac2-C001-133", "intents": ["@USE@"], "paper_id": "ABC_78afdf391c70d7992200b4071e4ac2_7", "text": "We evaluated our approach on the 160 CPs annotated in the course of this work (Wiki50 set), as well as on the 70 take CPs from Bonial et al. (2014) (take set) and compare our results to the baseline."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "Goldberg et al. (2009) follow this definition of wish and provide manually annotated datasets, where each sentence is labelled as wish or non-wish."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "Ramanand et al. (2010) worked on product review dataset of the wish corpus, with an objective to extract suggestions for improvements."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-118", "intents": ["@DIF@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "In the case of politics dataset, similar reason (big dataset) can be attributed for the better performance of unigrams over subjunctive features."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-32", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "In subsequent work, Fitzgerald et al. (2018) constructed a large-scale corpus and used it to train a parser."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-19", "intents": ["@MOT@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "As Fitzgerald et al. (2018) acknowledged, the main shortage of the large-scale 2018 dataset is the lack of recall, estimated by experts to be in the lower 70s."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-71", "intents": ["@USE@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "All aligned arguments from the previous step are inspected for label equivalence, similar to the joint evaluation reported in (Fitzgerald et al., 2018) ."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-82", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "We extend our metric for evaluating manual or automatic redundant annotations, like the Dense dataset or the parser in (Fitzgerald et al., 2018) , which predicts argument spans independently of each other."}
{"sent_id": "d72f0608fddd1bf1cdef7ca6a20bdf-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_d72f0608fddd1bf1cdef7ca6a20bdf_8", "text": "Advanced variants of DNNs, such as convolutional neural nets (CNNs) [12] , recurrent neural nets (RNNs) [13] , long short-term memory nets (LSTMs) [14] , time-delay neural nets (TDNNs) [15, 29] , VGG-nets [8] , have significantly improved recognition performance, bringing them closer to human performance [9] ."}
{"sent_id": "d72f0608fddd1bf1cdef7ca6a20bdf-C001-102", "intents": ["@SIM@"], "paper_id": "ABC_d72f0608fddd1bf1cdef7ca6a20bdf_8", "text": "The FSH corpus contains speech from quite a diverse set of speakers, helping to reduce the WER of the CH subset more significantly than the SWB subset, a trend reflected in results reported in the literature [8] ."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-65", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "Discriminator: In addition to the D RN N in the discriminator of hredGAN , if the attribute C i+1 is a sequence of tokens, then the same tattRN N is used to summarize the attribute token embeddings; otherwise the single attribute embedding is concatenated with the other inputs of D RN N in Fig. 1 of [7] ."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-101", "intents": ["@USE@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "For the modified noise sample, we perform a linear search for α with sample size L = 1 based on the average discriminator loss, −logD(G(.)) [7] using trained models run in autoregressive mode to reflect performance in actual deployment."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-137", "intents": ["@USE@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "In [7] , the authors recommend the version with word-level noise injection, hredGAN w , so we use this version in our comparison."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-136", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "We also compare our system to hredGAN from [7] in terms of perplexity, ROGUE, and distinct n-grams scores."}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-79", "intents": ["@USE@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "• preordered data by our re-implementation of (Hoshino et al., 2013) , and"}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "Hoshino et al. (2013) proposed to move a predicate after the subject (inter-chunk preordering)."}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-15", "intents": ["@MOT@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "However, these preordering methods are tested on limited domains: travel (Komachi et al., 2006) and patent (Katz-Brown and Collins, 2008; Hoshino et al., 2013) corpora."}
{"sent_id": "e9e733d38affa8a39a633ffb4d9d71-C001-156", "intents": ["@BACK@"], "paper_id": "ABC_e9e733d38affa8a39a633ffb4d9d71_8", "text": "It needs to be noted that the average length of a string in the Digital Corpus of Sanskrit is 6.7 (Krishna et al., 2016) ."}
{"sent_id": "e9e733d38affa8a39a633ffb4d9d71-C001-188", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_e9e733d38affa8a39a633ffb4d9d71_8", "text": "Those systems such as Krishna et al. (2016) can be used to identify the morphological tags of the system as they currently store the morphological information of predicted candidates, but do not use them for evaluation as of now."}
{"sent_id": "e9e733d38affa8a39a633ffb4d9d71-C001-179", "intents": ["@MOT@"], "paper_id": "ABC_e9e733d38affa8a39a633ffb4d9d71_8", "text": "This limits the scalability of those systems, as they cannot handle out of vocabulary words."}
{"sent_id": "e9e733d38affa8a39a633ffb4d9d71-C001-203", "intents": ["@USE@"], "paper_id": "ABC_e9e733d38affa8a39a633ffb4d9d71_8", "text": "Our experiments in line with the measures reported in Krishna et al. (2016) show that our system performs robustly across strings of varying word size."}
{"sent_id": "fdfb8fbdb8544dca17b1aeba768124-C001-17", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_fdfb8fbdb8544dca17b1aeba768124_8", "text": "We performed a comparison between the existing CFG filtering techniques for LTAG (Poller and Becker, 1998) and HPSG (Torisawa et al., 2000) , using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Treebank (Marcus et al., 1993) into HPSG-style."}
{"sent_id": "fdfb8fbdb8544dca17b1aeba768124-C001-34", "intents": ["@USE@", "@EXT@", "@DIF@"], "paper_id": "ABC_fdfb8fbdb8544dca17b1aeba768124_8", "text": "The parsers with CFG filtering used in our experiments follow the above parsing strategy, but are different in the way the CF approximation and the elimination of impossible parse trees in phase 2 are performed."}
{"sent_id": "fdfb8fbdb8544dca17b1aeba768124-C001-84", "intents": ["@USE@"], "paper_id": "ABC_fdfb8fbdb8544dca17b1aeba768124_8", "text": "We can thereby construct another CFG filtering for LTAG by combining this CFG filter with an existing LTAG parsing algorithm (van Noord, 1994) ."}
{"sent_id": "91c82c4a49815fb2de300d99312754-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_91c82c4a49815fb2de300d99312754_8", "text": "To effectively incorporate KB information and perform knowledge- * Corresponding Author based reasoning, memory augmented models have been proposed (Bordes et al., 2017; Seo et al., 2017; Eric and Manning, 2017b; Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Wu et al., 2019) ."}
{"sent_id": "91c82c4a49815fb2de300d99312754-C001-40", "intents": ["@USE@"], "paper_id": "ABC_91c82c4a49815fb2de300d99312754_8", "text": "We omit the subscript E or S 2 , following Madotto et al. (2018) to define each pointer index set:"}
{"sent_id": "91c82c4a49815fb2de300d99312754-C001-41", "intents": ["@USE@"], "paper_id": "ABC_91c82c4a49815fb2de300d99312754_8", "text": "Symbol Definition xi or yi a token in the dialog history or system response $ a special token used as a sentinel (Madotto et al., 2018 ) X X = {x1, . . . , xn, $}, the dialog history Y Y = {y1, · · · , ym}, the expected response bi one KB tuple, actually the corresponding entity B B = {b1, · · · , b l , $}, the KB tuples P T RE = {ptrE,1, · · · , ptrE,m}, dialog pointer index set."}
{"sent_id": "91c82c4a49815fb2de300d99312754-C001-67", "intents": ["@EXT@"], "paper_id": "ABC_91c82c4a49815fb2de300d99312754_8", "text": "We here use a rule-based word selection strategy by extending the sentinel idea in (Madotto et al., 2018) , which is shown in Figure 1 ."}
{"sent_id": "91c82c4a49815fb2de300d99312754-C001-97", "intents": ["@DIF@"], "paper_id": "ABC_91c82c4a49815fb2de300d99312754_8", "text": "Though multi-hop attention strengthens the reasoning ability and improves the results, we find that the performance difference between the hops K = 1 and K = 3 is not so obvious as shown in (Madotto et al., 2018; Wu et al., 2019) ."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-14", "intents": ["@USE@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "In this work, we experiment with the Self-reported Mental Health Diagnoses (SMHD) dataset (Cohan et al., 2018) , consisting of thousands of Reddit users diagnosed with one or more mental illnesses."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-38", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "For each diagnosed user, we draw exactly nine control users from the pool of 335,952 control users present in SMHD and proceed to train and test our binary classifiers on the newly created sub-datasets."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-39", "intents": ["@USE@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "In order to create a statistically-fair comparison, we run the selection process multiple times, as well as reimplement the benchmark models used in Cohan et al. (2018) ."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-36", "intents": ["@BACK@", "@EXT@", "@DIF@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "Cohan et al. (2018) select nine or more control users for each diagnosed user and run their experiments with these mappings."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-41", "intents": ["@MOT@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "Due to this reason, we build our technique on top of theirs."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-71", "intents": ["@MOT@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "Although this works well for a relatively clean parallel corpus considered in their work, i.e., novels, this does not work well for bug reports."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-113", "intents": ["@MOT@", "@USE@", "@EXT@", "@DIF@", "@SIM@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "Experimental Setup The baseline method we consider is the one in (Barzilay and McKeown, 2001 ) without sentence alignment -as the bug reports are usually of one sentence long."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-114", "intents": ["@MOT@", "@USE@", "@EXT@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "We call it BL."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-49", "intents": ["@USE@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "The authors first used identical words and phrases as seeds to find and score contextual patterns."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-2", "intents": ["@EXT@", "@MOT@", "@USE@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "We propose a variant of a well-known machine translation (MT) evaluation metric, HyTER (Dreyer and Marcu, 2012) , which exploits reference translations enriched with meaning equivalent expressions."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-5", "intents": ["@EXT@", "@MOT@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "We show that although the metric obtains good results on small and carefully curated data with both manually and automatically selected substitutes, it achieves medium performance on much larger and noisier datasets, demonstrating the limits of the metric for tuning and evaluation of current MT systems."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-22", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "Contrary to the original HyTER implementation, we do not consider permutations when transforming x into y as previous results (cf."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-67", "intents": ["@EXT@", "@SIM@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "This observation shows that replacing the handcrafted lattices with automatically built ones has only a moderate impact on the HyTER metric quality: automatic lattices result in a small drop of the correlation when evaluating hypotheses translated from Chinese, and slightly improve it for the Arabic to English condition."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-14", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "We show that HyTERA strongly correlates with HyTER with hand-crafted lattices, and approximates the hTER score (Snover et al., 2006) as measured using post-edits made by human annotators."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-99", "intents": ["@USE@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "We provide the first evaluation of HyTER on data from a recent WMT Metrics Shared task."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "The manually built networks attempt to encode the set of all correct translations for a sentence, and HyTER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-84", "intents": ["@BACK@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "It is important to note that the hTER scores used in the initial HyTER evaluation were produced by experienced LDC annotators, while the WMT16 Direct Assessment (DA) adequacy judgments were collected from non-experts through crowd-sourcing (Bojar et al., 2016) ."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-83", "intents": ["@DIF@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "HyTERA obtains medium performance on the WMT16 dataset, which is much larger and noisier than the dataset used for evaluation in (Dreyer and Marcu, 2012) : it is made, for each language, of 560 translations sampled from outputs of all systems taking part in the WMT15 campaign."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-14", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "This constraint drastically reduces the size of grammar for LR-Hiero in comparison to Hiero grammar (Siahbani et al., 2013) ."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-92", "intents": ["@USE@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "We use 3 baselines: (i) our implementation of (Watanabe et al., 2006) : LR-Hiero with beam search (LR-Hiero) and (ii) LR-Hiero with cube pruning (Siahbani et al., 2013) : (LR-Hiero+CP); and (iii) Kriya, an open-source implementation of Hiero in Python, which performs comparably to other open-source Hiero systems (Sankaran et al., 2012) ."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-102", "intents": ["@USE@", "@SIM@", "@DIF@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "But modifying rule type (c) does not show any improvement due to spurious ambiguity created by 5 We report results on Cs-En and De-En in (Siahbani et al., 2013) ."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-104", "intents": ["@DIF@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "We achieve better results than our previous work (Siahbani et al., 2013) type (c) rules."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-110", "intents": ["@DIF@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "LRHiero+CP with our modifications works substantially faster than LR-Hiero while obtain significantly better translation quality on Zh-En."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "In particular, Naseem et al. showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of parameters for all languages."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-142", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "Cohen et al. (2011) and Naseem et al. (2012) have shown that using expectation-maximization (EM) to this end can in some cases bring substantial accuracy gains."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-119", "intents": ["@DIF@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "Although this model still performs worse than NBG, it is an improvement over the Delex baseline and actually outperforms the former on 5 out of the 16 languages."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-226", "intents": ["@DIF@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "Comparing this model to the NBG+EM baseline, we observe an improvement by 3.6% absolute, outperforming it on 14 of the 16 languages."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-46", "intents": ["@MOT@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "In the ordering step, however, parameters are selectively shared between subsets of Naseem et al. (2012) restricts its potential performance."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-103", "intents": ["@USE@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "Inspired by Naseem et al. Table 2 from Naseem et al. (2012) ."}
{"sent_id": "87af486eb2e968d2055eeab094b3f9-C001-53", "intents": ["@DIF@"], "paper_id": "ABC_87af486eb2e968d2055eeab094b3f9_8", "text": "Although margin ranking loss has been the dominant choice for training cross-modal feature matching [8, 20, 25] , we find that log-exp-sum pairwise ranking [26] yields better results in terms of evaluation performance and efficiency."}
{"sent_id": "dcf84cf05e3e7950cabbdd8d8f304c-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_dcf84cf05e3e7950cabbdd8d8f304c_8", "text": "The hypothesis behind this line of work is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words."}
{"sent_id": "dcf84cf05e3e7950cabbdd8d8f304c-C001-88", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_dcf84cf05e3e7950cabbdd8d8f304c_8", "text": "For example, using comp 1 with representations of the MWE and component words obtained from word2vec (Mikolov et al., 2013) , Salehi et al. (2015) achieve correlations of 0.717, 0.289, and 0.400 for ENC, the verb component of EVPC, and GNC, respectively."}
{"sent_id": "dcf84cf05e3e7950cabbdd8d8f304c-C001-41", "intents": ["@USE@"], "paper_id": "ABC_dcf84cf05e3e7950cabbdd8d8f304c_8", "text": "2 Once vector representations of an MWE and its component words are obtained, following Salehi et al. (2015) , the following equations are then used to compute the compositionality of an MWE:"}
{"sent_id": "dcf84cf05e3e7950cabbdd8d8f304c-C001-58", "intents": ["@USE@"], "paper_id": "ABC_dcf84cf05e3e7950cabbdd8d8f304c_8", "text": "We train language models over a portion of English and German Wikipedia dumps -following Salehi et al. (2015) -from 20 January 2018."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-12", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b ) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al., 2006 ) SS dataset."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-87", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "We include three baselines LSA, LDA and WTMF using the setting described in (Guo and Diab, 2012b) ."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-115", "intents": ["@BACK@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "Our previous experiments (Guo and Diab, 2012b) show that WTMF is the state-of-the-art model on LI06."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-117", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "It should be noted that LI06 prefers a smaller similar word pair weight ( a δ = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms WTMF as shown in Figure 3d ."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-13", "intents": ["@MOT@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "However, all of these models make harsh simplifying assumptions on how a token is generated: (1) in LSA/WTMF, a token is generated by the inner product of the word latent vector and the document latent vector; (2) in LDA, all the tokens in a document are sampled from the same document level topic distribution."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-19", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "In this paper, we explicitly encode lexical semantics, both corpus-based and knowledge-based information, in the WTMF model, by which we are able to achieve even better results in SS task."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-22", "intents": ["@USE@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "Consider the following example: In WTMF/LSA/LDA, a word will receive semantics from all the other words in a sentence, hence, the word oil, in the above example, will be assigned the incorrect finance topic that reflects the sentence level semantics."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-74", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "where P ·,s(i) are the latent vectors of similar words of word i; the length of these vectors in the current iteration are stored in L s(i) (similarly L i is the current length of P ·,i ) (cf. (Steck, 2010; Guo and Diab, 2012b) for optimization details)."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-108", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "Figure 3c illustrates the impact of dimension K = {50, 75, 100, 125, 150} on WTMF and WTMF+PK."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-96", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "Using corpus-based selectional preference semantics alone (model 4 WTMF+P in Table  1 ) boosts the performance of WTMF by +1.17% on the test set, while using knowledge-based semantics alone (model 5 WTMF+K) improves the over the WTMF results by an absolute +2.31%."}
{"sent_id": "0f5c87e5434785a612c6578244543d-C001-89", "intents": ["@SIM@"], "paper_id": "ABC_0f5c87e5434785a612c6578244543d_8", "text": "It should be noted that the implementation by Faruqui and Dyer (2014) also length-normalizes the word embeddings in a preprocessing step."}
{"sent_id": "520437f612e678dcd4ec9c043cf701-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_520437f612e678dcd4ec9c043cf701_8", "text": "3 Numbers in italic are classification results reported in Ott et al. (2011) and Mihalcea and Strapparava (2009)."}
{"sent_id": "520437f612e678dcd4ec9c043cf701-C001-20", "intents": ["@USE@"], "paper_id": "ABC_520437f612e678dcd4ec9c043cf701_8", "text": "I. TripAdvisor-Gold: Introduced in Ott et al. (2011) , this dataset contains 400 truthful reviews obtained from www.tripadviser.com and 400 deceptive reviews gathered using Amazon Mechanical Turk, evenly distributed across 20 Chicago hotels."}
{"sent_id": "425148e63eb84bba50326e362cc5b8-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_425148e63eb84bba50326e362cc5b8_9", "text": "Recently, Agrawal et al. [1] published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where they claimed that the instability of topics is one major shortcoming of this technique."}
{"sent_id": "425148e63eb84bba50326e362cc5b8-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_425148e63eb84bba50326e362cc5b8_9", "text": "Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm [1] ."}
{"sent_id": "425148e63eb84bba50326e362cc5b8-C001-94", "intents": ["@BACK@"], "paper_id": "ABC_425148e63eb84bba50326e362cc5b8_9", "text": "Extended Jaccard measures have been used in LDA stability task optimization [1, 12] ."}
{"sent_id": "425148e63eb84bba50326e362cc5b8-C001-28", "intents": ["@UNSURE@"], "paper_id": "ABC_425148e63eb84bba50326e362cc5b8_9", "text": "Our method is not an alternative to the ones presented by Agrawal et al. [1] but additive."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "At the core of the Transformer is its attention mechanism, which is proposed to integrate the dependencies between the inputs."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "In all cases, the Transformer's attentions follow the same general mechanism."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Note that the kernel form k(x q , x k ) in the original Transformer (Vaswani et al., 2017 ) is a asymmetric exponential kernel with additional mapping W q and W k (Wilson et al., 2016; Li et al., 2017) 2 ."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-120", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "In the following, we itemize the corresponding designs for the variants in Transformers: (i) Encoder Self-Attention in original Transformer (Vaswani et al., 2017) : For each query x q in the encoded sequence, M (x q , S x k ) = S x k contains the keys being all the tokens in the encoded sequence."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-129", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "(v) Decoder Self-Attention in Sparse Transformer (Child et al., 2019) : For each query x q in the decoded sentence, M (x q , S x k ) returns a subset of S 1 (M (x q , S x k ) ⊂ S 1 )."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-244", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "In addition to the fundamental difference between graph-structured learning and kernel learning, the prior work (Wang et al., 2018; Shaw et al., 2018; Tsai et al., 2019b) focused on presenting Transformer for its particular application (e.g., video classification (Wang et al., 2018) and neural machine translation (Shaw et al., 2018) )."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-7", "intents": ["@MOT@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer's attention."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-23", "intents": ["@MOT@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "The above observation inspires us to connect Transformer's attention to kernel learning (Scholkopf and Smola, 2001) : they both concurrently and order-agnostically process all inputs by calculating the similarity between the inputs."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-8", "intents": ["@EXT@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "As an example, we propose a new variant of Transformer's attention which models the input as a product of symmetric kernels."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-33", "intents": ["@UNSURE@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "This section aims at providing an understanding of attention in Transformer via the lens of kernel."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-39", "intents": ["@UNSURE@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "To relate the notations in sequence to sequence learning (Vaswani et al., 2017 ), x represents a specific element of a sequence, x = [x 1 , x 2 , ⋯, x T ] denotes a sequence of features, S x = {x exp , x 2 , ⋯, x T } represents the set with its elements being the features in sequence x, and we refer the space of set S x as S."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-90", "intents": ["@UNSURE@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "For the rest of the paper, we will focus on the setting for sequence Transformer X = (F × T ) and discuss the kernel construction on it."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-135", "intents": ["@UNSURE@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "So far, we see how Eq. (2) connects to the variants of Transformers."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-146", "intents": ["@UNSURE@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "By viewing the attention mechanism with Eq. (2), we aims at answering the following questions regarding the Transformer's designs:"}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-160", "intents": ["@UNSURE@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "We fix the position-wise operations in Transformer 3 and only change the attention mechanism."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-184", "intents": ["@UNSURE@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "We conclude that the choice of kernel matters for the design of attention in Transformer."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-250", "intents": ["@UNSURE@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Via the lens of the kernel, we were able to better understand the role of individual components in Transformer's attention and categorize previous attention variants in a unified formulation."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-218", "intents": ["@DIF@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Nonetheless, the performance is slightly better than considering PE from the original Transformer (Vaswani et al., 2017) ."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-29", "intents": ["@BACK@", "@UNSURE@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "We also compared with a probabilistic model in (Ninomiya et al., 2006) ."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-56", "intents": ["@UNSURE@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "In the experiments, we compared our model with other two types of probabilistic models using a supertagger (Ninomiya et al., 2006) ."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-64", "intents": ["@UNSURE@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "The probabilities are given as the product of Ninomiya et al. (2006) 's model 1 and the probabilistic HPSG."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-148", "intents": ["@UNSURE@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "This advantage of Ninomiya et al. (2006)'s model can apparently be implemented in our model, but this requires many parts of rewriting of the implemented parser."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-151", "intents": ["@UNSURE@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "In Figure 3 , the line of our model crosses the line of Ninomiya et al. (2006) 's model."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-159", "intents": ["@UNSURE@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution and the probabilistic HPSG with supertagging (Ninomiya et al., 2006 )."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-73", "intents": ["@USE@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "In our model, Ninomiya et al. (2006) 's model 1 is used as a reference distribution."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-161", "intents": ["@DIF@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "Our model had around 2.65 points higher F-score than 's model and around 0.56 points higher F-score than the Ninomiya et al. (2006) 's model 3."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005) , (Zhang et al., 2006a) ."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-112", "intents": ["@BACK@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "When both results of CT and DS are available, the CM can be calculated according to the following formula in (Zhang et al., 2006a) :"}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-38", "intents": ["@MOT@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "Furthermore, our error analysis on the results of combination reveals that confidence measure in (Zhang et al., 2006a) has a representation flaw and we propose an EIV tag method to revise it."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-105", "intents": ["@MOT@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "We show in this section that there is a representation flaw in the formula of confidence measure in (Zhang et al., 2006a )."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-41", "intents": ["@UNSURE@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "In Section 2, we give a brief introduction to Zhang\"s DS method and subword-based tagging, which is a special CT method. And by comparing the results of this special CT method and DS according our detailed metric, we show that CT performs better on both IV and OOV."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-79", "intents": ["@UNSURE@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "Thus, we still call subwordbased tagging as a special CT method and in the reminder of this paper \"CT\" means subwordbased tagging in Zhang\"s paper and \"Pure CT\" means CT without subword."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-82", "intents": ["@UNSURE@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "In this paper, data provided by Bakeoff 2005 is used in our experiments in order to compare with the published results in (Zhang et al., 2006a) ."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-149", "intents": ["@UNSURE@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "Thus, there may be some similar strategies in Zhang\"s CM too but not presented in Zhang\"s paper."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-122", "intents": ["@DIF@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "In this paper,  = 0.8 and t = 0.7 (Parameters in two papers, Zhang et al. 2006a and Zhang et al. 2006b , are different. And our parameters are consistent with Zhang et al. 2006b which is confirmed by Dr Zhang through email) are used in CM, namely MP= 0.875 is the threshold."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-117", "intents": ["@SIM@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "In the experiment of this paper, MP is used as CM because it is equivalent to Zhang\"s CM but more convenient to express."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "Then, ABS outputs a summaryŶ given an input sentence X as follows:"}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-110", "intents": ["@BACK@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "ABS (re-run) represents the performance of ABS re-trained by the distributed scripts 7 ."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-18", "intents": ["@UNSURE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "We expect that the quality of headlines will improve with this reasonable combination (ABS and AMR)."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-90", "intents": ["@UNSURE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "Gigaword test data used Gigaword in (Rush et al., 2015) Our sampled test data (Rush et al., 2015)"}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-100", "intents": ["@UNSURE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "Additionally, we also evaluated the performance of the AMR encoder without the attention mechanism, which we refer to as \"ABS+AMR(w/o attn)\", to investigate the contribution of the attention mechanism on the AMR encoder."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-130", "intents": ["@UNSURE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "We can also observe that ABS+AMR achieved the best ROUGE-1 scores on all of the test data."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-131", "intents": ["@UNSURE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "According to this fact, ABS+AMR tends to successfully yield semantically important words."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-134", "intents": ["@UNSURE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "For example, ABS+AMR successfully added the correct modifier 'saudi' to \"crown prince\" in the first example."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-135", "intents": ["@UNSURE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "Moreover, ABS+AMR generated a consistent subject in the third example."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-136", "intents": ["@UNSURE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "The comparison between ABS+AMR(w/o attn) and ABS+AMR (with attention) suggests that the attention mechanism is necessary for AMR encoding."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-92", "intents": ["@USE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "The training data was obtained from the first sentence and the headline of a document in the annotated Gigaword corpus (Napoles et al., 2012) 4 ."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-113", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "However in contrast, we observed that the improvements on Gigaword (the same test data as Rush et al. (2015) ) seem to be limited compared with the DUC-2004 dataset."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-127", "intents": ["@USE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "To provide evidence of this assumption, we also evaluated the performance on our randomly selected 2,000 sentence-headline test data also taken from the test data section of the annotated Gigaword corpus."}
{"sent_id": "0a93feafef3ba2d4bb5360ff215171-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_0a93feafef3ba2d4bb5360ff215171_9", "text": "Several metrics have been proposed recently for evaluating VQA systems (see section 2), but accuracy is still the most commonly used evaluation criterion [4, 11, 23, 42, 44, 1, 5, 14, 45, 2] ."}
{"sent_id": "0a93feafef3ba2d4bb5360ff215171-C001-126", "intents": ["@BACK@"], "paper_id": "ABC_0a93feafef3ba2d4bb5360ff215171_9", "text": "This is crucial since, as shown in Figure 3 , in current datasets the proportion of samples with a perfect inter-annotator agreement (i.e., 1 unique answer) is relatively low: 35% in VQA 1.0 [4] , 33% in VQA 2.0 [14] , 43% in VQA-abstract [1] , and only 3% in VizWiz [16] ."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-95", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "On the convolutional side, Liu et al. (2017) generated the segment representation by combining the outputs of three parallel CNNs with different context window sizes, in order to capture different functional patterns."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-109", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "In this sense, similarly to our previous study using SVMs (Ribeiro et al., 2015) , Liu et al. (2017) concluded that providing that information in the form of the classification of the surrounding segments leads to better results than using their words, even when those classifications are obtained automatically."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-255", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Liu et al. (2017) also used Word2Vec embeddings, but trained on Facebook data."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-314", "intents": ["@SIM@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Starting with the dimensionality of the embedding space, in Table 3 we can see that using an embedding space with 200 dimensions, such as in the study by Liu et al. (2017) , leads to better results than any of the dimensionality values used by Khanpour et al. (2016) ."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-489", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "In the case of the study by Liu et al. (2017) , direct result comparison with those reported is not possible since they were obtained on different sets."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-395", "intents": ["@MOT@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Although both our previous study and that by Liu et al. (2017) used the classifications of preceding segments as context information, none of them took into account that those segments have a sequential nature and simply flattened the sequence before appending it to the segment representation."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-279", "intents": ["@UNSURE@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "To do so, we used the approach suggested by Kim (2014) in the paper that inspired the CNN approach used by Liu et al. (2017) ."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-470", "intents": ["@USE@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Starting with the typically used word-level, we have shown that using an embedding space with 200 dimensions as used by Liu et al. (2017) in their study leads to better results than any of the dimensionality values used by Khanpour et al. (2016) ."}
{"sent_id": "e803782890224294066ce447671981-C001-3", "intents": ["@EXT@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "The patternmatching approach proposed by Johnson (2002) for a similar task for phrase structure trees is extended with machine learning techniques."}
{"sent_id": "e803782890224294066ce447671981-C001-5", "intents": ["@DIF@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in (Johnson, 2002) ."}
{"sent_id": "e803782890224294066ce447671981-C001-104", "intents": ["@DIF@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "The results presented in the previous section show that it is possible to improve over the simple pattern matching algorithm of (Johnson, 2002) , using dependency rather than phrase structure information, more skeletal patterns, as was suggested by Johnson, and a set of features associated with instances of patterns."}
{"sent_id": "e803782890224294066ce447671981-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "In (Clark et al., 2002) long-range dependencies are included in parser's probabilistic model, while Johnson (2002) presents a method for recovering non-local dependencies after parsing has been performed."}
{"sent_id": "e803782890224294066ce447671981-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "From a training corpus with annotated empty nodes Johnson's algorithm first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus \"licensing\" corresponding non-local dependencies."}
{"sent_id": "e803782890224294066ce447671981-C001-14", "intents": ["@MOT@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "In (Johnson, 2002 ) the author notes that the biggest weakness of the algorithm seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem."}
{"sent_id": "e803782890224294066ce447671981-C001-51", "intents": ["@SIM@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "The definition of a structure matching a pattern, and the algorithms for pattern matching and pattern extraction from a corpus are straightforward and similar to those described in (Johnson, 2002) ."}
{"sent_id": "e803782890224294066ce447671981-C001-96", "intents": ["@UNSURE@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "These last columns give the results that can be compared to Johnson's results for section 23 (Table 4) Table 3 : Overall performance of our algorithm."}
{"sent_id": "e803782890224294066ce447671981-C001-100", "intents": ["@UNSURE@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "It is difficult to make a strict comparison of our results and those in (Johnson, 2002) ."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "Works addressing these datasets [28, 33, 39] show that they are indeed challenging in terms of visual recognition and automatic description."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-9", "intents": ["@UNSURE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "We explore different design choices to build and train the LSTM and achieve the best performance to date on the challenging MPII-MD dataset."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-111", "intents": ["@UNSURE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "Then, we compare our best system to prior work."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-122", "intents": ["@UNSURE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "The authors of CIDEr [32] showed that METEOR also outperforms CIDEr when the number of references is small and in the case of MPII-MD we have typically only a single reference."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-194", "intents": ["@UNSURE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "In this section we want to take a closer look at three methods, best SMT of [28] , S2VT [33] and ours, in order to understand where these methods succeed and where they fail."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-198", "intents": ["@UNSURE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "SMT [28] Someone is a man, someone is a man."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-202", "intents": ["@UNSURE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "SMT [28] The car is a water of the water."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-207", "intents": ["@UNSURE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "SMT [28] Someone is down the door, someone is a back of the door, and someone is a door."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-269", "intents": ["@UNSURE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "We analyze the challenges in the movie description task using our and two prior works."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-176", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "Exploring different strategies to label selection and classifier training, as well as various LSTM configurations allows to obtain best result to date on the MPII-MD dataset."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-178", "intents": ["@DIF@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "We outperform both prior works in terms of Correctness and Relevance, however we lose to S2VT in terms of Grammar."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-167", "intents": ["@USE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "In the following section we evaluate our best ensemble (last line of Table 4 ) on the test set of MPII-MD."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-193", "intents": ["@USE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "Despite the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets (MPII-MD [28] and M-VAD [31] ) remains relatively low."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-245", "intents": ["@SIM@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "The most frequent verbs in this topic are \"look up\" and \"nod\", which are also frequent in the dataset and in the sentences produced by [28] ."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-139", "intents": ["@USE@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "We thus performed manual analysis, following the procedure in Moryossef et al. (2019) ."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-156", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "We adopt the planning-based neural generation framework of Moryossef et al. (2019) and extend it to be orders of magnitude faster and produce more correct and more fluent text."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "In Moryossef et al. (2019) , the sentence plan trees were linearized into strings that were then fed to a neural machine translation decoder (Open-NMT) (Klein et al., 2017) with a copy mecha-nism."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-16", "intents": ["@UNSURE@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "See Moryossef et al. (2019) for further details."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-85", "intents": ["@UNSURE@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "In the remaining cases, we set the system to continue searching by trying other plans, by going down the list of plans (when using the exhaustive planner of Moryossef et al. (2019) ) or by sampling a new plan (when using the linear time planner suggested in this paper)."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-123", "intents": ["@UNSURE@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "We evaluate on the WebNLG dataset (Colin et al., 2016) , comparing to the step-bystep systems described in Moryossef et al. (2019) , which are state of the art."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-127", "intents": ["@UNSURE@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "We compare the exhaustive planner from Moryossef et al. (2019) to our neural planner, by replacing the planner component in the Moryossef et al. (2019) system."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-141", "intents": ["@UNSURE@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "5 We compare to the StrongNeural and BestPlan systems from Moryossef et al. (2019) ."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-172", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "Note that we only develop our work on the open-ended case in VQA dataset because it is the most popular task and we also think the open-ended task is closer to the real situation than multiple-choice one."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-100", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "We take the most popular VQA dataset (Antol et al. 2015) to develop our BQD."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-102", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "Then, we take all of the testing questions from the VQA dataset (Antol et al. 2015) to be our main question candidates."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-147", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "For the image part, there is already a rapidly growing research on evaluating the robustness of deep learning models (Fawzi, Moosavi Dezfooli, and Frossard 2017;  Table 3 : MUTAN without Attention model evaluation results on BQD and VQA dataset (Antol et al. 2015) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and VQA dataset and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and VQA dataset."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-184", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "The accuracy is given by the following: Table 5 : MLB with Attention model evaluation results on BQD and VQA dataset (Antol et al. 2015) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and VQA dataset and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and VQA dataset."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-239", "intents": ["@UNSURE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "Accordingly to the Table 9 , 96.84% of testing questions from VQA dataset cannot find the proper basic questions to help the accuracy by naive concatenation method."}
{"sent_id": "40d73d5fc22686c13a14946946dd18-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_40d73d5fc22686c13a14946946dd18_9", "text": "More recently, Tan et al. (2018) replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "Prabhumoye et al. (2018) propose to transfer style through backtranslation."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "They first transfer a sentence to one pivot language and use the encoding of the sentence in the pivot language to train the generative models corresponding to the two styles."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-26", "intents": ["@UNSURE@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "We compare the results with the cross-aligned auto-encoder (Shen et al., 2017) and the back-translation model with one pivot language (Prabhumoye et al., 2018) ."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-83", "intents": ["@UNSURE@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "We also compare our results with the back-translation model using only one pivot language and with no feedback loss (BST model)."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-30", "intents": ["@MOT@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "While the previous work (Prabhumoye et al., 2018) focuses on creating a representation by translating to a pivot language, preserving meaning in the generated sentences is still an unsolved question."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-50", "intents": ["@USE@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "We use these translation systems for training the style specific decoders following the procedure in (Prabhumoye et al., 2018) ."}
{"sent_id": "b71da01fb46900d81162b3a3c3cd41-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_b71da01fb46900d81162b3a3c3cd41_9", "text": "There may be more than one such permutation, but Nivre (2009) defines the canonical projective order < G for x given G as the order given by an inorder traversal of G that respects the order < between a node and its direct dependents."}
{"sent_id": "b71da01fb46900d81162b3a3c3cd41-C001-14", "intents": ["@DIF@"], "paper_id": "ABC_b71da01fb46900d81162b3a3c3cd41_9", "text": "In this paper, we show that the oracle used for training by Nivre (2009) is suboptimal because it eagerly swaps words as early as possible and therefore makes a large number of unnecessary transitions, which potentially affects both efficiency and accuracy."}
{"sent_id": "b71da01fb46900d81162b3a3c3cd41-C001-85", "intents": ["@UNSURE@"], "paper_id": "ABC_b71da01fb46900d81162b3a3c3cd41_9", "text": "We have presented a new training oracle for the transition system originally presented in Nivre (2009) ."}
{"sent_id": "59d9c7640e69fd0e19b12b6dbc392c-C001-16", "intents": ["@UNSURE@"], "paper_id": "ABC_59d9c7640e69fd0e19b12b6dbc392c_9", "text": "We will also report on the state of the art in automatic frame semantic role labeling for English (Swayamdipta et al., 2017) and for other languages."}
{"sent_id": "59d9c7640e69fd0e19b12b6dbc392c-C001-48", "intents": ["@UNSURE@"], "paper_id": "ABC_59d9c7640e69fd0e19b12b6dbc392c_9", "text": "She works with Noah Smith and Chris Dyer on developing efficient algorithms for broad-coverage semantic parsing, with a focus on exploiting the relationship between syntax and semantics (Swayamdipta et al., 2017) ."}
{"sent_id": "7b4a976ba6a43b5ba42cc350b4d132-C001-22", "intents": ["@USE@"], "paper_id": "ABC_7b4a976ba6a43b5ba42cc350b4d132_9", "text": "We propose here to extend the LRP application to a linguistically motivated network architecture, known as Kernel-Based Deep Architecture (KDA) [5] , which frames semantic information captured by linguistic Tree Kernel [2] methods within the neural-based learning paradigm."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "To address these problems, preordering (Xia and McCord, 2004; Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012; Nakagawa, 2015) and postordering (Goto et al., 2012 (Goto et al., , 2013 Hayashi et al., 2013 ) models have been proposed."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-35", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "The label is determined based on Kendall's τ (Kendall, 1938) as in (Nakagawa, 2015) , which is calculated by Equation (1)."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-72", "intents": ["@UNSURE@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "We compared our model with the state-of-theart preordering method proposed in (Nakagawa, 2015) , which is hereafter referred to as BTG."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-113", "intents": ["@UNSURE@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "Figure 3 shows the distribution of Kendall's τ in the original training data as well as the distributions after preordering by RvNN and BTG."}
{"sent_id": "9b203bfa690c4a79c1324360a4b8dc-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_9b203bfa690c4a79c1324360a4b8dc_11", "text": "Santus et al. (2016) proposed a supervised method based on a Random Forest algorithm to learn taxonomical semantic relations and they have shown that the model performs well for co-hyponymy detection."}
{"sent_id": "9b203bfa690c4a79c1324360a4b8dc-C001-82", "intents": ["@USE@"], "paper_id": "ABC_9b203bfa690c4a79c1324360a4b8dc_11", "text": "3.2. Experiment-2 (Santus et al., 2016) In the second experiment, we use ROOT9 dataset prepared by Santus et al. (2016) , containing 9,600 labeled pairs extracted from three datasets: EVALution (Santus et al., 2015) , Lenci/Benotto (?) and BLESS (Baroni and Lenci, 2011) ."}
{"sent_id": "3128481fa4e5d2c4af7deba2c28950-C001-137", "intents": ["@USE@"], "paper_id": "ABC_3128481fa4e5d2c4af7deba2c28950_11", "text": "The JTextTile software was used to implement Hearst (1994) on dialogue."}
{"sent_id": "3128481fa4e5d2c4af7deba2c28950-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_3128481fa4e5d2c4af7deba2c28950_11", "text": "Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (Hearst, 1994) , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size."}
{"sent_id": "1c1b524d2bfe00c62a5a2e1a05ffc7-C001-19", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_1c1b524d2bfe00c62a5a2e1a05ffc7_11", "text": "In Vaswani et al. (2017) , the authors introduce the Transformer network, a novel architecture that avoids the recurrence equation and maps the input sequences into hidden states solely using attention."}
{"sent_id": "1c1b524d2bfe00c62a5a2e1a05ffc7-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_1c1b524d2bfe00c62a5a2e1a05ffc7_11", "text": "Vaswani et al. (2017) proportionally reduce d k = d v = d model so that the computational load of the multi-head attention is the same as simple self-attention."}
{"sent_id": "1c1b524d2bfe00c62a5a2e1a05ffc7-C001-115", "intents": ["@SIM@"], "paper_id": "ABC_1c1b524d2bfe00c62a5a2e1a05ffc7_11", "text": "As in Vaswani et al. (2017) , we used the Adam optimizer (Kingma & Ba, 2014) with (β 1 , β 2 ) = (0.9, 0.98) and = 10 −9 ."}
{"sent_id": "79e96060492c3978dc5a7a0d5f293f-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_79e96060492c3978dc5a7a0d5f293f_11", "text": "Li and Zong (NLP-KE 2008) explore a classifier combination technique they call \"MultipleLabel Consensus Training\" which results in better accuracy than non-adapted models on the data sets used in Blitzer et al. (2007) ."}
{"sent_id": "79e96060492c3978dc5a7a0d5f293f-C001-127", "intents": ["@USE@"], "paper_id": "ABC_79e96060492c3978dc5a7a0d5f293f_11", "text": "In these experiments, we compare the results of our all-in-one classifier and the ensemble classifier trained and tested on the four datasets to the results of SCL and its variation SCL-MI domain adaptation as reported by Blitzer et al. (2007) baseline and ceiling in-domain classifiers for the four domains."}
{"sent_id": "79e96060492c3978dc5a7a0d5f293f-C001-182", "intents": ["@USE@"], "paper_id": "ABC_79e96060492c3978dc5a7a0d5f293f_11", "text": "They both are very close in some domains like When comparing the all-in-one and the ensemble approaches on the four datasets in Blitzer et al. (2007) , the all-in-one exceeds the ensemble only in the DVD domain."}
{"sent_id": "79e96060492c3978dc5a7a0d5f293f-C001-180", "intents": ["@DIF@"], "paper_id": "ABC_79e96060492c3978dc5a7a0d5f293f_11", "text": "The results in the previous section indicate that both the all-in-one and the ensemble approaches exceed both Daumé's domain adaptation technique on the 27 datasets (given our current implementation of Daumé's approach) and SCL on the four datasets in Blitzer et al. (2007) and that the all-in-one approach achieves comparable results in terms of transfer ratio to Glorot et al. (2011) ."}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-42", "intents": ["@EXT@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "Our work builds upon the work by Kiros et al. (2014) , in which the authors use a rank loss to optimize the embedding."}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-61", "intents": ["@USE@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "We define the similarity measure s(i, c) in the joint embedding space following Kiros et al. (2014) ."}
{"sent_id": "4a90cd18be0df0c41a94febe2f68ef-C001-124", "intents": ["@USE@"], "paper_id": "ABC_4a90cd18be0df0c41a94febe2f68ef_11", "text": "Since DROP does not indicate the answer type but only provides the answer string, we therefore adopt the weakly supervised annotation scheme, as suggested in Berant et al. (2013); Dua et al. (2019) ."}
{"sent_id": "4a90cd18be0df0c41a94febe2f68ef-C001-22", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_4a90cd18be0df0c41a94febe2f68ef_11", "text": "Third, to support numerical reasoning, prior work (Dua et al., 2019) learns to predict signed numbers for obtaining an arithmetic expression that can be executed by a symbolic system."}
{"sent_id": "4a90cd18be0df0c41a94febe2f68ef-C001-84", "intents": ["@SIM@"], "paper_id": "ABC_4a90cd18be0df0c41a94febe2f68ef_11", "text": "Arithmetic expression In order to model the process of performing addition or subtraction among multiple numbers mentioned in the passage, we assign a three-way categorical variable (plus, minus, or zero) for each number to indicate its sign, similar to Dua et al. (2019) ."}
{"sent_id": "fa3d20d5975ec59454abfca68f8935-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_fa3d20d5975ec59454abfca68f8935_11", "text": "Nonetheless remarkable progress have been achieved with the use of seq2seq models (Gehrmann et al., 2018; See et al., 2017; Chopra et al., 2016; Rush et al., 2015) and a reward instead of loss function via deep-reinforcement learning (Chen and Bansal, 2018; Paulus et al., 2017; Ranzato et al., 2015) ."}
{"sent_id": "fa3d20d5975ec59454abfca68f8935-C001-152", "intents": ["@USE@"], "paper_id": "ABC_fa3d20d5975ec59454abfca68f8935_11", "text": "Following previous works (See et al., 2017; Nallapati et al., 2017; Chen and Bansal, 2018) , we evaluate both datasets on standard ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004) ."}
{"sent_id": "fa3d20d5975ec59454abfca68f8935-C001-86", "intents": ["@DIF@"], "paper_id": "ABC_fa3d20d5975ec59454abfca68f8935_11", "text": "for each t th sentence in the reference summary, R j , per i th sentence in document D j , in contrast to Chen and Bansal (2018) 's that uses ROUGE-L recall score."}
{"sent_id": "81bdddc7d6b04c88407537f57c0580-C001-23", "intents": ["@USE@"], "paper_id": "ABC_81bdddc7d6b04c88407537f57c0580_11", "text": "We conduct evaluation using the same dataset and measure as in previous work (Yang et al., 2015; Jurczyk et al., 2016) , and our framework improves the F 1 score by 6.6% (from 36.65% to 43.27%), compared with the state of the art."}
{"sent_id": "81bdddc7d6b04c88407537f57c0580-C001-97", "intents": ["@USE@"], "paper_id": "ABC_81bdddc7d6b04c88407537f57c0580_11", "text": "Since the code from (Yang et al., 2015) is available, we use it (rather than (Jurczyk et al., 2016) ) to assist our analysis."}
{"sent_id": "81bdddc7d6b04c88407537f57c0580-C001-93", "intents": ["@DIF@"], "paper_id": "ABC_81bdddc7d6b04c88407537f57c0580_11", "text": "Different from Yang et al. (2015)'s results, combining with the QLen feature does not further improve the performance in our case, possibly because we choose Bi-RNN as our encoder, which may capture some question characteristics better than a length feature."}
{"sent_id": "69857bcd5ba67cb7ca0b4344a3a85f-C001-15", "intents": ["@MOT@"], "paper_id": "ABC_69857bcd5ba67cb7ca0b4344a3a85f_11", "text": "A frequently used testbed for coherence models is the discourse ordering problem, which occurs often in text generation, complex question answering, and multi-document summarization: given discourse units, what is the most coherent ordering of them (Marcu, 1996; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005) ? Because the problem is NP-complete (Althaus et al., 2005) , it is critical how coherence model evaluation is intertwined with search: if the search for the best ordering is greedy and has many errors, one is not able to properly evaluate whether a model is better than another."}
{"sent_id": "69857bcd5ba67cb7ca0b4344a3a85f-C001-125", "intents": ["@USE@"], "paper_id": "ABC_69857bcd5ba67cb7ca0b4344a3a85f_11", "text": "Lapata (2003) 0.48 0.07 Barzilay & Lee (2004) 0.81 0.44 Barzilay & Lee (reproduced) 0.39 0.36 Barzilay & Lapata (2005) 0 the original sentence order (OSO)."}
{"sent_id": "69857bcd5ba67cb7ca0b4344a3a85f-C001-157", "intents": ["@DIF@"], "paper_id": "ABC_69857bcd5ba67cb7ca0b4344a3a85f_11", "text": "On the other hand, we achieve the highest accuracy figure (0.50) on the ACCIDENTS corpus, outperforming the previous-highest figure (0.44) of Barzilay and Lee (2004) ."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-34", "intents": ["@USE@", "@UNSURE@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "Bohnet and Nivre (2012) parser: transitionbased dependency parser with joint tagger that implements global learning and beam search."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-118", "intents": ["@USE@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "The Bohnet and Nivre (2012) parser succeeds in finding the correct conjucts (shown in bold font) on DT and makes mistakes on SB and CD in some difficult cases like the following ones: a) "}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-97", "intents": ["@DIF@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "The Bohnet and Nivre (2012) parser predicts supertags with an average accuracy of 89.73% which is significantly lower than state-ofthe-art 95% (Ytrestøl, 2011) ."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "This is explained by the fact that the Bohnet and Nivre (2012) parser implements a novel approach to parsing: beam-search algorithm with global structure learning."}
{"sent_id": "547551e556d8aa919f731da99424c9-C001-165", "intents": ["@USE@"], "paper_id": "ABC_547551e556d8aa919f731da99424c9_11", "text": "The first 6 rows are taken from , the next 4 are from Tomar et al. (2017) , the next 5 from Shen et al. (2017) and The last 4 rows are our experiments using Infersent (Conneau et al., 2017) and our models."}
{"sent_id": "547551e556d8aa919f731da99424c9-C001-120", "intents": ["@SIM@"], "paper_id": "ABC_547551e556d8aa919f731da99424c9_11", "text": "We follow a similar evaluation protocol to those presented in ; Hill et al. (2016) ; Conneau et al. (2017) which is to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters of our sentence representation model."}
{"sent_id": "547551e556d8aa919f731da99424c9-C001-240", "intents": ["@EXT@"], "paper_id": "ABC_547551e556d8aa919f731da99424c9_11", "text": "In addition to the above tasks which were considered by Conneau et al. (2017) , we also evaluate on the recently published Quora duplicate question dataset 6 since it is an order of magnitude larger than the others (approximately 400,000 question pairs)."}
{"sent_id": "28038a4fa4182ccdc6134f2138c0da-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_28038a4fa4182ccdc6134f2138c0da_11", "text": "A major intended application of definition modeling is the explication and evaluation of distributed lexical representations, also known as word embeddings (Noraset et al., 2017) ."}
{"sent_id": "28038a4fa4182ccdc6134f2138c0da-C001-133", "intents": ["@USE@"], "paper_id": "ABC_28038a4fa4182ccdc6134f2138c0da_11", "text": "4 The dropout rate and warmup steps number were set using a hyperparameter search on the dataset from Noraset et al. (2017) , during which encoder and decoder vocabulary were merged for computational simplicity and models stopped after 12,000 steps."}
{"sent_id": "fa7475b6025d010dd6814dfb3905ef-C001-72", "intents": ["@BACK@"], "paper_id": "ABC_fa7475b6025d010dd6814dfb3905ef_11", "text": "Patient Knowledge Distillation (PKD) [33] designs multiple distillation losses between the module hidden states of the teacher and student models."}
{"sent_id": "fa7475b6025d010dd6814dfb3905ef-C001-167", "intents": ["@USE@"], "paper_id": "ABC_fa7475b6025d010dd6814dfb3905ef_11", "text": "We set up a baseline of vanilla Knowledge Distillation [14] as in [33] ."}
{"sent_id": "fa7475b6025d010dd6814dfb3905ef-C001-169", "intents": ["@USE@"], "paper_id": "ABC_fa7475b6025d010dd6814dfb3905ef_11", "text": "Under the setting of compressing 12-layer BERT-base to a 6-layer compact model, we choose BERT-PKD [33] , PD-BERT [37] , and DistillBERT [28] as strong baselines."}
{"sent_id": "fa7475b6025d010dd6814dfb3905ef-C001-187", "intents": ["@DIF@"], "paper_id": "ABC_fa7475b6025d010dd6814dfb3905ef_11", "text": "Also, our model obviously outperforms the vanilla KD [14] and Patient Knowledge Distillation (PKD) [33] , showing its supremacy over the KD-based compression approaches."}
{"sent_id": "d51bf6d22d21dcd91e080f6f0b5dcb-C001-176", "intents": ["@USE@"], "paper_id": "ABC_d51bf6d22d21dcd91e080f6f0b5dcb_11", "text": "We apply this method in our low resource setting experiments (Sec. 5.3) to compare our bipartite and tripartite results to Razmara et al. (2013) ."}
{"sent_id": "d51bf6d22d21dcd91e080f6f0b5dcb-C001-53", "intents": ["@SIM@"], "paper_id": "ABC_d51bf6d22d21dcd91e080f6f0b5dcb_11", "text": "A comparison of different association measures appears in (Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014) and our preliminary experiments validated the choice of the same association measure as in these papers, namely Pointwise Mutual Information (Lin, 1998) (PMI) ."}
{"sent_id": "b124e65938672691a5589fb5cdb21e-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_b124e65938672691a5589fb5cdb21e_11", "text": "There are no tweet instances annotated with the class Neither in the data set (Küçük, 2017) ."}
{"sent_id": "b124e65938672691a5589fb5cdb21e-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_b124e65938672691a5589fb5cdb21e_11", "text": "The corresponding results have indicated that using unigrams as features leads to favorable performance rates and using unigrams together with hashtag features improves these results further, while using bigrams as features of the SVM classifiers results in poor performance (Küçük, 2017) ."}
{"sent_id": "b124e65938672691a5589fb5cdb21e-C001-62", "intents": ["@USE@"], "paper_id": "ABC_b124e65938672691a5589fb5cdb21e_11", "text": "The favorable results corresponding to the former two settings are provided in Table 3 as excerpted from (Küçük, 2017) , in order to be used as reference results for comparison purposes."}
{"sent_id": "76476d80e1d3f65818592ec4caab0e-C001-79", "intents": ["@USE@"], "paper_id": "ABC_76476d80e1d3f65818592ec4caab0e_11", "text": "We evaluate RECODE with the Hearthstone (HS) (Ling et al., 2016) and Django (Oda et al., 2015) datasets, as preprocessed by Yin and Neubig (2017) ."}
{"sent_id": "76476d80e1d3f65818592ec4caab0e-C001-82", "intents": ["@USE@"], "paper_id": "ABC_76476d80e1d3f65818592ec4caab0e_11", "text": "For evaluation metrics, we use accuracy of exact match and the BLEU score following Yin and Neubig (2017) ."}
{"sent_id": "76476d80e1d3f65818592ec4caab0e-C001-118", "intents": ["@SIM@"], "paper_id": "ABC_76476d80e1d3f65818592ec4caab0e_11", "text": "The closest work to ours are Yin and Neubig (2017) and Rabinovich et al. (2017) which represent code as an AST."}
{"sent_id": "66392c3b6fa3744de79f056f615a75-C001-80", "intents": ["@EXT@"], "paper_id": "ABC_66392c3b6fa3744de79f056f615a75_11", "text": "We extend the approach of Moldovan et al. (2004) by adding similar words as features focusing on hypernyms, hyponyms and sister words of the modifier and head noun."}
{"sent_id": "66392c3b6fa3744de79f056f615a75-C001-156", "intents": ["@BACK@"], "paper_id": "ABC_66392c3b6fa3744de79f056f615a75_11", "text": "In the automatic interpretation of NCs, many claims have been made for the increase in performance, but these works make their own assumptions for interpretation (Barker and Szpakowicz, 1998; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; Seaghdha, 2007) ."}
{"sent_id": "74623c8d812e3c84e7bc6b46e982f5-C001-14", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_74623c8d812e3c84e7bc6b46e982f5_11", "text": "Over a decade of research has considered conversation disentanglement (Shen et al., 2006) , but using datasets that are either small (2,500 messages, Elsner and Charniak, 2008) or not released (Adams and Martell, 2008) ."}
{"sent_id": "74623c8d812e3c84e7bc6b46e982f5-C001-43", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_74623c8d812e3c84e7bc6b46e982f5_11", "text": "3 Related Work IRC Disentanglement Data: The most significant work on conversation disentanglement is a line of papers developing data and models for the #Linux IRC channel (Elsner and Charniak, 2008; Elsner and Schudy, 2009; Charniak, 2010, 2011) ."}
{"sent_id": "74623c8d812e3c84e7bc6b46e982f5-C001-127", "intents": ["@USE@"], "paper_id": "ABC_74623c8d812e3c84e7bc6b46e982f5_11", "text": "(2) One-to-One Overlap (1-1, Elsner and Charniak, 2008) ."}
{"sent_id": "74623c8d812e3c84e7bc6b46e982f5-C001-175", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_74623c8d812e3c84e7bc6b46e982f5_11", "text": "Decreasing the data size to match Elsner and Charniak (2008) 's training set leads to worse results, both if the sentences are from diverse contexts (3rd row), and if they are from just two contexts (bottom row)."}
{"sent_id": "74623c8d812e3c84e7bc6b46e982f5-C001-182", "intents": ["@USE@"], "paper_id": "ABC_74623c8d812e3c84e7bc6b46e982f5_11", "text": "We do not evaluate on graphs because Elsner and Charniak (2008) 's annotations do not include them."}
{"sent_id": "e0df566d073649431c3454a52813e9-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_e0df566d073649431c3454a52813e9_11", "text": "One of earliest and pioneer work on the disambiguation of discourse connectives, Pitler and Nenkova [20] , showed that four syntactic features (see Section 3.4 for details about the features) and the connective itself can disambiguate discourse connectives with an accuracy of 95.04% within the PDTB [22] ."}
{"sent_id": "e0df566d073649431c3454a52813e9-C001-85", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_e0df566d073649431c3454a52813e9_11", "text": "As mentioned in Section 2, Pitler and Nenkova [20] have shown that the context of discourse connectives in the syntactic tree is very discriminating for the disambiguation of English discourse connectives."}
{"sent_id": "eee36102d3feac0f673cd33562d40f-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_eee36102d3feac0f673cd33562d40f_11", "text": "To address the task of unknown named entity classification, Primadhanty et al. (2015) explored the use of sparse combinatorial features."}
{"sent_id": "eee36102d3feac0f673cd33562d40f-C001-60", "intents": ["@USE@"], "paper_id": "ABC_eee36102d3feac0f673cd33562d40f_11", "text": "We used a subset of features from experiments performed by Primadhanty et al. (2015) ."}
{"sent_id": "eee36102d3feac0f673cd33562d40f-C001-83", "intents": ["@DIF@"], "paper_id": "ABC_eee36102d3feac0f673cd33562d40f_11", "text": "The accuracy of LOC, however, was lower than that of the log-bilinear model (Primadhanty et al., 2015) ."}
{"sent_id": "950cc4a7fa2db3aa6786cc0ae802b5-C001-49", "intents": ["@USE@"], "paper_id": "ABC_950cc4a7fa2db3aa6786cc0ae802b5_12", "text": "In order to be able to compare our work with previous research, we use the Yahoo! Answers dataset that was first introduced by Jansen et al. (2014) and was later used by Sharp et al. (2015) and Fried et al. (2015) ."}
{"sent_id": "950cc4a7fa2db3aa6786cc0ae802b5-C001-77", "intents": ["@USE@"], "paper_id": "ABC_950cc4a7fa2db3aa6786cc0ae802b5_12", "text": "We only evaluate the 200-dimension DBOW model and its combinations with other models, comparing these to the baselines and the previous results on the same dataset (we use the same train/dev/test split as Jansen et al. (2014) )."}
{"sent_id": "d68bb5264d157cc4c2d9fa9c8f82b6-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_d68bb5264d157cc4c2d9fa9c8f82b6_12", "text": "The n-gram data show Heaps law in that, if N t is corpus size and v t is vocabulary size at time t, then v t ≈ N β t , with β ≈ 0.5, for all English words in the corpus [42] ."}
{"sent_id": "d68bb5264d157cc4c2d9fa9c8f82b6-C001-157", "intents": ["@BACK@"], "paper_id": "ABC_d68bb5264d157cc4c2d9fa9c8f82b6_12", "text": "The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41, 42, 1, 28] ."}
{"sent_id": "d68bb5264d157cc4c2d9fa9c8f82b6-C001-116", "intents": ["@SIM@"], "paper_id": "ABC_d68bb5264d157cc4c2d9fa9c8f82b6_12", "text": "Although it did not replicate exactly the particular 1-gram corpus we used here, the Heaps law exponent yielded by the PNM does fall within the range-from 0.44 to 0.54-observed in different English 1-gram corpora [42] ."}
{"sent_id": "392cbe849c1b8a69aae9923ade41aa-C001-77", "intents": ["@USE@"], "paper_id": "ABC_392cbe849c1b8a69aae9923ade41aa_12", "text": "In the next step, we train four different labelling models: the labeller of Zhang et al. (2017) that uses a rectifier neural network with two hidden layers (baseline), two bidirectional LSTM models (BILSTM(L) and BILSTM(B)), and one tree LSTM model (TREELSTM) ( §3)."}
{"sent_id": "392cbe849c1b8a69aae9923ade41aa-C001-85", "intents": ["@DIF@"], "paper_id": "ABC_392cbe849c1b8a69aae9923ade41aa_12", "text": "The scores for English are slightly lower since, in contrast to Zhang et al. (2017) , we do not use pre-trained embeddings."}
{"sent_id": "2c3a2999390b82f4e29b00d59f90f2-C001-95", "intents": ["@BACK@"], "paper_id": "ABC_2c3a2999390b82f4e29b00d59f90f2_12", "text": "Transformation-based learning (Florian et al., 2003) , Support Vector Machines (Mayfield et al., 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each."}
{"sent_id": "2c3a2999390b82f4e29b00d59f90f2-C001-118", "intents": ["@BACK@"], "paper_id": "ABC_2c3a2999390b82f4e29b00d59f90f2_12", "text": "One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al., 2003) ."}
{"sent_id": "e3ee86bbaca6ae00906e7ec64f0ac0-C001-23", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12", "text": "Zhiyuan Chen [2] ever proposed a approach to determine which domain dose a word have the sentiment orientation to achieve the goal of lifelong learning."}
{"sent_id": "e3ee86bbaca6ae00906e7ec64f0ac0-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12", "text": "Zhiyuan and Bing [2] improved the sentiment classification by involving knowledge."}
{"sent_id": "e3ee86bbaca6ae00906e7ec64f0ac0-C001-81", "intents": ["@BACK@"], "paper_id": "ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12", "text": "LSC [2] discussed a possible solution of P(w |c j )."}
{"sent_id": "e3ee86bbaca6ae00906e7ec64f0ac0-C001-90", "intents": ["@MOT@"], "paper_id": "ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12", "text": "Although LSC [2] considered the difference among domains, it still is a typical supervised learning approach."}
{"sent_id": "c705c0533600b9b93d2c89bcbc292b-C001-58", "intents": ["@USE@"], "paper_id": "ABC_c705c0533600b9b93d2c89bcbc292b_12", "text": "For SM, we use a reimplementation without the progress monitor, which was shown to be most important for search in inference (Ma et al., 2019) ."}
{"sent_id": "35233406ffd78d87743478454432d5-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_35233406ffd78d87743478454432d5_12", "text": "We discussed the benefit of CER over social network problem in (Xu et al., 2016 ) so we omit here but keep a performance comparison in Section 5."}
{"sent_id": "35233406ffd78d87743478454432d5-C001-82", "intents": ["@USE@"], "paper_id": "ABC_35233406ffd78d87743478454432d5_12", "text": "Then we briefly introduce the method for CER in (Xu et al., 2016) ."}
{"sent_id": "808e0a94b877182dc06447c8682a63-C001-9", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_808e0a94b877182dc06447c8682a63_12", "text": "Features from n-gram counts over resources like Web1T (Brants and Franz, 2006 ) have proven to be useful proxies for syntax (Bansal and Klein, 2011; Pitler, 2012) , but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences."}
{"sent_id": "da2429450c8d1f1f3e72383c86ec73-C001-4", "intents": ["@EXT@", "@USE@"], "paper_id": "ABC_da2429450c8d1f1f3e72383c86ec73_12", "text": "Our approach was to build up on the system of the last year's winning approach by NRC Canada 2013 (Mohammad et al., 2013) , with some modifications and additions of features, and additional sentiment lexicons."}
{"sent_id": "da2429450c8d1f1f3e72383c86ec73-C001-21", "intents": ["@EXT@"], "paper_id": "ABC_da2429450c8d1f1f3e72383c86ec73_12", "text": "Compared to the previous NRC Canada 2013 approach (Mohammad et al., 2013) , our main changes are the following three: First we use sparse linear classifiers instead of classical dense ones."}
{"sent_id": "da2429450c8d1f1f3e72383c86ec73-C001-132", "intents": ["@EXT@", "@USE@"], "paper_id": "ABC_da2429450c8d1f1f3e72383c86ec73_12", "text": "Our system is built up on the approach of NRC Canada (Mohammad et al., 2013) , with several modifications and extensions (e.g. sparse linear classifiers,"}
{"sent_id": "da2429450c8d1f1f3e72383c86ec73-C001-26", "intents": ["@USE@"], "paper_id": "ABC_da2429450c8d1f1f3e72383c86ec73_12", "text": "We tried to reproduce the same classifier as in (Mohammad et al., 2013) as a baseline for comparison."}
{"sent_id": "06db17253d76150772c0926e11131d-C001-27", "intents": ["@MOT@"], "paper_id": "ABC_06db17253d76150772c0926e11131d_12", "text": "However, it has been shown that they tend to exploit statistical regularities between answer occurrences and certain patterns in the question [24, 10, 25, 23, 13] ."}
{"sent_id": "06db17253d76150772c0926e11131d-C001-70", "intents": ["@MOT@"], "paper_id": "ABC_06db17253d76150772c0926e11131d_12", "text": "However, even with this additional balancing, statistical biases from the question remain and can be leveraged [10] ."}
{"sent_id": "06db17253d76150772c0926e11131d-C001-45", "intents": ["@USE@"], "paper_id": "ABC_06db17253d76150772c0926e11131d_12", "text": "We run extensive experiments on VQA-CP v2 [10] and demonstrate the ability of RUBi to surpass current state-of-the-art results from a significant margin."}
{"sent_id": "06db17253d76150772c0926e11131d-C001-168", "intents": ["@DIF@"], "paper_id": "ABC_06db17253d76150772c0926e11131d_12", "text": "This accuracy corresponds to a gain of +5.94 percentage points over the current state-of-the-art UpDn + Q-Adv + DoE. It also corresponds to a gain of +15.88 over GVQA [10] , which is a specific architecture designed for VQA-CP."}
{"sent_id": "7b9fc52e4479dc5ff9b8796a558981-C001-27", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_7b9fc52e4479dc5ff9b8796a558981_12", "text": "Artetxe et al. (2017) define the optimal mapping matrix W * with the following equation,"}
{"sent_id": "7b9fc52e4479dc5ff9b8796a558981-C001-76", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_7b9fc52e4479dc5ff9b8796a558981_12", "text": "We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictionary consisting only of numeral identity translations (such as 2-2, 3-3, et cetera) as in Artetxe et al. (2017) ."}
{"sent_id": "7b9fc52e4479dc5ff9b8796a558981-C001-89", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_7b9fc52e4479dc5ff9b8796a558981_12", "text": "Table 1 compares our methods against the system of Artetxe et al. (2017) , using scaling factors selected based on development data results."}
{"sent_id": "c34bbed419bddb6d63b3e3bccf595d-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_c34bbed419bddb6d63b3e3bccf595d_12", "text": "In recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status (IS) classification (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012) ."}
{"sent_id": "c34bbed419bddb6d63b3e3bccf595d-C001-20", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_c34bbed419bddb6d63b3e3bccf595d_12", "text": "In addition, making up between 5% and 20% of definite descriptions (Gardent and Manuélian, 2005; Caselli and Prodanof, 2006) and around 6% of all NPs (Markert et al., 2012) , bridging is still less frequent than many other IS classes and recognition of minority classes is well known to be more difficult."}
{"sent_id": "c34bbed419bddb6d63b3e3bccf595d-C001-53", "intents": ["@USE@"], "paper_id": "ABC_c34bbed419bddb6d63b3e3bccf595d_12", "text": "However, we observe that bridging anaphors are often licensed because of discourse structure Markert et al. (2012)"}
{"sent_id": "c34bbed419bddb6d63b3e3bccf595d-C001-108", "intents": ["@USE@"], "paper_id": "ABC_c34bbed419bddb6d63b3e3bccf595d_12", "text": "We perform experiments on the corpus provided in Markert et al. (2012) 6 ."}
{"sent_id": "bcf19914bb67ded47785d298969a7a-C001-30", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_bcf19914bb67ded47785d298969a7a_12", "text": "We combine the tasks and data from Shekhar et al. (2017b) and Shekhar et al. (2017a) ."}
{"sent_id": "710ec6f6d6d4c7c8c148833c0adfef-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_710ec6f6d6d4c7c8c148833c0adfef_12", "text": "They have been used for both rule-based (Taboada et al., 2011) and unsupervised (Turney, 2002; Hu and Liu, 2004; or supervised (Mohammad et al., 2013; Tang et al., 2014b; Vo and Zhang, 2015) machine-learning-based sentiment analysis."}
{"sent_id": "710ec6f6d6d4c7c8c148833c0adfef-C001-24", "intents": ["@DIF@"], "paper_id": "ABC_710ec6f6d6d4c7c8c148833c0adfef_12", "text": "The correlation between our method and the method of Mohammad et al. (2013) is analogous to the \"predicting\" vs \"counting\" correlation between distributional and distributed word representations (Baroni et al., 2014) ."}
{"sent_id": "710ec6f6d6d4c7c8c148833c0adfef-C001-117", "intents": ["@DIF@"], "paper_id": "ABC_710ec6f6d6d4c7c8c148833c0adfef_12", "text": "The polarity accuracy of our lexicon is 78.2%, in contrast to 76.9% by the lexicon of Mohammad et al. (2013) , demonstrating the relative strength of our method."}
{"sent_id": "710ec6f6d6d4c7c8c148833c0adfef-C001-112", "intents": ["@USE@"], "paper_id": "ABC_710ec6f6d6d4c7c8c148833c0adfef_12", "text": "Table 6 shows examples of our predicting-based lexicon and the counting-based lexicon of Mohammad et al. (2013) ."}
{"sent_id": "fc5de471ba4cc82a2156ed25d2c78b-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_fc5de471ba4cc82a2156ed25d2c78b_12", "text": "Previous experiments [5] showed that the encoder accounts for most of the benefits of transferring the parameters."}
{"sent_id": "fc5de471ba4cc82a2156ed25d2c78b-C001-77", "intents": ["@USE@"], "paper_id": "ABC_fc5de471ba4cc82a2156ed25d2c78b_12", "text": "Following the architecture and training procedure described in [5] , input speech features are fed into a stack of two CNN layers."}
{"sent_id": "24ee9b2bd8c97cbe923bc747b09806-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_24ee9b2bd8c97cbe923bc747b09806_12", "text": "While our model is not explicitly trained to recognise words or segment the speech signal, previous work has shown that such information can be extracted by visual grounding models [15, 28] ."}
{"sent_id": "26fbf9f4ae740513d8889160ad9f63-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_26fbf9f4ae740513d8889160ad9f63_12", "text": "In particular, (Otterbacher et al., 2002) experimentally showed that discourse relations can improve the coherence of multi-document summaries."}
{"sent_id": "26fbf9f4ae740513d8889160ad9f63-C001-50", "intents": ["@BACK@", "@SIM@", "@EXT@", "@DIF@"], "paper_id": "ABC_26fbf9f4ae740513d8889160ad9f63_12", "text": "The comparison, contingency, and illustration relations are also considered by most of the work in the field of discourse analysis such as the PDTB: Penn Discourse TreeBank research group (Prasad et al., 2008) and the RST Discourse Treebank research group (Carlson and Marcu, 2001 )."}
{"sent_id": "26fbf9f4ae740513d8889160ad9f63-C001-81", "intents": ["@BACK@"], "paper_id": "ABC_26fbf9f4ae740513d8889160ad9f63_12", "text": "A description and evaluation of these approaches can be found in (Mithun, 2012) ."}
{"sent_id": "26fbf9f4ae740513d8889160ad9f63-C001-80", "intents": ["@EXT@", "@USE@"], "paper_id": "ABC_26fbf9f4ae740513d8889160ad9f63_12", "text": "However, we have complemented this parser with three other approaches: (Jindal and Liu, 2006 )'s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (Fei et al., 2008) 's approach to identify topic-opinion relations; and we have proposed a new approach to tag attributive relations (Mithun, 2012) ."}
{"sent_id": "26fbf9f4ae740513d8889160ad9f63-C001-147", "intents": ["@USE@"], "paper_id": "ABC_26fbf9f4ae740513d8889160ad9f63_12", "text": "To ensure that the results were not specific to our summarizer, we performed the same experiments with two other systems: the MEAD summarizer (Radev et al., 2004) , a publicly available and a widely used summarizer, and with the output of the TAC best-scoring system."}
{"sent_id": "7d5c01ec5d744747413e42dcbc1a3c-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_7d5c01ec5d744747413e42dcbc1a3c_12", "text": "The unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling (e.g., Tomokiyo and Hurst (2003) ), graph-based ranking (e.g., Zha (2002) , Mihalcea and Tarau (2004) , Wan et al. (2007) , Wan and Xiao (2008) , Liu et al. (2009a) ), and clustering (e.g., Matsuo and Ishizuka (2004) , Liu et al. (2009b) )."}
{"sent_id": "7d5c01ec5d744747413e42dcbc1a3c-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_7d5c01ec5d744747413e42dcbc1a3c_12", "text": "A candidate phrase, typically a sequence of nouns and adjectives, is selected as a keyphrase if (1) it includes one or more of the top-ranked candidate words (Mihalcea and Tarau (2004) , Liu et al. (2009b) ), or (2) the sum of the ranking scores of its constituent words makes it a top scoring phrase (Wan and Xiao, 2008) ."}
{"sent_id": "7d5c01ec5d744747413e42dcbc1a3c-C001-188", "intents": ["@SIM@"], "paper_id": "ABC_7d5c01ec5d744747413e42dcbc1a3c_12", "text": "It is also worth mentioning that using our re-implementation of SingleRank, we are able to match the best scores reported by Mihalcea and Tarau (2004) on Inspec."}
{"sent_id": "40742bac72bbbaed4755ff0b74d599-C001-66", "intents": ["@USE@"], "paper_id": "ABC_40742bac72bbbaed4755ff0b74d599_12", "text": "2 Additionally, SGNS uses 5 negative samples (Mikolov et al., 2013b) , a window of size 10 ( Levy et al., 2015) , for 5 iterations with initial learning rate set to the default 0.025."}
{"sent_id": "0732eaa37366d7ae092f4de0ed72cb-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_0732eaa37366d7ae092f4de0ed72cb_12", "text": "Seq2seq models have also been applied to constituency parsing (Vinyals et al., 2015) and provided a fairly good result."}
{"sent_id": "0732eaa37366d7ae092f4de0ed72cb-C001-20", "intents": ["@USE@"], "paper_id": "ABC_0732eaa37366d7ae092f4de0ed72cb_12", "text": "Our starting point is an RNN-based Seq2seq model with an attention mechanism that was applied to constituency parsing (Vinyals et al., 2015) ."}
{"sent_id": "4d2488844c1f6f39f1f4b8f3487288-C001-96", "intents": ["@USE@"], "paper_id": "ABC_4d2488844c1f6f39f1f4b8f3487288_13", "text": "For evaluation, we compare using the same dataset and metrics as Kawahara et al. (2014) ."}
{"sent_id": "4d2488844c1f6f39f1f4b8f3487288-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_4d2488844c1f6f39f1f4b8f3487288_13", "text": "In Kawahara et al. (2014) , two identical DPMM's were used."}
{"sent_id": "9655fb9abfb1c30b39f3261680fafc-C001-18", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_9655fb9abfb1c30b39f3261680fafc_13", "text": "Previous work (Wiseman et al., 2017) notices the problem and proposes an extractive metric to evaluate the consistency between the generation and its input structured data."}
{"sent_id": "9655fb9abfb1c30b39f3261680fafc-C001-25", "intents": ["@USE@"], "paper_id": "ABC_9655fb9abfb1c30b39f3261680fafc_13", "text": "We evaluate our proposed method on the RO-TOWIRE dataset (Wiseman et al., 2017) , which targets at generating multi-sentence game summaries."}
{"sent_id": "9655fb9abfb1c30b39f3261680fafc-C001-86", "intents": ["@USE@"], "paper_id": "ABC_9655fb9abfb1c30b39f3261680fafc_13", "text": "Evaluation: We use automatic evaluation metric BLEU-4 (Papineni et al., 2002) and the extractive evaluation metrics proposed by (Wiseman et al., 2017) , which contains three criteria: content selection (CS), relation generation (RG), content ordering (CO)."}
{"sent_id": "e5bff4a27468139762496abdff3436-C001-52", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_e5bff4a27468139762496abdff3436_13", "text": "The linguistic coordination is systematically quantified by (Danescu-Niculescu-Mizil et al. 2012 ) and the arguments follow the principles of exchange theory examining behavior dynamics in low and high power groups (Willer 1999; Thye, Willer, and Markovsky 2006) : Lawyers tend to cooperate more to Justices than conversely and demonstrate strong linguistic coordination in their speech."}
{"sent_id": "e5bff4a27468139762496abdff3436-C001-71", "intents": ["@DIF@"], "paper_id": "ABC_e5bff4a27468139762496abdff3436_13", "text": "We further add another dimension in the relative power: Winners and Losers, haven't been investigated in the previous study (Danescu-Niculescu-Mizil et al. 2012) ."}
{"sent_id": "e5bff4a27468139762496abdff3436-C001-126", "intents": ["@DIF@"], "paper_id": "ABC_e5bff4a27468139762496abdff3436_13", "text": "Unlike the previous study (Danescu-Niculescu-Mizil et al. 2012) , entirely tracking back and forth utterances and proving the adaptation, e.g., linguistic coordination, by identifying the frequency of selected keywords, we directly utilize their overall conclusion and claim that linguistic relations already preserve the adaptation and any other complex collective linguistic process induced by both cooperation and competition in different power groups."}
{"sent_id": "8c202e3610599c9eee23724ef213de-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_8c202e3610599c9eee23724ef213de_13", "text": "Recent work has begun to model different aspects of these naturally occurring lay arguments, with tasks including stance classification (Somasundaran and Wiebe, 2009; Walker et al., 2012) , argument summarization (Misra et al., 2015) , sarcasm detection (Justo et al., 2014) and classification of propositions and arguments (Park and Cardie, 2014; Park et al., 2015; Oraby et al., 2015) ."}
{"sent_id": "8c202e3610599c9eee23724ef213de-C001-156", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_8c202e3610599c9eee23724ef213de_13", "text": "2 Park and Cardie (2014) and Park et al. (2015) used this corpus for examining each proposition with respect to its verifiability to determine the desirable types of support for the analysis of arguments."}
{"sent_id": "8ef47e16cd41aa3a606cf21c41adb7-C001-25", "intents": ["@USE@"], "paper_id": "ABC_8ef47e16cd41aa3a606cf21c41adb7_13", "text": "The heuristics are evaluated within the template-based abstractive summarization system of Oya et al. (2014) ."}
{"sent_id": "8ef47e16cd41aa3a606cf21c41adb7-C001-83", "intents": ["@USE@"], "paper_id": "ABC_8ef47e16cd41aa3a606cf21c41adb7_13", "text": "Following (Oya et al., 2014) , we removed 20 dialogs used by the authors for development, and use the remaining dialogs for the threefold cross-validation."}
{"sent_id": "c1eefe276c0ed46d7cd50f3f3bc3f3-C001-19", "intents": ["@USE@"], "paper_id": "ABC_c1eefe276c0ed46d7cd50f3f3bc3f3_13", "text": "(Illustration taken from Bordes et al., 2017) as location, type of cuisine and price range."}
{"sent_id": "c1eefe276c0ed46d7cd50f3f3bc3f3-C001-143", "intents": ["@USE@"], "paper_id": "ABC_c1eefe276c0ed46d7cd50f3f3bc3f3_13", "text": "Implementing the modifications to the Memory Network architecture described by Bordes et al. (2017) , we use the model as an end-to-end baseline and analyze its performance."}
{"sent_id": "c1eefe276c0ed46d7cd50f3f3bc3f3-C001-38", "intents": ["@EXT@"], "paper_id": "ABC_c1eefe276c0ed46d7cd50f3f3bc3f3_13", "text": "This work builds upon the bAbI dialog dataset described in Bordes et al. (2017) , which is aimed at testing end-to-end dialog systems in the goal-oriented domain of restaurant reservations."}
{"sent_id": "1f77b780c98093cd85966243471a1d-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_1f77b780c98093cd85966243471a1d_13", "text": "In the last couple of years, several variants of this problem have been considered (Fried et al., 2014; Abbar et al., 2015; Culotta, 2014; Ardehaly and Culotta, 2015) ."}
{"sent_id": "1f77b780c98093cd85966243471a1d-C001-32", "intents": ["@DIF@"], "paper_id": "ABC_1f77b780c98093cd85966243471a1d_13", "text": "Despite the model's simplicity, it outperforms Fried et al. (2014) 's best model by 2% accuracy."}
{"sent_id": "1f77b780c98093cd85966243471a1d-C001-60", "intents": ["@MOT@"], "paper_id": "ABC_1f77b780c98093cd85966243471a1d_13", "text": "Importantly, Fried et al. (2014) train and test their models on communities rather than individuals, which limits the applicability of their approach to individualized public health."}
{"sent_id": "45238fe9b493ccdf5921c8f5284097-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_45238fe9b493ccdf5921c8f5284097_13", "text": "Evaluation Metrics Despite differences in the exact definitions, the majority (e.g., Hsu et al., 2018; Celikyilmaz et al., 2018; Narayan et al., 2018b; Chen and Bansal, 2018; Peyrard and Gurevych, 2018) agree on both or either one of two broad quality definitions: coverage determines how much of the salient content of the source document is captured in the summary, and informativeness, how much of the content captured in the summary is salient with regards to the original document."}
{"sent_id": "45238fe9b493ccdf5921c8f5284097-C001-29", "intents": ["@USE@"], "paper_id": "ABC_45238fe9b493ccdf5921c8f5284097_13", "text": "To validate our proposed approach we use the recently introduced eXtreme SUMmarization dataset (XSUM, Narayan et al., 2018b) to evaluate two state-of-the-art abstractive summarization methods, Pointer Generator Networks (See et al., 2017) and Topic-aware Convolutional Networks (Narayan et al., 2018b) , using crowd-sourcing for both highlight annotation and quality judgments."}
{"sent_id": "45238fe9b493ccdf5921c8f5284097-C001-132", "intents": ["@USE@"], "paper_id": "ABC_45238fe9b493ccdf5921c8f5284097_13", "text": "We assessed summaries from two state-ofthe-art abstractive summarization systems using our highlight-based evaluation: (i) the PointerGenerator model (PTGEN) introduced by See et al. (2017) is an RNN-based abstractive systems which allows to copy words from the source text, and (ii) the Topic-aware Convolutional Sequence to Sequence model (TCONVS2S) introduced by Narayan et al. (2018b) is an abstractive model which is conditioned on the article's topics and based entirely on Convolutional Neural Networks."}
{"sent_id": "45238fe9b493ccdf5921c8f5284097-C001-181", "intents": ["@SIM@"], "paper_id": "ABC_45238fe9b493ccdf5921c8f5284097_13", "text": "When comparing the reference summaries against the original documents, both ROUGE and HROUGE confirm that the reference summaries are rather abstractive as reported by Narayan et al. (2018b) , and they in fact score below the system summaries."}
{"sent_id": "71a72cfca17b0b15938ed590f9c868-C001-2", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_71a72cfca17b0b15938ed590f9c868_13", "text": "Previous work on classifying information status (Nissim, 2006; Rahman and Ng, 2011) is restricted to coarse-grained classification and focuses on conversational dialogue."}
{"sent_id": "03c57679549ff600a024d436d5a107-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_03c57679549ff600a024d436d5a107_13", "text": "Ponzetto and Strube (2006) and Ratinov and Roth (2012) precompute a fixed alignment of the mentions to the knowledge base entities."}
{"sent_id": "03c57679549ff600a024d436d5a107-C001-89", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_03c57679549ff600a024d436d5a107_13", "text": "Given these entities, there are many possible features that may be used for disambiguation of the mentions, such as gender and fine-grained Wikipedia categories as used by Ratinov and Roth (2012) , however most of these features may not be relevant to the task of within-document coreference."}
{"sent_id": "03c57679549ff600a024d436d5a107-C001-27", "intents": ["@USE@"], "paper_id": "ABC_03c57679549ff600a024d436d5a107_13", "text": "Our paper provides the following contributions: (1) an approach that jointly reasons about both within-doc entities and their alignment to KBentities by dynamically adjusting a ranked list of candidate alignments, during coreference, (2) Utilization of a larger set of surface string variations for each entity candidate by using links that appear all over the web (Spitkovsky and Chang, 2012) , (3) A combination of these approaches that improves upon a competitive baseline without a knowledge base by 1.09 B 3 F1 points on the ACE 2004 data, and outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B 3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012) , and documents that contain a large number of mentions."}
{"sent_id": "46050691971ea46ce7e18fef5f6d2d-C001-20", "intents": ["@MOT@"], "paper_id": "ABC_46050691971ea46ce7e18fef5f6d2d_13", "text": "To what extent does Goldberg's (2019) result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations?"}
{"sent_id": "46050691971ea46ce7e18fef5f6d2d-C001-21", "intents": ["@EXT@"], "paper_id": "ABC_46050691971ea46ce7e18fef5f6d2d_13", "text": "Building on Goldberg's (2019) work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples."}
{"sent_id": "46050691971ea46ce7e18fef5f6d2d-C001-107", "intents": ["@USE@"], "paper_id": "ABC_46050691971ea46ce7e18fef5f6d2d_13", "text": "3 Goldberg (2019) showed that in his experiments the base BERT model performed better than the larger model, so we restrict our attention to the base model."}
{"sent_id": "46050691971ea46ce7e18fef5f6d2d-C001-105", "intents": ["@DIF@"], "paper_id": "ABC_46050691971ea46ce7e18fef5f6d2d_13", "text": "The average example in our cloze data is evaluated using 1,468 words, compared with 2 in Goldberg (2019) ."}
{"sent_id": "e264c45391853fb008c838aa7ccca8-C001-116", "intents": ["@DIF@"], "paper_id": "ABC_e264c45391853fb008c838aa7ccca8_13", "text": "We successfully obtained more than 1.73 million hyponymy relations with 85.2% precision, which greatly outperformed the results of Sumida and Torisawa (2008) in terms of both the precision and the number of acquired hyponymy relations."}
{"sent_id": "e264c45391853fb008c838aa7ccca8-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_e264c45391853fb008c838aa7ccca8_13", "text": "We expect that the readers will refer to the literature (Sumida and Torisawa, 2008) to see the effect of the features proposed by Sumida and Torisawa."}
{"sent_id": "c3c09df34cf9f81c1cc4fc63a18bf0-C001-220", "intents": ["@BACK@"], "paper_id": "ABC_c3c09df34cf9f81c1cc4fc63a18bf0_13", "text": "Attention mechanisms in neural networks have been successfully applied to a variety of problems, like machine translation [Bahdanau et al., 2014] , image captioning [Xu et al., 2015] , supervised relation extraction [Shen and Huang, 2016] , distantly-supervised relation extraction [Zheng et al., 2016] etc."}
{"sent_id": "c3c09df34cf9f81c1cc4fc63a18bf0-C001-85", "intents": ["@USE@"], "paper_id": "ABC_c3c09df34cf9f81c1cc4fc63a18bf0_13", "text": "For a given bag of sentences, learning is done using the setting proposed by [Zeng et al., 2015] , wherein the sentence with the highest probability of expressing a relation in a bag is selected to train the model in each iteration."}
{"sent_id": "c3c09df34cf9f81c1cc4fc63a18bf0-C001-98", "intents": ["@USE@"], "paper_id": "ABC_c3c09df34cf9f81c1cc4fc63a18bf0_13", "text": "It uses PCNN [Zeng et al., 2015] assumption to select the sentence with highest probability of any relation."}
{"sent_id": "8ae44e74146d3f40845741fac4dff9-C001-27", "intents": ["@SIM@"], "paper_id": "ABC_8ae44e74146d3f40845741fac4dff9_13", "text": "We evaluate the capacity of our vector space model to discriminate between literal and figurative paraphrases on its own, as well as integrating it with a selectional preference-based model similar to that of Shutova (2010) and thus evaluating the latter in an unsupervised setting."}
{"sent_id": "8ae44e74146d3f40845741fac4dff9-C001-31", "intents": ["@USE@"], "paper_id": "ABC_8ae44e74146d3f40845741fac4dff9_13", "text": "The comparison against a paraphrasing gold standard provided by Shutova (2010) is complemented by an evaluation against direct human judgements of system output."}
{"sent_id": "2504d707a8123774791d98b755551a-C001-148", "intents": ["@DIF@"], "paper_id": "ABC_2504d707a8123774791d98b755551a_13", "text": "The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus, which would be relatively much harder to incorporate in approaches like (Bunescu and Mooney, 2004) and (Finkel et al., 2005) ."}
{"sent_id": "2504d707a8123774791d98b755551a-C001-62", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_2504d707a8123774791d98b755551a_13", "text": "A very common case of this in the CoNLL dataset is that of documents containing references to both The China Daily, a newspaper, and China, the country (Finkel et al., 2005) ."}
{"sent_id": "2504d707a8123774791d98b755551a-C001-160", "intents": ["@BACK@"], "paper_id": "ABC_2504d707a8123774791d98b755551a_13", "text": "Recent work looking to directly model non-local dependencies and do approximate inference are that of Bunescu and Mooney (2004) , who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly model long-distance dependencies, Sutton and McCallum (2004) , who introduce skip-chain CRFs, which add additional non-local edges to the underlying CRF sequence model (which Bunescu and Mooney (2004) lack) and Finkel et al. (2005) who hand-set penalties for inconsistency in labels based on the training data and then use Gibbs Sampling for doing approximate inference where the goal is to obtain the label sequence that maximizes the product of the CRF objective function and their penalty."}
{"sent_id": "a3ad95d75b7750b8a879fa183e30f6-C001-22", "intents": ["@BACK@", "@EXT@", "@MOT@"], "paper_id": "ABC_a3ad95d75b7750b8a879fa183e30f6_13", "text": "Variational autoencoder-based data augmentation (VAE-DA) is a domain adaptation method proposed in [13] , which pools in-domain and out-domain to train a VAE that learns factorized latent representations of speech segments."}
{"sent_id": "a3ad95d75b7750b8a879fa183e30f6-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_a3ad95d75b7750b8a879fa183e30f6_13", "text": "A key observation made in [13] is that nuisance factors, such as speaker identity and room acoustics, are generally constant over segments within an utterance, while linguistic content changes from segment to segment."}
{"sent_id": "4bc12aca138835b5ed80b0cf69febf-C001-61", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_4bc12aca138835b5ed80b0cf69febf_13", "text": "Commonly known as a cloze task, Devlin et al. (2018) introduced a framework that pretrained transformers (Vaswani et al., 2017) based on masked token prediction."}
{"sent_id": "4bc12aca138835b5ed80b0cf69febf-C001-55", "intents": ["@SIM@"], "paper_id": "ABC_4bc12aca138835b5ed80b0cf69febf_13", "text": "For language modeling, we create training data similar to those in BERT (Devlin et al., 2018) ."}
{"sent_id": "4bc12aca138835b5ed80b0cf69febf-C001-66", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_4bc12aca138835b5ed80b0cf69febf_13", "text": "Unlike Devlin et al. (2018) , we run the randomization script once per each training epoch."}
{"sent_id": "4bc12aca138835b5ed80b0cf69febf-C001-136", "intents": ["@USE@"], "paper_id": "ABC_4bc12aca138835b5ed80b0cf69febf_13", "text": "For masked tokens, we predict that token through bidirectional context, the same as Devlin et al. (2018) ."}
{"sent_id": "4bc12aca138835b5ed80b0cf69febf-C001-137", "intents": ["@USE@"], "paper_id": "ABC_4bc12aca138835b5ed80b0cf69febf_13", "text": "For next sentence prediction, we use the unbiased method previously introduced as well as in Devlin et al. (2018) ."}
{"sent_id": "518b281a4bc04b3504d3b385a5dc62-C001-101", "intents": ["@USE@"], "paper_id": "ABC_518b281a4bc04b3504d3b385a5dc62_13", "text": "Stop Condition To identify words with no parents we use two types of binary features suggested by Narasimhan et al. (2015) ."}
{"sent_id": "518b281a4bc04b3504d3b385a5dc62-C001-76", "intents": ["@BACK@"], "paper_id": "ABC_518b281a4bc04b3504d3b385a5dc62_13", "text": "Semantic similarity was an important feature in MorphoChains: Narasimhan et al. (2015) concluded that up to 25 percent of their model's precision was due to the semantic similarity feature."}
{"sent_id": "c4e2f43e223f61d81d81ac2c9aaa3f-C001-59", "intents": ["@USE@"], "paper_id": "ABC_c4e2f43e223f61d81d81ac2c9aaa3f_13", "text": "We explicitly add inductive bias to the model via the regularizer Ω below, but our model also implicitly learns regularization through multi-task learning (Caruana 1993 ) mediated by the α parameters, while the β parameters are used to learn the mixture functions f (·), as detailed in the following."}
{"sent_id": "412c2daf6d060f520850d187c6eb36-C001-153", "intents": ["@SIM@"], "paper_id": "ABC_412c2daf6d060f520850d187c6eb36_13", "text": "For some of the words, more than one sense listed in LDOCE corresponds to a sense as used in Yarowsky (1992) ."}
{"sent_id": "412c2daf6d060f520850d187c6eb36-C001-149", "intents": ["@USE@"], "paper_id": "ABC_412c2daf6d060f520850d187c6eb36_13", "text": "3. The senses marked with * are used in Yarowsky (1992) but no corresponding sense is found in LDOCE."}
{"sent_id": "412c2daf6d060f520850d187c6eb36-C001-132", "intents": ["@DIF@"], "paper_id": "ABC_412c2daf6d060f520850d187c6eb36_13", "text": "9 The average sentence length in the Brown corpus is 19.41° words which is 5 times smaller than the 100 word window used in Gale et al. (1992) and Yarowsky (1992) ."}
{"sent_id": "412c2daf6d060f520850d187c6eb36-C001-188", "intents": ["@DIF@"], "paper_id": "ABC_412c2daf6d060f520850d187c6eb36_13", "text": "such as that described in Yarowsky (1992) are very unlikely to be capable of acquiring this finer knowledge because the problem of data sparseness becomes even more serious with the introduction of syntactic constraints."}
{"sent_id": "1c89c8f4849d1c8214a3e5f6b9ff1a-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14", "text": "Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context (Sproat and Jaitly, 2016; Yolchuyeva et al., 2018) ."}
{"sent_id": "1c89c8f4849d1c8214a3e5f6b9ff1a-C001-101", "intents": ["@USE@"], "paper_id": "ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14", "text": "Our datasets were randomly sampled from a set of 4.9M sentences in the training data portion of the Sproat and Jaitly (2016) data release and split into training, validation, and test data."}
{"sent_id": "1c89c8f4849d1c8214a3e5f6b9ff1a-C001-115", "intents": ["@USE@"], "paper_id": "ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14", "text": "We follow Sproat and Jaitly (2016) in down-sampling window-based training data to constrain the proportion of \"\" tokens to 10% of the data."}
{"sent_id": "22da24997f66a6dafa911f83f061e5-C001-79", "intents": ["@BACK@"], "paper_id": "ABC_22da24997f66a6dafa911f83f061e5_14", "text": "ESIM has been widely used among the FEVER challenge participants (Nie et al., 2019; Yoneda et al., 2018; Hanselowski et al., 2018) ."}
{"sent_id": "260489da0fb3f7a201a6a1cce8f03b-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_260489da0fb3f7a201a6a1cce8f03b_14", "text": "While early NMT models encode a source sentence as a fixed-length vector, Bahdanau et al. [2015] advocate the use of attention in NMT."}
{"sent_id": "260489da0fb3f7a201a6a1cce8f03b-C001-86", "intents": ["@USE@"], "paper_id": "ABC_260489da0fb3f7a201a6a1cce8f03b_14", "text": "1. Moses [Koehn and Hoang, 2007] : a phrase-based SMT system; 2. GroundHog [Bahdanau et al., 2015] : an attention-based NMT system."}
{"sent_id": "8853d810b364ae47a2da71c2502b3e-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_8853d810b364ae47a2da71c2502b3e_14", "text": "Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging (Plank and Agić, 2018) ."}
{"sent_id": "8853d810b364ae47a2da71c2502b3e-C001-66", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_8853d810b364ae47a2da71c2502b3e_14", "text": "Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available (Plank and Agić, 2018) ."}
{"sent_id": "8853d810b364ae47a2da71c2502b3e-C001-31", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_8853d810b364ae47a2da71c2502b3e_14", "text": "The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger (Plank and Agić, 2018) ."}
{"sent_id": "8853d810b364ae47a2da71c2502b3e-C001-56", "intents": ["@USE@"], "paper_id": "ABC_8853d810b364ae47a2da71c2502b3e_14", "text": "Annotation projection To build the taggers for new languages, we resort to annotation projection following Plank and Agić (2018) ."}
{"sent_id": "8853d810b364ae47a2da71c2502b3e-C001-191", "intents": ["@USE@"], "paper_id": "ABC_8853d810b364ae47a2da71c2502b3e_14", "text": "Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model (Plank and Agić, 2018) ."}
{"sent_id": "bdd0ebe147e277f8f7f04fc351464a-C001-3", "intents": ["@BACK@"], "paper_id": "ABC_bdd0ebe147e277f8f7f04fc351464a_14", "text": "In a previous paper [5] we have presented a methodology for the automatic summarization of documents, emitted by multiple sources, which describe the evolution of an event."}
{"sent_id": "bdd0ebe147e277f8f7f04fc351464a-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_bdd0ebe147e277f8f7f04fc351464a_14", "text": "2 Due to space limitations this section contains a very brief introduction to a methodology for the creation of summaries from evolving events that we have earlier presented [5] ."}
{"sent_id": "bdd0ebe147e277f8f7f04fc351464a-C001-132", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_bdd0ebe147e277f8f7f04fc351464a_14", "text": "the claim that the same trivial information might be contained in all the documents and thus such trivial information will have a high probability of being included in the final summary-this claim is rebuffed by the nature of the methodology that we have briefly presented in Section 2 and more fully exposed in [1] and [5] ."}
{"sent_id": "bdd0ebe147e277f8f7f04fc351464a-C001-135", "intents": ["@MOT@"], "paper_id": "ABC_bdd0ebe147e277f8f7f04fc351464a_14", "text": "As we have argued in [5] the creation of the ontology and the specifications of the messages require a considerable amount of human labor."}
{"sent_id": "cc66b46b34a0d716414e8b845707f9-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_cc66b46b34a0d716414e8b845707f9_14", "text": "Other effective approaches include the use of tree conditional random fields (Lu et al., 2009) and template extraction within a log-linear framework (Angeli et al., 2010) ."}
{"sent_id": "cc66b46b34a0d716414e8b845707f9-C001-148", "intents": ["@DIF@"], "paper_id": "ABC_cc66b46b34a0d716414e8b845707f9_14", "text": "Table 1 compares our test results against previous methods that include KL12 (Konstas and Lapata, 2012) , KL13 (Konstas and Lapata, 2013) , and ALK10 (Angeli et al., 2010) ."}
{"sent_id": "7f234ecfb4cf880502faa8b89cd07b-C001-16", "intents": ["@USE@"], "paper_id": "ABC_7f234ecfb4cf880502faa8b89cd07b_14", "text": "We show that it is a generalization of various multi-task learning algorithms such as hard parameter sharing [7] , low supervision [25] , and cross-stitch networks [21] , as well as transfer learning algorithms such as frustratingly easy domain adaptation [9] ."}
{"sent_id": "7f234ecfb4cf880502faa8b89cd07b-C001-87", "intents": ["@BACK@"], "paper_id": "ABC_7f234ecfb4cf880502faa8b89cd07b_14", "text": "Low Supervision [25] propose a model where only the inner layers of two deep recurrent works are shared."}
{"sent_id": "7f234ecfb4cf880502faa8b89cd07b-C001-47", "intents": ["@MOT@"], "paper_id": "ABC_7f234ecfb4cf880502faa8b89cd07b_14", "text": "Recall that our architecture is partly motivated by the observation that for loosely related tasks, only certain features in specific layers should be shared, while many of the layers and subspaces may remain more taskspecific [25] ."}
{"sent_id": "7f234ecfb4cf880502faa8b89cd07b-C001-171", "intents": ["@SIM@"], "paper_id": "ABC_7f234ecfb4cf880502faa8b89cd07b_14", "text": "The figure in 3 shows that hard parameter sharing, while learning faster because of the smoother loss surface in multi-task learning, is a good regularizer, confirming the findings in [25] , whereas the sluice network is even better at fitting noise than the single-task models."}
{"sent_id": "a0af9cf22996a245af9d66cf1d358f-C001-14", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_a0af9cf22996a245af9d66cf1d358f_14", "text": "A spike in attention directed toward a particular location may signal an important update, such as the need for aid for the location (Varga et al. 2013) ."}
{"sent_id": "a0af9cf22996a245af9d66cf1d358f-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_a0af9cf22996a245af9d66cf1d358f_14", "text": "Crisis events such as hurricanes present a useful case study for the development of collective attention, due to the large volume of online participation and large uncertainty among event observers towards the situation during the crisis events (Varga et al. 2013) ."}
{"sent_id": "8ff1560ac0241a763b4b0d93718b40-C001-121", "intents": ["@DIF@"], "paper_id": "ABC_8ff1560ac0241a763b4b0d93718b40_14", "text": "APCNN (dos Santos et al., 2016) and ABCNN (Yin et al., 2016) both employ an attentive pooling mechanism."}
{"sent_id": "de9eb9b7dff69743252b3ff0ef8894-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_de9eb9b7dff69743252b3ff0ef8894_14", "text": "In order to measure the extent to which various societal biases are captured by word embeddings, Caliskan et al. (2017) proposed the Word Embedding Association Test (WEAT)."}
{"sent_id": "de9eb9b7dff69743252b3ff0ef8894-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_de9eb9b7dff69743252b3ff0ef8894_14", "text": "where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work (Caliskan et al., 2017) ."}
{"sent_id": "de9eb9b7dff69743252b3ff0ef8894-C001-55", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_de9eb9b7dff69743252b3ff0ef8894_14", "text": "We adopt the general bias-testing framework from Caliskan et al. (2017) , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora."}
{"sent_id": "e68d09937d522dc5acac9637eb2a8b-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_e68d09937d522dc5acac9637eb2a8b_14", "text": "An alternative representation for baseNPs has been put forward by (Ramshaw and Marcus, 1995) ."}
{"sent_id": "e68d09937d522dc5acac9637eb2a8b-C001-95", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_e68d09937d522dc5acac9637eb2a8b_14", "text": "The data was segmented into baseNP parts and non-baseNP parts in a similar fashion as the data used by (Ramshaw and Marcus, 1995) ."}
{"sent_id": "e68d09937d522dc5acac9637eb2a8b-C001-117", "intents": ["@USE@"], "paper_id": "ABC_e68d09937d522dc5acac9637eb2a8b_14", "text": "We have applied it to the two data sets mentioned in (Ramshaw and Marcus, 1995) ."}
{"sent_id": "26b00c6e5b499eea30e9cef0bbaf9f-C001-65", "intents": ["@USE@"], "paper_id": "ABC_26b00c6e5b499eea30e9cef0bbaf9f_14", "text": "We report results from Salle et al. (2016) and use the same training corpus and parameters to train LexVec with positional contexts and external memory."}
{"sent_id": "9e8af6ca401cd74adc9a4137ae05ec-C001-15", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_9e8af6ca401cd74adc9a4137ae05ec_14", "text": "The pronunciation variation model is used to generate multiple pronunciations for each canonical pronunciation in a pronouncing dictionary and these variations are used in the spelling correction approach developed by Toutanova and Moore (2002) , which uses statistical models of spelling errors that consider both orthography and pronunciation."}
{"sent_id": "9e8af6ca401cd74adc9a4137ae05ec-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_9e8af6ca401cd74adc9a4137ae05ec_14", "text": "Since a spelling correction model needs to rank candidate words rather than candidate pronunciations, Toutanova and Moore (2002) derive an error model that determines the probability that a word w was spelled as the non-word r based on their pronunciations."}
{"sent_id": "9e8af6ca401cd74adc9a4137ae05ec-C001-173", "intents": ["@USE@"], "paper_id": "ABC_9e8af6ca401cd74adc9a4137ae05ec_14", "text": "We incorporated a pronouncing dictionary extended for Japanese writers of English into the spelling correction model developed by Toutanova and Moore (2002) , which combines orthography-based and pronunciation-based models."}
{"sent_id": "967c78ccf905c69d732a4c1ef00289-C001-40", "intents": ["@DIF@"], "paper_id": "ABC_967c78ccf905c69d732a4c1ef00289_14", "text": "Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016; Fu et al., 2017 ) that require additional parsers."}
{"sent_id": "967c78ccf905c69d732a4c1ef00289-C001-166", "intents": ["@USE@"], "paper_id": "ABC_967c78ccf905c69d732a4c1ef00289_14", "text": "We add chunk embedding and on dep path embedding (Nguyen and Grishman, 2016) ."}
{"sent_id": "17252628fa9c03c2fe0b44763fc7a2-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_17252628fa9c03c2fe0b44763fc7a2_14", "text": "Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004) , German-English (Collins et al., 2005) , Chinese-English (Wang et al., 2007; Zhang et al., 2008) , and English-Japanese (Lee et al., 2010) ."}
{"sent_id": "17252628fa9c03c2fe0b44763fc7a2-C001-26", "intents": ["@MOT@"], "paper_id": "ABC_17252628fa9c03c2fe0b44763fc7a2_14", "text": "We argue that even though the rules by Wang et al. (2007) exist, it is almost impossible to automatically convert their rules into rules that are applicable to dependency parsers."}
{"sent_id": "17252628fa9c03c2fe0b44763fc7a2-C001-23", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_17252628fa9c03c2fe0b44763fc7a2_14", "text": "The most similar work to this paper is that of Wang et al. (2007) ."}
{"sent_id": "17252628fa9c03c2fe0b44763fc7a2-C001-116", "intents": ["@SIM@"], "paper_id": "ABC_17252628fa9c03c2fe0b44763fc7a2_14", "text": "Notice that some of the incorrect pre-orderings may be caused by erroneous parsing as also suggested by Wang et al. (2007) ."}
{"sent_id": "ab8c43bf5a37c436d166960af459a8-C001-35", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_ab8c43bf5a37c436d166960af459a8_14", "text": "110K informal sentences were collected from Yahoo Answers and they were rewritten in a formal style via crowd-sourcing, which made it possible to benchmark style transfer systems based on both PBMT and NMT models (Rao and Tetreault, 2018) ."}
{"sent_id": "ab8c43bf5a37c436d166960af459a8-C001-152", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_ab8c43bf5a37c436d166960af459a8_14", "text": "6 Formality Transfer Experiments 6.1 Baseline Models from Rao and Tetreault (2018) PBMT is a phrase-based machine translation model trained on the GYAFC corpus using a training regime consisting of self-training, data sub-selection and a large language model."}
{"sent_id": "ab8c43bf5a37c436d166960af459a8-C001-169", "intents": ["@DIF@"], "paper_id": "ABC_ab8c43bf5a37c436d166960af459a8_14", "text": "As shown in Table 1 , our NMT baselines yield surprisingly better BLEU scores than those of Rao and Tetreault (2018) , even without using rule-processed source training data and pretrained word embeddings."}
{"sent_id": "4f111ff06afd5523d65fc1d1a9ff83-C001-20", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_4f111ff06afd5523d65fc1d1a9ff83_14", "text": "Alignment is an integral part of statistical MT (Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006) but the task is often substantively different from monolingual alignment, which poses unique challenges depending on the application (MacCartney et al., 2008) ."}
{"sent_id": "4f111ff06afd5523d65fc1d1a9ff83-C001-26", "intents": ["@USE@"], "paper_id": "ABC_4f111ff06afd5523d65fc1d1a9ff83_14", "text": "Our alignment system is structured identically to MANLI (MacCartney et al., 2008) and uses the same phrase-based alignment representation."}
{"sent_id": "4f111ff06afd5523d65fc1d1a9ff83-C001-37", "intents": ["@USE@"], "paper_id": "ABC_4f111ff06afd5523d65fc1d1a9ff83_14", "text": "In our experiments, we merge the annotations using majority rule in the same manner as MacCartney et al. (2008) ."}
{"sent_id": "4f111ff06afd5523d65fc1d1a9ff83-C001-43", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_4f111ff06afd5523d65fc1d1a9ff83_14", "text": "Our implementation uses the same set of features as MacCartney et al. (2008) with some minor changes: we use a shallow parser (Daumé and Marcu, 2005) for detecting constituents and employ only string similarity and WordNet for determining semantic relatedness, forgoing NomBank and the distributional similarity resources used in the original MANLI implementation."}
{"sent_id": "d8168f4596878807d22ddc7474ffc8-C001-66", "intents": ["@USE@"], "paper_id": "ABC_d8168f4596878807d22ddc7474ffc8_14", "text": "In this experimental setup, we consider all nine grammars from Eskander et al. (2016) using both the Standard and the Cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set."}
{"sent_id": "d8168f4596878807d22ddc7474ffc8-C001-69", "intents": ["@EXT@", "@SIM@"], "paper_id": "ABC_d8168f4596878807d22ddc7474ffc8_14", "text": "To approximate the effect of Scholar-seeded-Knowledge in Eskander et al. (2016), we used the training set to derive affixes and use them as scholar-seeded knowledge added to the grammars (before the learning happens)."}
{"sent_id": "23119eff3cfd71370e8ad408fc75e1-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_23119eff3cfd71370e8ad408fc75e1_15", "text": "Very recently, Lee et al. (2017) proposed the first state-of-the-art end-to-end neural coreference resolution system."}
{"sent_id": "23119eff3cfd71370e8ad408fc75e1-C001-38", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_23119eff3cfd71370e8ad408fc75e1_15", "text": "We adopt the same span representation approach as in Lee et al. (2017) using bidirectional LSTMs and a headfinding attention."}
{"sent_id": "23119eff3cfd71370e8ad408fc75e1-C001-85", "intents": ["@DIF@"], "paper_id": "ABC_23119eff3cfd71370e8ad408fc75e1_15", "text": "In particular, compared with Lee et al. (2017) , our improvement mainly results from the precision scores."}
{"sent_id": "c5ec401f42f79c4707770dac4f5013-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_c5ec401f42f79c4707770dac4f5013_15", "text": "Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et al., 2016c) and context-sensitive (Tian et al., 2017; Xing et al., 2017) ."}
{"sent_id": "c5ec401f42f79c4707770dac4f5013-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_c5ec401f42f79c4707770dac4f5013_15", "text": "One is the attention-based approach (Xing et al., 2017) , the other is the sequential integration approach (Tian et al., 2017) ."}
{"sent_id": "b7a718664f395f048abb3655fb1d8d-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_b7a718664f395f048abb3655fb1d8d_15", "text": "Since its inception, papers have applied different methods such as feature based (Kiritchenko et al., 2014) , Recursive Neural Networks (RecNN) (Dong et al., 2014) , Recurrent Neural Networks (RNN) (Tang et al., 2016a) , attention applied to RNN (Wang et al., 2016; Chen et al., 2017; Tay et al., 2017) , Neural Pooling (NP) Wang et al., 2017) , RNN combined with NP (Zhang et al., 2016) , and attention based neural networks (Tang et al., 2016b) ."}
{"sent_id": "b7a718664f395f048abb3655fb1d8d-C001-31", "intents": ["@MOT@"], "paper_id": "ABC_b7a718664f395f048abb3655fb1d8d_15", "text": "In this paper, we therefore reproduce three papers chosen as they employ widely differing methods: Neural Pooling (NP) , NP with dependency parsing (Wang et al., 2017) , and RNN (Tang et al., 2016a) , as well as having been applied largely to different datasets."}
{"sent_id": "b7a718664f395f048abb3655fb1d8d-C001-178", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_b7a718664f395f048abb3655fb1d8d_15", "text": "However for the neural network/deep learning approach of Tang et al. (2016a) we agree with Reimers and Gurevych (2017) that reporting multiple runs of the system over different seed values is required as the single performance scores can be misleading, which could explain why previous papers obtained different results to the original for the TDLSTM method (Chen et al., 2017; Tay et al., 2017) ."}
{"sent_id": "5fc7df69445712a50228d0bf80f30a-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_5fc7df69445712a50228d0bf80f30a_15", "text": "While early works tend to frame humor recognition as a binary classification task (Mihalcea and Strapparava, 2005; Yang et al., 2015) , the last few years have seen the emergence of humor recognition as a pairwise relative ranking task (Cattle and Ma, 2016; Shahaf et al., 2015) ."}
{"sent_id": "5fc7df69445712a50228d0bf80f30a-C001-99", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_5fc7df69445712a50228d0bf80f30a_15", "text": "To show the effectiveness of label propagation of our tensor embedding method for small sample humor recognition, we conduct an experiment on two humor classification datasets 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day (Yang et al., 2015) ."}
{"sent_id": "b13bc55709f5040cf100bd5f466ff2-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_b13bc55709f5040cf100bd5f466ff2_15", "text": "Skerry-Ryan et al. used convolutional neural networks and a Gated Recurrent Unit (GRU) [9] to compress the prosody of the reference speech [4] ."}
{"sent_id": "b13bc55709f5040cf100bd5f466ff2-C001-75", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_b13bc55709f5040cf100bd5f466ff2_15", "text": "Unless otherwise stated, we used the same hyperparameter settings used in earlier work [4] ."}
{"sent_id": "b13bc55709f5040cf100bd5f466ff2-C001-37", "intents": ["@DIF@"], "paper_id": "ABC_b13bc55709f5040cf100bd5f466ff2_15", "text": "In addition, variablelength prosody embedding was also implemented using the output of the GRU at every time step [4] ."}
{"sent_id": "d8a250a1a0495ee824837839b74f26-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_d8a250a1a0495ee824837839b74f26_15", "text": "ReplaceHead covers two distinct actions in Wang et al. (2015b) ; ReplaceHead and Merge."}
{"sent_id": "d8a250a1a0495ee824837839b74f26-C001-50", "intents": ["@DIF@"], "paper_id": "ABC_d8a250a1a0495ee824837839b74f26_15", "text": "Unlike Wang et al. (2015b) we do not parameterise Swap or Reattach actions with a label."}
{"sent_id": "b0083488650bc98477fb10a9c5a808-C001-44", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_b0083488650bc98477fb10a9c5a808_15", "text": "Compared to [13] , our model is much deeper."}
{"sent_id": "b0083488650bc98477fb10a9c5a808-C001-124", "intents": ["@DIF@"], "paper_id": "ABC_b0083488650bc98477fb10a9c5a808_15", "text": "This algorithm is faster than [13] , where at each output step, they recompute all previous states of 2DLSTM from scratch which are not required."}
{"sent_id": "da8f30113f1126a78cefed06a15076-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_da8f30113f1126a78cefed06a15076_15", "text": "A number of approaches for question answering have been proposed recently that use reinforcement learning to reason over a knowledge graph (Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Zhang et al., 2018) ."}
{"sent_id": "da8f30113f1126a78cefed06a15076-C001-122", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_da8f30113f1126a78cefed06a15076_15", "text": "Both the public dataset and the proprietary dataset are Das et al. (2018) , using the same train/val/test splits for FB15k-237."}
{"sent_id": "da8f30113f1126a78cefed06a15076-C001-148", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_da8f30113f1126a78cefed06a15076_15", "text": "The final QA Score also increased from 28.72% to 39.55%, and also significantly improved over Das et al. (2018) and Lin et al. (2018) ."}
{"sent_id": "4adcc28c6d1906d74874b8fca371dc-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_4adcc28c6d1906d74874b8fca371dc_15", "text": "Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description Plummer et al., 2015) in a common space."}
{"sent_id": "4adcc28c6d1906d74874b8fca371dc-C001-46", "intents": ["@MOT@"], "paper_id": "ABC_4adcc28c6d1906d74874b8fca371dc_15", "text": "A similar loss function is useful for learning multimodal embeddings in a single language (Kiros et al., 2015) ."}
{"sent_id": "4adcc28c6d1906d74874b8fca371dc-C001-68", "intents": ["@SIM@"], "paper_id": "ABC_4adcc28c6d1906d74874b8fca371dc_15", "text": "We used the L2 norm to mitigate over-fitting (Kiros et al., 2015) ."}
{"sent_id": "6597d733f13b06f61cb653f86c4460-C001-2", "intents": ["@BACK@"], "paper_id": "ABC_6597d733f13b06f61cb653f86c4460_15", "text": "Recent work by Nerbonne and Wiersma (2006) has provided a foundation for measuring syntactic differences between corpora."}
{"sent_id": "6597d733f13b06f61cb653f86c4460-C001-35", "intents": ["@MOT@"], "paper_id": "ABC_6597d733f13b06f61cb653f86c4460_15", "text": "Fortunately, the permutation test used by Nerbonne and Wiersma (2006) is already designed to normalize the effects of differing sentence length when combining POS trigrams into a single vector per region."}
{"sent_id": "6597d733f13b06f61cb653f86c4460-C001-49", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6597d733f13b06f61cb653f86c4460_15", "text": "The primary source is the syntactic comparison of Nerbonne and Wiersma (2006) , which uses a permutation test, explained in Good (1995) and in particular for linguistic purposes in Kessler (2001) ."}
{"sent_id": "6597d733f13b06f61cb653f86c4460-C001-136", "intents": ["@EXT@"], "paper_id": "ABC_6597d733f13b06f61cb653f86c4460_15", "text": "Our work extends that of Nerbonne and Wiersma (2006) in a number of ways."}
{"sent_id": "a7f4154081f4045390e662c6e6f3ac-C001-5", "intents": ["@MOT@"], "paper_id": "ABC_a7f4154081f4045390e662c6e6f3ac_15", "text": "We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014) ."}
{"sent_id": "a7f4154081f4045390e662c6e6f3ac-C001-18", "intents": ["@MOT@"], "paper_id": "ABC_a7f4154081f4045390e662c6e6f3ac_15", "text": "Finally, we show how to integrate these networks with a preexisting entity linking system (Durrett and Klein, 2014) ."}
{"sent_id": "a7f4154081f4045390e662c6e6f3ac-C001-62", "intents": ["@EXT@", "@SIM@"], "paper_id": "ABC_a7f4154081f4045390e662c6e6f3ac_15", "text": "f Q and f E are both sparse features vectors and are taken from previous work (Durrett and Klein, 2014) ."}
{"sent_id": "a7f4154081f4045390e662c6e6f3ac-C001-79", "intents": ["@DIF@"], "paper_id": "ABC_a7f4154081f4045390e662c6e6f3ac_15", "text": "Our results outperform those of Durrett and Klein (2014) and Nguyen et al. (2014) ."}
{"sent_id": "57e65909baf823ff00a9a10a64fffd-C001-201", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_57e65909baf823ff00a9a10a64fffd_15", "text": "Even the classifier trained on expert-labeled data (Waseem, 2016) flags black-aligned tweets as sexist at almost twice the rate of white-aligned tweets."}
{"sent_id": "c3f6140bd69d1eef0124665e651c0c-C001-38", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c3f6140bd69d1eef0124665e651c0c_15", "text": "Similarly to Klebanov et al. (2014) , we classify each content word (i.e., adjective, noun, verb or adverb) appearing in a proverb as being used metaphorically or not."}
{"sent_id": "c3f6140bd69d1eef0124665e651c0c-C001-51", "intents": ["@SIM@"], "paper_id": "ABC_c3f6140bd69d1eef0124665e651c0c_15", "text": "Similarly to Klebanov et al. (2014) , the mean concreteness ratings, ranging from 1 to 5, are binned in 0.25 increments."}
{"sent_id": "c3f6140bd69d1eef0124665e651c0c-C001-119", "intents": ["@BACK@"], "paper_id": "ABC_c3f6140bd69d1eef0124665e651c0c_15", "text": "Finally, Table 3 shows the effect of the different feature sets on VUAMC used by Klebanov et al. (2014) ."}
{"sent_id": "34b73a56bd9b80dc415ca2c5608596-C001-69", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_34b73a56bd9b80dc415ca2c5608596_15", "text": "Finally, we define bias towards refugees similar to how the authors of (Garg et al., 2018) define bias against Asians during the 20th century, measuring to what extent radio shows associate \"outsider\" adjectives like \"aggressive\", \"frightening\", \"illegal\", etc."}
{"sent_id": "93cf4d4fd9cd875e8e148d1bbd8e2c-C001-25", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_93cf4d4fd9cd875e8e148d1bbd8e2c_15", "text": "Second, we perform qualitative and quantitative comparison of the maps generated by state-of-the-art attention-based VQA models (Yang et al., 2015; Lu et al., 2016 ) and a task-independent saliency baseline (Judd et al., 2009 ) against our human attention maps through visualizations and rank-order correlation."}
{"sent_id": "93cf4d4fd9cd875e8e148d1bbd8e2c-C001-104", "intents": ["@MOT@"], "paper_id": "ABC_93cf4d4fd9cd875e8e148d1bbd8e2c_15", "text": "• Hierarchical Co-Attention Network (HieCoAtt) (Lu et al., 2016) with word-level (HieCoAtt-W), phrase-level (HieCoAtt-P) and question-level (HieCoAtt-Q) attention maps; we evaluate all three maps 3 ."}
{"sent_id": "93cf4d4fd9cd875e8e148d1bbd8e2c-C001-137", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_93cf4d4fd9cd875e8e148d1bbd8e2c_15", "text": "HieCoAtt-W (Lu et al., 2016) 0.062 ± 0.012 HieCoAtt-P (Lu et al., 2016) 0.048 ± 0.010 HieCoAtt-Q (Lu et al., 2016) 0 Table 3 : Mean rank-correlation coefficients (higher is better) on the reduced set without center bias; error bars show standard error of means."}
{"sent_id": "9ea14a9fe422451901ad221bee5714-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_9ea14a9fe422451901ad221bee5714_15", "text": "In this pretext, we revisit the problem of compound type identification in Sanskrit (Krishna et al., 2016) and experiment with various neural architectures for solving the task."}
{"sent_id": "9ea14a9fe422451901ad221bee5714-C001-34", "intents": ["@DIF@"], "paper_id": "ABC_9ea14a9fe422451901ad221bee5714_15", "text": "Unlike the feature-rich representation of Krishna et al. (2016) , we rely on various word embedding approaches, which include character level, sub-word level, and word-level embedding approaches."}
{"sent_id": "9ea14a9fe422451901ad221bee5714-C001-46", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_9ea14a9fe422451901ad221bee5714_15", "text": "Here, similar to Krishna et al. (2016) , we expect the users to provide a compound in its component-wise segmented form as input to the model."}
{"sent_id": "b86a5a8ec1f27354057bb45ff27588-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_b86a5a8ec1f27354057bb45ff27588_15", "text": "It differs significantly from various spoken varieties of Arabic (Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013) ."}
{"sent_id": "b86a5a8ec1f27354057bb45ff27588-C001-23", "intents": ["@SIM@"], "paper_id": "ABC_b86a5a8ec1f27354057bb45ff27588_15", "text": "Some improvements in terms of classification accuracy and 10-fold cross validation under the same data conditions as (Zaidan and Callison-Burch, 2011; Elfardy and Diab, 2013) are presented."}
{"sent_id": "b86a5a8ec1f27354057bb45ff27588-C001-74", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_b86a5a8ec1f27354057bb45ff27588_15", "text": "For our experiments, we use the data set provided in (Zaidan and Callison-Burch, 2011 ) which also has been used in the experiments in (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2014) ."}
{"sent_id": "b86a5a8ec1f27354057bb45ff27588-C001-50", "intents": ["@MOT@"], "paper_id": "ABC_b86a5a8ec1f27354057bb45ff27588_15", "text": "While most of the above work focuses on document-level language classification, recent work on handling Arabic dialect data addresses the problem of sentence-level classification (Zaidan and CallisonBurch, 2011; Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2014 )."}
{"sent_id": "b86a5a8ec1f27354057bb45ff27588-C001-150", "intents": ["@DIF@"], "paper_id": "ABC_b86a5a8ec1f27354057bb45ff27588_15", "text": "In comparison, (Elfardy and Diab, 2013) reports an accuracy of 80.4 % as perplexity-based baseline."}
{"sent_id": "4d528117dd7751d0cd6413430e1ec1-C001-86", "intents": ["@SIM@"], "paper_id": "ABC_4d528117dd7751d0cd6413430e1ec1_15", "text": "For justice, we use the same model selection strategy with Zhang et al. (2017) , i.e. we choose the model whose sum of reconstruction loss and classification accuracy is the least."}
{"sent_id": "4d528117dd7751d0cd6413430e1ec1-C001-93", "intents": ["@USE@"], "paper_id": "ABC_4d528117dd7751d0cd6413430e1ec1_15", "text": "For this set of experiments, we use the same data as Zhang et al. (2017) ."}
{"sent_id": "4d528117dd7751d0cd6413430e1ec1-C001-98", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_4d528117dd7751d0cd6413430e1ec1_15", "text": "The results of baseline models are cited from Zhang et al. (2017) ."}
{"sent_id": "4d528117dd7751d0cd6413430e1ec1-C001-111", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_4d528117dd7751d0cd6413430e1ec1_15", "text": "In this section, we integrate our method with Conneau et al. (2018) , whose method improves Zhang et al. (2017) by more sophiscated refinement procedure and validation criterion."}
{"sent_id": "70dc108166d6b5fb9da39c451c3229-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_70dc108166d6b5fb9da39c451c3229_16", "text": "Thorne et al. (2018) used a modified document retrieval component of DrQA (Chen et al., 2017) to select the top most similar sentences w.r.t the claim, using bigram TF-IDF with binning."}
{"sent_id": "70dc108166d6b5fb9da39c451c3229-C001-52", "intents": ["@DIF@"], "paper_id": "ABC_70dc108166d6b5fb9da39c451c3229_16", "text": "We see that our best approach (combined) achieved a high coverage 94.4% compared to the baseline (Thorne et al., 2018) of 55.3%."}
{"sent_id": "70dc108166d6b5fb9da39c451c3229-C001-58", "intents": ["@DIF@"], "paper_id": "ABC_70dc108166d6b5fb9da39c451c3229_16", "text": "Our evidence recall is 78.4 as compared to 45.05 in the development set of FEVER (Thorne et al., 2018) , which demonstrates the importance of document retrieval in fact extraction and verification."}
{"sent_id": "195f41862b929318787aad9d8e5a1c-C001-55", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_195f41862b929318787aad9d8e5a1c_16", "text": "We closely follow the implementation of Huang et al. [12] described in their paper except for the changes above."}
{"sent_id": "195f41862b929318787aad9d8e5a1c-C001-125", "intents": ["@DIF@"], "paper_id": "ABC_195f41862b929318787aad9d8e5a1c_16", "text": "Our FastFusionNet reaches F1 75% in 4 epochs and achieves at F1 82.5% at the end which matches the reported F1 82.5% of FusionNet without CoVe on SQuAD development set [12] ."}
{"sent_id": "195f41862b929318787aad9d8e5a1c-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_195f41862b929318787aad9d8e5a1c_16", "text": "FusionNet [12] is reading comprehension model built on top of DrQA by introducing Fully-aware attention layers (context-question attention and context self-attention), contextual embeddings [21] , and more RNN layers."}
{"sent_id": "195f41862b929318787aad9d8e5a1c-C001-56", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_195f41862b929318787aad9d8e5a1c_16", "text": "Following Huang et al. [12] , the hidden size of each SRU is set to 125, resulting in a 250-d output feature of each BiSRU regardless of the input size."}
{"sent_id": "f587fc2bbbf3c1327b03d556e4bc05-C001-22", "intents": ["@DIF@"], "paper_id": "ABC_f587fc2bbbf3c1327b03d556e4bc05_16", "text": "This makes our work distinguished from to the work of (Le and Mikolov, 2014; Hill et al., 2016; Kiros et al., 2015) where they study the interrelationship of words in the text snippet."}
{"sent_id": "f587fc2bbbf3c1327b03d556e4bc05-C001-49", "intents": ["@DIF@"], "paper_id": "ABC_f587fc2bbbf3c1327b03d556e4bc05_16", "text": "This is an advantage compared to existing methods for generating paragraph vectors, such as (Le and Mikolov, 2014; Hill et al., 2016) ."}
{"sent_id": "f587fc2bbbf3c1327b03d556e4bc05-C001-60", "intents": ["@SIM@"], "paper_id": "ABC_f587fc2bbbf3c1327b03d556e4bc05_16", "text": "Some efforts aimed at finding a global representation of a text snippet using a paragraph-level representation such as paragraph vectors (Le and Mikolov, 2014) ."}
{"sent_id": "2adb3a645a57b8f441a80bb5a46045-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_2adb3a645a57b8f441a80bb5a46045_16", "text": "This was, for instance, shown by a recent shared task on three-category stance classification of tweets, where an F-score of 0.59 was achieved by a classifier that outperformed submissions from 19 shared task teams (Mohammad et al., 2017) ."}
{"sent_id": "2adb3a645a57b8f441a80bb5a46045-C001-85", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_2adb3a645a57b8f441a80bb5a46045_16", "text": "Following the principle of the guidelines by Mohammad et al. (2017) , we classified the posts as taking a stance against or for vaccination, or to be undecided."}
{"sent_id": "6a90ecc147618b3909609fc6c2e2b3-C001-117", "intents": ["@SIM@"], "paper_id": "ABC_6a90ecc147618b3909609fc6c2e2b3_16", "text": "The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of Baldridge (2008) that uses variational Bayes EM (33%)."}
{"sent_id": "6a90ecc147618b3909609fc6c2e2b3-C001-118", "intents": ["@DIF@"], "paper_id": "ABC_6a90ecc147618b3909609fc6c2e2b3_16", "text": "Our complexity based initialization is not directly comparable to the results in Baldridge (2008) because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism."}
{"sent_id": "e7c947a02bb0e81d6b6b4b9da74024-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_e7c947a02bb0e81d6b6b4b9da74024_16", "text": "Bolukbasi et al. (2016b) show that using word embeddings for simple analogies surfaces many gender stereotypes."}
{"sent_id": "e7c947a02bb0e81d6b6b4b9da74024-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_e7c947a02bb0e81d6b6b4b9da74024_16", "text": "2 In a seminal work, Bolukbasi et al. (2016b) use a post-processing debiasing method."}
{"sent_id": "e7c947a02bb0e81d6b6b4b9da74024-C001-78", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_e7c947a02bb0e81d6b6b4b9da74024_16", "text": "Male-and female-biased words cluster together We take the most biased words in the vocabulary according to the original bias (500 malebiased and 500 female-biased 8 ), and cluster them 6 We use the embeddings provided by Bolukbasi et al. (2016b) in https://github.com/tolga-b/ debiaswe and by Zhao et al. (2018) in https:// github.com/uclanlp/gn_glove."}
{"sent_id": "e7c947a02bb0e81d6b6b4b9da74024-C001-69", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_e7c947a02bb0e81d6b6b4b9da74024_16", "text": "6 Unless otherwise specified, we follow Bolukbasi et al. (2016b) and use a reduced version of the vocabulary for both word embeddings: we take the most frequent 50,000 words and phrases and remove words with upper-case letters, digits, or punctuation, and words longer than 20 characters."}
{"sent_id": "09dad2fd96cd1d48936cd5b99a38e7-C001-72", "intents": ["@DIF@"], "paper_id": "ABC_09dad2fd96cd1d48936cd5b99a38e7_16", "text": "The former situation is present, due to the fact that F-Score does not consider the make-up of the clusters beyond the majority class (Rosenberg and Hirschberg, 2007) ."}
{"sent_id": "09dad2fd96cd1d48936cd5b99a38e7-C001-97", "intents": ["@SIM@"], "paper_id": "ABC_09dad2fd96cd1d48936cd5b99a38e7_16", "text": "V-measure assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness (Rosenberg and Hirschberg, 2007) ."}
{"sent_id": "7616f6f8c1c188b32cd3a8374b61dd-C001-3", "intents": ["@MOT@"], "paper_id": "ABC_7616f6f8c1c188b32cd3a8374b61dd_16", "text": "We motivate the need to build NLP-QT as a resource in its own right, by comparing the Penn Treebank-style annotation scheme used for QuestionBank (Judge et al., 2006) with the modified NP annotation for the Penn Treebank introduced by Vadas and Curran (2007) ."}
{"sent_id": "7616f6f8c1c188b32cd3a8374b61dd-C001-123", "intents": ["@MOT@"], "paper_id": "ABC_7616f6f8c1c188b32cd3a8374b61dd_16", "text": "We have motivated the need to build NLP-QT as a resource in its own right by comparing the Penn Treebank-style annotation scheme used for QuestionBank with the modified NP annotation for the Penn Treebank introduced by Vadas and Curran (2007) ."}
{"sent_id": "7616f6f8c1c188b32cd3a8374b61dd-C001-62", "intents": ["@SIM@"], "paper_id": "ABC_7616f6f8c1c188b32cd3a8374b61dd_16", "text": "This section summarizes the set of experiments that we have conducted with the Vadas and Curran (2007) annotation style for NPs and in particular with the NLP-QT data set."}
{"sent_id": "7616f6f8c1c188b32cd3a8374b61dd-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_7616f6f8c1c188b32cd3a8374b61dd_16", "text": "It is precisely this type of shortcoming that led Vadas and Curran (2007) to revise the Penn Treebank annotation style for NPs along the following lines:"}
{"sent_id": "7616f6f8c1c188b32cd3a8374b61dd-C001-76", "intents": ["@DIF@"], "paper_id": "ABC_7616f6f8c1c188b32cd3a8374b61dd_16", "text": "The decrease in performance caused by adding the QuestionBank training data together with the modified NP annotation on section 23 is comparable to the one caused by adding the modified NP annotation alone (a decrease from 90.263 to 90.04, whereas for the original Penn Treebank data the F-score decreased from 90.43 to 89.96), but this slight decrease is more than offset by the increase in semantic information obtained from the Vadas and Curran (2007) annotation for complex base NPs."}
{"sent_id": "2bb41cea97a0375f67eab3a77c3a97-C001-14", "intents": ["@MOT@"], "paper_id": "ABC_2bb41cea97a0375f67eab3a77c3a97_16", "text": "To remedy these problems, recent years have seen interest in the distant supervision approach for relation extraction (Wu and Weld, 2007; Mintz et al., 2009) ."}
{"sent_id": "2bb41cea97a0375f67eab3a77c3a97-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_2bb41cea97a0375f67eab3a77c3a97_16", "text": "Since Mintz et al. (2009) coined the name \"distant supervision,\" there has been growing interest in this technique."}
{"sent_id": "2bb41cea97a0375f67eab3a77c3a97-C001-87", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_2bb41cea97a0375f67eab3a77c3a97_16", "text": "To generate negative examples for each relation, we follow the assumption in Mintz et al. (2009) that relations are disjoint and sample from other relations, i.e., R"}
{"sent_id": "2bb41cea97a0375f67eab3a77c3a97-C001-195", "intents": ["@DIF@"], "paper_id": "ABC_2bb41cea97a0375f67eab3a77c3a97_16", "text": "In addition to the TAC-KBP benchmark, we also follow prior work (Mintz et al., 2009; Hoffmann et al., 2011) and measure the quality using held-out data from Freebase."}
{"sent_id": "4f75f73b4eac8aecdde9312a846a1d-C001-56", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_4f75f73b4eac8aecdde9312a846a1d_16", "text": "For this, we extend Vaswani et al. (2017) by introducing an additional identical encoding block by which both the enc src and the enc mt encoders communicate with the decoder."}
{"sent_id": "4f75f73b4eac8aecdde9312a846a1d-C001-49", "intents": ["@EXT@"], "paper_id": "ABC_4f75f73b4eac8aecdde9312a846a1d_16", "text": "Our single-source model (SS) is based on an encoder-decoder-based transformer architecture (Vaswani et al., 2017) ."}
{"sent_id": "4f75f73b4eac8aecdde9312a846a1d-C001-35", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4f75f73b4eac8aecdde9312a846a1d_16", "text": "Finally, this joint encoder is fed to the decoder which follows a similar architecture as described in Vaswani et al. (2017) ."}
{"sent_id": "4f75f73b4eac8aecdde9312a846a1d-C001-100", "intents": ["@SIM@"], "paper_id": "ABC_4f75f73b4eac8aecdde9312a846a1d_16", "text": "The learning rate is varied throughout the training process, first increasing linearly for the first training steps warmup steps = 4000 and then adjusted as described in (Vaswani et al., 2017) ."}
{"sent_id": "c5a10f46c253f0da005622661b12a1-C001-79", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_c5a10f46c253f0da005622661b12a1_16", "text": "However, some evaluation settings differ: Li et al. (2013) train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types."}
{"sent_id": "c5a10f46c253f0da005622661b12a1-C001-95", "intents": ["@SIM@"], "paper_id": "ABC_c5a10f46c253f0da005622661b12a1_16", "text": "We compare our system's performance to the published trigger classification results of the baseline system of (Li et al., 2013 ) (its globally optimized run, when labeling both triggers and arguments)."}
{"sent_id": "4da1c39dbbeaa2c9dac22118d0c698-C001-79", "intents": ["@SIM@"], "paper_id": "ABC_4da1c39dbbeaa2c9dac22118d0c698_16", "text": "From the various feature combinations we tested, we found the following to be most successful: ratio_words, pos_sim_all, language_detection, mt_cfs, mt_bleu, ratio_chars (as described in Section 3.1), alongside cg_score, only_capletters_dif, and punctuation_similarity (from Barbu, 2015) ."}
{"sent_id": "fa5413db2c8e0a32bc3805d25cd0e7-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_fa5413db2c8e0a32bc3805d25cd0e7_16", "text": "Unsupervised machine translation has become an emerging research interest in recent years (Artetxe et al., 2017; Artetxe et al., 2018b; Marie and Fujita, 2018; Ren et al., 2019; Lample and Conneau, 2019) ."}
{"sent_id": "fa5413db2c8e0a32bc3805d25cd0e7-C001-215", "intents": ["@BACK@"], "paper_id": "ABC_fa5413db2c8e0a32bc3805d25cd0e7_16", "text": "More recently, Lample and Conneau (2019) reach new state-of-the-art performance on unsupervised en-fr and en-de translation tasks."}
{"sent_id": "fa5413db2c8e0a32bc3805d25cd0e7-C001-112", "intents": ["@DIF@"], "paper_id": "ABC_fa5413db2c8e0a32bc3805d25cd0e7_16", "text": "It seems that pre-training decoders has a smaller effect on the final performance than pre-training encoders (Lample and Conneau, 2019) , one reason for which could be that the encoder-to-decoder attention is not pre-trained."}
{"sent_id": "fa5413db2c8e0a32bc3805d25cd0e7-C001-154", "intents": ["@FUT@"], "paper_id": "ABC_fa5413db2c8e0a32bc3805d25cd0e7_16", "text": "The improvement made by Lample and Conneau (2019) compared with the first five baselines shows that cross-lingual pre-training can be necessary for unsupervised MT."}
{"sent_id": "43a52325987ea035136a6a718389d9-C001-83", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_43a52325987ea035136a6a718389d9_16", "text": "Following Sun and Korhonen (2009) we used the MNCut spectral clustering (Meila and Shi, 2001 ) which has a wide applicability and a clear probabilistic interpretation (von Luxburg, 2007; Verma and Meila, 2005) ."}
{"sent_id": "43a52325987ea035136a6a718389d9-C001-50", "intents": ["@SIM@"], "paper_id": "ABC_43a52325987ea035136a6a718389d9_16", "text": "Systems similar to ASSCI have been used in recent verb classification works e.g. (Schulte im Walde, 2006; Li and Brew, 2008; Sun and Korhonen, 2009 )."}
{"sent_id": "43a52325987ea035136a6a718389d9-C001-75", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_43a52325987ea035136a6a718389d9_16", "text": "We adopt a fully unsupervised approach to SP acquisition using the method of Sun and Korhonen (2009) , with the difference that we determine the optimal number of SP clusters automatically following Zelnik-Manor and Perona (2004) ."}
{"sent_id": "be67496882917c2a44afb42e6f9f15-C001-175", "intents": ["@BACK@"], "paper_id": "ABC_be67496882917c2a44afb42e6f9f15_16", "text": "Later on, many improvements were described in the Google neural machine translation system (Wu et al., 2016) , including utilizing coverage penalty (Tu et al., 2016) while decoding."}
{"sent_id": "be67496882917c2a44afb42e6f9f15-C001-168", "intents": ["@DIF@"], "paper_id": "ABC_be67496882917c2a44afb42e6f9f15_16", "text": "Previous work based on coverage based approaches (Tu et al., 2016; See et al., 2017) either imposed an extra term to the loss function or used an extra vector to keep track of which parts of the input sequences had been attended to, thereby focusing the attention weights in subsequent steps on tokens that received little attention before."}
{"sent_id": "ca7db62af4457ca887fe220c43b10e-C001-49", "intents": ["@DIF@"], "paper_id": "ABC_ca7db62af4457ca887fe220c43b10e_16", "text": "Unfortunately, the connection to Collobert and Weston (2008) was not recognized in either of these two studies; vice versa, neither of the above were referenced in Collobert et al. (2011) ."}
{"sent_id": "ca7db62af4457ca887fe220c43b10e-C001-12", "intents": ["@MOT@"], "paper_id": "ABC_ca7db62af4457ca887fe220c43b10e_16", "text": "Recently, Collobert et al. (2011) proposed \"deep architecture\" models for sequence labeling (named Sentence-level Likelihood Neural Nets, abbreviated as SLNN henceforth), and showed promising results on a range of tasks (POS tagging, NER, Chunking, and SRL)."}
{"sent_id": "ca7db62af4457ca887fe220c43b10e-C001-34", "intents": ["@SIM@"], "paper_id": "ABC_ca7db62af4457ca887fe220c43b10e_16", "text": "1 Normalizing locally in a logistic regression is equivalent to adding a softmax layer to the output layer of the IONN, which was commonly done in neural networks, such as in Collobert et al. (2011) ."}
{"sent_id": "ca7db62af4457ca887fe220c43b10e-C001-135", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_ca7db62af4457ca887fe220c43b10e_16", "text": "Four binary features are also appended to each word embedding to capture capitalization patterns, as described in Collobert et al. (2011) ."}
{"sent_id": "f3012301e42a4075ed6d4d2b39b528-C001-117", "intents": ["@DIF@"], "paper_id": "ABC_f3012301e42a4075ed6d4d2b39b528_16", "text": "Table 2 : Comparative results for Tweet-A using rule-based algorithm and statistical classifiers using our feature combinations 6 Evaluation Table 2 shows the performance of our classifiers in terms of Precision (P), Recall (R) and F-score Riloff et al. (2013) 's two rule-based algorithms: the ordered version predicts a tweet as sarcastic if it has a positive verb phrase followed by a negative situation/noun phrase, while the unordered does so if the two are present in any order."}
{"sent_id": "f3012301e42a4075ed6d4d2b39b528-C001-155", "intents": ["@DIF@"], "paper_id": "ABC_f3012301e42a4075ed6d4d2b39b528_16", "text": "Our system also outperforms two past works (Riloff et al., 2013; Maynard and Greenwood, 2014) with 10-20% improvement in F-score."}
{"sent_id": "f3012301e42a4075ed6d4d2b39b528-C001-43", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f3012301e42a4075ed6d4d2b39b528_16", "text": "Our feature engineering is based on Riloff et al. (2013) and Ramteke et al. (2013) ."}
{"sent_id": "c2952b2da147d5f128cdbd5d8074a5-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_c2952b2da147d5f128cdbd5d8074a5_16", "text": "Gonen and Goldberg (2019) contest the approaches to debiasing word embeddings presented by Bolukbasi et al. (2016b) and Zhao et al. (2018) , arguing that while the bias is reduced when measured according to its definition, i.e., dampening the impact of the general gender direction in the vector space, \"the actual effect is mostly hiding the bias, not removing it\"."}
{"sent_id": "f29baa099b13f38badeb4cbd8789f6-C001-46", "intents": ["@USE@"], "paper_id": "ABC_f29baa099b13f38badeb4cbd8789f6_17", "text": "In this paper, we train a readability classification model using a corpus compiled from textbooks and features inherited from our previous works Islam et al. (2012; and features from Sinha et al. (2012) ."}
{"sent_id": "f29baa099b13f38badeb4cbd8789f6-C001-213", "intents": ["@USE@"], "paper_id": "ABC_f29baa099b13f38badeb4cbd8789f6_17", "text": "Table 3 : Performance of Bangla readability models proposed by Sinha et al. (Sinha et al., 2012) ."}
{"sent_id": "f29baa099b13f38badeb4cbd8789f6-C001-224", "intents": ["@BACK@"], "paper_id": "ABC_f29baa099b13f38badeb4cbd8789f6_17", "text": "The traditional readability formulas that were proposed for English texts do not work for Bangla texts (Islam et al., 2012; Islam et al., 2014; Sinha et al., 2012) ."}
{"sent_id": "b0a50145121eb797cf8e6ebc2f49e0-C001-121", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_b0a50145121eb797cf8e6ebc2f49e0_17", "text": "(Refer to Algorithm 1 in Durrani et al. (2011) )."}
{"sent_id": "b0a50145121eb797cf8e6ebc2f49e0-C001-214", "intents": ["@USE@"], "paper_id": "ABC_b0a50145121eb797cf8e6ebc2f49e0_17", "text": "We follow the training steps described in Durrani et al. (2011) , consisting of i) post-processing the alignments to remove discontinuous and unaligned target cepts, ii) conversion of bilingual alignments into operation sequences, iii) estimation of the n-gram language models."}
{"sent_id": "b0a50145121eb797cf8e6ebc2f49e0-C001-38", "intents": ["@EXT@"], "paper_id": "ABC_b0a50145121eb797cf8e6ebc2f49e0_17", "text": "In this work, we extend the N-gram model, based on operation sequences (Durrani et al., 2011) , to use phrases during decoding."}
{"sent_id": "b0a50145121eb797cf8e6ebc2f49e0-C001-231", "intents": ["@DIF@"], "paper_id": "ABC_b0a50145121eb797cf8e6ebc2f49e0_17", "text": "We mark a result as sig-8 Discontinuous source-side units did not lead to any improvements in (Durrani et al., 2011) and increased the decoding times by multiple folds."}
{"sent_id": "fed51218e78d35aae39d287c95a95a-C001-53", "intents": ["@USE@"], "paper_id": "ABC_fed51218e78d35aae39d287c95a95a_17", "text": "With this definition we train a supervised classifier based on convolution kernels (Collins and Duffy, 2001 ) as this method has been shown to be quite effective for OH extraction (Wiegand and Klakow, 2010) ."}
{"sent_id": "fed51218e78d35aae39d287c95a95a-C001-61", "intents": ["@EXT@"], "paper_id": "ABC_fed51218e78d35aae39d287c95a95a_17", "text": "In addition to Wiegand and Klakow (2010) , we have to discard the content of candidate NPs (e.g. the candidate opinion holder NP [N P Cand [N N S advocates] ] is reduced to [N P Cand ]), the reason for this being that in our automatically generated training set, OHs will always be protoOHs."}
{"sent_id": "fed51218e78d35aae39d287c95a95a-C001-170", "intents": ["@DIF@"], "paper_id": "ABC_fed51218e78d35aae39d287c95a95a_17", "text": "As a maximum amount of labeled training data we chose 60000 instances (i.e. NPs) which is even a bit more than used in (Wiegand and Klakow, 2010) ."}
{"sent_id": "fed51218e78d35aae39d287c95a95a-C001-182", "intents": ["@SIM@"], "paper_id": "ABC_fed51218e78d35aae39d287c95a95a_17", "text": "This observation is consistent with (Wiegand and Klakow, 2010) where, however, AL and SL are considered for augmentation."}
{"sent_id": "c7778abb2f1890ba896ccef2c3e13b-C001-91", "intents": ["@USE@"], "paper_id": "ABC_c7778abb2f1890ba896ccef2c3e13b_17", "text": "Baseline 2: Lau et al. [16] : The authors proposed an unsupervised approach based on topic modeling for sense induction, and showed novel sense identification as one of its applications."}
{"sent_id": "730738d63cabcd4e63ec4300a8091b-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_730738d63cabcd4e63ec4300a8091b_17", "text": "In addition to reducing search errors compared to greedy search, it also enables the use of global models that accommodate richer non-local features without overfitting, leading to recent state-of-the-art accuracies of transition-based dependency parsing (Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012) that are competitive with the best graph-based dependency parsers."}
{"sent_id": "730738d63cabcd4e63ec4300a8091b-C001-17", "intents": ["@MOT@"], "paper_id": "ABC_730738d63cabcd4e63ec4300a8091b_17", "text": "For another example, it would be interesting to know whether a local, greedy, transition-based parser can be equipped with the rich features of Zhang and Nivre (2011) to improve its accuracy, and in particular whether MaltParser (Nivre et al., 2006) can achieve the same level of accuracies as ZPar (Zhang and Nivre, 2011) by using the same range of rich feature definitions."}
{"sent_id": "730738d63cabcd4e63ec4300a8091b-C001-21", "intents": ["@USE@"], "paper_id": "ABC_730738d63cabcd4e63ec4300a8091b_17", "text": "Second, we show that the accuracies of a local, greedy transition-based parser cannot be improved by adding the rich features of Zhang and Nivre (2011) ."}
{"sent_id": "f4792ef9808a1a3c415f6f57351335-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_f4792ef9808a1a3c415f6f57351335_17", "text": "Sporleder and Lapata (2005) also used the RST Treebank as training data for data-driven discourse parsing algorithms, though their focus, in contrast to Soricut and Marcu (2003) , was to avoid contextfree parsing and rely exclusively on features in their model that could be derived via finite-state chunkers and taggers."}
{"sent_id": "f4792ef9808a1a3c415f6f57351335-C001-20", "intents": ["@MOT@"], "paper_id": "ABC_f4792ef9808a1a3c415f6f57351335_17", "text": "In this paper we take up the question posed by the results of Sporleder and Lapata (2005) : how much, if any, accuracy reduction should we expect if we choose to use only finite-state derived features, rather than those derived from full contextfree parses? If little accuracy is lost, as their results suggest, then it would make sense to avoid relatively expensive context-free parsing, particularly if the amount of text to be processed is large or if there are real-time processing constraints on the system."}
{"sent_id": "f4792ef9808a1a3c415f6f57351335-C001-76", "intents": ["@USE@"], "paper_id": "ABC_f4792ef9808a1a3c415f6f57351335_17", "text": "Table 1 compares segmentation results of three systems on the Sporleder and Lapata (2005) 608 sentence subset of the evaluation data: (1) their best reported system; (2) the SPADE system results reported in that paper; and (3) the SPADE system results with our current configuration."}
{"sent_id": "e3c735811b2ea08d92659272ddcbdd-C001-74", "intents": ["@BACK@"], "paper_id": "ABC_e3c735811b2ea08d92659272ddcbdd_17", "text": "In our previous work, we observed that gang members use curse words nearly five times more than the average curse words use on Twitter [BWDS16] ."}
{"sent_id": "e3c735811b2ea08d92659272ddcbdd-C001-148", "intents": ["@BACK@"], "paper_id": "ABC_e3c735811b2ea08d92659272ddcbdd_17", "text": "Specific details about our data curation procedure are discussed in [BWDS16] ."}
{"sent_id": "e3c735811b2ea08d92659272ddcbdd-C001-145", "intents": ["@USE@"], "paper_id": "ABC_e3c735811b2ea08d92659272ddcbdd_17", "text": "We consider a dataset of curated gang and non-gang members' Twitter profiles collected from our previous work [BWDS16] ."}
{"sent_id": "ae67018df3a74e0fd4ae90522499a3-C001-15", "intents": ["@MOT@"], "paper_id": "ABC_ae67018df3a74e0fd4ae90522499a3_17", "text": "Although a significant number of studies (e.g. (Huang et al., 2013; Ganguly et al., 2015; Zheng and Callan, 2015; Guo et al., 2016; Zamani and Croft, 2016; Dehghani et al., 2017; ) try to apply neural networks in IR, there have been few studies reporting the performance that is comparable to state-of-the-art IR models."}
{"sent_id": "ae67018df3a74e0fd4ae90522499a3-C001-136", "intents": ["@SIM@"], "paper_id": "ABC_ae67018df3a74e0fd4ae90522499a3_17", "text": "These collections have been broadly used in recent studies (Zheng and Callan, 2015; Guo et al., 2016; Dehghani et al., 2017) ."}
{"sent_id": "9dd9ac975c6f55797615f0e52aa296-C001-87", "intents": ["@USE@"], "paper_id": "ABC_9dd9ac975c6f55797615f0e52aa296_17", "text": "Following the previous studies (Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015b) , we evaluate the models on the ACE 2005 corpus with 33 event subtypes."}
{"sent_id": "8084b5077b2a8db755b1bbd0f6fe60-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_8084b5077b2a8db755b1bbd0f6fe60_17", "text": "As Baldwin and Lui (2010) or Tiedemann and Ljubešić (2012) point out, language identification is erroneously considered an easy and solved problem 2 , in part because of some general purpose systems being available, notably TextCat 3 , Xerox Language Identifier 4 and, more recently, langid.py (Lui and Baldwin, 2012) ."}
{"sent_id": "8084b5077b2a8db755b1bbd0f6fe60-C001-131", "intents": ["@SIM@"], "paper_id": "ABC_8084b5077b2a8db755b1bbd0f6fe60_17", "text": "Moreover, the classifier could be used, as Tiedemann and Ljubešić (2012) suggest, to learn varieties discriminators to label texts beyond national classes (e.g. both Caribbean and Andean Spanish cross-cut national borders and, conversely, nations involved are known not to be dialectally uniform)."}
{"sent_id": "8084b5077b2a8db755b1bbd0f6fe60-C001-117", "intents": ["@DIF@"], "paper_id": "ABC_8084b5077b2a8db755b1bbd0f6fe60_17", "text": "Despite findings by Tiedemann and Ljubešić (2012) , character n-grams performed better during tenfold cross-validation on the training dataset for different feature settings on the DSL dataset for group A (Bosnian, Croatian and Serbian)."}
{"sent_id": "8084b5077b2a8db755b1bbd0f6fe60-C001-137", "intents": ["@USE@"], "paper_id": "ABC_8084b5077b2a8db755b1bbd0f6fe60_17", "text": "Following the suggestion of Tiedemann and Ljubešić (2012) , we envisage the use of parallel texts such as versions of the Bible from different areas to learn the differences among varieties."}
{"sent_id": "2a84615479af66bbf875517a3a753b-C001-33", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_2a84615479af66bbf875517a3a753b_17", "text": "In our previous work [7] , we applied a dual RNN in order to obtain a richer representation by blending the content and acoustic knowledge."}
{"sent_id": "2a84615479af66bbf875517a3a753b-C001-64", "intents": ["@MOT@"], "paper_id": "ABC_2a84615479af66bbf875517a3a753b_17", "text": "Motivated by the architecture used in [7, 17, 19] , we train a recurrent encoder to predict the categorical class of a given audio signal."}
{"sent_id": "2a84615479af66bbf875517a3a753b-C001-106", "intents": ["@USE@"], "paper_id": "ABC_2a84615479af66bbf875517a3a753b_17", "text": "For consistent comparison with previous works [7, 18] , all utterances labeled \"excitement\" are merged with those labeled \"happiness\"."}
{"sent_id": "d2b9c678a3d4920919f59c3b5903d3-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_d2b9c678a3d4920919f59c3b5903d3_17", "text": "where Y(x) lists all possible structures of x. If we think of a tree (x, y) as a sequence (z) (Vinyals et al., 2015) as illustrated in Figure 1 , we can define a probability distribution over (x, y) as follows:"}
{"sent_id": "bd2a718f75d206ef3f2cb5648585d5-C001-130", "intents": ["@USE@"], "paper_id": "ABC_bd2a718f75d206ef3f2cb5648585d5_17", "text": "We emulated the expert annotation process carried out by Wachsmuth et al. (2017a) on CrowdFlower in order to evaluate whether lay annotators suffice for a theory-based quality assessment."}
{"sent_id": "489d0077e05269327e7fe4e7f7e4a3-C001-71", "intents": ["@DIF@"], "paper_id": "ABC_489d0077e05269327e7fe4e7f7e4a3_17", "text": "We chose 512-dimensional word embedding for our model with self-attention, whereas [17] and [3] chose a vector length of 300."}
{"sent_id": "489d0077e05269327e7fe4e7f7e4a3-C001-137", "intents": ["@UNSURE@"], "paper_id": "ABC_489d0077e05269327e7fe4e7f7e4a3_17", "text": "MADiMa '19, October 21, 2019, Nice, France [17] and AdaMine [3] re-implementation."}
{"sent_id": "4e1b01c1faebc447891bc0b847316d-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_4e1b01c1faebc447891bc0b847316d_17", "text": "Lau et al. (2014) proposed an automated approach to the word intrusion task."}
{"sent_id": "4e1b01c1faebc447891bc0b847316d-C001-86", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_4e1b01c1faebc447891bc0b847316d_17", "text": "Newman et al. (2010) found PMI to be the best association measure, and later studies (Aletras and Stevenson, 2013; Lau et al., 2014) found that normalised PMI (NPMI: Bouma (2009)) improves PMI further."}
{"sent_id": "4e1b01c1faebc447891bc0b847316d-C001-29", "intents": ["@USE@"], "paper_id": "ABC_4e1b01c1faebc447891bc0b847316d_17", "text": "We experiment with the automatic word intrusion (Lau et al., 2014) and discover that correlation with human ratings decreases systematically as cardinality increases."}
{"sent_id": "4e1b01c1faebc447891bc0b847316d-C001-76", "intents": ["@SIM@"], "paper_id": "ABC_4e1b01c1faebc447891bc0b847316d_17", "text": "This result is consistent with previous studies (Lau et al., 2014) ."}
{"sent_id": "197b557d7b5c7c2d195be84990719b-C001-34", "intents": ["@EXT@"], "paper_id": "ABC_197b557d7b5c7c2d195be84990719b_17", "text": "We are interested in modifying the Skipgram Negative-Sampling (SGNS) objective in (Mikolov et al., 2013) to utilize document-wide feature vectors while simultaneously learning continuous document weights loading onto topic vectors."}
{"sent_id": "197b557d7b5c7c2d195be84990719b-C001-154", "intents": ["@EXT@"], "paper_id": "ABC_197b557d7b5c7c2d195be84990719b_17", "text": "This work demonstrates a simple model, lda2vec, that extends SGNS (Mikolov et al., 2013) to build unsupervised document representations that yield coherent topics."}
{"sent_id": "197b557d7b5c7c2d195be84990719b-C001-47", "intents": ["@USE@"], "paper_id": "ABC_197b557d7b5c7c2d195be84990719b_17", "text": "Unless stated otherwise, the negative sampling power beta is set to 3/4 and the number of negative samples is fixed to n = 15 as in Mikolov et al. (2013) ."}
{"sent_id": "197b557d7b5c7c2d195be84990719b-C001-43", "intents": ["@DIF@"], "paper_id": "ABC_197b557d7b5c7c2d195be84990719b_17", "text": "Each word is represented with a fixedlength dense distributed-representation vector, but unlike Mikolov et al. (2013) the same word vectors are used in both the pivot and target representations."}
{"sent_id": "9c5baf669470fe4dd18277591591f1-C001-11", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9c5baf669470fe4dd18277591591f1_17", "text": "The analyses have focused on wordlevel models, yet character-level models have been shown to outperform word-level models in some NLP tasks, such as text classification (Zhang et al., 2015) , named entity recognition (Kuru et al., 2016) , and time normalization (Laparra et al., 2018a) ."}
{"sent_id": "9c5baf669470fe4dd18277591591f1-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_9c5baf669470fe4dd18277591591f1_17", "text": "There are three types of outputs per Laparra et al. (2018a) 's encoding of the SCATE schema, so there is a separate stack of bi-GRUs and a softmax for each output type."}
{"sent_id": "9c5baf669470fe4dd18277591591f1-C001-40", "intents": ["@EXT@"], "paper_id": "ABC_9c5baf669470fe4dd18277591591f1_17", "text": "We keep the original neural architecture and parameter settings in Laparra et al. (2018a) , and experiment with the following embedding layers: Rand(128): the original setting of Laparra et al. (2018a) , where 128-dimensional character embeddings are randomly initialized."}
{"sent_id": "397e593f8f282d4951402d83036c12-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_397e593f8f282d4951402d83036c12_17", "text": "Capable of capturing longdistance dependencies with gating (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) and attention (Bahdanau et al., 2015) mechanisms, NMT has proven to outperform conventional statistical machine translation systematically across a variety of language pairs (Junczys-Dowmunt et al., 2016) ."}
{"sent_id": "f633ceffdf53849159574a2891eda1-C001-29", "intents": ["@USE@"], "paper_id": "ABC_f633ceffdf53849159574a2891eda1_17", "text": "We use the same exact setup as the \"paragraph\" variant of BERTserini (Yang et al., 2019) , where the input corpus is pre-segmented into paragraphs at index time, each of which is treated as a \"document\" for retrieval purposes."}
{"sent_id": "f633ceffdf53849159574a2891eda1-C001-35", "intents": ["@DIF@"], "paper_id": "ABC_f633ceffdf53849159574a2891eda1_17", "text": "One major shortcoming with BERTserini is that Yang et al. (2019) only fine tune on SQuAD, which means that the BERT reader is exposed to an impoverished set of examples; all SQuAD data come from a total of only 442 documents."}
{"sent_id": "04461d946dadc759e4be1207655159-C001-23", "intents": ["@SIM@"], "paper_id": "ABC_04461d946dadc759e4be1207655159_18", "text": "Like Chiang (2005) and Chiang (2007) , our HD-HPB translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar."}
{"sent_id": "04461d946dadc759e4be1207655159-C001-36", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_04461d946dadc759e4be1207655159_18", "text": "This is the same as the hierarchical rules defined in Chiang's HPB model (Chiang, 2007) , except that we use head POSinformed non-terminal symbols in the source language."}
{"sent_id": "04461d946dadc759e4be1207655159-C001-38", "intents": ["@DIF@"], "paper_id": "ABC_04461d946dadc759e4be1207655159_18", "text": "Given the word alignment in Figure 1 , Table 1 demonstrates the difference between hierarchical rules in Chiang (2007) and HD-HRs defined here."}
{"sent_id": "04461d946dadc759e4be1207655159-C001-67", "intents": ["@UNSURE@"], "paper_id": "ABC_04461d946dadc759e4be1207655159_18", "text": "We evaluate the performance of our HD-HPB model and compare it with our implementation of Chiang's HPB model (Chiang, 2007) , a source-side SAMTstyle refined version of HPB (SAMT-HPB), and the Moses implementation of HPB."}
{"sent_id": "af9b884710f8198f008a9687153db6-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_af9b884710f8198f008a9687153db6_18", "text": "Several different neural network methods have been proven to be effective for NER (Collobert et al., 2011; Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016) ."}
{"sent_id": "af9b884710f8198f008a9687153db6-C001-18", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_af9b884710f8198f008a9687153db6_18", "text": "2 Word-level Neural CRF As a baseline method, we use word-level neural CRF proposed by (Ma and Hovy, 2016) since their method achieves state-of-the-art performance on NER."}
{"sent_id": "af9b884710f8198f008a9687153db6-C001-87", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_af9b884710f8198f008a9687153db6_18", "text": "To generate a segment lattice, we train word-level BLSTM-CNN with the same hyper-parameters used in Ma and Hovy (2016) level CNN, and 100 dimentional pre-trained word embedding of GloVe (Pennington et al., 2014) ."}
{"sent_id": "af9b884710f8198f008a9687153db6-C001-29", "intents": ["@DIF@"], "paper_id": "ABC_af9b884710f8198f008a9687153db6_18", "text": "Then, A ∈ R |T |×|T | is a transition score matrix, A y i−1 ,y i is a transition 1 While (Ma and Hovy, 2016) define ϕ(yi−1, yi, oi) = exp(Wy i−1 ,y i oi + Ay i−1 ,y i ) as the potential function where W is the weight vector corresponding to label pair (yi−1, yi), we use the simple potential function here."}
{"sent_id": "975413dd6b3d3df9c5d111d94e8eb7-C001-53", "intents": ["@USE@"], "paper_id": "ABC_975413dd6b3d3df9c5d111d94e8eb7_18", "text": "Following (Su et al., 2018) , we then collect the global co-occurrence statistics of textual and KB relations."}
{"sent_id": "975413dd6b3d3df9c5d111d94e8eb7-C001-99", "intents": ["@MOT@"], "paper_id": "ABC_975413dd6b3d3df9c5d111d94e8eb7_18", "text": "As shown in previous work (Su et al., 2018) , on NYT dataset, due to a significant amount of false negatives, the PR curve on the held-out set may not be an accurate measure of performance."}
{"sent_id": "45d4d6f0ac4a4f3bf7b2ac70fbcf7f-C001-43", "intents": ["@MOT@"], "paper_id": "ABC_45d4d6f0ac4a4f3bf7b2ac70fbcf7f_18", "text": "Although capable of classifying utterances with CDAs, Qu et al. [14] 's model only concerns a strictly-local context range and thus cannot include distant information."}
{"sent_id": "45d4d6f0ac4a4f3bf7b2ac70fbcf7f-C001-104", "intents": ["@USE@"], "paper_id": "ABC_45d4d6f0ac4a4f3bf7b2ac70fbcf7f_18", "text": "We use the MSDialog-Intent dataset [14] to conduct experiments."}
{"sent_id": "d9c0e641f8ceb61e5d6e416bfc6492-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_d9c0e641f8ceb61e5d6e416bfc6492_18", "text": "Nilsson et al. (2007) modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy."}
{"sent_id": "d9c0e641f8ceb61e5d6e416bfc6492-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_d9c0e641f8ceb61e5d6e416bfc6492_18", "text": "Nilsson et al. (2007) show that making the auxiliary the head of the dependency as in Figure 2 is useful for parsing Czech and Slovenian."}
{"sent_id": "d9c0e641f8ceb61e5d6e416bfc6492-C001-20", "intents": ["@USE@"], "paper_id": "ABC_d9c0e641f8ceb61e5d6e416bfc6492_18", "text": "In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by Nilsson et al. (2007) on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD."}
{"sent_id": "f2db88c0d4e0ec4c34fc295a5d59ba-C001-2", "intents": ["@UNSURE@"], "paper_id": "ABC_f2db88c0d4e0ec4c34fc295a5d59ba_18", "text": "We show that the automatically induced latent variable grammars of Petrov et al. (2006) vary widely in their underlying representations, depending on their EM initialization point."}
{"sent_id": "f2db88c0d4e0ec4c34fc295a5d59ba-C001-32", "intents": ["@UNSURE@"], "paper_id": "ABC_f2db88c0d4e0ec4c34fc295a5d59ba_18", "text": "We focus on the randomness introduced by the EM algorithm and refer the reader to Matsuzaki et al. (2005) and Petrov et al. (2006) for a more general introduction."}
{"sent_id": "f2db88c0d4e0ec4c34fc295a5d59ba-C001-128", "intents": ["@BACK@"], "paper_id": "ABC_f2db88c0d4e0ec4c34fc295a5d59ba_18", "text": "Using weights learned on a held-out set and rescoring 50-best lists from Charniak (2000) and Petrov et al. (2006) , they obtain an F 1 score of 91.0 (which they further improve to 91.4 using a voting scheme)."}
{"sent_id": "f2db88c0d4e0ec4c34fc295a5d59ba-C001-46", "intents": ["@DIF@"], "paper_id": "ABC_f2db88c0d4e0ec4c34fc295a5d59ba_18", "text": "While the split&merge procedure described above is shown in Petrov et al. (2006) to reduce the variance in final performance, we found after closer examination that there are substantial differences in the patterns learned by the grammars."}
{"sent_id": "e7b1c00e747f5bfbb96499d7223496-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_e7b1c00e747f5bfbb96499d7223496_18", "text": "As explained in Lazaridou et al. (2015) , the majority of this literature focuses on constructing textual and visual representations independently and then combining them under some metrics."}
{"sent_id": "e7b1c00e747f5bfbb96499d7223496-C001-107", "intents": ["@DIF@"], "paper_id": "ABC_e7b1c00e747f5bfbb96499d7223496_18", "text": "In the cases of capturing general relatedness and pure visual similarity, the multimodal model of Lazaridou et al. (2015) performs better."}
{"sent_id": "9d1699d4ca3b4026ed5aab125a737d-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_9d1699d4ca3b4026ed5aab125a737d_18", "text": "In proposing bigrams as concepts for their system, Gillick and Favre (2009) explain that:"}
{"sent_id": "67b6d87aa2a943a854251fada6e183-C001-57", "intents": ["@USE@", "@BACK@", "@SIM@", "@MOT@"], "paper_id": "ABC_67b6d87aa2a943a854251fada6e183_18", "text": "Han et al. (2012) found that using feature selection to identify \"location indicative words\" led to improvements in geolocation performance."}
{"sent_id": "67b6d87aa2a943a854251fada6e183-C001-83", "intents": ["@USE@"], "paper_id": "ABC_67b6d87aa2a943a854251fada6e183_18", "text": "We base our evaluation on the publicly-available WORLD dataset of Han et al. (2012) ."}
{"sent_id": "67b6d87aa2a943a854251fada6e183-C001-51", "intents": ["@EXT@"], "paper_id": "ABC_67b6d87aa2a943a854251fada6e183_18", "text": "As such, we build off the text-based naive Bayes-based geolocation system of Han et al. (2012) , which our experiments have shown to have a good balance of tractability and accuracy."}
{"sent_id": "67b6d87aa2a943a854251fada6e183-C001-96", "intents": ["@SIM@"], "paper_id": "ABC_67b6d87aa2a943a854251fada6e183_18", "text": "Of the two original models, we can see that MB is comparable to KL, in line with the findings of Han et al. (2012) ."}
{"sent_id": "43622e43d6ef5291b64320d2d68b95-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_43622e43d6ef5291b64320d2d68b95_18", "text": "Self-attention networks (SANs) (Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017) , natural language inference (Shen et al., 2018a) , and acoustic modeling (Sperber et al., 2018) ."}
{"sent_id": "43622e43d6ef5291b64320d2d68b95-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_43622e43d6ef5291b64320d2d68b95_18", "text": "where ATT(·) is an attention model (Bahdanau et al., 2015; Vaswani et al., 2017) that retrieves the keys K h with the query q h i ."}
{"sent_id": "43622e43d6ef5291b64320d2d68b95-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_43622e43d6ef5291b64320d2d68b95_18", "text": "Experimental results demonstrate that our approach consistently improves performance over the strong TRANSFORMER model (Vaswani et al., 2017) across language pairs."}
{"sent_id": "b49e6f8181d51a998c6c27a830b98e-C001-101", "intents": ["@USE@"], "paper_id": "ABC_b49e6f8181d51a998c6c27a830b98e_18", "text": "Our current work was limited to English compounds from Reddy et al. (2011) ."}
{"sent_id": "b49e6f8181d51a998c6c27a830b98e-C001-86", "intents": ["@DIF@"], "paper_id": "ABC_b49e6f8181d51a998c6c27a830b98e_18", "text": "We speculate that these differences are potentially due to the use of different data sets, the fact that we use a considerably smaller context window for constructing the word vectors (5 due to the restrictions of Google Ngram corpus vs. 100 in Reddy et al. (2011) and 40 in Schulte im Walde et al. (2016b) ) and the use of a compound-centric setting (as described in 4.1)."}
{"sent_id": "878c6cf1c47c86f36a7ff3f04e2998-C001-78", "intents": ["@SIM@"], "paper_id": "ABC_878c6cf1c47c86f36a7ff3f04e2998_18", "text": "Same as Thorne et al. (2018) , we use the decomposable attention (DA) between the claim and the evidence for RTE."}
{"sent_id": "878c6cf1c47c86f36a7ff3f04e2998-C001-134", "intents": ["@SIM@"], "paper_id": "ABC_878c6cf1c47c86f36a7ff3f04e2998_18", "text": "Most closely related to our work, Thorne et al. (2018) addresses large-scale fact extraction and verification task using a pipeline approach."}
{"sent_id": "878c6cf1c47c86f36a7ff3f04e2998-C001-96", "intents": ["@USE@"], "paper_id": "ABC_878c6cf1c47c86f36a7ff3f04e2998_18", "text": "For the final fullpipeline, we compare to and follow the metric defined in Thorne et al. (2018) ."}
{"sent_id": "3bc48bea420e4977027832240450ec-C001-117", "intents": ["@BACK@"], "paper_id": "ABC_3bc48bea420e4977027832240450ec_18", "text": "One major difficulty with the description of the method in the paper and re-implementation is handling the same target multiple appearances issue as originally raised by Wang et al. (2017) ."}
{"sent_id": "af6c68ef5f80eac2274bf33a894d1f-C001-2", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_af6c68ef5f80eac2274bf33a894d1f_18", "text": "We replicate the syntactic experiments of Mikolov et al. (2013b) on English, and expand them to include morphologically complex languages."}
{"sent_id": "af6c68ef5f80eac2274bf33a894d1f-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_af6c68ef5f80eac2274bf33a894d1f_18", "text": "1 Introduction Mikolov et al. (2013b) demonstrate that vector representations of words obtained from a neural network language model provide a way of capturing both semantic and syntactic regularities in language."}
{"sent_id": "af6c68ef5f80eac2274bf33a894d1f-C001-20", "intents": ["@MOT@"], "paper_id": "ABC_af6c68ef5f80eac2274bf33a894d1f_18", "text": "For comparison with the results of Mikolov et al. (2013b) , we limit the data to the first 320M lowercased tokens of the corpus."}
{"sent_id": "3356313ee5cdf186816cd6fecfce84-C001-72", "intents": ["@BACK@"], "paper_id": "ABC_3356313ee5cdf186816cd6fecfce84_18", "text": "DSConv have been successfully applied to the domain of computer vision [8, 13] , neural translation [7] and KWS [2] ."}
{"sent_id": "3356313ee5cdf186816cd6fecfce84-C001-54", "intents": ["@UNSURE@"], "paper_id": "ABC_3356313ee5cdf186816cd6fecfce84_18", "text": "Regarding real-time capability, our model is designed to operate on a single-core microcontroller capable of 50 MOps per second [2] ."}
{"sent_id": "3356313ee5cdf186816cd6fecfce84-C001-109", "intents": ["@DIF@"], "paper_id": "ABC_3356313ee5cdf186816cd6fecfce84_18", "text": "Compared to the DSConv network in [2] , our network is more efficient in terms of accuracy for a given parameter count."}
{"sent_id": "08d3f7a0938ab85d9a251b6a2364ed-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_08d3f7a0938ab85d9a251b6a2364ed_18", "text": "A key observation is that a degree-D Attardi system does not contain all possible transitions of degree within D. Since prior empirical work has ascertained that transition systems using more transitions with degree greater than 1 can handle more non-projective treebank trees (Attardi, 2006; Gómez-Rodríguez, 2016) , we hypothesize that adding some of these \"missing\" reduce transitions into the system's inventory should increase coverage."}
{"sent_id": "08d3f7a0938ab85d9a251b6a2364ed-C001-27", "intents": ["@UNSURE@"], "paper_id": "ABC_08d3f7a0938ab85d9a251b6a2364ed_18", "text": "Having one has the practical advantage of allowing generative models, as in Cohen et al. (2011) , and transition-based scoring functions, which have yielded good projective-parsing results (Shi et al., 2017) Attardi's (2006) transition system of degree 2 and our variants."}
{"sent_id": "08d3f7a0938ab85d9a251b6a2364ed-C001-31", "intents": ["@UNSURE@"], "paper_id": "ABC_08d3f7a0938ab85d9a251b6a2364ed_18", "text": "Thick arrows and gray dotted arrows represent additional and deleted transitions with respect to the original Attardi (2006) system."}
{"sent_id": "08d3f7a0938ab85d9a251b6a2364ed-C001-91", "intents": ["@USE@"], "paper_id": "ABC_08d3f7a0938ab85d9a251b6a2364ed_18", "text": "On the other hand, since their addition doesn't affect the asymptotic run-time, we define ALLDEG1 to include all five degree-1 transitions from R into the Attardi (2006) system."}
{"sent_id": "123d8e8ddef15fed120908c5c20656-C001-53", "intents": ["@DIF@"], "paper_id": "ABC_123d8e8ddef15fed120908c5c20656_18", "text": "For both architectures, we also propose to keep this low-pass filter fixed while learning the convolution filter weights, a setting that was not explored by Zeghidour et al. [8] , who learnt the lowpass filter weights when randomly initializing the convolutions."}
{"sent_id": "123d8e8ddef15fed120908c5c20656-C001-133", "intents": ["@DIF@"], "paper_id": "ABC_123d8e8ddef15fed120908c5c20656_18", "text": "More importantly, using either an Han-fixed or Han-learnt filter when learning scatteringbased filterbanks from a random initialization removes the gap in performance with the Gabor wavelet initialization that was observed in [8] where the lowpass filter was also initialized randomly."}
{"sent_id": "0f66e9a5c51cff004d97e4aaddf4d0-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_0f66e9a5c51cff004d97e4aaddf4d0_18", "text": "While previous methods train sentence embeddings in an unsupervised manner, a recent work (Conneau et al., 2017) argued that better representations can be achieved via supervised training on a general sentence inference dataset (Bowman et al., 2015) ."}
{"sent_id": "0f66e9a5c51cff004d97e4aaddf4d0-C001-26", "intents": ["@USE@"], "paper_id": "ABC_0f66e9a5c51cff004d97e4aaddf4d0_18", "text": "BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of (Conneau et al., 2017) which is the baseline for our work."}
{"sent_id": "0f66e9a5c51cff004d97e4aaddf4d0-C001-78", "intents": ["@UNSURE@"], "paper_id": "ABC_0f66e9a5c51cff004d97e4aaddf4d0_18", "text": "All other hyper-parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of (Conneau et al., 2017) ."}
{"sent_id": "0f66e9a5c51cff004d97e4aaddf4d0-C001-80", "intents": ["@UNSURE@"], "paper_id": "ABC_0f66e9a5c51cff004d97e4aaddf4d0_18", "text": "We compared out method against the baseline BiL-STM implementation of (Conneau et al., 2017) and included FastSent (Hill et al., 2016) and SkipThought vectors (Kiros et al., 2015) as a reference."}
{"sent_id": "0f66e9a5c51cff004d97e4aaddf4d0-C001-86", "intents": ["@MOT@"], "paper_id": "ABC_0f66e9a5c51cff004d97e4aaddf4d0_18", "text": "Leveraging supervision given by some general task aided in obtaining state-of-the-art sentence representations (Conneau et al., 2017) ."}
{"sent_id": "759c1c892361f62ad8f2c46e569e8a-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_759c1c892361f62ad8f2c46e569e8a_18", "text": "However according to a recent comparison (Casanueva et al., 2017) in the context of dialog policy learning, it performed worse than other RL methods such as Gaussian Process in many testing conditions."}
{"sent_id": "759c1c892361f62ad8f2c46e569e8a-C001-71", "intents": ["@USE@"], "paper_id": "ABC_759c1c892361f62ad8f2c46e569e8a_18", "text": "Evaluation results in Casanueva et al. (2017) with several dialog policy types, e.g. a handcrafted policy and the best reported policies serve as baselines in our experiments."}
{"sent_id": "7293ab5db16d3fe1fee48d45154697-C001-16", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_7293ab5db16d3fe1fee48d45154697_19", "text": "For our word-level submissions we have applied the approach proposed by Esplà-Gomis et al. (2015) , where we used black-box bilingual on-line resources."}
{"sent_id": "7293ab5db16d3fe1fee48d45154697-C001-85", "intents": ["@SIM@"], "paper_id": "ABC_7293ab5db16d3fe1fee48d45154697_19", "text": "Incidentally, and in spite of the changes in languages and machine translation systems, the results obtained for word-level MTQE are very similar to those obtained by Esplà-Gomis et al. (2015) for the translation from English into Spanish."}
{"sent_id": "1c0d971cf771f351b51661950f4b14-C001-49", "intents": ["@SIM@"], "paper_id": "ABC_1c0d971cf771f351b51661950f4b14_19", "text": "Training Data We use comparable Wikipedia data introduced in (Vulić and Moens, 2013a; Vulić and Moens, 2013b ) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18, 898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (NL-EN)."}
{"sent_id": "1c0d971cf771f351b51661950f4b14-C001-65", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_1c0d971cf771f351b51661950f4b14_19", "text": "The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; Vulić and Moens, 2013b) ."}
{"sent_id": "1c0d971cf771f351b51661950f4b14-C001-51", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_1c0d971cf771f351b51661950f4b14_19", "text": "Following prior work (Haghighi et al., 2008; Prochasson and Fung, 2011; Vulić and Moens, 2013b) , we retain only nouns that occur at least 5 times in the corpus."}
{"sent_id": "1c0d971cf771f351b51661950f4b14-C001-67", "intents": ["@BACK@"], "paper_id": "ABC_1c0d971cf771f351b51661950f4b14_19", "text": "Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; Tamura et al., 2012; Vulić and Moens, 2013a; Vulić and Moens, 2013b) ."}
{"sent_id": "1c0d971cf771f351b51661950f4b14-C001-103", "intents": ["@MOT@"], "paper_id": "ABC_1c0d971cf771f351b51661950f4b14_19", "text": "Finally, we may use the knowledge of BWEs obtained by BWESG from document-aligned data to learn bilingual correspondences (e.g., word translation pairs or lists of semantically similar words across languages) which may in turn be used for representation learning from large unaligned multilingual datasets as proposed in (Haghighi et al., 2008; Mikolov et al., 2013b; Vulić and Moens, 2013b) ."}
{"sent_id": "f5d1c0d3ac45ea4949f7d01d1704f6-C001-41", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_f5d1c0d3ac45ea4949f7d01d1704f6_19", "text": "However, the proposed techniques extend to non-linear hypotheses, as mentioned in (Daumé III, 2007) ."}
{"sent_id": "f5d1c0d3ac45ea4949f7d01d1704f6-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_f5d1c0d3ac45ea4949f7d01d1704f6_19", "text": "A good intuitive insight into why this simple algorithm works so well in practice and outperforms most state-of-the-art algorithms is given in (Daumé III, 2007) ."}
{"sent_id": "f5d1c0d3ac45ea4949f7d01d1704f6-C001-138", "intents": ["@SIM@"], "paper_id": "ABC_f5d1c0d3ac45ea4949f7d01d1704f6_19", "text": "We also note that EA performs poorly for some cases, as was shown (Daumé III, 2007) earlier."}
{"sent_id": "c2bfe3534597a8f192ec846619f6b1-C001-16", "intents": ["@MOT@"], "paper_id": "ABC_c2bfe3534597a8f192ec846619f6b1_19", "text": "Therefore, mapping documents in different languages into a common latent topic space can be of great benefit when detecting document translation pairs (Mimno et al., 2009; Platt et al., 2010) ."}
{"sent_id": "c2bfe3534597a8f192ec846619f6b1-C001-122", "intents": ["@DIF@"], "paper_id": "ABC_c2bfe3534597a8f192ec846619f6b1_19", "text": "In addition, we show how the results as reported by Platt et al. (2010) can be obtained using the PLTM representation with a significant speed improvement."}
{"sent_id": "c2bfe3534597a8f192ec846619f6b1-C001-148", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_c2bfe3534597a8f192ec846619f6b1_19", "text": "On the same data set, (Platt et al., 2010) report accuracy of 98.9% using 50 topics, a slightly different prior distribution, and MAP instead of posterior inference."}
{"sent_id": "8ca479895b028ea6dedb0e99cacae6-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_8ca479895b028ea6dedb0e99cacae6_19", "text": "Most rely on convolutional neural nets (Zeng et al., 2014 (Zeng et al., , 2015 Grishman, 2015, 2016; Fu et al., 2017) or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations."}
{"sent_id": "faeac0a0e3c0cad79d39dea04ec59a-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_faeac0a0e3c0cad79d39dea04ec59a_19", "text": "Furthermore, while some argue that detailed guidelines can help annotators to make more subtle distinctions (Davidson et al., 2017) , others find that they do not improve the reliability of non-expert classifications (Ross et al., 2016) ."}
{"sent_id": "faeac0a0e3c0cad79d39dea04ec59a-C001-82", "intents": ["@BACK@"], "paper_id": "ABC_faeac0a0e3c0cad79d39dea04ec59a_19", "text": "A number of studies on hate speech use part-of-speech sequences to model the expression of hatred (Warner and Hirschberg, 2012; Gitari et al., 2015; Davidson et al., 2017) ."}
{"sent_id": "a6954db741df61f014cc622c5b8263-C001-220", "intents": ["@BACK@"], "paper_id": "ABC_a6954db741df61f014cc622c5b8263_19", "text": "This is a partial explanation of the utility of verbal distance in Collins (1999) ."}
{"sent_id": "a6954db741df61f014cc622c5b8263-C001-45", "intents": ["@DIF@"], "paper_id": "ABC_a6954db741df61f014cc622c5b8263_19", "text": "Specifically, we construct an unlexicalized PCFG which outperforms the lexicalized PCFGs of Magerman (1995) and Collins (1996) (though not more recent models, such as Charniak (1997) or Collins (1999) )."}
{"sent_id": "a6954db741df61f014cc622c5b8263-C001-91", "intents": ["@SIM@"], "paper_id": "ABC_a6954db741df61f014cc622c5b8263_19", "text": "The raw treebank grammar corresponds to v = 1, h = ∞ (the upper right corner), while the parent annotation in (Johnson, 1998) corresponds to v = 2, h = ∞, and the second-order model in Collins (1999) , is broadly a smoothed version of v = 2, h = 2."}
{"sent_id": "bb133ba3dfe483412672b44b777c4a-C001-10", "intents": ["@MOT@"], "paper_id": "ABC_bb133ba3dfe483412672b44b777c4a_19", "text": "Neural machine translation has achieved great success in the last few years (Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) ."}
{"sent_id": "bb133ba3dfe483412672b44b777c4a-C001-11", "intents": ["@MOT@"], "paper_id": "ABC_bb133ba3dfe483412672b44b777c4a_19", "text": "The Transformer (Vaswani et al., 2017) , which has outperformed previous RNN/CNN based translation models (Bahdanau et al., 2014; Gehring et al., 2017) , is based on multi-layer self-attention networks and can be trained very efficiently."}
{"sent_id": "bb133ba3dfe483412672b44b777c4a-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_bb133ba3dfe483412672b44b777c4a_19", "text": "The official implementation of the Transformer uses a different computation sequence (Figure 1 b) compared to the published version (Vaswani et al., 2017) (Figure 1 a), since it seems better for harder-to-learn models 1 ."}
{"sent_id": "6e92b1fa4f3b78a099cb222b3eb9a9-C001-15", "intents": ["@SIM@"], "paper_id": "ABC_6e92b1fa4f3b78a099cb222b3eb9a9_19", "text": "In this work, we focus on Arabic DID which can can be posed as a five class classification problem, given that the Arabic language can be divided into five major dialects; Egyptian (EGY), Gulf (GLF), Lavantine (LAV), Modern Standard Arabic (MSA) and North African (NOR) [2] ."}
{"sent_id": "6e92b1fa4f3b78a099cb222b3eb9a9-C001-58", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6e92b1fa4f3b78a099cb222b3eb9a9_19", "text": "We use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works [2, 13] ."}
{"sent_id": "6e92b1fa4f3b78a099cb222b3eb9a9-C001-25", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_6e92b1fa4f3b78a099cb222b3eb9a9_19", "text": "One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model [2, 11] ."}
{"sent_id": "6e92b1fa4f3b78a099cb222b3eb9a9-C001-78", "intents": ["@BACK@"], "paper_id": "ABC_6e92b1fa4f3b78a099cb222b3eb9a9_19", "text": "This method has been shown to improve DID (LID) performance [2, 11] ."}
{"sent_id": "866ae880aa0de1e60d306eac2e66fc-C001-96", "intents": ["@BACK@"], "paper_id": "ABC_866ae880aa0de1e60d306eac2e66fc_19", "text": "Afterwards, the fusion module models the joint relationship J attn ∈ R O×H×W between questions and images, mapping them to a common space of dimension O. In the simplest case, one can implement the fusion module using either concatenation or Hadamard product [1] , but more effective pooling schemes can be applied [6, 11, 25, 26] ."}
{"sent_id": "866ae880aa0de1e60d306eac2e66fc-C001-82", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_866ae880aa0de1e60d306eac2e66fc_19", "text": "We mostly build upon the MCB model in [6] , which exemplifies current state-of-the-art techniques for this problem."}
{"sent_id": "866ae880aa0de1e60d306eac2e66fc-C001-112", "intents": ["@EXT@"], "paper_id": "ABC_866ae880aa0de1e60d306eac2e66fc_19", "text": "It is mostly based on the model presented in [6] ."}
{"sent_id": "866ae880aa0de1e60d306eac2e66fc-C001-168", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_866ae880aa0de1e60d306eac2e66fc_19", "text": "We follow the processing scheme from [6] , where non-informative words in the questions and answers such as \"a\" and \"is\" are removed."}
{"sent_id": "2407cfa8572ccbab7f9a081f45a4ad-C001-12", "intents": ["@MOT@"], "paper_id": "ABC_2407cfa8572ccbab7f9a081f45a4ad_19", "text": "To circumvent this problem, Khapra et al. (2009) proposed a WSD method that can be applied to a language even when no sense tagged corpus for that language is available."}
{"sent_id": "2407cfa8572ccbab7f9a081f45a4ad-C001-86", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_2407cfa8572ccbab7f9a081f45a4ad_19", "text": "Ours is thus a 3-factor economic model (crosslinking, annotation and accuracy) as opposed to the 2-factor model (cross-linking, accuracy) proposed by Khapra et al. (2009) ."}
{"sent_id": "6678c19792be8d9ad66cf923d00c23-C001-20", "intents": ["@DIF@"], "paper_id": "ABC_6678c19792be8d9ad66cf923d00c23_19", "text": "In addition to the improvements, through the learned ranking models, we also discover meaningful words that are financially risk-related, some of which were not identified in (Kogan et al., 2009 )."}
{"sent_id": "6678c19792be8d9ad66cf923d00c23-C001-73", "intents": ["@DIF@"], "paper_id": "ABC_6678c19792be8d9ad66cf923d00c23_19", "text": "Almost all the terms found by our ranking approach are financially meaningful; in addition, some of highly risk-correlated terms are not even reported in (Kogan et al., 2009) ."}
{"sent_id": "6678c19792be8d9ad66cf923d00c23-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_6678c19792be8d9ad66cf923d00c23_19", "text": "In recent year, there have been some studies conducted on mining financial reports, such as (Lin et al., 2008; Kogan et al., 2009; Leidner and Schilder, 2010) ."}
{"sent_id": "6678c19792be8d9ad66cf923d00c23-C001-63", "intents": ["@SIM@"], "paper_id": "ABC_6678c19792be8d9ad66cf923d00c23_19", "text": "In this paper, the 10-K Corpus (Kogan et al., 2009 ) is used to conduct the experiments; only Section 7 \"management's discussion and analysis of financial conditions and results of operations\" (MD&A) is included in the experiments since typically Section 7 contains the most important forward-looking statements."}
{"sent_id": "be77eed8430b6492c81ae6535f1dd5-C001-21", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_be77eed8430b6492c81ae6535f1dd5_19", "text": "3. Even though it simultaneously performs constituency parsing, our parser does not use any explicit syntactic feature, nor does it need any binarization of discourse trees, thanks to the powerful span-based framework of Cross and Huang (2016) (Section 3)."}
{"sent_id": "be77eed8430b6492c81ae6535f1dd5-C001-99", "intents": ["@DIF@"], "paper_id": "ABC_be77eed8430b6492c81ae6535f1dd5_19", "text": "The result is shown in Table 1 . Note that in constituency level, the accuracy is not directly comparable with the accuracy reported in Cross and Huang (2016) , since: a) our parser is trained on a much smaller dataset (RST Treebank is about 1/6 of Penn Treebank); b) the parser is trained to optimize the discourse-level accuracy."}
{"sent_id": "be77eed8430b6492c81ae6535f1dd5-C001-65", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_be77eed8430b6492c81ae6535f1dd5_19", "text": "As mentioned above, the input sequences are substantially longer than PTB parsing, so we choose linear-time parsing, by adapting a popular greedy constituency parser, the span-based constituency parser of Cross and Huang (2016) ."}
{"sent_id": "be77eed8430b6492c81ae6535f1dd5-C001-77", "intents": ["@SIM@"], "paper_id": "ABC_be77eed8430b6492c81ae6535f1dd5_19", "text": "The scoring functions in the deductive system (Figure 4 ) are calculated by an underlying neural model, which is similar to the bi-directional LSTM model in Cross and Huang (2016) that evaluates based on span boundary features."}
{"sent_id": "983ef31a44646d8e6276ee1933e41d-C001-82", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_983ef31a44646d8e6276ee1933e41d_19", "text": "Training details are as described in the Appendix of Aharoni and Goldberg (2018) using the OpenNMT-py framework (Klein et al., 2017) ."}
{"sent_id": "983ef31a44646d8e6276ee1933e41d-C001-93", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_983ef31a44646d8e6276ee1933e41d_19", "text": "AG18 is the previous best model by Aharoni and Goldberg (2018) , which used the full WebSplit training set, whereas we downsampled it."}
{"sent_id": "ebd4488438579946c23904cc0f5932-C001-15", "intents": ["@SIM@"], "paper_id": "ABC_ebd4488438579946c23904cc0f5932_19", "text": "To our knowledge, Gildea and Jurafsky (2000) is the only work that uses FrameNet to build a statistical semantic classifier."}
{"sent_id": "ebd4488438579946c23904cc0f5932-C001-23", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_ebd4488438579946c23904cc0f5932_19", "text": "Frame: We extend Gildea and Jurafsky (2000) 's initial effort in three ways."}
{"sent_id": "ebd4488438579946c23904cc0f5932-C001-35", "intents": ["@DIF@"], "paper_id": "ABC_ebd4488438579946c23904cc0f5932_19", "text": "Because human-annotated syntactic information could only be obtained for a subset of their data, the training, development, and test sets used here are approximately 10% smaller than those used in Gildea and Jurafsky (2000) ."}
{"sent_id": "79ff6e23cc951aa18ae53763e9c982-C001-4", "intents": ["@DIF@"], "paper_id": "ABC_79ff6e23cc951aa18ae53763e9c982_19", "text": "Following Webber et al. (2003) , the paper argues that in contrast crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as then, otherwise, and nevertheless."}
{"sent_id": "79ff6e23cc951aa18ae53763e9c982-C001-139", "intents": ["@DIF@"], "paper_id": "ABC_79ff6e23cc951aa18ae53763e9c982_19", "text": "The semantics proposed for in-contrast, thus, provides further evidence for the distinction between coordinating and subordinating conjunctions and discourse adverbials that has been put forth by Webber et al. (2003) ."}
{"sent_id": "79ff6e23cc951aa18ae53763e9c982-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_79ff6e23cc951aa18ae53763e9c982_19", "text": "The third generalization is further elaborated by Webber et al. (2003) who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and discourse adverbials such as then, otherwise, nevertheless, and instead on the other hand."}
{"sent_id": "79ff6e23cc951aa18ae53763e9c982-C001-103", "intents": ["@BACK@"], "paper_id": "ABC_79ff6e23cc951aa18ae53763e9c982_19", "text": "B0H (0476) Another property that distinguishes anaphoric discourse adverbials from structural connectives in the sense of Webber et al. (2003) , i.e. coordinating and subordinating conjunctions, concerns the type of dependencies that the arguments of the types of connectives can enter into."}
{"sent_id": "79ff6e23cc951aa18ae53763e9c982-C001-55", "intents": ["@SIM@"], "paper_id": "ABC_79ff6e23cc951aa18ae53763e9c982_19", "text": "Following Webber et al. (2003), we will argue that it resembles other discourse adverbials such as then, otherwise, and nevertheless in that it crucially involves the notion of discourse anaphora."}
{"sent_id": "90522b5ac99d1657bf9af9d165c36e-C001-15", "intents": ["@MOT@"], "paper_id": "ABC_90522b5ac99d1657bf9af9d165c36e_19", "text": "This problem has lately raised severe concerns in the word embedding community (e.g., Hellrich and Hahn (2016b) ; Antoniak and Mimno (2018) ; Wendlandt et al. (2018) ) and is also of interest to the wider machine learning community due to the influence of probabilistic-and thus unstablemethods on experimental results (Reimers and Gurevych, 2017; Henderson et al., 2018) , as well as replicability and reproducibility (Ivie and Thain, 2018, pp."}
{"sent_id": "90522b5ac99d1657bf9af9d165c36e-C001-23", "intents": ["@MOT@"], "paper_id": "ABC_90522b5ac99d1657bf9af9d165c36e_19", "text": "We show how the choice of down-sampling strategies, a seemingly minor detail, leads to major differences in the characterization of SVD PPMI in recent studies (Hellrich and Hahn, 2017; Antoniak and Mimno, 2018) ."}
{"sent_id": "90522b5ac99d1657bf9af9d165c36e-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_90522b5ac99d1657bf9af9d165c36e_19", "text": "Most stability experiments focused on repeatedly training the same algorithm on one corpus Hahn, 2016a,b, 2017; Antoniak and Mimno, 2018; Pierrejean and Tanguy, 2018; Chugh et al., 2018) , whereas Wendlandt et al. (2018) quantified stability by comparing word similarity for models trained with different algorithms."}
{"sent_id": "90522b5ac99d1657bf9af9d165c36e-C001-82", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_90522b5ac99d1657bf9af9d165c36e_19", "text": "We used both the corpora as-is, as well as independently drawn random subsamples (see also Hellrich and Hahn (2016a) ; Antoniak and Mimno (2018) ) to simulate the arbitrary content selection in most corpora-texts could be removed or replaced with similar ones without changing the overall nature of a corpus, e.g., Wikipedia articles are continuously edited."}
{"sent_id": "90522b5ac99d1657bf9af9d165c36e-C001-91", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_90522b5ac99d1657bf9af9d165c36e_19", "text": "As proposed by Antoniak and Mimno (2018) , we further modified our SVD PPMI implementation to use random numbers generated with a non-fixed seed for probabilistic down-sampling."}
{"sent_id": "2e7df0912d9aac8bf97f4061de613f-C001-81", "intents": ["@BACK@"], "paper_id": "ABC_2e7df0912d9aac8bf97f4061de613f_19", "text": "* marks the (reimplemented and recomputed) best system from [7] ."}
{"sent_id": "2e7df0912d9aac8bf97f4061de613f-C001-14", "intents": ["@SIM@"], "paper_id": "ABC_2e7df0912d9aac8bf97f4061de613f_19", "text": "We evaluate our established system [7] in a crossspeaker setting, observing a drastic performance drop on unknown speakers."}
{"sent_id": "46b9079fb1dd6b4626f20819ccfa07-C001-95", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_46b9079fb1dd6b4626f20819ccfa07_19", "text": "A second parameter estimation method, which was used in (Roark et al., 2004b) , is to optimize the log-likelihood under a log-linear model."}
{"sent_id": "46b9079fb1dd6b4626f20819ccfa07-C001-37", "intents": ["@SIM@"], "paper_id": "ABC_46b9079fb1dd6b4626f20819ccfa07_19", "text": "Previous work (Roark et al., 2004a; Roark et al., 2004b) has shown that discriminative methods within an ngram approach can lead to significant reductions in WER, in spite of the features being of the same type as the original language model."}
{"sent_id": "46b9079fb1dd6b4626f20819ccfa07-C001-67", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_46b9079fb1dd6b4626f20819ccfa07_19", "text": "We follow the framework of Collins (2002; 2004) , recently applied to language modeling in Roark et al. (2004a; 2004b) ."}
{"sent_id": "46b9079fb1dd6b4626f20819ccfa07-C001-196", "intents": ["@FUT@"], "paper_id": "ABC_46b9079fb1dd6b4626f20819ccfa07_19", "text": "In addition, we plan to explore the alternative parameter estimation methods described in (Roark et al., 2004a; Roark et al., 2004b) , which were shown in this previous work to give further improvements over the perceptron."}
{"sent_id": "b6efc2f5239a0c5d9210d7da8466ab-C001-13", "intents": ["@MOT@"], "paper_id": "ABC_b6efc2f5239a0c5d9210d7da8466ab_19", "text": "Assuming access to a small amount * Performed while faculty at Johns Hopkins University of parallel text is realistic, especially considering the recent success of crowdsourcing translations (Zaidan and Callison-Burch, 2011; Ambati, 2011; Post et al., 2012) ."}
{"sent_id": "b6efc2f5239a0c5d9210d7da8466ab-C001-103", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_b6efc2f5239a0c5d9210d7da8466ab_19", "text": "9 Post et al. (2012) gathered up to six translations for each source word, so some have multiple correct translations appending the top-k translations for OOV words to our model instead of just the top-1."}
{"sent_id": "b6efc2f5239a0c5d9210d7da8466ab-C001-84", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_b6efc2f5239a0c5d9210d7da8466ab_19", "text": "In order to make our results comparable to those of Post et al. (2012) , we follow that work and use Table 3 : Percent of word types in a held out portion of the training data which are translated correctly by our bilingual lexicon induction technique."}
{"sent_id": "b6efc2f5239a0c5d9210d7da8466ab-C001-164", "intents": ["@USE@"], "paper_id": "ABC_b6efc2f5239a0c5d9210d7da8466ab_19", "text": "In the experiments above, we only evaluated our methods for improving the accuracy and coverage of models trained on small amounts of bitext using the full parallel training corpora released by Post et al. (2012) ."}
{"sent_id": "320a5c79d9884e652c42f85847172b-C001-59", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_320a5c79d9884e652c42f85847172b_20", "text": "Most recently, Conneau et al. [3] have proposed an adversarial learning-based model in order to automatically, in a fully unsupervised fashion, create word translation pairs that can then be used to learn the same projection functions f S and f T as in the model of Smith et al. [19] . Let X be the set of all monolingual word embeddings from the source language, and Y the set of all target language embeddings."}
{"sent_id": "320a5c79d9884e652c42f85847172b-C001-124", "intents": ["@SIM@"], "paper_id": "ABC_320a5c79d9884e652c42f85847172b_20", "text": "The reported performance on bilingual lexicon extraction (BLE) using cross-lingual embedding spaces is also lower for EN-NL compared to EN-IT (see, e.g., [19] )."}
{"sent_id": "320a5c79d9884e652c42f85847172b-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_320a5c79d9884e652c42f85847172b_20", "text": "2 Technically, the method of Smith et al. [19] learns two projection functions f S ( S |θ S ) and f S ( T |θ T ), projecting the source and target monolingual embedding spaces, respectively, to the new shared space."}
{"sent_id": "320a5c79d9884e652c42f85847172b-C001-92", "intents": ["@DIF@"], "paper_id": "ABC_320a5c79d9884e652c42f85847172b_20", "text": "The CL-WT embeddings of Smith et al. [19] use 10K translation pairs obtained from Google Translate to learn the linear mapping functions."}
{"sent_id": "0cc576e90c5ee2af043e09234792f5-C001-28", "intents": ["@FUT@"], "paper_id": "ABC_0cc576e90c5ee2af043e09234792f5_20", "text": "Finally, it would be interesting to determine whether using ASs extracted from a corpus of native texts enables a better prediction than that obtained by using the simple frequency of the unigrams and bigrams (Yannakoudakis et al., 2011) ."}
{"sent_id": "0cc576e90c5ee2af043e09234792f5-C001-69", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_0cc576e90c5ee2af043e09234792f5_20", "text": "Since the quality ratings are distributed on a zero to 40 scale, I chose Pearson's correlation coefficient, also used by Yannakoudakis et al. (2011) , as the measure of performance."}
{"sent_id": "0cc576e90c5ee2af043e09234792f5-C001-92", "intents": ["@SIM@"], "paper_id": "ABC_0cc576e90c5ee2af043e09234792f5_20", "text": "To get an idea of how well the collocational and lexical features perform, the correlations in Table 2 can be compared to the average correlation between the Examiners' scores reported by Yannakoudakis et al. (2011) , which give an upper bound of 0.80 while the All models with more than three bins obtain a correlation of at least 0.75."}
{"sent_id": "0cc576e90c5ee2af043e09234792f5-C001-67", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_0cc576e90c5ee2af043e09234792f5_20", "text": "Supervised Learning Approach and Evaluation: As in Yannakoudakis et al. (2011) , the automated scoring task was treated as a rankpreference learning problem by means of the SVM-Rank package (Joachims, 2006) , which is a much faster version of the SVM-Light package used by Yannakoudakis et al. (2011) ."}
{"sent_id": "d66ca5ff22e508da239fc7fdf5ac29-C001-162", "intents": ["@SIM@"], "paper_id": "ABC_d66ca5ff22e508da239fc7fdf5ac29_20", "text": "An informative traditional measure was sentence length, similarly to the results of previous studies (Beinborn et al., 2012; Dell'Orletta et al., 2011; François and Fairon, 2012; Heimann Mühlenbock, 2013; Vajjala and Meurers, 2012) ."}
{"sent_id": "d66ca5ff22e508da239fc7fdf5ac29-C001-168", "intents": ["@DIF@"], "paper_id": "ABC_d66ca5ff22e508da239fc7fdf5ac29_20", "text": "Moreover, in the case of Swedish L1 text readability the noun/pronoun ratio and modifiers proved to be indicative of textlevel difficulty (Heimann Mühlenbock, 2013 ), but at the sentence level from the L2 perspective only the latter seemed influential in our experiments."}
{"sent_id": "b49807b058e5e1e50eae524e592401-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_b49807b058e5e1e50eae524e592401_20", "text": "The state-of-the-art approach (Shimaoka et al., 2017) for fine-grained entity typing employs an attentive neural architecture to learn representations of the entity mention as well as its context."}
{"sent_id": "b49807b058e5e1e50eae524e592401-C001-38", "intents": ["@DIF@"], "paper_id": "ABC_b49807b058e5e1e50eae524e592401_20", "text": "This is different from Shimaoka et al. (2017) who use two separate bi-directional RNNs for context on each side of the entity mention."}
{"sent_id": "b49807b058e5e1e50eae524e592401-C001-78", "intents": ["@DIF@"], "paper_id": "ABC_b49807b058e5e1e50eae524e592401_20", "text": "Note that (1) without adaptive thresholds or document-level contexts, our approach still outperforms other approaches on macro F 1 and micro F 1 ; (2) adding hand-crafted features (Shimaoka et al., 2017) does not improve the performance."}
{"sent_id": "b49807b058e5e1e50eae524e592401-C001-89", "intents": ["@DIF@"], "paper_id": "ABC_b49807b058e5e1e50eae524e592401_20", "text": "Ling and Weld (2012) 52.30 69.90 69.30 PLE (Ren et al., 2016b) 49.44 68.75 64.54 Ma et al. (2016) 53.54 68.06 66.53 AFET (Ren et al., 2016a) 53.30 69.30 66.40 NEURAL (Shimaoka et al., 2017) proach still achieves the state-of-the-art strict and micro F 1 ."}
{"sent_id": "ca98f16fa3a118f83b16586bba04c8-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_ca98f16fa3a118f83b16586bba04c8_20", "text": "In previous studies, researchers have proposed a variety of approaches to address the problem of universal replies, ranging from heuristically modified training objectives [2] , diversified decoding algorithms [9] , to content-introducing approaches [3, 10] ."}
{"sent_id": "ca98f16fa3a118f83b16586bba04c8-C001-105", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_ca98f16fa3a118f83b16586bba04c8_20", "text": "These metrics are used in previous work [4, 3] , and are related to our research question."}
{"sent_id": "649eff228a47b484d01872a980e58f-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_649eff228a47b484d01872a980e58f_20", "text": "Combining the strengths of CNNs and RNNs, convolutional recurrent neural network based KWS is investigated in [7] and demonstrate the robustness of the model to noise."}
{"sent_id": "649eff228a47b484d01872a980e58f-C001-157", "intents": ["@BACK@"], "paper_id": "ABC_649eff228a47b484d01872a980e58f_20", "text": "[5, 6, 7, 8] trained on the speech commands dataset [9] ."}
{"sent_id": "48e3715c55fcc188367dcfdc26c05f-C001-26", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_48e3715c55fcc188367dcfdc26c05f_20", "text": "We experiment in two different tasks with promising results: First, we repeat the disambiguation experiment of Grefenstette and Sadrzadeh (2011a) for transitive verbs."}
{"sent_id": "48e3715c55fcc188367dcfdc26c05f-C001-95", "intents": ["@SIM@"], "paper_id": "ABC_48e3715c55fcc188367dcfdc26c05f_20", "text": "We also remind to the reader that the relational method for constructing a tensor for the meaning of a verb (Grefenstette and Sadrzadeh, 2011a) provides us with a matrix in N 2 ."}
{"sent_id": "48e3715c55fcc188367dcfdc26c05f-C001-113", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_48e3715c55fcc188367dcfdc26c05f_20", "text": "We train our vectors from a lemmatised version of the British National Corpus (BNC), following closely the parameters of the setting described in Mitchell and Lapata (2008) , later used by Grefenstette and Sadrzadeh (2011a) ."}
{"sent_id": "48e3715c55fcc188367dcfdc26c05f-C001-74", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_48e3715c55fcc188367dcfdc26c05f_20", "text": "An important consequence of our design decision is that it enables us to reduce the space complexity of the implementation from Θ(d n ) (Grefenstette and Sadrzadeh, 2011a) to Θ(d), making the problem much more tractable."}
{"sent_id": "48e3715c55fcc188367dcfdc26c05f-C001-110", "intents": ["@DIF@"], "paper_id": "ABC_48e3715c55fcc188367dcfdc26c05f_20", "text": "Furthermore, note that the nesting problem of Grefenstette and Sadrzadeh (2011a) does not arise here, since the linguistic and concrete types are the same."}
{"sent_id": "48e3715c55fcc188367dcfdc26c05f-C001-141", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_48e3715c55fcc188367dcfdc26c05f_20", "text": "1 The original relational model of Grefenstette and Sadrzadeh (2011a) with S = N 2 , provided a ρ of 0.21."}
{"sent_id": "44916cd85311c78666839a3376ccc6-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_44916cd85311c78666839a3376ccc6_20", "text": "Further, Ajjour et al. (2017) proposed a setup with three bidirectional LSTMs (Bi-LSTMs) (Schuster and Paliwal, 1997) in total as their best solution."}
{"sent_id": "44916cd85311c78666839a3376ccc6-C001-80", "intents": ["@SIM@"], "paper_id": "ABC_44916cd85311c78666839a3376ccc6_20", "text": "This framework has been applied previously for the same task (Stab, 2017; Eger et al., 2017; Ajjour et al., 2017) ."}
{"sent_id": "44916cd85311c78666839a3376ccc6-C001-159", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_44916cd85311c78666839a3376ccc6_20", "text": "For our re-implementation of the baseline, we are able to approximately reproduce the results reported by Ajjour et al. (2017) ."}
{"sent_id": "3bbc588f06e326e1d75985fe253a5f-C001-139", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_3bbc588f06e326e1d75985fe253a5f_20", "text": "In this study, we apply the USMT method of Artetxe et al. (2018b) and Marie and Fujita (2018) to GEC."}
{"sent_id": "3bbc588f06e326e1d75985fe253a5f-C001-71", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_3bbc588f06e326e1d75985fe253a5f_20", "text": "The implementation made by Artetxe et al. (2018b) 6 was modified to conduct the experiments."}
{"sent_id": "bb2609c568540390a560757dd40b32-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_bb2609c568540390a560757dd40b32_20", "text": "The Ubuntu Dialogue Corpus is the largest freely available multi-turn based dialog corpus [1] 1 ."}
{"sent_id": "bb2609c568540390a560757dd40b32-C001-155", "intents": ["@BACK@"], "paper_id": "ABC_bb2609c568540390a560757dd40b32_20", "text": "This agrees with Figure 3 of the previous evaluation [1] ."}
{"sent_id": "831342435ca0a4695e2a7f149891e4-C001-73", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_831342435ca0a4695e2a7f149891e4_20", "text": "We study two Spanish-to-English translation datasets: the large scale \"conversational\" corpus of parallel text and read speech pairs from [21] , and the Spanish Fisher corpus of telephone conversations and corresponding English translations [38] , which is smaller and more challenging due to the spontaneous and informal speaking style."}
{"sent_id": "831342435ca0a4695e2a7f149891e4-C001-88", "intents": ["@SIM@"], "paper_id": "ABC_831342435ca0a4695e2a7f149891e4_20", "text": "Input feature frames are created by stacking 3 adjacent frames of an 80-channel log-mel spectrogram as in [21] ."}
{"sent_id": "831342435ca0a4695e2a7f149891e4-C001-90", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_831342435ca0a4695e2a7f149891e4_20", "text": "Table 2 shows performance of the model trained using different combinations of auxiliary losses, compared to a baseline ST → TTS cascade model using a speech-to-text translation model [21] trained on the same data, and the same Tacotron 2 TTS model used to synthesize training targets."}
{"sent_id": "831342435ca0a4695e2a7f149891e4-C001-84", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_831342435ca0a4695e2a7f149891e4_20", "text": "In this section, instead of using the human target speech, we use a TTS model to synthesize target In addition, we augment the input source speech by adding background noise and reverberation in the same manner as [21] ."}
{"sent_id": "4f5a25d7a961e7e61c2caef81418e0-C001-20", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_4f5a25d7a961e7e61c2caef81418e0_20", "text": "Our model can be seen as an extension of the recently proposed model for the same problem by Rush et al. (2015) ."}
{"sent_id": "4f5a25d7a961e7e61c2caef81418e0-C001-146", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_4f5a25d7a961e7e61c2caef81418e0_20", "text": "We extend the state-of-the-art model for abstractive sentence summarization (Rush et al., 2015) to a recurrent neural network architecture."}
{"sent_id": "4f5a25d7a961e7e61c2caef81418e0-C001-26", "intents": ["@SIM@"], "paper_id": "ABC_4f5a25d7a961e7e61c2caef81418e0_20", "text": "Empirically we show that our model beats the state-of-the-art systems of Rush et al. (2015) on multiple data sets."}
{"sent_id": "4f5a25d7a961e7e61c2caef81418e0-C001-85", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4f5a25d7a961e7e61c2caef81418e0_20", "text": "The data is pre-processed in the same way as Rush et al. (2015) and we use the same splits for training, validation, and testing."}
{"sent_id": "73eaa7d5a54b2d60bd8128e0270683-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_73eaa7d5a54b2d60bd8128e0270683_20", "text": "Of particular interest to this paper is the work by Ma and Hovy (2016) introduce a strong end-to-end model combining a bi-directional Long Short-Term Memory (Bi-LSTM) network with Convolutional Neural Network (CNN) character encoding in a Conditional Random Field (CRF)."}
{"sent_id": "73eaa7d5a54b2d60bd8128e0270683-C001-33", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_73eaa7d5a54b2d60bd8128e0270683_20", "text": "We build on a highly competitive sequence labelling model, namely Bi-LSTM-CNN-CRF, first introduced by Ma and Hovy (2016) ."}
{"sent_id": "73eaa7d5a54b2d60bd8128e0270683-C001-87", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_73eaa7d5a54b2d60bd8128e0270683_20", "text": "In addition to reporting a number of prior results of competitive baseline models, as listed in Table 2 , we also re-implement the Bi-LSTM-CNN-CRF model by Ma and Hovy (2016) (referred to as Neural-CRF in Table 2 ) and report its average performance."}
{"sent_id": "73eaa7d5a54b2d60bd8128e0270683-C001-39", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_73eaa7d5a54b2d60bd8128e0270683_20", "text": "Here, our character-level CNN is similar to that used in Ma and Hovy (2016) but differs in that we use a ReLU activation (Nair and Hinton, 2010) ."}
{"sent_id": "4e7ee576b07a8a21a42472bf921291-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_4e7ee576b07a8a21a42472bf921291_20", "text": "K-NRM [10] is an interaction-based model that uses kernel pooling to summarize word-word interactions."}
{"sent_id": "4e7ee576b07a8a21a42472bf921291-C001-57", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4e7ee576b07a8a21a42472bf921291_20", "text": "Testing Labels: Following Xiong et al. [10] , three testing conditions were used."}
{"sent_id": "4e7ee576b07a8a21a42472bf921291-C001-54", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_4e7ee576b07a8a21a42472bf921291_20", "text": "Xiong, et al. [10] built the vocabulary from queries and titles, but we built it from the queries, titles and URLs for better term coverage."}
{"sent_id": "7cb7cfed8b7e7bf2f0a810e02e6cbc-C001-19", "intents": ["@MOT@"], "paper_id": "ABC_7cb7cfed8b7e7bf2f0a810e02e6cbc_20", "text": "Consequently, the main previous work on AMR-based abstractive summarization (Liu et al., 2015) only generated bag-of-words from the summary AMR graph."}
{"sent_id": "7cb7cfed8b7e7bf2f0a810e02e6cbc-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_7cb7cfed8b7e7bf2f0a810e02e6cbc_20", "text": "The encoder computes the hidden representation of the input, {z 1 , z 2 , . . . , z k }, which is the linearized summary AMR graph, G ′ from Liu et al. (2015) , following Van Noord and Bos (2017)'s preprocessing steps."}
{"sent_id": "7cb7cfed8b7e7bf2f0a810e02e6cbc-C001-94", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_7cb7cfed8b7e7bf2f0a810e02e6cbc_20", "text": "Guided NLG for full summarization In this experiment we combine our guided NLG model with Liu et al. (2015) 's work in order to generate fluent texts from their summary AMR graphs using the hyper-parameters tuned in the previous paragraph."}
{"sent_id": "7cb7cfed8b7e7bf2f0a810e02e6cbc-C001-97", "intents": ["@SIM@"], "paper_id": "ABC_7cb7cfed8b7e7bf2f0a810e02e6cbc_20", "text": "We compare our result against Liu et al. (2015) 's bag of words 1 , the unguided AMR-to-text model from §3.2, and a seq2seq summarization model (Open-NMT BRNN) 2,3 which summarizes directly from the source document to summary sentence without using AMR as an interlingua and is trained on CNN/DM corpus (Hermann et al., 2015) using the same settings as See et al. (2017) ."}
{"sent_id": "9c8c5da4cdd13efb187690e7d3aa20-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_9c8c5da4cdd13efb187690e7d3aa20_20", "text": "Gupta et al. (2018) sampled 4K sentences as their test set, but did not specify which sentences they used."}
{"sent_id": "e9779b09826d709f8851550d958df7-C001-28", "intents": ["@SIM@"], "paper_id": "ABC_e9779b09826d709f8851550d958df7_20", "text": "This preliminary result, in line with previous findings of [8] , confirms that neural speech-image models can capture a cross-lingual semantic signal, a first step in the perspective of learning speech-to-speech translation systems without text supervision."}
{"sent_id": "e9779b09826d709f8851550d958df7-C001-151", "intents": ["@SIM@"], "paper_id": "ABC_e9779b09826d709f8851550d958df7_20", "text": "is paired with image I, we assess the ability of our approach to rank the matching spoken caption in language tgt paired with image I in the top 1, 5, and 10 results and give its median rank r. We report our results in Table 4 as well as results from [8] who performed speechto-speech retrieval using crowd-sourced spoken captions in English and Hindi."}
{"sent_id": "e9779b09826d709f8851550d958df7-C001-136", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_e9779b09826d709f8851550d958df7_20", "text": "This possibility was introduced in [8] , but required training jointly or alternatively two speech encoders within the same architecture and a parallel bilingual speech dataset while we experiment with separately trained models for both languages."}
{"sent_id": "e9779b09826d709f8851550d958df7-C001-153", "intents": ["@DIF@"], "paper_id": "ABC_e9779b09826d709f8851550d958df7_20", "text": "Nevertheless, it is also important to mention that [8] experimented on real speech with multiple speakers while we used synthetic speech with only one voice."}
{"sent_id": "e9779b09826d709f8851550d958df7-C001-171", "intents": ["@FUT@"], "paper_id": "ABC_e9779b09826d709f8851550d958df7_20", "text": "ces so that there would be only one target caption for each query in order to compare our results with [8] ."}
{"sent_id": "d0007c7f1f9ecfbdd7b6ad7c59cc92-C001-115", "intents": ["@BACK@"], "paper_id": "ABC_d0007c7f1f9ecfbdd7b6ad7c59cc92_20", "text": "All of their experiments are performed on Dong et al. (2014) Twitter data set."}
{"sent_id": "d0007c7f1f9ecfbdd7b6ad7c59cc92-C001-144", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_d0007c7f1f9ecfbdd7b6ad7c59cc92_20", "text": "We test all of the methods on the test data set of Dong et al. (2014) and show the difference between the original and reproduced models in figure 2 ."}
{"sent_id": "bbd8c1007b573758fc78a6d16e2b77-C001-135", "intents": ["@BACK@"], "paper_id": "ABC_bbd8c1007b573758fc78a6d16e2b77_20", "text": "We should note, however, that most other stochastic parsers do include counts of single nonheadwords: they appear in the backed-off statistics of these parsers (see Collins 1997 Collins , 1999 Charniak 1997; Goodman 1998) ."}
{"sent_id": "bbd8c1007b573758fc78a6d16e2b77-C001-177", "intents": ["@SIM@"], "paper_id": "ABC_bbd8c1007b573758fc78a6d16e2b77_20", "text": "The importance of including counts of (single) nonheadwords is now also quite uncontroversial (e.g. Collins 1997 Collins , 1999 Charniak 2000) , and the current paper has shown the importance of including two and more nonheadwords."}
{"sent_id": "bbd8c1007b573758fc78a6d16e2b77-C001-154", "intents": ["@DIF@"], "paper_id": "ABC_bbd8c1007b573758fc78a6d16e2b77_20", "text": "A major difference between our approach and most other models tested on the WSJ is that the DOP model uses frontier lexicalization while most other models use, what we might call, constituent lexicalization (in that it associates each constituent nonterminal with its lexical head --see Collins 1997 Collins , 1999 Charniak 1997; Eisner 1997) ."}
{"sent_id": "1ffadfc2d4961beeb1621502298a70-C001-21", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_1ffadfc2d4961beeb1621502298a70_21", "text": "The remainder of this paper is organized as follows: Section 3 presents the shared task description and the sub-tasks included in OffensEval and Section 4 includes a brief description of OLID based on Zampieri et al. (2019) ."}
{"sent_id": "1ffadfc2d4961beeb1621502298a70-C001-40", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_1ffadfc2d4961beeb1621502298a70_21", "text": "While each of these sub-tasks tackle a particular type of abuse or offense, they share similar properties and the hierarchical annotation model pro-posed proposed in OLID (Zampieri et al., 2019) and used in OffensEval aims to capture this."}
{"sent_id": "1ffadfc2d4961beeb1621502298a70-C001-140", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_1ffadfc2d4961beeb1621502298a70_21", "text": "In OffensEval we used OLID (Zampieri et al., 2019) , a dataset containing English tweets annotated with a hierarchical three-layer annotation model which considers 1) whether a message is offensive or not (sub-task A); 2) what is the type of the offensive 7 In the camera-ready version of this report we will be including a Table with references to all system descriptions papers."}
{"sent_id": "1ffadfc2d4961beeb1621502298a70-C001-20", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_1ffadfc2d4961beeb1621502298a70_21", "text": "In OffensEval 1 we use OLID (Zampieri et al., 2019) and propose one sub-task for each layer of annotation as presented in Section 3."}
{"sent_id": "2357152e66ad3ae1c23738ac95f971-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_2357152e66ad3ae1c23738ac95f971_21", "text": "Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011; Nuhn et al., 2013) ."}
{"sent_id": "2357152e66ad3ae1c23738ac95f971-C001-19", "intents": ["@USE@"], "paper_id": "ABC_2357152e66ad3ae1c23738ac95f971_21", "text": "We use the notation from Nuhn et al. (2013) ."}
{"sent_id": "2357152e66ad3ae1c23738ac95f971-C001-132", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_2357152e66ad3ae1c23738ac95f971_21", "text": "We modify the beam search algorithm for decipherment from Nuhn et al. (2013; and extend it to use global scoring of the plaintext message using neural LMs."}
{"sent_id": "c91781e76a8d7d6de7d7bc4407e799-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_c91781e76a8d7d6de7d7bc4407e799_21", "text": "Although many authors released their code along with their sequence labeling papers (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018) , the implementations are mostly focused on specific model structures and specific tasks."}
{"sent_id": "c91781e76a8d7d6de7d7bc4407e799-C001-122", "intents": ["@BACK@"], "paper_id": "ABC_c91781e76a8d7d6de7d7bc4407e799_21", "text": "Most of state-of-the-art models utilize the framework of word LSTM-CRF with character LSTM or CNN features (correspond to \"CLSTM+WLSTM+CRF\" and \"CCNN+WLSTM+CRF\" of our models) (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2017; Peters et al., 2017) ."}
{"sent_id": "c91781e76a8d7d6de7d7bc4407e799-C001-29", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c91781e76a8d7d6de7d7bc4407e799_21", "text": "• Effective and efficient: we reimplement several state-of-the-art neural models (Lample et al., 2016; Ma and Hovy, 2016) using NCRF++."}
{"sent_id": "c91781e76a8d7d6de7d7bc4407e799-C001-111", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c91781e76a8d7d6de7d7bc4407e799_21", "text": "Hyperparameters are mostly following Ma and Hovy (2016) and almost keep the same in all these experiments 5 ."}
{"sent_id": "92e43071b2a9b05b5d96277c1aa250-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_92e43071b2a9b05b5d96277c1aa250_21", "text": "Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014) ."}
{"sent_id": "92e43071b2a9b05b5d96277c1aa250-C001-30", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_92e43071b2a9b05b5d96277c1aa250_21", "text": "To learn the restaurant script from our dataset, we implement the models of Chambers and Jurafsky (2008) and Jans et al. (2012) , as well as the unigram baseline of Pichotta and Mooney (2014) ."}
{"sent_id": "92e43071b2a9b05b5d96277c1aa250-C001-35", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_92e43071b2a9b05b5d96277c1aa250_21", "text": "This section provides an overview of each of the different methods and parameter settings we employ to learn narrative chains from the Dinners from Hell corpus, starting with the original model (Chambers and Jurafsky, 2008) and extending to the modifications of Jans et al. (2012) ."}
{"sent_id": "92e43071b2a9b05b5d96277c1aa250-C001-47", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_92e43071b2a9b05b5d96277c1aa250_21", "text": "(Jans et al., 2012) Coreference Chain Length The original model counts co-occurrences in all coreference chains; we include Jans et al. (2012) 's option to count over only the longest chains in each document, or to count only over chains of length 5 or greater (long)."}
{"sent_id": "92e43071b2a9b05b5d96277c1aa250-C001-120", "intents": ["@DIF@"], "paper_id": "ABC_92e43071b2a9b05b5d96277c1aa250_21", "text": "Furthermore, it is apparent from the results that the bigram probability models perform better overall than PMI-based models, a finding also reported in Jans et al. (2012) ."}
{"sent_id": "92e43071b2a9b05b5d96277c1aa250-C001-54", "intents": ["@FUT@"], "paper_id": "ABC_92e43071b2a9b05b5d96277c1aa250_21", "text": "Two additional models are introduced by Jans et al. (2012) and we use them here, as well."}
{"sent_id": "ee8163c5a76ed9f929a960b3086356-C001-72", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_ee8163c5a76ed9f929a960b3086356_21", "text": "In this section we compare our method to two prior methods, our reimplementation of the supervised word-vector-based method of Mikolov et al. (2013b) (using the same vectors as our method), and the reported results of an EM-based method of Haghighi et al. (2008) ."}
{"sent_id": "ee8163c5a76ed9f929a960b3086356-C001-106", "intents": ["@SIM@"], "paper_id": "ABC_ee8163c5a76ed9f929a960b3086356_21", "text": "We evaluate the induced lexicon after 40 iterations of bidirectional bootstrapping by comparing it to the lexicon after the first iteration in a single direction, which is equivalent to the method of Mikolov et al. (2013b) ."}
{"sent_id": "a82bdc55c15bb2bcee77c57641b1b5-C001-96", "intents": ["@BACK@"], "paper_id": "ABC_a82bdc55c15bb2bcee77c57641b1b5_21", "text": "The best performance for siding ideological debates in previous work is approximately 64% accuracy over all topics, for a collection of 2nd Amendment, Abortion, Evolution, and Gay Rights debate posts (Somasundaran and Wiebe, 2010) ."}
{"sent_id": "a82bdc55c15bb2bcee77c57641b1b5-C001-148", "intents": ["@BACK@"], "paper_id": "ABC_a82bdc55c15bb2bcee77c57641b1b5_21", "text": "Previous research in this area suggests the utility of dependency structure to determine the TARGET of an opinion word (Joshi and Penstein-Rosé, 2009; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010) ."}
{"sent_id": "a82bdc55c15bb2bcee77c57641b1b5-C001-192", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_a82bdc55c15bb2bcee77c57641b1b5_21", "text": "While our method of extracting the GDepP features is identical to (Joshi and Penstein-Rosé, 2009 ), our method for extracting GDepO is an approximation of the method of (Somasundaran and Wiebe, 2010) , that does not rely on selecting particular patterns indicating the topics of arguing by using a development set."}
{"sent_id": "3bb6243de9f77fc6ebf2dc24de7faa-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_3bb6243de9f77fc6ebf2dc24de7faa_21", "text": "Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (Mitchell and Lapata, 2010) , it seems that more complex approaches usually yield better performance (Cheng et al., 2018; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; Hill et al., 2016; Kiros et al., 2015; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 ."}
{"sent_id": "3bb6243de9f77fc6ebf2dc24de7faa-C001-91", "intents": ["@BACK@"], "paper_id": "ABC_3bb6243de9f77fc6ebf2dc24de7faa_21", "text": "In the learning stage, we employ the Support Vector Machines (SVM) implementation provided by LibSVM (Chang and Lin, 2011 Table 1 : Performance results (in %) of our approach (VLAWE) versus several state-of-the-art methods Cheng et al., 2018; Fu et al., 2018; Hill et al., 2016; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 on the Reuters-21578, RT-2k, MR, TREC and Subj data sets."}
{"sent_id": "3bb6243de9f77fc6ebf2dc24de7faa-C001-97", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_3bb6243de9f77fc6ebf2dc24de7faa_21", "text": "We follow the same evaluation procedure as Kiros et al. (2015) and Hill et al. (2016) , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set."}
{"sent_id": "3bb6243de9f77fc6ebf2dc24de7faa-C001-103", "intents": ["@DIF@"], "paper_id": "ABC_3bb6243de9f77fc6ebf2dc24de7faa_21", "text": "First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (Le and Mikolov, 2014; Hill et al., 2016) ."}
{"sent_id": "fe8d369d4a6f940a1eb25aa7c9b4fe-C001-116", "intents": ["@BACK@"], "paper_id": "ABC_fe8d369d4a6f940a1eb25aa7c9b4fe_21", "text": "The simplest of these is length normalization which penalizes short translations in decoding (Wu et al., 2016) ."}
{"sent_id": "fe8d369d4a6f940a1eb25aa7c9b4fe-C001-41", "intents": ["@SIM@"], "paper_id": "ABC_fe8d369d4a6f940a1eb25aa7c9b4fe_21", "text": "Given a source position i, we define its coverage as the sum of the past attention probabilities c i = |y| j a ij (Wu et al., 2016; Tu et al., 2016) ."}
{"sent_id": "fe8d369d4a6f940a1eb25aa7c9b4fe-C001-80", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_fe8d369d4a6f940a1eb25aa7c9b4fe_21", "text": "For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods (Wu et al., 2016) ."}
{"sent_id": "fe8d369d4a6f940a1eb25aa7c9b4fe-C001-54", "intents": ["@DIF@"], "paper_id": "ABC_fe8d369d4a6f940a1eb25aa7c9b4fe_21", "text": "Note that our way of truncation is different from Wu et al. (2016) 's, where they clip the coverage into [0, 1] and ignore the fact that a source word may be translated into multiple target words and its coverage should be of a value larger than 1."}
{"sent_id": "f32bbd580d93f77ef764c5341b93db-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_f32bbd580d93f77ef764c5341b93db_21", "text": "But other network architectures such as RNN-LM [13] can also be used here."}
{"sent_id": "f32bbd580d93f77ef764c5341b93db-C001-80", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f32bbd580d93f77ef764c5341b93db_21", "text": "During testing,ỹ andy are integrated into a single output sequence just as done in the previous work [13] ."}
{"sent_id": "f32bbd580d93f77ef764c5341b93db-C001-107", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f32bbd580d93f77ef764c5341b93db_21", "text": "For the ASR model, the encoder included a 6layer VGG extractor with downsampling used in the previous work [13] and a 5-layer BLSTM with 320 units per direction."}
{"sent_id": "5a000efaa052588f6cfbb69f8ced2d-C001-20", "intents": ["@DIF@"], "paper_id": "ABC_5a000efaa052588f6cfbb69f8ced2d_21", "text": "When combined with BIUTEE, we achieved 74.27% accuracy on MC160 and 65.96% on MC500, which are significantly better than those reported by Richardson et al. (2013) ."}
{"sent_id": "5a000efaa052588f6cfbb69f8ced2d-C001-89", "intents": ["@DIF@"], "paper_id": "ABC_5a000efaa052588f6cfbb69f8ced2d_21", "text": "Our proposed baseline outperforms the baseline of Richardson et al. (2013) by 4 and 3 points in accuracy on MC160 and MC500 respectively."}
{"sent_id": "5a000efaa052588f6cfbb69f8ced2d-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_5a000efaa052588f6cfbb69f8ced2d_21", "text": "More concretely, the algorithm of Richardson et al. (2013) passes a sliding window over the story, size of which is equal to the number of words in the question-answer pair."}
{"sent_id": "5a000efaa052588f6cfbb69f8ced2d-C001-51", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_5a000efaa052588f6cfbb69f8ced2d_21", "text": "Similar to Richardson et al. (2013) , we use a linear combination of this score with their distancebased scoring function, and we weigh tokens with their inverse document frequencies in each individual story."}
{"sent_id": "5e85f66e9971497e5e21af6893418d-C001-65", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_5e85f66e9971497e5e21af6893418d_21", "text": "Following Vendrov et al. (2016) we set the dimensionality of the embedding space and the GRU hidden layer N to 1024 for both English and German."}
{"sent_id": "c82c31a3e7b229b5aed8faeff21efa-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_c82c31a3e7b229b5aed8faeff21efa_21", "text": "RNNs and their variants such as LSTM (Hochreiter and Schmidhuber, 1997) have shown success in several natural language processing (NLP) tasks, such as entity extraction (Lample et al., 2016; Ma and Hovy, 2016) , relation extraction (Vu et al., 2016a; Miwa and Bansal, 2016; Gupta et al., 2016 Gupta et al., , 2018c , language modeling (Mikolov et al., 2010; Peters et al., 2018) , slot filling (Mesnil et al., 2015; Vu et al., 2016b) , machine translation (Bahdanau et al., 2014) , sentiment analysis (Wang et al., 2016; Tang et al., 2015) , semantic textual similarity (Mueller and Thyagarajan, 2016; Gupta et al., 2018a) and dynamic topic modeling (Gupta et al., 2018d) ."}
{"sent_id": "c82c31a3e7b229b5aed8faeff21efa-C001-48", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_c82c31a3e7b229b5aed8faeff21efa_21", "text": "Ranking Objective: Similar to Santos et al. (2015) and Vu et al. (2016a) , we applied the ranking loss function to train C-BRNN."}
{"sent_id": "c82c31a3e7b229b5aed8faeff21efa-C001-84", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_c82c31a3e7b229b5aed8faeff21efa_21", "text": "Following Vu et al. (2016a) , we use N-grams (e.g., tri-grams) representation for each word in each subsequence S ≤k that is input to C-BRNN to compute P (R|S ≤k ), where the N-gram (N=3) subsequence S ≤k is given by,"}
{"sent_id": "c82c31a3e7b229b5aed8faeff21efa-C001-117", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_c82c31a3e7b229b5aed8faeff21efa_21", "text": "We use the similar experimental setup as Vu et al. (2016a) ."}
{"sent_id": "10de18ba49c0da530b15ff2d14f343-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_10de18ba49c0da530b15ff2d14f343_21", "text": "Xu et al. [3] proposed a Convolutional Neural Network (CNN) model employing two types of pre-trained word embeddings, general-purpose embeddings and domainspecific embeddings, for aspect term extraction."}
{"sent_id": "10de18ba49c0da530b15ff2d14f343-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_10de18ba49c0da530b15ff2d14f343_21", "text": "Xu et al. [3] use double embeddings that leverage both general embeddings and domain embeddings as a feature for a CNN model and let the CNN model decide which embeddings have more useful information."}
{"sent_id": "10de18ba49c0da530b15ff2d14f343-C001-51", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_10de18ba49c0da530b15ff2d14f343_21", "text": "As stated previously, the goal of this work is to extract aspect and opinion terms in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism [3] ."}
{"sent_id": "14fcaa3645771e9ca183558eb2e9a1-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_14fcaa3645771e9ca183558eb2e9a1_21", "text": "Good examples of the later are character-based models like Flair (Akbik et al., 2018) and masked language models like BERT (Devlin et al., 2019) ."}
{"sent_id": "14fcaa3645771e9ca183558eb2e9a1-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_14fcaa3645771e9ca183558eb2e9a1_21", "text": "In that case, only multilingual versions are available, where each language shares the quota of substrings and parameters with the rest of the languages, leading to a decrease in performance (Devlin et al., 2019) ."}
{"sent_id": "14fcaa3645771e9ca183558eb2e9a1-C001-57", "intents": ["@MOT@"], "paper_id": "ABC_14fcaa3645771e9ca183558eb2e9a1_21", "text": "More recently, Devlin et al. (2019) introduced BERT, a model based on the transformer architecture trained as a masked language model, which has obtained very good results on a variety of NLP tasks."}
{"sent_id": "14fcaa3645771e9ca183558eb2e9a1-C001-145", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_14fcaa3645771e9ca183558eb2e9a1_21", "text": "For comparison between BERT models we fine-tune on the training data provided for each of the four tasks with both the official multilingual BERT (Devlin et al., 2019) model and with our BERTeus model (trained as described in Section 3.3.)."}
{"sent_id": "14fcaa3645771e9ca183558eb2e9a1-C001-120", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_14fcaa3645771e9ca183558eb2e9a1_21", "text": "Model Architecture In the same way as the original BERT architecture proposed by Devlin et al. (2019) our model is composed by stacked layers of Transformer encoders (Vaswani et al., 2017) ."}
{"sent_id": "6388fd7167389982be2f01fbe594cd-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_6388fd7167389982be2f01fbe594cd_21", "text": "Recently, Bicknell and Levy (2010) presented a model of eye movement control in reading that directly models the process of identifying the text from visual input, and makes eye movements to maximize the efficiency of the identification process."}
{"sent_id": "6388fd7167389982be2f01fbe594cd-C001-195", "intents": ["@BACK@"], "paper_id": "ABC_6388fd7167389982be2f01fbe594cd_21", "text": "We described the failure of the rational model presented in Bicknell and Levy (2010) to obtain humanlike effects of word length, despite including all of these factors, suggesting that our understanding of word length effects in reading is incomplete."}
{"sent_id": "6388fd7167389982be2f01fbe594cd-C001-22", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_6388fd7167389982be2f01fbe594cd_21", "text": "We present an extension of Bicknell and Levy's (2010) model which does not make this simplifying assumption, and show in two sets of simulations that effects of word length produced by the extended model look more like those of humans."}
{"sent_id": "6388fd7167389982be2f01fbe594cd-C001-50", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_6388fd7167389982be2f01fbe594cd_21", "text": "In the remainder of this paper, we describe an extension of Bicknell and Levy's (2010) model in which visual input provides stochastic -rather than veridical -information about the length of words, yielding uncertainty about word length, and in which the amount of uncertainty grows with length."}
{"sent_id": "6388fd7167389982be2f01fbe594cd-C001-141", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_6388fd7167389982be2f01fbe594cd_21", "text": "We test the model on a corpus of 33 sentences from the Schilling corpus slightly modified by Bicknell and Levy (2010) so that every bigram occurred in the BNC, ensuring that the results do not depend on smoothing."}
{"sent_id": "6388fd7167389982be2f01fbe594cd-C001-54", "intents": ["@EXT@"], "paper_id": "ABC_6388fd7167389982be2f01fbe594cd_21", "text": "In this section, we describe our extension of Bicknell and Levy's (2010) rational model of eye movement control in reading."}
{"sent_id": "6388fd7167389982be2f01fbe594cd-C001-38", "intents": ["@SIM@"], "paper_id": "ABC_6388fd7167389982be2f01fbe594cd_21", "text": "The model presented by Bicknell and Levy (2010) fits this description, and includes visual acuity limitations (in fact, identical to the visual acuity function in SWIFT)."}
{"sent_id": "c796e11db9203d35c1fad61d1329ef-C001-30", "intents": ["@SIM@"], "paper_id": "ABC_c796e11db9203d35c1fad61d1329ef_21", "text": "Indeed, this was the pattern demonstrated by Levy et al. (2015) :"}
{"sent_id": "c796e11db9203d35c1fad61d1329ef-C001-89", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c796e11db9203d35c1fad61d1329ef_21", "text": "In our experiments, we use the implementations of word2vec Skip-Gram with Negative Sampling (SGNS) and PMI matrix factorization via Singular Value Decomposition (SVD) by Levy et al. (2015) ."}
{"sent_id": "c796e11db9203d35c1fad61d1329ef-C001-186", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_c796e11db9203d35c1fad61d1329ef_21", "text": "The equivalent post-processing of the matrices in SVD for explicit inclusion of firstorder similarity suggested by Levy et al. (2015) enhanced the performance of this model in the syntagmatic (relatedness) task only in the expense of making it worse for the paradigmatic (similarity) task."}
{"sent_id": "92d9291093f4ff9f10cca1b8ad2a27-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_92d9291093f4ff9f10cca1b8ad2a27_21", "text": "However, most of the recent approaches (Mohammed et al., 2018; Bordes et al., 2015; Dai et al., 2016; He and Golub, 2016; Yu et al., 2017) are based on automatically extracted features of terms; thanks to the prominent performance of neural network on representation learning (Mikolov et al., 2013a,b) ."}
{"sent_id": "92d9291093f4ff9f10cca1b8ad2a27-C001-65", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_92d9291093f4ff9f10cca1b8ad2a27_21", "text": "Additionally, following Yu et al. (2017) , we add another neural network (Q'-R), the right part of the architecture, to compute the matching score of Q' with the relation of Q (R)."}
{"sent_id": "65f6a6fce98c511473a3ab144a73e4-C001-12", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_65f6a6fce98c511473a3ab144a73e4_21", "text": "Our submission to this competition closely follows Cherry and Guo (2015) , who advocate the use of a semi-Markov tagger trained online with standard discriminative tagging features, gazetteer matches, Brown clusters, and word embeddings."}
{"sent_id": "65f6a6fce98c511473a3ab144a73e4-C001-27", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_65f6a6fce98c511473a3ab144a73e4_21", "text": "The same corpus is used by Cherry and Guo (2015) ."}
{"sent_id": "65f6a6fce98c511473a3ab144a73e4-C001-142", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_65f6a6fce98c511473a3ab144a73e4_21", "text": "The hyper-parameters suggested by Cherry and Guo (2015) (E=10, C=0.01, P=10) were selected to work well with and without representations."}
{"sent_id": "65f6a6fce98c511473a3ab144a73e4-C001-67", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_65f6a6fce98c511473a3ab144a73e4_21", "text": "Note that we do not include part-of-speech tags as features, as they were not found to be useful by Cherry and Guo (2015) ."}
{"sent_id": "65f6a6fce98c511473a3ab144a73e4-C001-186", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_65f6a6fce98c511473a3ab144a73e4_21", "text": "Our entry extends the work of Cherry and Guo (2015) with updated lexicons, phrase embeddings, and gazetteer-infused phrase embeddings."}
{"sent_id": "65f6a6fce98c511473a3ab144a73e4-C001-188", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_65f6a6fce98c511473a3ab144a73e4_21", "text": "Taken together with improved hyper-parameters, these extensions improve the approach of Cherry and Guo (2015) by 2.6 Fmeasure on a completely blind test."}
{"sent_id": "65f6a6fce98c511473a3ab144a73e4-C001-94", "intents": ["@BACK@"], "paper_id": "ABC_65f6a6fce98c511473a3ab144a73e4_21", "text": "In recent years, two sources of information have been found to be valuable to boost the performance for NER: distributed representation learned from a large corpus and domain-specific lexicons (Turian et al., 2010; Cherry and Guo, 2015) ."}
{"sent_id": "7e73137c97a84fe7fa4941ecd06a91-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_7e73137c97a84fe7fa4941ecd06a91_21", "text": "Most modern Q/A systems, however, follow work done by (Li and Roth, 2002) in using machine learning classifiers in order to select the one (or more) EATs from a fixed hierarchy of answer types which are most appropriate for a particular question."}
{"sent_id": "7e73137c97a84fe7fa4941ecd06a91-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_7e73137c97a84fe7fa4941ecd06a91_21", "text": "(A graphical representation of a portion of the LCC ATH is presented in Figure 1 .) The UIUC answer type hierarchy (Li and Roth, 2002) We feel that the time is right for work in ATD to move beyond the UIUC ATH and to begin to tackle problems of organizing and learning answer type hierarchies that encompass several hundreds of diverse expected answer types."}
{"sent_id": "7e73137c97a84fe7fa4941ecd06a91-C001-36", "intents": ["@DIF@"], "paper_id": "ABC_7e73137c97a84fe7fa4941ecd06a91_21", "text": "in contrast, the (Li and Roth, 2002) UIUC ATH is designed especially for questions, but lacks the ability to extend the depth of the hierarchy when Q/A systems are capable of handling more detailed answer types."}
{"sent_id": "7e73137c97a84fe7fa4941ecd06a91-C001-68", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_7e73137c97a84fe7fa4941ecd06a91_21", "text": "In this section, we describe how we used the large ATH introduced in Section 3 in order to annotate a corpus drawn from more than 10,000 questions compiled from (1) existing annotated question corpora (Li and Roth, 2002) , (2) collections of questions mined from the web, and (3) questions submitted to LCC's FERRET question-answering system (Hickl et al., 2006a) ."}
{"sent_id": "fa33495582abd0c6efe8f599c73d0e-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_fa33495582abd0c6efe8f599c73d0e_21", "text": "Recently, a third component has been added to many of these models: an attention mechanism, whereby the decoder can attend directly to localized information from the input sentence during the output generation process (Bahdanau et al., 2015; Luong et al., 2015) ."}
{"sent_id": "fa33495582abd0c6efe8f599c73d0e-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_fa33495582abd0c6efe8f599c73d0e_21", "text": "Models may use computed alignments between source and target sentences to directly copy or transform a word from the input sentence whose corresponding translation is not present in the vocabulary (Luong et al., 2015) or they may conduct sentence tokenization at the level of individual characters (Ling et al., 2015) or subword units such as morphemes (Sennrich et al., 2015b) ."}
{"sent_id": "fa33495582abd0c6efe8f599c73d0e-C001-14", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_fa33495582abd0c6efe8f599c73d0e_21", "text": "The attention mechanism in our four-layer model is what Luong (2015) describes as \"Global attention (dot)\"; the mechanism in our five-layer Y-LSTM model is described in Section 2.1."}
{"sent_id": "fa33495582abd0c6efe8f599c73d0e-C001-34", "intents": ["@DIF@"], "paper_id": "ABC_fa33495582abd0c6efe8f599c73d0e_21", "text": "The beam search decoder differs slightly from Luong (2015) in that we normalize output sentence probabilities by length, following , rather than performing ad-hoc adjustments to correct for short output sentences."}
{"sent_id": "13fe4afa75c5a02727cb8ce3a73297-C001-134", "intents": ["@USE@"], "paper_id": "ABC_13fe4afa75c5a02727cb8ce3a73297_22", "text": "Table 4 : Sample generated by GPT2 → BC → WP + SWAG + SYNTH primed with the same prompt as Fan et al. (2018) ."}
{"sent_id": "13fe4afa75c5a02727cb8ce3a73297-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_13fe4afa75c5a02727cb8ce3a73297_22", "text": "Because GPT2 uses subword tokenization (Sennrich et al., 2016) , it is not directly comparable to the wordlevel perplexity obtained in Fan et al. (2018) ."}
{"sent_id": "13fe4afa75c5a02727cb8ce3a73297-C001-100", "intents": ["@BACK@"], "paper_id": "ABC_13fe4afa75c5a02727cb8ce3a73297_22", "text": "Story Generation: Recent work in neural story generation (Kiros et al., 2015; Roemmele, 2016) has shown success in using hierarchical methods (Yao et al., 2018; Fan et al., 2018) to generate stories."}
{"sent_id": "2ef456a3f6b043350121c4c5cfd404-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_2ef456a3f6b043350121c4c5cfd404_22", "text": "Furthermore, we can show that this solution optimally approximates the sampling from a unigram distribution, which has been shown to be a good noise distribution choice [13, 16] ."}
{"sent_id": "2ef456a3f6b043350121c4c5cfd404-C001-141", "intents": ["@BACK@"], "paper_id": "ABC_2ef456a3f6b043350121c4c5cfd404_22", "text": "Moreover, the performance of the small ReLu-LSTM is comparable to the LSTM models proposed in [16] and [18] which use large hidden layers."}
{"sent_id": "d13502d44435988822e59bcf66b635-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_d13502d44435988822e59bcf66b635_22", "text": "The traditional approach to FSD (Petrovic et al., 2010) computes the distance of each incoming document 1 e.g. a natural disaster or a scandal 2 TDT by NIST -1998 NIST - -2004 ."}
{"sent_id": "d13502d44435988822e59bcf66b635-C001-98", "intents": ["@BACK@"], "paper_id": "ABC_d13502d44435988822e59bcf66b635_22", "text": "It is known for its high effectiveness in the TDT2 and TDT3 competitions (Fiscus, 2001) and widely used as a benchmark for FSD systems (Petrovic et al., 2010; Kasiviswanathan et al., 2011; Petrovic 2013; ) ."}
{"sent_id": "d13502d44435988822e59bcf66b635-C001-104", "intents": ["@USE@"], "paper_id": "ABC_d13502d44435988822e59bcf66b635_22", "text": "We configure their system using the default parameters (Petrovic et al., 2010) ."}
{"sent_id": "375b9c865d9f1b559387aa01a20a78-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_375b9c865d9f1b559387aa01a20a78_22", "text": "By enriching the information of the word, sub-words are useful for capturing morphological changes (Bojanowski et al., 2017) and the meaning of short phrases (Wieting et al., 2016) ."}
{"sent_id": "375b9c865d9f1b559387aa01a20a78-C001-35", "intents": ["@SIM@"], "paper_id": "ABC_375b9c865d9f1b559387aa01a20a78_22", "text": "Each n-gram is explicitly modeled as a composition of its sub-n-grams just like each word is modeled as a composition of sub-words in the subword information skipgram model (SISG) (Bojanowski et al., 2017) ."}
{"sent_id": "375b9c865d9f1b559387aa01a20a78-C001-72", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_375b9c865d9f1b559387aa01a20a78_22", "text": "In order to show comparable results, we use the null vector for these OOV words following Bojanowski et al. (2017) ."}
{"sent_id": "375b9c865d9f1b559387aa01a20a78-C001-42", "intents": ["@UNSURE@"], "paper_id": "ABC_375b9c865d9f1b559387aa01a20a78_22", "text": "Our method SCNE is inspired by recent successful sub-word models (Bojanowski et al., 2017; Zhang et al., 2015) as well as by the segmentation-free character n-gram embedding for unsegmented languages (Oshikiri, 2017) ."}
{"sent_id": "87a190b1df5a7a941ba7b9a98064a3-C001-22", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_87a190b1df5a7a941ba7b9a98064a3_22", "text": "Similar to Zhang et al. (2006) , we employ a convolution parse tree kernel in order to model syntactic structures."}
{"sent_id": "87a190b1df5a7a941ba7b9a98064a3-C001-108", "intents": ["@DIF@"], "paper_id": "ABC_87a190b1df5a7a941ba7b9a98064a3_22", "text": "Compared with the composite kernel (Zhang et al, 2006) , our system further prunes the parse tree and incorporates entity features into the convolution parse tree kernel."}
{"sent_id": "be39cfec0479ace0a7e08508239cb0-C001-3", "intents": ["@MOT@"], "paper_id": "ABC_be39cfec0479ace0a7e08508239cb0_22", "text": "In this paper, we investigate a state-of-the-art paragraph embedding method proposed by Zhang et al. (2017) and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not."}
{"sent_id": "be39cfec0479ace0a7e08508239cb0-C001-5", "intents": ["@MOT@"], "paper_id": "ABC_be39cfec0479ace0a7e08508239cb0_22", "text": "This result motivates us to replace the reconstructionbased objective of Zhang et al. (2017) with our sentence content probe objective in a semisupervised setting."}
{"sent_id": "be39cfec0479ace0a7e08508239cb0-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_be39cfec0479ace0a7e08508239cb0_22", "text": "Methods that embed a paragraph into a single vector have been successfully integrated into many NLP applications, including text classification (Zhang et al., 2017) , document retrieval (Le and Mikolov, 2014) , and semantic similarity and relatedness (Dai et al., 2015; Chen, 2017) ."}
{"sent_id": "be39cfec0479ace0a7e08508239cb0-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_be39cfec0479ace0a7e08508239cb0_22", "text": "Paragraphs to train our classifiers are extracted from the Hotel Reviews corpus (Li et al., 2015) , which has previously been used for evaluating the quality of paragraph embeddings (Li et al., 2015; Zhang et al., 2017) ."}
{"sent_id": "be39cfec0479ace0a7e08508239cb0-C001-86", "intents": ["@USE@"], "paper_id": "ABC_be39cfec0479ace0a7e08508239cb0_22", "text": "With fine-tuning, CNN-SC substantially boosts accuracy and generalization We switch gears Table 4 : CNN-SC outperforms other baseline models that do not use external data, including CNN-R. All baseline models are taken from Zhang et al. (2017) ."}
{"sent_id": "be39cfec0479ace0a7e08508239cb0-C001-20", "intents": ["@UNSURE@"], "paper_id": "ABC_be39cfec0479ace0a7e08508239cb0_22", "text": "Surprisingly, our experiments (Section 2) reveal that despite its impressive downstream performance, the model of Zhang et al. (2017) substantially underperforms a simple bagof-words model on our sentence content probe."}
{"sent_id": "45551e674210bb9bbb56c8778d2f8c-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_45551e674210bb9bbb56c8778d2f8c_22", "text": "This is the state of the art method (Mishra et al., 2018a) for the dataset we are using."}
{"sent_id": "45551e674210bb9bbb56c8778d2f8c-C001-108", "intents": ["@SIM@"], "paper_id": "ABC_45551e674210bb9bbb56c8778d2f8c_22", "text": "However, on the racism class, its recall is hindered by the same factor that Mishra et al. (2018a) highlighted for their node2vec-only method, i.e., that racist tweets come from 5 unique authors only who have also contributed sexist or clean tweets."}
{"sent_id": "45551e674210bb9bbb56c8778d2f8c-C001-127", "intents": ["@EXT@"], "paper_id": "ABC_45551e674210bb9bbb56c8778d2f8c_22", "text": "In this paper, we built on the work of Mishra et al. (2018a) that introduces community-based profiling of authors for abusive language detection."}
{"sent_id": "ebb79e6e223d4747987aa4abfd1a58-C001-43", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_ebb79e6e223d4747987aa4abfd1a58_22", "text": "The SMT system trained on synthetic data eliminates the noisy phrase pairs using 2 As in Artetxe et al. (2018b) , τ is estimated by maximizing the phrase translation probability between an embedding and the nearest embedding on the opposite side."}
{"sent_id": "ebb79e6e223d4747987aa4abfd1a58-C001-114", "intents": ["@BACK@"], "paper_id": "ABC_ebb79e6e223d4747987aa4abfd1a58_22", "text": "Unsupervised Machine Translation Studies on unsupervised methods have been conducted for both NMT (Lample et al., 2018; Marie and Fujita, 2018) and SMT (Artetxe et al., 2018b Table 4 : Error types for which our best system corrected errors well or mostly did not correct on the dev data."}
{"sent_id": "3ced64da2c64b0963c4c3d88fd60e0-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_3ced64da2c64b0963c4c3d88fd60e0_22", "text": "The CRF-based method presented by Kudo et al. (2004) is generally accepted as the state-of-the-art in this paradigm."}
{"sent_id": "3ced64da2c64b0963c4c3d88fd60e0-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_3ced64da2c64b0963c4c3d88fd60e0_22", "text": "1 More fine-grained POS tags have provided small boosts in accuracy in previous research (Kudo et al., 2004) , but these increase the annotation burden, which is contrary to our goal."}
{"sent_id": "3ced64da2c64b0963c4c3d88fd60e0-C001-14", "intents": ["@DIF@"], "paper_id": "ABC_3ced64da2c64b0963c4c3d88fd60e0_22", "text": "We find experimental evidence that pointwise MA can exceed the accuracy of a state-of-the-art structured approach (Kudo et al., 2004) on in-domain data, and is significantly more robust to out-of-domain data."}
{"sent_id": "3ced64da2c64b0963c4c3d88fd60e0-C001-31", "intents": ["@USE@"], "paper_id": "ABC_3ced64da2c64b0963c4c3d88fd60e0_22", "text": "We follow Kudo et al. (2004) in defining our feature set, as summarized in Table 1 1 ."}
{"sent_id": "3ced64da2c64b0963c4c3d88fd60e0-C001-77", "intents": ["@UNSURE@"], "paper_id": "ABC_3ced64da2c64b0963c4c3d88fd60e0_22", "text": "As a representative of joint sequence-based MA described in 2.1, we used MeCab (Kudo, 2006) , an open source implementation of Kudo et al. (2004) 's CRF-based method (we will call this JOINT)."}
{"sent_id": "ce86cf36ee3b359c34b68e5d82b563-C001-9", "intents": ["@EXT@", "@BACK@"], "paper_id": "ABC_ce86cf36ee3b359c34b68e5d82b563_22", "text": "Covington (1996) presents a workable alignment algorithm for comparing two languages."}
{"sent_id": "ce86cf36ee3b359c34b68e5d82b563-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_ce86cf36ee3b359c34b68e5d82b563_22", "text": "Covington (1996) treats this as a process that steps through both strings and, at each step, performs either a \"match\" (accepting a character from both strings), a \"skip-l\" (skipping a character in the first string), or a \"skip-2\" (skipping a character in the second string)."}
{"sent_id": "6cd4235e66a6e6e9768250c3db7fc6-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_6cd4235e66a6e6e9768250c3db7fc6_22", "text": "The most recent work is the learningbased language-independent discriminative parsing approach for normalizing temporal expressions by Angeli and Uszkoreit (2013 There are also (semi-)automatic approaches to port a temporal tagger from one language to another."}
{"sent_id": "6cd4235e66a6e6e9768250c3db7fc6-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_6cd4235e66a6e6e9768250c3db7fc6_22", "text": "1 This issue was also reported by Angeli and Uszkoreit (2013) ."}
{"sent_id": "6cd4235e66a6e6e9768250c3db7fc6-C001-121", "intents": ["@UNSURE@"], "paper_id": "ABC_6cd4235e66a6e6e9768250c3db7fc6_22", "text": "Finally, we compare the normalization quality of our approach to the multilingual parsing approach of Angeli and Uszkoreit (2013) ."}
{"sent_id": "022049c0e75a490978b2c49da41deb-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_022049c0e75a490978b2c49da41deb_22", "text": "The state-of-the-art method for this dataset (Salehi et al., 2014b ) is a supervised support vector regression model, trained over the distributional method from Section 3.1 as applied to both English and 51 target languages (under word and MWE translation)."}
{"sent_id": "022049c0e75a490978b2c49da41deb-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_022049c0e75a490978b2c49da41deb_22", "text": "The state-of-the-art method for this dataset (Salehi et al., 2014b ) is a linear combination of: (1) the distributional method from Section 3.1; (2) the same method applied to 10 target languages (under word and MWE translation, selecting the languages using supervised learning); and (3) the string similarity method of Salehi and Cook (2013) ."}
{"sent_id": "022049c0e75a490978b2c49da41deb-C001-76", "intents": ["@USE@"], "paper_id": "ABC_022049c0e75a490978b2c49da41deb_22", "text": "For comp 1 , α is set to 1.0 for EVPC, and 0.7 for both ENC and GNC, also based on the findings of Salehi et al. (2014b) ."}
{"sent_id": "022049c0e75a490978b2c49da41deb-C001-98", "intents": ["@SIM@", "@FUT@"], "paper_id": "ABC_022049c0e75a490978b2c49da41deb_22", "text": "In future work we intend to explore the contribution of information from word embeddings of a target expression and its component words under translation into many languages, along the lines of Salehi et al. (2014b) ."}
{"sent_id": "742d9ca22bf801b0ade5fd1671473c-C001-107", "intents": ["@BACK@"], "paper_id": "ABC_742d9ca22bf801b0ade5fd1671473c_22", "text": "The state of the art for Math23K, described in Wang et al. (2017) , uses a hybrid Jaccard retrieval and seq2seq model."}
{"sent_id": "742d9ca22bf801b0ade5fd1671473c-C001-43", "intents": ["@USE@"], "paper_id": "ABC_742d9ca22bf801b0ade5fd1671473c_22", "text": "Following Wang et al. (2017) , we use Jaccard distance in this model."}
{"sent_id": "742d9ca22bf801b0ade5fd1671473c-C001-133", "intents": ["@EXT@"], "paper_id": "ABC_742d9ca22bf801b0ade5fd1671473c_22", "text": "More recently, Wang et al. (2017) provide a large dataset of Chinese algebra word problems and learn a hybrid model consisting of both retrieval and seq2seq components."}
{"sent_id": "a5f00f524fdf18e62a4e98a92a2d82-C001-58", "intents": ["@USE@"], "paper_id": "ABC_a5f00f524fdf18e62a4e98a92a2d82_22", "text": "We then use the Stanford Sentiment Classifier (SSC) developed by Socher et al. (2013) to automatically assign sentiment labels (positive, negative) to translated tweets."}
{"sent_id": "a5f00f524fdf18e62a4e98a92a2d82-C001-96", "intents": ["@SIM@"], "paper_id": "ABC_a5f00f524fdf18e62a4e98a92a2d82_22", "text": "Note that our ML baseline systems as well as the English SA classifier by Socher et al. (2013) are trained on balanced data sets, i.e. we can assume no prior bias towards one class."}
{"sent_id": "a5f00f524fdf18e62a4e98a92a2d82-C001-120", "intents": ["@FUT@"], "paper_id": "ABC_a5f00f524fdf18e62a4e98a92a2d82_22", "text": "Note that the Stanford SA system pays particular attention to sentence structure due to its \"deep\" architecture that adds to the model the feature of being sensitive to word ordering (Socher et al., 2013) ."}
{"sent_id": "26658b95c9bac96f1206da96b95921-C001-67", "intents": ["@EXT@"], "paper_id": "ABC_26658b95c9bac96f1206da96b95921_22", "text": "In this section, we introduce the notion of a graded matrix grammar which constitutes a slight variation of matrix grammars as introduced by Rudolph and Giesbrecht (2010) ."}
{"sent_id": "26658b95c9bac96f1206da96b95921-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_26658b95c9bac96f1206da96b95921_22", "text": "More formally, according to Rudolph and Giesbrecht (2010) , the underlying idea can be described as follows: \"Given a mapping · : Σ → S from a set of tokens in Σ into some semantical space S, the composition operation is defined by mapping sequences of meanings to meanings: : S → S. So, the meaning of the sequence of tokens σ 1 · · · σ n can be obtained by first applying the function · to each token and then to the sequence σ 1 · · · σ n , as shown in Figure 2 \"."}
{"sent_id": "26658b95c9bac96f1206da96b95921-C001-95", "intents": ["@BACK@"], "paper_id": "ABC_26658b95c9bac96f1206da96b95921_22", "text": "Inspired by the work of Rudolph and Giesbrecht (2010) they use CMSMs to model composition, and present an algorithm for learning a matrix for each word via ordered logistic regression, which is evaluated with promising results."}
{"sent_id": "26658b95c9bac96f1206da96b95921-C001-40", "intents": ["@UNSURE@"], "paper_id": "ABC_26658b95c9bac96f1206da96b95921_22", "text": "In this section, we provide the definitions of weighted automata in (Balle and Mohri, 2015; Sakarovitch, 2009 ) and matrix-space models of language in (Rudolph and Giesbrecht, 2010) ."}
{"sent_id": "237ac6f9b635e56119be956d7521e1-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_237ac6f9b635e56119be956d7521e1_22", "text": "They usually employ a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with CRF as the output layer, and a CNN (Ma and Hovy, 2016) or LSTM (Lample et al., 2016; Rei et al., 2016) composes the character embeddings."}
{"sent_id": "237ac6f9b635e56119be956d7521e1-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_237ac6f9b635e56119be956d7521e1_22", "text": "The loss function contains not only the log likelihood of the training data and the similarity score but also a language modeling loss, which is not mentioned in (Rei et al., 2016) but discussed in the subsequent work (Rei, 2017) ."}
{"sent_id": "237ac6f9b635e56119be956d7521e1-C001-27", "intents": ["@USE@"], "paper_id": "ABC_237ac6f9b635e56119be956d7521e1_22", "text": "We employed the neural sequence labeling model of (Rei et al., 2016) and experimented with two word representation models: word-level and character-level."}
{"sent_id": "237ac6f9b635e56119be956d7521e1-C001-66", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_237ac6f9b635e56119be956d7521e1_22", "text": "We used an almost identical setting to Rei et al. (2016) : words are lowercased, but characters are not, digits are replaced with zeros, singleton words in the training set are converted into unknown tokens, word and character embedding sizes are 300 and 50 respectively."}
{"sent_id": "237ac6f9b635e56119be956d7521e1-C001-127", "intents": ["@USE@"], "paper_id": "ABC_237ac6f9b635e56119be956d7521e1_22", "text": "We reported an empirical evaluation of neural sequence labeling models by Rei et al. (2016) on NER in Indonesian conversational texts."}
{"sent_id": "237ac6f9b635e56119be956d7521e1-C001-132", "intents": ["@UNSURE@"], "paper_id": "ABC_237ac6f9b635e56119be956d7521e1_22", "text": "While the character model by Rei et al. (2016) has produced good results, it is still quite slow because of the LSTM used for composing character embeddings."}
{"sent_id": "cd56849805cdb43bba567f74b31b87-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_cd56849805cdb43bba567f74b31b87_22", "text": "The decoder of Liu et al. (2018) reconstructs the DRS in three steps, by first predicting the overall structure (the 'boxes'), then the predicates and finally the referents, with each subsequent step being conditioned on the output of the previous."}
{"sent_id": "cd56849805cdb43bba567f74b31b87-C001-111", "intents": ["@BACK@"], "paper_id": "ABC_cd56849805cdb43bba567f74b31b87_22", "text": "Amongst these, tree or graph-structured decoders have recently shown to be state-of-the-art Lapata, 2016, 2018; Liu et al., 2018; Cheng et al., 2017; Yin and Neubig, 2017) ."}
{"sent_id": "cd56849805cdb43bba567f74b31b87-C001-22", "intents": ["@EXT@"], "paper_id": "ABC_cd56849805cdb43bba567f74b31b87_22", "text": "To test our approach we leverage the DRT parser of Liu et al. (2018) , an encoder-decoder architecture where the meaning representation is reconstructed in three stages, coarse-to-fine, by first building the DRS skeleton (i.e. the 'box' structures) and then fill each DRS with predicates and variables."}
{"sent_id": "cd10d509dacd8f55993396258eb92a-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_cd10d509dacd8f55993396258eb92a_22", "text": "Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general [16, 17] , but shows better recognition of rare words and proper nouns."}
{"sent_id": "cd10d509dacd8f55993396258eb92a-C001-34", "intents": ["@MOT@"], "paper_id": "ABC_cd10d509dacd8f55993396258eb92a_22", "text": "This approach also mitigates accuracy regressions that have been observed when using phoneme-only E2E models [16, 17] ."}
{"sent_id": "cd10d509dacd8f55993396258eb92a-C001-183", "intents": ["@DIF@"], "paper_id": "ABC_cd10d509dacd8f55993396258eb92a_22", "text": "However, we note that the regression is significantly smaller than the all-phoneme model in [16] ."}
{"sent_id": "a0c0076fa8c3be914d93ec1d66d0c1-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_a0c0076fa8c3be914d93ec1d66d0c1_22", "text": "AvgAdd is a re-implementation of the best method in Kisselew et al. (2015) :"}
{"sent_id": "a0c0076fa8c3be914d93ec1d66d0c1-C001-27", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_a0c0076fa8c3be914d93ec1d66d0c1_22", "text": "As in Kisselew et al. (2015) , we treat every derivation type as a specific learning problem: we take a set of word pairs with a particular derivation pattern (e.g., \"-in\", Bäcker::Bäckerin), and divide this set into training and test pairs by performing 10-fold cross-validation."}
{"sent_id": "a0c0076fa8c3be914d93ec1d66d0c1-C001-53", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_a0c0076fa8c3be914d93ec1d66d0c1_22", "text": "We thus apply a more informed baseline, the same as in Kisselew et al. (2015) , and predict the derived term at exactly the same position as the base term."}
{"sent_id": "c022b7cf4568e26c7408a835eaafb7-C001-18", "intents": ["@EXT@"], "paper_id": "ABC_c022b7cf4568e26c7408a835eaafb7_22", "text": "We then adapt parse combination methods by Henderson and Brill (1999) , Sagae and Lavie (2006) , and Fossum and Knight (2009) to fuse the constituents from the n parses into a single tree."}
{"sent_id": "c022b7cf4568e26c7408a835eaafb7-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_c022b7cf4568e26c7408a835eaafb7_22", "text": "Varying the threshold changes the precision/recall balance since a high threshold adds only the most confident constituents to the chart (Sagae and Lavie, 2006) ."}
{"sent_id": "c022b7cf4568e26c7408a835eaafb7-C001-86", "intents": ["@SIM@"], "paper_id": "ABC_c022b7cf4568e26c7408a835eaafb7_22", "text": "The threshold was set to 0.42 to optimize model combination F 1 on development data (similar to Setting 2 for constituency parsing in Sagae and Lavie (2006) )."}
{"sent_id": "0143619c1c54129702aafb585463d2-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_0143619c1c54129702aafb585463d2_23", "text": "To our knowledge, the only published results on TAC 2014 is [4] , where the authors utilized query reformulation (QR) based on UMLS ontology."}
{"sent_id": "0143619c1c54129702aafb585463d2-C001-81", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_0143619c1c54129702aafb585463d2_23", "text": "In addition to [4] , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in [4] ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] ."}
{"sent_id": "b1c06a67b03d81b249b320413a6e7e-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_b1c06a67b03d81b249b320413a6e7e_23", "text": "Some (Ravichandran and Hovy, 2002; Bunescu and Mooney, 2007) targeted specific relations like BornInYear, CorporationAcquired, others (Wu and Weld, 2010; Fader et al., 2011 ) extracted any phrase denoting a relation in an English sentence."}
{"sent_id": "b1c06a67b03d81b249b320413a6e7e-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_b1c06a67b03d81b249b320413a6e7e_23", "text": "However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (Kim and Moldovan, 1993; Soderland et al., 1999) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance (Fader et al., 2011) ."}
{"sent_id": "b1c06a67b03d81b249b320413a6e7e-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_b1c06a67b03d81b249b320413a6e7e_23", "text": "However, recently developed OpenIE systems like TextRunner (Banko et al., 2007; Banko, 2009) and ReVerb (Fader et al., 2011) surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences."}
{"sent_id": "b1c06a67b03d81b249b320413a6e7e-C001-21", "intents": ["@DIF@"], "paper_id": "ABC_b1c06a67b03d81b249b320413a6e7e_23", "text": "• We conduct a comparative study with the verb-based relation extraction system ReVerb (Fader et al., 2011) and show that our approach accurately extracts more verb-based relations."}
{"sent_id": "b1c06a67b03d81b249b320413a6e7e-C001-113", "intents": ["@SIM@"], "paper_id": "ABC_b1c06a67b03d81b249b320413a6e7e_23", "text": "For our comparative study with existing systems, we used ReVerb 4 (Fader et al., 2011) , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts."}
{"sent_id": "a334cda78f8ba6dea709809f0999b6-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_a334cda78f8ba6dea709809f0999b6_23", "text": "It has been shown that word embeddings capture human biases (such as gender bias) present in these corpora in how they relate words to each other (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018) ."}
{"sent_id": "a334cda78f8ba6dea709809f0999b6-C001-78", "intents": ["@BACK@"], "paper_id": "ABC_a334cda78f8ba6dea709809f0999b6_23", "text": "In Caliskan et al. (2017) H o is tested through a permutation test, in which X ∪ Y is partitioned into alternative target listsX andŶ exhaustively and computing the one-sided p-value p[s(X,Ŷ , M, F ) > s(X, Y, M, F )], i.e. the proportion of partition permutationsX,Ŷ in which the test statistic s(X,Ŷ , M, F ) is greater than the observed test statistic s(X, Y, M, F )."}
{"sent_id": "a334cda78f8ba6dea709809f0999b6-C001-24", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_a334cda78f8ba6dea709809f0999b6_23", "text": "To address this, we applied the WEAT test on four sets of word embeddings trained on corpora from four domains: social media (Twit-ter), a Wikipedia-based gender-balanced corpus (GAP) and a biomedical corpus (PubMed) and news (Google News, in order to reproduce and validate our results against those of Caliskan et al. (2017) ) (see Section 3)."}
{"sent_id": "a334cda78f8ba6dea709809f0999b6-C001-90", "intents": ["@SIM@"], "paper_id": "ABC_a334cda78f8ba6dea709809f0999b6_23", "text": "Although one would hope to find little gender bias in a news corpus, given that its authors are professional journalists, bias had already been detected by Caliskan et al. (2017) and Garg et al. (2018) using methods similar to ours."}
{"sent_id": "cc992a7a918858f9e04b9bb5c15c3f-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_cc992a7a918858f9e04b9bb5c15c3f_23", "text": "RTMs use Machine Translation Performance Prediction (MTPP) System Biçici and Way, 2014b) , which is a state-of-the-art (SoA) performance predictor of translation even without using the translation."}
{"sent_id": "cc992a7a918858f9e04b9bb5c15c3f-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_cc992a7a918858f9e04b9bb5c15c3f_23", "text": "MTPP features for translation acts are provided in (Biçici and Way, 2014b) ."}
{"sent_id": "cc992a7a918858f9e04b9bb5c15c3f-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_cc992a7a918858f9e04b9bb5c15c3f_23", "text": "Domain specific RTM models obtain improved performance in those domains (Biçici and Way, 2014b) ."}
{"sent_id": "cc992a7a918858f9e04b9bb5c15c3f-C001-19", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_cc992a7a918858f9e04b9bb5c15c3f_23", "text": "We present positive results from a fully automated judge for semantic similarity based on Referential Translation Machines (Biçici and Way, 2014b) in two semantic similarity tasks at SemEval-2015, Semantic Evaluation Exercises -International Workshop on Semantic Evaluation (Nakov et al., 2015) ."}
{"sent_id": "86af8f2cc08b00821a7a83abdfd964-C001-3", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_86af8f2cc08b00821a7a83abdfd964_23", "text": "Specifically, we extend the method recently proposed by Täckström et al. (2012) , which is based on cross-lingual word cluster features."}
{"sent_id": "86af8f2cc08b00821a7a83abdfd964-C001-21", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_86af8f2cc08b00821a7a83abdfd964_23", "text": "Specifically, we extend the direct transfer method proposed by Täckström et al. (2012) in two ways."}
{"sent_id": "86af8f2cc08b00821a7a83abdfd964-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_86af8f2cc08b00821a7a83abdfd964_23", "text": "Although semi-supervised approaches have been shown to reduce the need for manual annotation (Freitag, 2004; Miller et al., 2004; Ando and Zhang, 2005; Suzuki and Isozaki, 2008; Lin and Wu, 2009; Turian et al., 2010; Dhillon et al., 2011; Täckström et al., 2012) , these methods still require a substantial amount of manual annotation for each target language."}
{"sent_id": "86af8f2cc08b00821a7a83abdfd964-C001-153", "intents": ["@BACK@"], "paper_id": "ABC_86af8f2cc08b00821a7a83abdfd964_23", "text": "We therefore suggest that such clusters could be of general use in multilingual learning of linguistic structure, in the same way that monolingual word clusters have been shown to be a robust way to bring improvements in many monolingual applications (Turian et al., 2010; Täckström et al., 2012) ."}
{"sent_id": "918caabbc0bcad04cd07761b29e767-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_918caabbc0bcad04cd07761b29e767_23", "text": "Li and Roth (2002) use a Sparse Network of Winnows (SNoW) to classify questions with respect to their expected answer type."}
{"sent_id": "918caabbc0bcad04cd07761b29e767-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_918caabbc0bcad04cd07761b29e767_23", "text": "This taxonomy has been chosen since it is the most frequently used one in earlier work in the field (Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003) ."}
{"sent_id": "918caabbc0bcad04cd07761b29e767-C001-45", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_918caabbc0bcad04cd07761b29e767_23", "text": "The same taxonomy, training and testing data was used as in Li and Roth (2002)"}
{"sent_id": "918caabbc0bcad04cd07761b29e767-C001-54", "intents": ["@USE@"], "paper_id": "ABC_918caabbc0bcad04cd07761b29e767_23", "text": "The taxonomy used is the taxonomy proposed by Li and Roth (2002) ."}
{"sent_id": "918caabbc0bcad04cd07761b29e767-C001-71", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_918caabbc0bcad04cd07761b29e767_23", "text": "First, we have used the corpus originally developed by (Li and Roth, 2002) , but since the test corpus used consists of questions solely from TREC-10 and the TREC conferences have a specific agenda the test corpus might be slightly different from the training data."}
{"sent_id": "fb1061d28dbf80858c1a630621a975-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_fb1061d28dbf80858c1a630621a975_23", "text": "Recent work by (Jiang et al., 2018; Mayfield et al., 2012) adopt deep learning approaches to compute message pair similarity, using a combination of message content and simple contextual features (e.g., authorship and timestamps)."}
{"sent_id": "fb1061d28dbf80858c1a630621a975-C001-103", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_fb1061d28dbf80858c1a630621a975_23", "text": "Reddit Dataset Improvement: We use the same pre-processing method in (Jiang et al., 2018) : we discard the messages which have less than 10 words or more than 100 words."}
{"sent_id": "fb1061d28dbf80858c1a630621a975-C001-122", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_fb1061d28dbf80858c1a630621a975_23", "text": "833 .431 .433 Evaluation Metrics: Normalized mutual information (NMI), Adjusted rand index (ARI) and F1 score, following (Jiang et al., 2018) ."}
{"sent_id": "fb1061d28dbf80858c1a630621a975-C001-76", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_fb1061d28dbf80858c1a630621a975_23", "text": "Training Procedure: Following (Jiang et al., 2018) , apart from a new thread, we consider the candidate threads (Active Threads) in Eq. 1 only from those appearing in one hour time-frame before m i ."}
{"sent_id": "520588fbf0643725153b07a09430d1-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_520588fbf0643725153b07a09430d1_23", "text": "Our previous work [9] demonstrates that the lexicon independent models can outperform lexicon dependent models on Mandarin Chinese ASR tasks by the ASR Transformer and the character based model establishes a new state-of-the-art character error rate (CER) on HKUST datasets."}
{"sent_id": "520588fbf0643725153b07a09430d1-C001-89", "intents": ["@BACK@"], "paper_id": "ABC_520588fbf0643725153b07a09430d1_23", "text": "We perform our experiments on the big model (D1024-H16) [9, 17] of the ASR Transformer."}
{"sent_id": "520588fbf0643725153b07a09430d1-C001-24", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_520588fbf0643725153b07a09430d1_23", "text": "Building on our work [9] , we employ sub-words generated by byte pair encoding (BPE) [11] as the multilingual modeling unit, which do not need any pronunciation lexicon."}
{"sent_id": "520588fbf0643725153b07a09430d1-C001-25", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_520588fbf0643725153b07a09430d1_23", "text": "The ASR Transformer is chosen to be the basic architecture of sequence-to-sequence attention-based model [9, 12] ."}
{"sent_id": "520588fbf0643725153b07a09430d1-C001-44", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_520588fbf0643725153b07a09430d1_23", "text": "The ASR Transformer architecture used in this work is the same as our work [9, 12] which is shown in Figure 1 ."}
{"sent_id": "29294f2ed3cc2772ca57fd4294274c-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_29294f2ed3cc2772ca57fd4294274c_23", "text": "These algorithms have been incorporated into a tool which has been used to create characters for a variety of applications (e.g. Leuski et al., 2006; Artstein et al., 2009; Swartout et al., 2010) ."}
{"sent_id": "29294f2ed3cc2772ca57fd4294274c-C001-74", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_29294f2ed3cc2772ca57fd4294274c_23", "text": "This measure does not take into account non-understanding, that is the classifier's determination that the best response is not good enough (Leuski et al., 2006) , since this capability was not implemented; however, since all of our test questions are known to have at least one appropriate response, any non-understanding of a question would necessarily count against accuracy anyway."}
{"sent_id": "29294f2ed3cc2772ca57fd4294274c-C001-48", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_29294f2ed3cc2772ca57fd4294274c_23", "text": "We reimplemented parts of the response ranking algorithms of Leuski et al. (2006) , including both the language modeling (LM) and cross-language modeling (CLM) approaches."}
{"sent_id": "29294f2ed3cc2772ca57fd4294274c-C001-63", "intents": ["@DIF@"], "paper_id": "ABC_29294f2ed3cc2772ca57fd4294274c_23", "text": "We also do not use the response threshold parameter, which Leuski et al. (2006) use to determine whether the top-ranked response is good enough."}
{"sent_id": "29294f2ed3cc2772ca57fd4294274c-C001-78", "intents": ["@DIF@"], "paper_id": "ABC_29294f2ed3cc2772ca57fd4294274c_23", "text": "The LM approach almost invariably produced better results than the CLM approach; this is the opposite of the findings of Leuski et al. (2006) , where CLM fared consistently better."}
{"sent_id": "bce5c3bf551a8aa211dfd962cde7a8-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_bce5c3bf551a8aa211dfd962cde7a8_23", "text": "As an example of such work, Zhang et al. (2008) have shown in the past that deep linguistic parsing outputs can be integrated to help improve the performance of the English semantic role labeling task."}
{"sent_id": "bce5c3bf551a8aa211dfd962cde7a8-C001-26", "intents": ["@SIM@"], "paper_id": "ABC_bce5c3bf551a8aa211dfd962cde7a8_23", "text": "It is similar to the architecture used by Zhang et al. (2008) ."}
{"sent_id": "bce5c3bf551a8aa211dfd962cde7a8-C001-102", "intents": ["@SIM@"], "paper_id": "ABC_bce5c3bf551a8aa211dfd962cde7a8_23", "text": "The semantic role labeling component used in the submitted system is similar to the one described by Zhang et al. (2008) ."}
{"sent_id": "bce5c3bf551a8aa211dfd962cde7a8-C001-32", "intents": ["@DIF@"], "paper_id": "ABC_bce5c3bf551a8aa211dfd962cde7a8_23", "text": "Comparing to Zhang et al. (2008) , this architecture simplified the syntactic component, and puts more focus on the integration of deep parsing outputs."}
{"sent_id": "836992d035c4be0c8eacdd419f151e-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_836992d035c4be0c8eacdd419f151e_23", "text": "While this approach is suitable for entity linking settings such as newswire (Bentivogli, 2010) and Wikipedia (Ratinov et al., 2011) , we cannot always rely on this information in other settings like Twitter (Guo et al., 2013; Fang and Chang, 2014; Huang et al., 2014; Dredze et al., 2016) , Snapchat (Moon et al., 2018) , other web platforms (Eshel et al., 2017) , or dialogue systems (Bowden et al., 2018) ."}
{"sent_id": "836992d035c4be0c8eacdd419f151e-C001-14", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_836992d035c4be0c8eacdd419f151e_23", "text": "In this work, we investigate this problem of effectively using context in the setting of the WikilinksNED dataset from Eshel et al. (2017) ."}
{"sent_id": "836992d035c4be0c8eacdd419f151e-C001-16", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_836992d035c4be0c8eacdd419f151e_23", "text": "We build off a state-of-the-art attentive LSTM model from prior work (Eshel et al., 2017) and show that despite its good performance, it fails to resolve some examples that human readers would find trivial."}
{"sent_id": "836992d035c4be0c8eacdd419f151e-C001-27", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_836992d035c4be0c8eacdd419f151e_23", "text": "We therefore follow prior work (Eshel et al., 2017) and take as candidates all gold entities in the training set whose mention was m rather than relying on a separate candidate generation scheme."}
{"sent_id": "836992d035c4be0c8eacdd419f151e-C001-32", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_836992d035c4be0c8eacdd419f151e_23", "text": "Embedding entities We follow the method of Eshel et al. (2017) for generating entity embeddings, using word2vecf (Levy and Goldberg, 2014) to jointly train word and entity embeddings simultaneously using Wikipedia article text."}
{"sent_id": "836992d035c4be0c8eacdd419f151e-C001-29", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_836992d035c4be0c8eacdd419f151e_23", "text": "This model, depicted in Figure 1 , roughly follows that of Eshel et al. (2017) , with some key differences, as we discuss in the rest of this section."}
{"sent_id": "836992d035c4be0c8eacdd419f151e-C001-41", "intents": ["@DIF@"], "paper_id": "ABC_836992d035c4be0c8eacdd419f151e_23", "text": "Unlike Eshel et al. (2017), we structure training as a multiclass decision among these titles rather than a binary prediction problem over each title as gold or not."}
{"sent_id": "4b65a59fc2331b9771ea09a12f32de-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_4b65a59fc2331b9771ea09a12f32de_23", "text": "Locascio et al. (2016) proposed the Deep-Regex model based on Seq2Seq for generating regular expressions from natural language descriptions together with a dataset of 10,000 NL-RX pairs."}
{"sent_id": "4b65a59fc2331b9771ea09a12f32de-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_4b65a59fc2331b9771ea09a12f32de_23", "text": "Datasets: Locascio et al. (2016) created a set of NL-RX pair data by arbitrarily creating and combining data in a tree form."}
{"sent_id": "4b65a59fc2331b9771ea09a12f32de-C001-133", "intents": ["@BACK@"], "paper_id": "ABC_4b65a59fc2331b9771ea09a12f32de_23", "text": "We now describe how Locascio et al. (2016) generated their synthetic regular expression data."}
{"sent_id": "4b65a59fc2331b9771ea09a12f32de-C001-75", "intents": ["@SIM@"], "paper_id": "ABC_4b65a59fc2331b9771ea09a12f32de_23", "text": "Similar to Locascio et al. (2016) , we randomly generate regular expression pairs up to depth three and label the equivalence between each pair."}
{"sent_id": "dd603c79f87e98d23f6f8e13028ae9-C001-4", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_dd603c79f87e98d23f6f8e13028ae9_23", "text": "In this paper we leverage the Machine Translation of Noisy Text (MTNT) dataset (Michel and Neubig, 2018) to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data."}
{"sent_id": "dd603c79f87e98d23f6f8e13028ae9-C001-16", "intents": ["@USE@"], "paper_id": "ABC_dd603c79f87e98d23f6f8e13028ae9_23", "text": "We present a series of experiments based on the Machine Translation of Noisy Text (MTNT) data set (Michel and Neubig, 2018 ) through which we demonstrate improved resilience of a vanilla MT system by adaptation using artificially noised data."}
{"sent_id": "dd603c79f87e98d23f6f8e13028ae9-C001-51", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_dd603c79f87e98d23f6f8e13028ae9_23", "text": "For this method, we inject artificial noise in the clean data according to the distribution of types of noise in MTNT specified in Michel and Neubig (2018) ."}
{"sent_id": "dd603c79f87e98d23f6f8e13028ae9-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_dd603c79f87e98d23f6f8e13028ae9_23", "text": "This is particularly pronounced in systems trained on clean, formalized parallel data such as Europarl (Koehn, 2005) , are tasked with translation of unedited, human generated text such as is common in domains such as social media, where accurate translation is becoming of widespread relevance (Michel and Neubig, 2018) ."}
{"sent_id": "dd603c79f87e98d23f6f8e13028ae9-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_dd603c79f87e98d23f6f8e13028ae9_23", "text": "Human generated text on the internet and social media are a particularly rich source of natural noise (Eisenstein, 2013; Baldwin et al., 2015) which causes pronounced problems for MT (Michel and Neubig, 2018) ."}
{"sent_id": "dd603c79f87e98d23f6f8e13028ae9-C001-43", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_dd603c79f87e98d23f6f8e13028ae9_23", "text": "For expediency and convenience of experimentation we have chosen to deploy a smaller, faster variant of the model used in Michel and Neubig (2018) , which allows us to provide comparative results across a variety of settings."}
{"sent_id": "874a8d4f847aff2895deb7c7560c56-C001-57", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_874a8d4f847aff2895deb7c7560c56_23", "text": "We used the dataset provided by [9] to extract the othering lexicon content."}
{"sent_id": "874a8d4f847aff2895deb7c7560c56-C001-216", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_874a8d4f847aff2895deb7c7560c56_23", "text": "This row is then converted to vector space using the paragraph2vec algorithm:  Each part was extracted as following:  Our lexicon content was extracted from negative samples of an annotated dataset [9] ."}
{"sent_id": "874a8d4f847aff2895deb7c7560c56-C001-256", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_874a8d4f847aff2895deb7c7560c56_23", "text": "In addition to that, we implemented semi-supervised classification by training in the positive samples of the [9] dataset and training in only the lexicon as negative samples."}
{"sent_id": "874a8d4f847aff2895deb7c7560c56-C001-266", "intents": ["@SIM@"], "paper_id": "ABC_874a8d4f847aff2895deb7c7560c56_23", "text": "For semi-supervised learning, when we tested our lexicon on unseen datasets, we trained the embedding algorithm in our othering lexicon as negative and the positive samples of the [9] dataset (see Figure 9 )."}
{"sent_id": "b3f7051cbba3344f0aec0f2e80d5e0-C001-16", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_b3f7051cbba3344f0aec0f2e80d5e0_23", "text": "As we summarize in Section 2, this paper relies on the same data set and evaluation metric as DeVault et al. (2011) , which reports results for learned policies based on maximum entropy models."}
{"sent_id": "b3f7051cbba3344f0aec0f2e80d5e0-C001-35", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_b3f7051cbba3344f0aec0f2e80d5e0_23", "text": "We perform our experiments and evaluation using an existing set of 19 annotated Amani dialogues (DeVault et al., 2011) ."}
{"sent_id": "b3f7051cbba3344f0aec0f2e80d5e0-C001-62", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_b3f7051cbba3344f0aec0f2e80d5e0_23", "text": "To measure the performance of the dialogue policy, we follow the approach of DeVault et al. (2011) , which counts an automatically produced system SA as correct if that SA was chosen by at least one referee for that dialogue turn in the data set."}
{"sent_id": "b3f7051cbba3344f0aec0f2e80d5e0-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_b3f7051cbba3344f0aec0f2e80d5e0_23", "text": "We refer the reader to DeVault et al. (2011) for additional details."}
{"sent_id": "b3f7051cbba3344f0aec0f2e80d5e0-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_b3f7051cbba3344f0aec0f2e80d5e0_23", "text": "The system builders' intended policy for Amani is detailed in DeVault et al. (2011) ."}
{"sent_id": "b3f7051cbba3344f0aec0f2e80d5e0-C001-112", "intents": ["@BACK@"], "paper_id": "ABC_b3f7051cbba3344f0aec0f2e80d5e0_23", "text": "As previously reported in DeVault et al. (2011) , a performance of .66 is achieved with the MaxEnt policy when trained on text-based features."}
{"sent_id": "b5149b6136c8baaed8356b562d3f96-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_b5149b6136c8baaed8356b562d3f96_23", "text": "With a more accurate model, however, we found (Moore, 2014 ) that while Ratnaparkhi's tag dictionary decreased the average number of tags per token from 45 to 3.7 on the current standard WSJ development set, it also decreased per-tag accuracy from 97.31% to 97.19%."}
{"sent_id": "b5149b6136c8baaed8356b562d3f96-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_b5149b6136c8baaed8356b562d3f96_23", "text": "For full details of the feature set, see our previous paper (Moore, 2014) ."}
{"sent_id": "b5149b6136c8baaed8356b562d3f96-C001-116", "intents": ["@BACK@"], "paper_id": "ABC_b5149b6136c8baaed8356b562d3f96_23", "text": "The last column shows the tagging speed in tokens per second for each of the three tag dictionaries, using the fast tagging method we previously described (Moore, 2014) , in a singlethreaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors."}
{"sent_id": "b5149b6136c8baaed8356b562d3f96-C001-19", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_b5149b6136c8baaed8356b562d3f96_23", "text": "While Ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed (Moore, 2014 ) makes tagging as fast as with Ratnaparkhi's tag dictionary, but with no decrease in accuracy."}
{"sent_id": "09dfa2f17283fe6b3fc28383f36732-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_09dfa2f17283fe6b3fc28383f36732_23", "text": "In addition, several works have demonstrated how combining robot affordance learning with language grounding can provide cognitive robots with new and useful skills, such as learning the association of spoken words with sensorimotor experience [10, 11] or sensorimotor representations [12] , learning tool use capabilities [13, 14] , and carrying out complex manipulation tasks expressed in natural language instructions which require planning and reasoning [15] ."}
{"sent_id": "09dfa2f17283fe6b3fc28383f36732-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_09dfa2f17283fe6b3fc28383f36732_23", "text": "This joint probability distribution, that is illustrated by the part of Fig. 2 enclosed in the dashed box, is estimated by the robot in an ego-centric way through interaction with the environment, as in [10] ."}
{"sent_id": "09dfa2f17283fe6b3fc28383f36732-C001-41", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_09dfa2f17283fe6b3fc28383f36732_23", "text": "In this paper, we combine (1) the robot affordance model of [10] , which associates verbal descriptions to the physical interactions of an agent with the environment, with (2) the gesture recognition system of [4] , which infers the type of action from human user movements."}
{"sent_id": "09dfa2f17283fe6b3fc28383f36732-C001-46", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_09dfa2f17283fe6b3fc28383f36732_23", "text": "Our main contribution is that of extending [10] by relaxing the assumption that the action is known during the learning phase."}
{"sent_id": "09dfa2f17283fe6b3fc28383f36732-C001-74", "intents": ["@EXT@"], "paper_id": "ABC_09dfa2f17283fe6b3fc28383f36732_23", "text": "In this study we wish to generalize the model of [10] by observing external (human) agents, as shown in Fig. 1 ."}
{"sent_id": "09dfa2f17283fe6b3fc28383f36732-C001-54", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_09dfa2f17283fe6b3fc28383f36732_23", "text": "Following the method adopted in [10] , we use a Bayesian probabilistic framework to allow a robot to ground the basic world behavior and verbal descriptions associated to it."}
{"sent_id": "4cf805818bed233fabb81f5f64f4cc-C001-17", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4cf805818bed233fabb81f5f64f4cc_23", "text": "Additionally, we also experiment with different transition systems, most notably the integrated parsing and part-of-speech (POS) tagging system of Bohnet and Nivre (2012) and also the swap system of Nivre (2009) ."}
{"sent_id": "4cf805818bed233fabb81f5f64f4cc-C001-52", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4cf805818bed233fabb81f5f64f4cc_23", "text": "Inspired by the work of Bohnet and Nivre (2012) , we embed the set of top tags according to a first-stage tagger."}
{"sent_id": "4cf805818bed233fabb81f5f64f4cc-C001-85", "intents": ["@DIF@"], "paper_id": "ABC_4cf805818bed233fabb81f5f64f4cc_23", "text": "For ease of experimentation, we deviate from Bohnet and Nivre (2012) and use a single unstructured beam, rather than separate beams for POS tag and parse differences."}
{"sent_id": "4cf805818bed233fabb81f5f64f4cc-C001-107", "intents": ["@DIF@"], "paper_id": "ABC_4cf805818bed233fabb81f5f64f4cc_23", "text": "It surpasses its linear analog, the work of Bohnet and Nivre (2012) on Stanford Dependencies UAS by 0.9% UAS and by 1.14% LAS."}
{"sent_id": "1bcd442a685e5fb2d0f3f44d3c66c3-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_1bcd442a685e5fb2d0f3f44d3c66c3_23", "text": "In (Lazaridou, Peysakhovich, and Baroni 2016) , the authors have restricted the message to be a single symbol token picked from a fixed vocabulary while in (Havrylov and Titov 2017) , the message is considered to be a sequence of symbols."}
{"sent_id": "1bcd442a685e5fb2d0f3f44d3c66c3-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_1bcd442a685e5fb2d0f3f44d3c66c3_23", "text": "(Mordatch and Abbeel 2018) further extends the scope of mode of communication by also studying the emergence of non-verbal communication."}
{"sent_id": "1bcd442a685e5fb2d0f3f44d3c66c3-C001-29", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_1bcd442a685e5fb2d0f3f44d3c66c3_23", "text": "In our work, we have used two referential game setups that are slight modifications to the ones used in (Lazaridou, Peysakhovich, and Baroni 2016; Lazaridou et al. 2018 )."}
{"sent_id": "982991efdb6b14f187702e0a577bac-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_982991efdb6b14f187702e0a577bac_23", "text": "Until now, their dataset was the only publicly available set of messages with annotated conversations (partially re-annotated by Mehri and Carenini (2017) with reply-structure graphs), and has been used for training and evaluation in subsequent work (Wang and Oard, 2009; Mehri and Carenini, 2017; Jiang et al., 2018) ."}
{"sent_id": "982991efdb6b14f187702e0a577bac-C001-119", "intents": ["@DIF@"], "paper_id": "ABC_982991efdb6b14f187702e0a577bac_23", "text": "Values are in the good agreement range proposed by Altman (1990) , and slightly higher than for Mehri and Carenini (2017)'s annotations."}
{"sent_id": "982991efdb6b14f187702e0a577bac-C001-129", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_982991efdb6b14f187702e0a577bac_23", "text": "We follow Mehri and Carenini (2017) and keep system messages."}
{"sent_id": "c0cac496ec0abdfd3f6bd9914f4cc4-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_c0cac496ec0abdfd3f6bd9914f4cc4_24", "text": "Since sentiment lexicons helped in improving the accuracy of sentiment classification models (Liu and Zhang, 2012; Al-Sallab et al., 2017; Badaro et al., 2014a Badaro et al., ,b, 2015 , several researchers are working on developing emotion lexicons for different languages such as English, French, Polish and Chinese (Mohammad, 2017; Bandhakavi et al., 2017; Yang et al., 2007; Mohammad and Turney, 2013; Abdaoui et al., 2017; Staiano and Guerini, 2014; Maziarz et al., 2016; Janz et al., 2017) ."}
{"sent_id": "c0cac496ec0abdfd3f6bd9914f4cc4-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_c0cac496ec0abdfd3f6bd9914f4cc4_24", "text": "For example, DepecheMood (Staiano and Guerini, 2014) , one of the largest publicly available emotion lexicon for English, includes around 37K terms while SentiWordNet (SWN) (Esuli and Sebastiani, 2007; Baccianella et al., 2010) , a large scale English sentiment lexicon semi-automatically generated using English WordNet (EWN) (Fellbaum, 1998) , includes around 150K terms annotated with three sentiment scores: positive, negative and objective."}
{"sent_id": "c0cac496ec0abdfd3f6bd9914f4cc4-C001-62", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_c0cac496ec0abdfd3f6bd9914f4cc4_24", "text": "We select to expand the DepecheMood variation with normalized scores since this variation performed best according to the presented results in (Staiano and Guerini, 2014) ."}
{"sent_id": "c0cac496ec0abdfd3f6bd9914f4cc4-C001-94", "intents": ["@SIM@"], "paper_id": "ABC_c0cac496ec0abdfd3f6bd9914f4cc4_24", "text": "We considered the same emotion mapping assumptions presented in the work of (Staiano and Guerini, 2014) : Fear → Afraid, Anger → Angry, Joy → Happy, Sadness → Sad and Surprise → Inspired."}
{"sent_id": "c0cac496ec0abdfd3f6bd9914f4cc4-C001-110", "intents": ["@SIM@"], "paper_id": "ABC_c0cac496ec0abdfd3f6bd9914f4cc4_24", "text": "As stated in (Staiano and Guerini, 2014) paper, 'Disgust' emotion was excluded since there was no corresponding mapping in EmoWordNet/DepecheMood."}
{"sent_id": "6891aebc7bb1152884d2236a893b55-C001-12", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6891aebc7bb1152884d2236a893b55_24", "text": "Our approach to the annotation projection builds upon the approach recently introduced by (Grishina and Stede, 2017) , who experimented with projecting manually annotated coreference chains from two source languages to the target language."}
{"sent_id": "6891aebc7bb1152884d2236a893b55-C001-28", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6891aebc7bb1152884d2236a893b55_24", "text": "We use the English-German-Russian unannotated corpus of (Grishina and Stede, 2017) as the basis for our experiment, which contains texts in two genres -newswire texts (229 sentences per language) and short stories (184 sentences per language)."}
{"sent_id": "6891aebc7bb1152884d2236a893b55-C001-45", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6891aebc7bb1152884d2236a893b55_24", "text": "Thereafter, we re-implement the multi-source approach as described in (Grishina and Stede, 2017) ."}
{"sent_id": "6891aebc7bb1152884d2236a893b55-C001-47", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6891aebc7bb1152884d2236a893b55_24", "text": "In our experiment, we apply the following strategies from (Grishina and Stede, 2017):"}
{"sent_id": "6891aebc7bb1152884d2236a893b55-C001-58", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6891aebc7bb1152884d2236a893b55_24", "text": "As for the multi-source settings, we were able to achieve the highest F1 of 36.2 by combining disjoint chains (Setting 1), which is 1.9 point higher than the best single-source projection scores and constitutes almost 62% of the quality of the projection of gold standard annotations reported in (Grishina and Stede, 2017) ."}
{"sent_id": "6891aebc7bb1152884d2236a893b55-C001-75", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6891aebc7bb1152884d2236a893b55_24", "text": "Following the work of (Grishina and Stede, 2017) , we analyse the projection accuracy for common nouns ('Nc'), named entities ('Np') and pronouns ('P') separately 4 : Table 5 shows the percentage of correctly projected markables of each type out of all the projected markables of this type."}
{"sent_id": "6891aebc7bb1152884d2236a893b55-C001-14", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_6891aebc7bb1152884d2236a893b55_24", "text": "Therefore, in contrast to (Grishina and Stede, 2017) , we use automatic source annotations produced by two state-of-the-art coreference systems, and we combine the output of our projection method for two source languages (English and German) to obtain target annotations for a third language (Russian)."}
{"sent_id": "6891aebc7bb1152884d2236a893b55-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_6891aebc7bb1152884d2236a893b55_24", "text": "Thereafter, (Grishina and Stede, 2017) proposed a multi-source method for annotation projection: They used a manually annotated trilingual coreference corpus and two source languages (English-German, English-Russian) to transfer annotations to the target language (Russian and German, respectively)."}
{"sent_id": "939274ae40a68acc322b34d8f91f7e-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_939274ae40a68acc322b34d8f91f7e_24", "text": "Recently, Verga et al. (2018) introduced multi-instance learning (MIL) (Riedel et al., 2010; Surdeanu et al., 2012) to treat multiple mentions of target entities in a document."}
{"sent_id": "939274ae40a68acc322b34d8f91f7e-C001-27", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_939274ae40a68acc322b34d8f91f7e_24", "text": "We infer relations between entities using MIL-based bi-affine pairwise scoring function (Verga et al., 2018) on the entity node representations."}
{"sent_id": "939274ae40a68acc322b34d8f91f7e-C001-47", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_939274ae40a68acc322b34d8f91f7e_24", "text": "Next, it encodes the graph structure using a stacked GCNN layer and classifies the relation between the target entities by applying MIL (Verga et al., 2018) to aggregate all 1 The dataset is publicly available at http://nactem."}
{"sent_id": "939274ae40a68acc322b34d8f91f7e-C001-76", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_939274ae40a68acc322b34d8f91f7e_24", "text": "Since each target entity can have multiple mentions in a document, we employ a multi-instance learning (MIL)-based classification scheme to aggregate the predictions of all target mention pairs using bi-affine pairwise scoring (Verga et al., 2018) ."}
{"sent_id": "939274ae40a68acc322b34d8f91f7e-C001-96", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_939274ae40a68acc322b34d8f91f7e_24", "text": "For the CDR dataset, we performed hypernym filtering similar to Gu et al. (2017) and Verga et al. (2018) ."}
{"sent_id": "939274ae40a68acc322b34d8f91f7e-C001-105", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_939274ae40a68acc322b34d8f91f7e_24", "text": "For the CDR dataset, we compare with five stateof-the-art models: SVM , ensemble of feature-based and neural-based models (Zhou et al., 2016a) , CNN and Maximum Entropy (Gu et al., 2017) , Piece-wise CNN (Li et al., 2018) and Transformer (Verga et al., 2018) ."}
{"sent_id": "939274ae40a68acc322b34d8f91f7e-C001-111", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_939274ae40a68acc322b34d8f91f7e_24", "text": "Unlike Verga et al. (2018), we used the pre-trained word embeddings in place of sub-word embeddings to align with our word graphs."}
{"sent_id": "939274ae40a68acc322b34d8f91f7e-C001-138", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_939274ae40a68acc322b34d8f91f7e_24", "text": "Our work is different from Verga et al. (2018) in that we replace Transformer with a GCNN model for full-abstract encoding using non-local dependencies such as entity coreference."}
{"sent_id": "dd875dd5c0f2558bb173f31bbdea00-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_dd875dd5c0f2558bb173f31bbdea00_24", "text": "Qualia Structures have been originally introduced by (Pustejovsky, 1991) and are used for a variety of purposes in Natural Language processing such as the analysis of compounds (Johnston and Busa, 1996) , co-composition and coercion (Pustejovsky, 1991) as well as for bridging reference resolution (Bos et al., 1995) ."}
{"sent_id": "dd875dd5c0f2558bb173f31bbdea00-C001-94", "intents": ["@BACK@"], "paper_id": "ABC_dd875dd5c0f2558bb173f31bbdea00_24", "text": "Busa, 1996) or (Pustejovsky, 1991) , as well as computer, an abstract noun, i.e. conversation, as well as two very specific multi-term words, i.e. natural language processing and data mining."}
{"sent_id": "dd875dd5c0f2558bb173f31bbdea00-C001-51", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_dd875dd5c0f2558bb173f31bbdea00_24", "text": "In general it is important to mention that by this approach we are not able to detect and separate multiple meanings of words, i.e. to handle polysemy, which is appropriately accounted for in the framework of the Generative Lexicon (Pustejovsky, 1991) ."}
{"sent_id": "dd875dd5c0f2558bb173f31bbdea00-C001-117", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_dd875dd5c0f2558bb173f31bbdea00_24", "text": "For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare (Pustejovsky, 1991) ."}
{"sent_id": "dd875dd5c0f2558bb173f31bbdea00-C001-118", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_dd875dd5c0f2558bb173f31bbdea00_24", "text": "As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to (Pustejovsky, 1991) ."}
{"sent_id": "dd875dd5c0f2558bb173f31bbdea00-C001-121", "intents": ["@MOT@"], "paper_id": "ABC_dd875dd5c0f2558bb173f31bbdea00_24", "text": "It seems that give is emphasizing the role of a book as a gift, read is referring to the most obvious purpose of a book as specified in the ideal qualia structures of (Pustejovsky, 1991) as well as (Johnston and Busa, 1996) and purchase denotes the more general purpose of a book, i.e. to be bought."}
{"sent_id": "dd875dd5c0f2558bb173f31bbdea00-C001-136", "intents": ["@MOT@"], "paper_id": "ABC_dd875dd5c0f2558bb173f31bbdea00_24", "text": "Considering the results for the Formal role, the elements drink (1st), alcohol (2nd) and beverage (4th) are much more specific than liquid as given in (Pustejovsky, 1991) , while thing at the 3rd position is certainly too general."}
{"sent_id": "aa496bd71f380e02dd392cda969999-C001-79", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_aa496bd71f380e02dd392cda969999_24", "text": "We experimented with four datasets widely used in literature: BLESS (Baroni and Lenci, 2011) , EVALution (Santus et al., 2015) , Lenci/Benotto (Benotto, 2015) , and Weeds (Weeds et al., 2014) taken from the repository provided by (Shwartz et al., 2017) ."}
{"sent_id": "aa496bd71f380e02dd392cda969999-C001-91", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_aa496bd71f380e02dd392cda969999_24", "text": "Similar to SLQS, our depth measure is motivated by distributional informativeness hypothesis (Shwartz et al., 2017) ."}
{"sent_id": "aa496bd71f380e02dd392cda969999-C001-100", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_aa496bd71f380e02dd392cda969999_24", "text": "We compared our numbers with those given in (Shwartz et al., 2017) ."}
{"sent_id": "aa496bd71f380e02dd392cda969999-C001-104", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_aa496bd71f380e02dd392cda969999_24", "text": "For the case of hypernym vs all other relations, except for EVALution, in all other data sets, our average precision (AP ) using both Jaccard and word2vec ( (Shwartz et al., 2017) call this as AP @all) is better than the best unsu- Table 2 : AP = average precision."}
{"sent_id": "aa496bd71f380e02dd392cda969999-C001-105", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_aa496bd71f380e02dd392cda969999_24", "text": "The Best AP and Best Measure is taken from (Shwartz et al., 2017) ."}
{"sent_id": "aa496bd71f380e02dd392cda969999-C001-106", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_aa496bd71f380e02dd392cda969999_24", "text": "pervised measure as reported in (Shwartz et al., 2017) ."}
{"sent_id": "aa496bd71f380e02dd392cda969999-C001-109", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_aa496bd71f380e02dd392cda969999_24", "text": "Our systems performs worse than the best measure whenever an Informativeness Measure (Shwartz et al., 2017) , like SLQS and its variants perform well."}
{"sent_id": "aa496bd71f380e02dd392cda969999-C001-114", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_aa496bd71f380e02dd392cda969999_24", "text": "For finding the best measure, (Shwartz et al., 2017) finds the best by varying the measures as well as the features, whereas we have a fixed system."}
{"sent_id": "642aa9fe999d0b2b3793cb1603c04c-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_642aa9fe999d0b2b3793cb1603c04c_24", "text": "Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer [22] ), sentiment analysis [25] , and other tasks."}
{"sent_id": "642aa9fe999d0b2b3793cb1603c04c-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_642aa9fe999d0b2b3793cb1603c04c_24", "text": "The first sublayer performs multi-head, scaled dot-product, self-attention [22] ."}
{"sent_id": "642aa9fe999d0b2b3793cb1603c04c-C001-55", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_642aa9fe999d0b2b3793cb1603c04c_24", "text": "Maximum path length Table 1 : Operation complexity of each layer type, based on [22] ."}
{"sent_id": "2fbf5397a8219923d1d9bc0464cb59-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_2fbf5397a8219923d1d9bc0464cb59_24", "text": "Related work on exploring syntactic structured information in pronoun resolution can be typically classified into three categories: parse tree-based search algorithms ( Hobbs 1978) , feature-based (Lappin and Leass 1994; Bergsma and Lin 2006) and tree kernel-based methods (Yang et al 2006) ."}
{"sent_id": "2fbf5397a8219923d1d9bc0464cb59-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_2fbf5397a8219923d1d9bc0464cb59_24", "text": "Compared with Collins and Duffy's kernel and its application in pronoun resolution (Yang et al 2006) , the context-sensitive convolution tree kernel enumerates not only context-free sub-trees but also context-sensitive sub-trees by taking their ancestor node paths into consideration."}
{"sent_id": "2fbf5397a8219923d1d9bc0464cb59-C001-67", "intents": ["@BACK@"], "paper_id": "ABC_2fbf5397a8219923d1d9bc0464cb59_24", "text": "It is found (Yang et al 2006) that the simpleexpansion tree span scheme performed best on the ACE 2003 corpus in pronoun resolution."}
{"sent_id": "2fbf5397a8219923d1d9bc0464cb59-C001-96", "intents": ["@BACK@"], "paper_id": "ABC_2fbf5397a8219923d1d9bc0464cb59_24", "text": "This convolution tree kernel has been successfully applied by Yang et al (2006) in pronoun resolution."}
{"sent_id": "2fbf5397a8219923d1d9bc0464cb59-C001-118", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_2fbf5397a8219923d1d9bc0464cb59_24", "text": "Table 1 systematically evaluates the impact of different m in our context-sensitive convolution tree kernel and compares our dynamic-expansion tree span scheme with the existing three tree span schemes, min-, simple-and full-expansions as described in Yang et al (2006) ."}
{"sent_id": "f3f61d50929f862e263e3f658852bc-C001-121", "intents": ["@BACK@"], "paper_id": "ABC_f3f61d50929f862e263e3f658852bc_24", "text": "3 Also, Table 4 reveals which reasons predict absolute differences most: The mean ratings of 7-3 (off-topic) are very low, indicating a strong negative impact, while 6-3 (irrelevant reasons) still shows rather 3 While the differences seem not very large, this is expected, as in many argument pairs from Habernal and Gurevych (2016a) both arguments are strong or weak respectively."}
{"sent_id": "f797e7439bd78af2ef86271214f991-C001-8", "intents": ["@MOT@"], "paper_id": "ABC_f797e7439bd78af2ef86271214f991_24", "text": "In Martschat and Strube (2014) , we propose a framework for error analysis for coreference resolution."}
{"sent_id": "f797e7439bd78af2ef86271214f991-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_f797e7439bd78af2ef86271214f991_24", "text": "The idea underlying the analysis framework of Martschat and Strube (2014) is to employ spanning trees in a graph-based entity representation."}
{"sent_id": "f797e7439bd78af2ef86271214f991-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_f797e7439bd78af2ef86271214f991_24", "text": "First, it includes multigraph, which is a deterministic approach using a few strong features (Martschat and Strube, 2014) ."}
{"sent_id": "f797e7439bd78af2ef86271214f991-C001-22", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f797e7439bd78af2ef86271214f991_24", "text": "In Martschat and Strube (2014) , we propose an algorithm based on Ariel's accessibility theory (Ariel, 1990) for reference entities."}
{"sent_id": "f797e7439bd78af2ef86271214f991-C001-47", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f797e7439bd78af2ef86271214f991_24", "text": "We already implemented the algorithms discussed in Martschat and Strube (2014) ."}
{"sent_id": "f797e7439bd78af2ef86271214f991-C001-54", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f797e7439bd78af2ef86271214f991_24", "text": "Our system also supports other use cases, such as the cross-system analysis described in Martschat and Strube (2014) ."}
{"sent_id": "f797e7439bd78af2ef86271214f991-C001-67", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f797e7439bd78af2ef86271214f991_24", "text": "Hence, following Martschat and Strube (2014) , we categorize all errors by coarse mention type of anaphor and antecedent (proper name, noun, pronoun, demonstrative pronoun or verb) 4 ."}
{"sent_id": "f797e7439bd78af2ef86271214f991-C001-114", "intents": ["@DIF@"], "paper_id": "ABC_f797e7439bd78af2ef86271214f991_24", "text": "Compared to our original implementation of the error analysis framework (Martschat and Strube, 2014) , we made the analysis interface more userfriendly and provide more analysis functionality."}
{"sent_id": "497b717bc4ff6b9e2160ee823f6b42-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_497b717bc4ff6b9e2160ee823f6b42_24", "text": "To mitigate this the \"RUSSE: The First Workshop on Russian Semantic Similarity\" [10] was conducted, producing RUSSE Human-Judgements evaluation dataset (we will refer to it as HJ-dataset)."}
{"sent_id": "497b717bc4ff6b9e2160ee823f6b42-C001-124", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_497b717bc4ff6b9e2160ee823f6b42_24", "text": "To compute correlation we use Spearman coefficient, since it was used as accuracy measure in RUSSE [10] ."}
{"sent_id": "497b717bc4ff6b9e2160ee823f6b42-C001-166", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_497b717bc4ff6b9e2160ee823f6b42_24", "text": "For convenience we replicate results for these corpora, originally presented in [10] , alongside with our result in Table 5 ."}
{"sent_id": "242aacd35fb92d836fea9eb33961a3-C001-41", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_242aacd35fb92d836fea9eb33961a3_24", "text": "We provide several popular schedulers, e.g., the inverse square-root scheduler from Vaswani et al. (2017) and cyclical schedulers based on warm restarts (Loshchilov and Hutter, 2016) ."}
{"sent_id": "242aacd35fb92d836fea9eb33961a3-C001-86", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_242aacd35fb92d836fea9eb33961a3_24", "text": "For En-De we replicate the setup of Vaswani et al. (2017) which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14."}
{"sent_id": "242aacd35fb92d836fea9eb33961a3-C001-92", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_242aacd35fb92d836fea9eb33961a3_24", "text": "All results use beam search with a beam width of 4 and length penalty of 0.6, following Vaswani et al. 2017 ."}
{"sent_id": "242aacd35fb92d836fea9eb33961a3-C001-74", "intents": ["@BACK@"], "paper_id": "ABC_242aacd35fb92d836fea9eb33961a3_24", "text": "FAIRSEQ provides fast inference for non-recurrent models (Gehring et al., 2017; Vaswani et al., 2017; Fan et al., 2018b; Wu et al., 2019) through incremental decoding, where the model states of previously generated tokens are cached in each active beam and re-used."}
{"sent_id": "242aacd35fb92d836fea9eb33961a3-C001-84", "intents": ["@BACK@"], "paper_id": "ABC_242aacd35fb92d836fea9eb33961a3_24", "text": "We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (Luong et al., 2015) , convolutional models (Gehring et al., 2017; Wu et al., 2019) and Transformer (Vaswani et al., 2017) ."}
{"sent_id": "242aacd35fb92d836fea9eb33961a3-C001-98", "intents": ["@BACK@"], "paper_id": "ABC_242aacd35fb92d836fea9eb33961a3_24", "text": "Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. Gehring et al. (2017) 25.2 40.5 b. Vaswani et al. (2017) 28.4 41.0 c. Ahmed et al. (2017) 28.9 41.4 d. Shaw et al. (2018) 29 et al., 2016), adaptive softmax (Grave et al., 2017) , and adaptive inputs ."}
{"sent_id": "817576dbe36f79ac3e0031211f400d-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_817576dbe36f79ac3e0031211f400d_24", "text": "For example, Multilingual-BERT is trained on a collection of corpora in 104 different languages (Devlin et al., 2019) , and generalizes language components well across languages (Pires et al., 2019) ."}
{"sent_id": "817576dbe36f79ac3e0031211f400d-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_817576dbe36f79ac3e0031211f400d_24", "text": "That Devlin et al. (2019) trained a better model when using NSP than without NSP is likely due to the model learning long-range dependencies in text from its inputs, which are longer than just the single sentence on itself."}
{"sent_id": "817576dbe36f79ac3e0031211f400d-C001-74", "intents": ["@SIM@"], "paper_id": "ABC_817576dbe36f79ac3e0031211f400d_24", "text": "The architecture of our language model is thus equal to the original BERT model with 12 self-attention layers with 12 heads (Devlin et al., 2019) ."}
{"sent_id": "652534f801dbff0c009c4a39fdef4d-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_652534f801dbff0c009c4a39fdef4d_24", "text": "This is the key problem addressed by research on ASR quality estimation C. de Souza et al., 2015; Jalalvand et al., 2015b) , and the task for which TranscRater, the tool described in this paper, has been designed."}
{"sent_id": "652534f801dbff0c009c4a39fdef4d-C001-100", "intents": ["@BACK@"], "paper_id": "ABC_652534f801dbff0c009c4a39fdef4d_24", "text": "Based on the empirical results reported in C. de Souza et al., 2015; Jalalvand et al., 2015b) , which indicate that Extremely Randomized Trees (XRT (Geurts et al., 2006) ) is a very competitive algorithm in several WER prediction tasks, the current version of the tool exploits XRT."}
{"sent_id": "652534f801dbff0c009c4a39fdef4d-C001-106", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_652534f801dbff0c009c4a39fdef4d_24", "text": "The current version of the tool includes an interface to the Random Forest algorithm (RF (Breiman, 2001) ), the same used in (Jalalvand et al., 2015b) ."}
{"sent_id": "db42e01dbc86b77335a0e488ff85e2-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_db42e01dbc86b77335a0e488ff85e2_24", "text": "Recent papers [3] , [4] in neural machine translation have proposed the strict use of attention mechanisms in networks such as the Transformer over previous approaches such as recurrent neural networks (RNNs) [5] and convolutional neural networks (CNNs) [6] ."}
{"sent_id": "db42e01dbc86b77335a0e488ff85e2-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_db42e01dbc86b77335a0e488ff85e2_24", "text": "The attention mechanism used by Vaswani et al. [3] can be thought of as a function that maps a query and set of keyvalue pairs to an output."}
{"sent_id": "db42e01dbc86b77335a0e488ff85e2-C001-92", "intents": ["@BACK@"], "paper_id": "ABC_db42e01dbc86b77335a0e488ff85e2_24", "text": "All proposed architectures including the base Transformer model [3] are trained over the International Workshop on Spoken Language Translation (IWSLT) 2016 corpus and tested similarly over the IWSLT 2014 test corpus [15] ."}
{"sent_id": "db42e01dbc86b77335a0e488ff85e2-C001-118", "intents": ["@DIF@"], "paper_id": "ABC_db42e01dbc86b77335a0e488ff85e2_24", "text": "On the much larger WMT English-German test set, all our models achieve better results then Vaswani et al. [3] ."}
{"sent_id": "db42e01dbc86b77335a0e488ff85e2-C001-120", "intents": ["@DIF@"], "paper_id": "ABC_db42e01dbc86b77335a0e488ff85e2_24", "text": "Our approach also takes considerably less time than the large Transformer model with a stack of eight encoder attention heads, although it is a little slower than the smaller Transformer model reported by Vaswani et al. [3] ."}
{"sent_id": "0763666190b6b4be1bcf494d7c6fe2-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_0763666190b6b4be1bcf494d7c6fe2_24", "text": "They also proposed an algorithm that uses successive splits and merges of semantic roles clusters in order to improve their quality in (Lang and Lapata, 2011a) ."}
{"sent_id": "0763666190b6b4be1bcf494d7c6fe2-C001-32", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_0763666190b6b4be1bcf494d7c6fe2_24", "text": "Following common practice (Lang and Lapata, 2011a; Titov and Klementiev, 2012) , we assume oracle argument identification and focus on argument labeling."}
{"sent_id": "0763666190b6b4be1bcf494d7c6fe2-C001-89", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_0763666190b6b4be1bcf494d7c6fe2_24", "text": "As done in (Lang and Lapata, 2011a) and (Titov and Klementiev, 2012) , we use purity and collocation measures to assess the quality of our role induction process."}
{"sent_id": "0763666190b6b4be1bcf494d7c6fe2-C001-94", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_0763666190b6b4be1bcf494d7c6fe2_24", "text": "In the same way as (Lang and Lapata, 2011a) , we use the micro-average obtained by weighting the scores for individual verbs proportionally to the number of argument instances for that verb."}
{"sent_id": "0763666190b6b4be1bcf494d7c6fe2-C001-103", "intents": ["@SIM@"], "paper_id": "ABC_0763666190b6b4be1bcf494d7c6fe2_24", "text": "The baseline model is the \"syntactic function\" used for instance in (Lang and Lapata, 2011a) , which simply clusters predicate arguments according to the dependency relation to their head."}
{"sent_id": "0763666190b6b4be1bcf494d7c6fe2-C001-110", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_0763666190b6b4be1bcf494d7c6fe2_24", "text": "We made our best to follow the setup used in previous work (Lang and Lapata, 2011a; Titov and Kle-mentiev, 2012) , in order to compare with the current state of the art."}
{"sent_id": "8bd97eb118175c9fd2147b6456421c-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_8bd97eb118175c9fd2147b6456421c_24", "text": "Shallow fusion has been used in E2E models for decoding [10] and contextual biasing [6] ."}
{"sent_id": "8bd97eb118175c9fd2147b6456421c-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_8bd97eb118175c9fd2147b6456421c_24", "text": "All these improvements lead to significantly better biasing which is comparable to the state-of-the-art conventional model [6] ."}
{"sent_id": "8bd97eb118175c9fd2147b6456421c-C001-132", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_8bd97eb118175c9fd2147b6456421c_24", "text": "Similarly to [6] , an input utterance is divided to 25-ms frames, windowed and shifted at a rate of 10 ms."}
{"sent_id": "8bd97eb118175c9fd2147b6456421c-C001-136", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_8bd97eb118175c9fd2147b6456421c_24", "text": "Similar to [6] , the encoder of the RNN-T consists of 8 Long Short-Term Memory (LSTM) [21] layers and the prediction network contains 2 layers."}
{"sent_id": "8bd97eb118175c9fd2147b6456421c-C001-197", "intents": ["@FUT@"], "paper_id": "ABC_8bd97eb118175c9fd2147b6456421c_24", "text": "Lastly, since wordpieces perform better than graphemes [6] in E2E modeling, it would be interesting to explore longer phonemic units such as phoneme pieces for biasing."}
{"sent_id": "537ec54aac2c3e3c62070468dcd8a3-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_537ec54aac2c3e3c62070468dcd8a3_24", "text": "In a phrase-based statistical translation (Koehn et al., 2003) , a bilingual text is decomposed as K phrase translation pairs (ē 1 ,fā 1 ), (ē 2 ,fā 2 ), ...: The input foreign sentence is segmented into phrasesf K 1 , mapped into corresponding Englishē K 1 , then, reordered to form the output English sentence according to a phrase alignment index mappingā."}
{"sent_id": "537ec54aac2c3e3c62070468dcd8a3-C001-30", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_537ec54aac2c3e3c62070468dcd8a3_24", "text": "The phrase extraction algorithm is based on those presented by Koehn et al. (2003) ."}
{"sent_id": "537ec54aac2c3e3c62070468dcd8a3-C001-32", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_537ec54aac2c3e3c62070468dcd8a3_24", "text": "Second, phrase translation pairs are extracted from the word aligned corpus (Koehn et al., 2003) ."}
{"sent_id": "537ec54aac2c3e3c62070468dcd8a3-C001-41", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_537ec54aac2c3e3c62070468dcd8a3_24", "text": "The decoding process is very similar to those described in (Koehn et al., 2003) : It starts from an initial empty hypothesis."}
{"sent_id": "537ec54aac2c3e3c62070468dcd8a3-C001-49", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_537ec54aac2c3e3c62070468dcd8a3_24", "text": "Our phrase-based model uses a standard pharaoh feature functions listed as follows (Koehn et al., 2003) :"}
{"sent_id": "537ec54aac2c3e3c62070468dcd8a3-C001-68", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_537ec54aac2c3e3c62070468dcd8a3_24", "text": "For each differently tokenized corpus, we computed word alignments by a HMM translation model (Och and Ney, 2003) and by a word alignment refinement heuristic of \"grow-diagfinal\" (Koehn et al., 2003) ."}
{"sent_id": "3e0704e0928f2df8b7c2ffa9863a55-C001-14", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_3e0704e0928f2df8b7c2ffa9863a55_24", "text": "this setting, we apply the methodology used by Shoemark et al. (2017) in the context of the 2014 Scottish independence referendum to a dataset of tweets related to the Catalonian referendum."}
{"sent_id": "3e0704e0928f2df8b7c2ffa9863a55-C001-31", "intents": ["@SIM@"], "paper_id": "ABC_3e0704e0928f2df8b7c2ffa9863a55_24", "text": "Our main methodological divergence from Shoemark et al. (2017) relates to the linguistic phenomenon at hand: while Scots is mainly manifested as interleaving individual words within English text (code-mixing), Catalan is a distinct language which, when used, usually replaces Spanish altogether for the entire tweet (code-switching)."}
{"sent_id": "3e0704e0928f2df8b7c2ffa9863a55-C001-41", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_3e0704e0928f2df8b7c2ffa9863a55_24", "text": "After removing retweets and tweets from users whose tweets frequently contained URLs (i.e., likely bots), our final \"Catalonian Independence Tweets\" (CT) dataset is made up of 11,670 tweets from 10,498 users (cf. the Scottish referendum set IT with 59,664 tweets and 18,589 users in Shoemark et al. (2017) )."}
{"sent_id": "3e0704e0928f2df8b7c2ffa9863a55-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_3e0704e0928f2df8b7c2ffa9863a55_24", "text": "With respect to political separatism, Shoemark et al. (2017) studied the use of Scots, a language local to Scotland, in the context of the 2014 Scotland independence referendum."}
{"sent_id": "3e0704e0928f2df8b7c2ffa9863a55-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_3e0704e0928f2df8b7c2ffa9863a55_24", "text": "the 693,815 control tweets in Table 6 of Shoemark et al. (2017) )."}
{"sent_id": "805935a672f5d706bd878a73fa8171-C001-10", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_805935a672f5d706bd878a73fa8171_24", "text": "We extend the tests made in Agirre et al. (2008) , who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing."}
{"sent_id": "805935a672f5d706bd878a73fa8171-C001-132", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_805935a672f5d706bd878a73fa8171_24", "text": "Our results extend those of Agirre et al. (2008) , which showed improvements on a subset of the PTB."}
{"sent_id": "805935a672f5d706bd878a73fa8171-C001-67", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_805935a672f5d706bd878a73fa8171_24", "text": "We use the same train-test split of Agirre et al. (2008) , with a total of 8,669 sentences containing 151,928 words partitioned into 3 sets: 80% training, 10% development and 10% test data."}
{"sent_id": "805935a672f5d706bd878a73fa8171-C001-76", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_805935a672f5d706bd878a73fa8171_24", "text": "We will experiment with the range of semantic representations used in Agirre et al. (2008) , all of which are based on WordNet 2.1."}
{"sent_id": "805935a672f5d706bd878a73fa8171-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_805935a672f5d706bd878a73fa8171_24", "text": "Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes."}
{"sent_id": "805935a672f5d706bd878a73fa8171-C001-64", "intents": ["@USE@"], "paper_id": "ABC_805935a672f5d706bd878a73fa8171_24", "text": "We used two different datasets: the full PTB and the Semcor/PTB intersection (Agirre et al. 2008 )."}
{"sent_id": "f881f6c65301fdfe2fffe7a18e05c4-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_f881f6c65301fdfe2fffe7a18e05c4_25", "text": "For example, by indicating the presence of a structural boundary or a relationship between parts of a discourse, cue phrases caa assist in the resolution of anaphora [5, 4, 17] and in the identification of rhetorical relations [10, 12, 17] ."}
{"sent_id": "f881f6c65301fdfe2fffe7a18e05c4-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_f881f6c65301fdfe2fffe7a18e05c4_25", "text": "Grosz and Sidner [4] classify cue phrases based on changes to the attentional stack and intentional structure found in their theory of discourse."}
{"sent_id": "f881f6c65301fdfe2fffe7a18e05c4-C001-76", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_f881f6c65301fdfe2fffe7a18e05c4_25", "text": "8Our set of cue phrases was derived from extensional definitions provided by ourselves and othel~ [3, 4, 17, 18, 21] ."}
{"sent_id": "c7821d22613ad91f77ea454d50a5ce-C001-9", "intents": ["@MOT@"], "paper_id": "ABC_c7821d22613ad91f77ea454d50a5ce_25", "text": "Question generation (QG) task, which takes a context and an answer as input and generates a question that targets the given answer, have received tremendous interests in recent years from both industrial and academic communities (Zhao et al., 2018) (Zhou et al., 2017) (Du et al., 2017) ."}
{"sent_id": "c7821d22613ad91f77ea454d50a5ce-C001-16", "intents": ["@MOT@"], "paper_id": "ABC_c7821d22613ad91f77ea454d50a5ce_25", "text": "However, as indicated by (Du et al., 2017) , providing paragraph-level information can improve QG performance."}
{"sent_id": "c7821d22613ad91f77ea454d50a5ce-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_c7821d22613ad91f77ea454d50a5ce_25", "text": "As a result, the existing QG models (Du et al., 2017) (Zhou et al., 2017 ) mainly use only sentence-level information as context."}
{"sent_id": "c7821d22613ad91f77ea454d50a5ce-C001-88", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_c7821d22613ad91f77ea454d50a5ce_25", "text": "We follow the same data split settings as previous work on the QG tasks (Du et al., 2017) (Zhao et al., 2018) to directly compare the state-of-theart results on QG tasks."}
{"sent_id": "c7821d22613ad91f77ea454d50a5ce-C001-90", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_c7821d22613ad91f77ea454d50a5ce_25", "text": "• SQuAD 73K In this set, we follow the same setting as (Du et al., 2017) ; the accessible parts of the SQuAD training data are randomly divided into a training set (80%), a development set (10%), and a test set (10%)."}
{"sent_id": "c7821d22613ad91f77ea454d50a5ce-C001-112", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_c7821d22613ad91f77ea454d50a5ce_25", "text": "• NQG-RC (Du et al., 2017) : A seq2seq question generation model based on bidirectional LSTM."}
{"sent_id": "c7821d22613ad91f77ea454d50a5ce-C001-110", "intents": ["@SIM@"], "paper_id": "ABC_c7821d22613ad91f77ea454d50a5ce_25", "text": "In this paper, we compare our models with the best performing models (Du et al., 2017) (Zhao et al., 2018) in the literature."}
{"sent_id": "09493a62815b4b826248d6d9be47cb-C001-21", "intents": ["@MOT@"], "paper_id": "ABC_09493a62815b4b826248d6d9be47cb_25", "text": "However, while these tools have proven to be effective for patient records and research papers, they achieve moderate results on social media texts (Nikfarjam et al., 2015; Limsopatham and Collier, 2016) ."}
{"sent_id": "09493a62815b4b826248d6d9be47cb-C001-123", "intents": ["@BACK@"], "paper_id": "ABC_09493a62815b4b826248d6d9be47cb_25", "text": "These tasks are devoted to the normalization of (1) (Limsopatham and Collier, 2016) 73.39 ----CNN (Limsopatham and Collier, 2016) 81.41 ----RNN (Limsopatham and Collier, 2016) 79.98 ----Attentional Char-CNN (Niu et al., 2018) 84.65 ----Hierarchical Char-CNN (Han et al., 2017) - Table 2 : The performance of the proposed models and the state-of-the-art methods in terms of accuracy."}
{"sent_id": "e99baf9c4b8650f29f410501c5165b-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_e99baf9c4b8650f29f410501c5165b_25", "text": "Promising results has been published since attention was introduced in [16] then later refined in [18] ."}
{"sent_id": "e99baf9c4b8650f29f410501c5165b-C001-26", "intents": ["@MOT@"], "paper_id": "ABC_e99baf9c4b8650f29f410501c5165b_25", "text": "To overcome these limitations both for machine translation and image captioning, some new models were proposed by using the attention mechanism [3, 16, 18] ."}
{"sent_id": "e99baf9c4b8650f29f410501c5165b-C001-56", "intents": ["@MOT@"], "paper_id": "ABC_e99baf9c4b8650f29f410501c5165b_25", "text": "Inspired by the use of attention in sequence-to-sequence learning for machine translation [16, 18] , visual attention has been proved to be a very effective way of improving image captioning."}
{"sent_id": "e99baf9c4b8650f29f410501c5165b-C001-37", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_e99baf9c4b8650f29f410501c5165b_25", "text": "We use the Luong style of attention [18] which is a refined version of attention mechanism and that to the best of our knowledge, there has not been any published work reporting the performance of an image captioning model that is built following only the encoder-decoder pipeline with Luong style of attention."}
{"sent_id": "e99baf9c4b8650f29f410501c5165b-C001-89", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_e99baf9c4b8650f29f410501c5165b_25", "text": "In our model, we use the general form described in [18] :"}
{"sent_id": "e99baf9c4b8650f29f410501c5165b-C001-94", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_e99baf9c4b8650f29f410501c5165b_25", "text": "In this paper, we use a LSTM cell wrapped with the attention mechanism described in [18] to form R. LSTM [24] is a powerful form of recurrent neural network that is widely used now because of its ability to deal with issues like vanishing and exploding gradients."}
{"sent_id": "d9aa77a03ff98cae29701eddb414d3-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_d9aa77a03ff98cae29701eddb414d3_25", "text": "Thus, our main contributions are as follows: first, we develop a novel web application with an in-browser KWS system based on previous state-of-the-art [3] models."}
{"sent_id": "d9aa77a03ff98cae29701eddb414d3-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_d9aa77a03ff98cae29701eddb414d3_25", "text": "To achieve this goal, resource-efficient architectures using convolutional neural networks (CNNs) [3, 1] and recurrent neural networks (RNNs) [2] have been proposed, while other works make use of low-bitwidth weights [4, 9] ."}
{"sent_id": "d9aa77a03ff98cae29701eddb414d3-C001-47", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_d9aa77a03ff98cae29701eddb414d3_25", "text": "For consistency with past results [3, 5] , we train our models on the first version of the Google Speech Commands dataset [10] , which comprises a total of 65,000 spoken utterances for 30 short, one-second phrases."}
{"sent_id": "d9aa77a03ff98cae29701eddb414d3-C001-48", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_d9aa77a03ff98cae29701eddb414d3_25", "text": "To compare with past work [3] , we pick the following twelve classes: \"yes,\" \"no,\" \"stop,\" \"go,\" \"left,\" \"right,\" \"on,\" \"off,\" unknown, and silence."}
{"sent_id": "d9aa77a03ff98cae29701eddb414d3-C001-51", "intents": ["@SIM@"], "paper_id": "ABC_d9aa77a03ff98cae29701eddb414d3_25", "text": "We use the standard 80%, 10%, and 10% splits for the training, validation, and test sets, respectively [3, 10] ."}
{"sent_id": "d9aa77a03ff98cae29701eddb414d3-C001-59", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_d9aa77a03ff98cae29701eddb414d3_25", "text": "We use the res8 and res8-narrow architectures from Tang and Lin [3] as a starting point, which represent prior state of the art in residual CNNs [13] for KWS."}
{"sent_id": "e0e21b4e473ad6fde28378b2dc4f34-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_e0e21b4e473ad6fde28378b2dc4f34_25", "text": "Methods to learn sparse word-based translation correspondences from supervised ranking signals have been presented by Bai et al. (2010) and Sokolov et al. (2013) ."}
{"sent_id": "e0e21b4e473ad6fde28378b2dc4f34-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_e0e21b4e473ad6fde28378b2dc4f34_25", "text": "The baseline consensus-based voting Borda Count procedure endows each voter with a fixed amount of voting points which he is free to distribute among the scored documents (Aslam and Montague, 2001; Sokolov et al., 2013) ."}
{"sent_id": "e0e21b4e473ad6fde28378b2dc4f34-C001-37", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_e0e21b4e473ad6fde28378b2dc4f34_25", "text": "Our approach extends the work of Sokolov et al. (2013) by presenting an alternative learningto-rank approach that can be used for supervised model combination to integrate dense and sparse features, and by evaluating both approaches on cross-lingual retrieval for patents and Wikipedia."}
{"sent_id": "e0e21b4e473ad6fde28378b2dc4f34-C001-101", "intents": ["@USE@"], "paper_id": "ABC_e0e21b4e473ad6fde28378b2dc4f34_25", "text": "A JP-EN system was trained on data described and preprocessed by Sokolov et al. (2013) , consisting of 1.8M parallel sentences from the NTCIR-7 JP-EN PatentMT subtask (Fujii et al., 2008) and 2k parallel sentences for parameter development from the NTCIR-8 test collection."}
{"sent_id": "820fa732cc4cedf2d5d94b2afb90fc-C001-39", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_820fa732cc4cedf2d5d94b2afb90fc_25", "text": "einsum ( \" hv , hdv−>d \" , o , P_o) r e t u r n y Note: [Vaswani et al., 2017] include a constant scaling factor on the logits."}
{"sent_id": "820fa732cc4cedf2d5d94b2afb90fc-C001-48", "intents": ["@USE@"], "paper_id": "ABC_820fa732cc4cedf2d5d94b2afb90fc_25", "text": "Following [Vaswani et al., 2017] , in an autoregressive model, we can prevent backward-information-flow by adding a \"mask\" to the logits containing the value −∞ in the illegal positions."}
{"sent_id": "820fa732cc4cedf2d5d94b2afb90fc-C001-52", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_820fa732cc4cedf2d5d94b2afb90fc_25", "text": "h , as suggested by [Vaswani et al., 2017]"}
{"sent_id": "820fa732cc4cedf2d5d94b2afb90fc-C001-92", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_820fa732cc4cedf2d5d94b2afb90fc_25", "text": "We introduce multi-query Attention as a variation of multi-head attention as described in [Vaswani et al., 2017] ."}
{"sent_id": "820fa732cc4cedf2d5d94b2afb90fc-C001-108", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_820fa732cc4cedf2d5d94b2afb90fc_25", "text": "Following [Vaswani et al., 2017] , we evaluate on the WMT 2014 English-German translation task."}
{"sent_id": "d4563562cd0dfd8ef6cdb57117fb22-C001-61", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_d4563562cd0dfd8ef6cdb57117fb22_25", "text": "In the case of the two PMI-based models, we use the discount score described in Pantel and Ravichandran (2004) and used by Chambers and Jurafsky (2008) ."}
{"sent_id": "d4563562cd0dfd8ef6cdb57117fb22-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_d4563562cd0dfd8ef6cdb57117fb22_25", "text": "Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001) , Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains."}
{"sent_id": "d4563562cd0dfd8ef6cdb57117fb22-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_d4563562cd0dfd8ef6cdb57117fb22_25", "text": "As defined by Chambers and Jurafsky (2008) , a narrative chain is \"a partially ordered set of narrative events that share a common actor,\" where a narrative event is \"a tuple of an event (most simply a verb) and its participants, represented as typed dependencies.\" To learn narrative chains from text, Chambers and Jurafsky extract chains of narrative events linked by a common coreferent within a document."}
{"sent_id": "0706cab049274ffc82c5e2ef6f7b99-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_0706cab049274ffc82c5e2ef6f7b99_25", "text": "For example, the coding manual for the Switchboard DAMSL dialogue act annotation scheme (Jurafsky, Shriberg, and Biasca 1997, page 2) states that kappa is used to \"assess labelling accuracy,\" and Di Eugenio and Glass (2004) relate reliability to \"the objectivity of decisions,\" whereas Carletta (1996) regards reliability as the degree to which we understand the judgments that annotators are asked to make."}
{"sent_id": "0706cab049274ffc82c5e2ef6f7b99-C001-79", "intents": ["@BACK@"], "paper_id": "ABC_0706cab049274ffc82c5e2ef6f7b99_25", "text": "The second class of agreement measure recommended in Di Eugenio and Glass (2004) is that of chance-corrected tests that do not assume an equal distribution of categories between coders."}
{"sent_id": "0706cab049274ffc82c5e2ef6f7b99-C001-105", "intents": ["@BACK@"], "paper_id": "ABC_0706cab049274ffc82c5e2ef6f7b99_25", "text": "Di Eugenio and Glass (2004) conclude with the proposal that these three forms of agreement measure collectively provide better means with which to judge agreement than any individual test."}
{"sent_id": "0706cab049274ffc82c5e2ef6f7b99-C001-62", "intents": ["@DIF@"], "paper_id": "ABC_0706cab049274ffc82c5e2ef6f7b99_25", "text": "The justification given for using percentage agreement is that it does not suffer from what Di Eugenio and Glass (2004) referred to as the \"prevalence problem."}
{"sent_id": "7b5ca6526f460139f273484bd276bc-C001-5", "intents": ["@USE@"], "paper_id": "ABC_7b5ca6526f460139f273484bd276bc_25", "text": "Experimental results show that DCR achieves stateof-the-art exact match and F1 scores on the SQuAD dataset (Rajpurkar et al. 2016) ."}
{"sent_id": "7b5ca6526f460139f273484bd276bc-C001-46", "intents": ["@USE@"], "paper_id": "ABC_7b5ca6526f460139f273484bd276bc_25", "text": "For the answer selection task this paper focuses on, several datasets exist, e.g. TREC-QA for factoid answer extraction from multiple given passages, bAbI (Weston, Chopra, and Bordes 2014) designed for inference purpose, and the SQuAD dataset (Rajpurkar et al. 2016) used in this paper."}
{"sent_id": "7b5ca6526f460139f273484bd276bc-C001-111", "intents": ["@USE@"], "paper_id": "ABC_7b5ca6526f460139f273484bd276bc_25", "text": "Dataset We used the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al. 2016) for the experiment."}
{"sent_id": "7b5ca6526f460139f273484bd276bc-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_7b5ca6526f460139f273484bd276bc_25", "text": "As a result, apart from a few exceptions (Rajpurkar et al. 2016; Wang and Jiang 2016) , this research direction has not been fully explored yet."}
{"sent_id": "7b5ca6526f460139f273484bd276bc-C001-23", "intents": ["@DIF@"], "paper_id": "ABC_7b5ca6526f460139f273484bd276bc_25", "text": "First, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in (Rajpurkar et al. 2016) ."}
{"sent_id": "7b5ca6526f460139f273484bd276bc-C001-24", "intents": ["@SIM@"], "paper_id": "ABC_7b5ca6526f460139f273484bd276bc_25", "text": "Second, it represents answer candidates as chunks, as in (Rajpurkar et al. 2016 ), instead of word-level representations (Wang and Jiang 2016) , to make the model aware of the subtle differences among candidates (importantly, overlapping candidates)."}
{"sent_id": "7b5ca6526f460139f273484bd276bc-C001-53", "intents": ["@SIM@"], "paper_id": "ABC_7b5ca6526f460139f273484bd276bc_25", "text": "As a result, this baseline can be viewed as a deep learning based counterpart of the system in (Rajpurkar et al. 2016 )."}
{"sent_id": "e48a1eac39987cb2f504b66d135572-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_e48a1eac39987cb2f504b66d135572_25", "text": "Later in the year, Alsentzer et al. (2019) and Si et al. (2019) published almost at the same time BERT models pre-trained trained on publicly available clinical notes from MIMIC3 either starting from trained parameters of original BERT or BioBERT model and show improvement of clinical NLP tasks."}
{"sent_id": "e48a1eac39987cb2f504b66d135572-C001-75", "intents": ["@BACK@"], "paper_id": "ABC_e48a1eac39987cb2f504b66d135572_25", "text": "Clinical BERT pre-trained on MIMIC corpus has been reported to have superior performance on NER tasks in Inside-Outside-Beginning (IOB) format (Ramshaw and Marcus, 1999) using i2b2 2010 (Uzuner et al., 2011) and 2012 (Sun et al., 2013) data (Alsentzer et al., 2019) .Original training/development/test splits in the challenges were used."}
{"sent_id": "e48a1eac39987cb2f504b66d135572-C001-51", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_e48a1eac39987cb2f504b66d135572_25", "text": "The prepossessing and tokenization pipeline from Alsentzer et al. 2019 (Alsentzer et al., 2019 was adapted."}
{"sent_id": "e48a1eac39987cb2f504b66d135572-C001-88", "intents": ["@SIM@"], "paper_id": "ABC_e48a1eac39987cb2f504b66d135572_25", "text": "We also looked at the scenarios where BERTbase model was pre-trained by MIMIC3 discharge summaries in a centralized manner (Alsentzer et al., 2019) ."}
{"sent_id": "60c1245eff625441383913f947a8b1-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_60c1245eff625441383913f947a8b1_25", "text": "Waseem and Hovy (2016) collected 130k tweets containing one of seventeen different terms or phrases they considered to be hateful."}
{"sent_id": "60c1245eff625441383913f947a8b1-C001-134", "intents": ["@DIF@"], "paper_id": "ABC_60c1245eff625441383913f947a8b1_25", "text": "The only case where blackaligned tweets are classified into a negative class less frequently than white-aligned tweets is the racism class in the Waseem and Hovy (2016) classifier."}
{"sent_id": "d44648766e68cb914c5489e385f42e-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_d44648766e68cb914c5489e385f42e_25", "text": "2 Related Work (Munteanu and Marcu, 2006) is the first attempt to extract parallel fragments from comparable sentences."}
{"sent_id": "d44648766e68cb914c5489e385f42e-C001-81", "intents": ["@BACK@"], "paper_id": "ABC_d44648766e68cb914c5489e385f42e_25", "text": "Munteanu and Marcu (2006) develop a smoothing filter applying this advantage."}
{"sent_id": "d44648766e68cb914c5489e385f42e-C001-147", "intents": ["@BACK@"], "paper_id": "ABC_d44648766e68cb914c5489e385f42e_25", "text": "Note that exact match criteria has a bias against (Munteanu and Marcu, 2006) , because their method extacts subsentential fragments which are quite long."}
{"sent_id": "d44648766e68cb914c5489e385f42e-C001-82", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_d44648766e68cb914c5489e385f42e_25", "text": "We extract the LLR lexicon from a word-aligned parallel corpus using the same method as (Munteanu and Marcu, 2006) ."}
{"sent_id": "d44648766e68cb914c5489e385f42e-C001-88", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_d44648766e68cb914c5489e385f42e_25", "text": "Aiming to gain new knowledge that does not exist in the lexicon, we apply a smoothing filter similar to (Munteanu and Marcu, 2006) ."}
{"sent_id": "d44648766e68cb914c5489e385f42e-C001-131", "intents": ["@SIM@"], "paper_id": "ABC_d44648766e68cb914c5489e385f42e_25", "text": "We compared our proposed method with (Munteanu and Marcu, 2006)."}
{"sent_id": "d44648766e68cb914c5489e385f42e-C001-99", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_d44648766e68cb914c5489e385f42e_25", "text": "Therefore, unlike (Munteanu and Marcu, 2006), we only apply the averaging filter to the words with negative scores."}
{"sent_id": "d44648766e68cb914c5489e385f42e-C001-166", "intents": ["@DIF@"], "paper_id": "ABC_d44648766e68cb914c5489e385f42e_25", "text": "Adding the fragments extracted by (Munteanu and Marcu, 2006) has a negative impact, compared to appending the sentences."}
{"sent_id": "3188ee1583a9c711cf147fc596768d-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_3188ee1583a9c711cf147fc596768d_25", "text": "So far, Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; )."}
{"sent_id": "3188ee1583a9c711cf147fc596768d-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_3188ee1583a9c711cf147fc596768d_25", "text": "Pivots are features occurring frequently and behaving similarly in both domains (Blitzer et al., 2006) ."}
{"sent_id": "3188ee1583a9c711cf147fc596768d-C001-43", "intents": ["@SIM@"], "paper_id": "ABC_3188ee1583a9c711cf147fc596768d_25", "text": "Intuitively, if we are able to find good correspondences through 'linking' pivots, then the augmented source data should transfer better to a target domain (Blitzer et al., 2006) ."}
{"sent_id": "3188ee1583a9c711cf147fc596768d-C001-88", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_3188ee1583a9c711cf147fc596768d_25", "text": "In our empirical setup, we follow Blitzer et al. (2006) and balance the size of source and target data."}
{"sent_id": "8ad5f7a658a8bb5377981ba6b098dc-C001-23", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_8ad5f7a658a8bb5377981ba6b098dc_25", "text": "We build on the paraphrasing approach of [1] in that we use a fixed set of templates to generate a set of candidate logical forms to answer a given query and map each logical form to a natural language expression, its canonical utterance."}
{"sent_id": "8ad5f7a658a8bb5377981ba6b098dc-C001-32", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_8ad5f7a658a8bb5377981ba6b098dc_25", "text": "Approaches to this task include schema matching [9] , inducing latent logical forms [10] , application of paraphrasing techniques [1, 11] , information extraction [12] , learning low dimensional embeddings of words and knowledge base constituents [13] and application of logical reasoning in conjunction with statistical techniques [11] ."}
{"sent_id": "8ad5f7a658a8bb5377981ba6b098dc-C001-90", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_8ad5f7a658a8bb5377981ba6b098dc_25", "text": "We built our implementation on top of the ParaSempre system [1] , and so our evaluation exactly matches theirs."}
{"sent_id": "8ad5f7a658a8bb5377981ba6b098dc-C001-104", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_8ad5f7a658a8bb5377981ba6b098dc_25", "text": "Since we have adopted the logical form templates of ParaSempre, our upper bound or oracle F1 score is the same, 63% [1] ."}
{"sent_id": "8ad5f7a658a8bb5377981ba6b098dc-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_8ad5f7a658a8bb5377981ba6b098dc_25", "text": "The ParaSempre system of [1] is based on the idea of generating a set of candidate logical forms from the query using a set of templates."}
{"sent_id": "8ad5f7a658a8bb5377981ba6b098dc-C001-55", "intents": ["@MOT@"], "paper_id": "ABC_8ad5f7a658a8bb5377981ba6b098dc_25", "text": "One way of doing this to consider a fixed number of logical forms for each query sentence, and train a classifier to choose the best logical form given a sentence [1] ."}
{"sent_id": "8ad5f7a658a8bb5377981ba6b098dc-C001-113", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_8ad5f7a658a8bb5377981ba6b098dc_25", "text": "Average F1 score Sempre [10] 35.7 ParaSempre [1] 39.9 Facebook [13] 41.8 DeepQA [11] 45.3 Tensor kernel with unigrams 40.1 In development, we found that ordering the training alphabetically by the text of the query lead to a large reduction in accuracy."}
{"sent_id": "d987872352e4602fd48936cf2fdab8-C001-29", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_d987872352e4602fd48936cf2fdab8_25", "text": "We take a hypothesis reranking approach, which is widely used in large-scale domain classification for higher scalabil- ity [14, 5] ."}
{"sent_id": "d987872352e4602fd48936cf2fdab8-C001-84", "intents": ["@SIM@"], "paper_id": "ABC_d987872352e4602fd48936cf2fdab8_25", "text": "Figure 2 shows the overall architecture of the hypothesis reranker that is similar to [5] ."}
{"sent_id": "22dc2a38e29a1f5ac55c9ac220782b-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_22dc2a38e29a1f5ac55c9ac220782b_25", "text": "Vaswani et al. (2017) defined attention as a function with as input a triplet containing queries Q, keys K with associated values V ."}
{"sent_id": "22dc2a38e29a1f5ac55c9ac220782b-C001-33", "intents": ["@SIM@"], "paper_id": "ABC_22dc2a38e29a1f5ac55c9ac220782b_25", "text": "The concept of self-attention (Cheng et al., 2016; Parikh et al., 2016) , central to our proposed approach, has shown great promises in natural language processing; It produced state-of-the-art results for machine translation (Vaswani et al., 2017) ."}
{"sent_id": "22dc2a38e29a1f5ac55c9ac220782b-C001-40", "intents": ["@DIF@"], "paper_id": "ABC_22dc2a38e29a1f5ac55c9ac220782b_25", "text": "3 SANet: Self-Attention Network Inspired by the Transformer architecture (Vaswani et al., 2017) which performed machine translation without recurrent or convolutional layers, we propose the Self-Attention Network (SANet) architecture targeting instead text classification."}
{"sent_id": "22dc2a38e29a1f5ac55c9ac220782b-C001-56", "intents": ["@DIF@"], "paper_id": "ABC_22dc2a38e29a1f5ac55c9ac220782b_25", "text": "Contrary to Vaswani et al. (2017) , we only use a single attention head, with attention performed on the complete sequence with constant d-dimensional inputs."}
{"sent_id": "1527ce2786adfe0decf8c926a3d846-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_1527ce2786adfe0decf8c926a3d846_25", "text": "The vast majority of prior methods assume a domain independent context, and rely on Wikipedia and Simple English Wikipedia, a subset of Wikipedia using simplified grammar and terminology, to learn simplifications (Biran et al., 2011; Paetzold and Specia, 2015) , with translationbased approaches using an aligned version (Coster and Kauchak, 2011; Horn et al., 2014; Yatskar et al., 2010) ."}
{"sent_id": "1527ce2786adfe0decf8c926a3d846-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_1527ce2786adfe0decf8c926a3d846_25", "text": "One approach identifies all pairwise permutations of 'content' terms and then applies semantic (i.e., WordNet) and simplicity filters to eliminate pairs that are not simplifications (Biran et al., 2011) ."}
{"sent_id": "1527ce2786adfe0decf8c926a3d846-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_1527ce2786adfe0decf8c926a3d846_25", "text": "The second is the cosine similarity of a minimum shared frequency co-occurrence matrix for the words in the pair and the co-occurrence matrix for the input sentence (Biran et al., 2011) ."}
{"sent_id": "1527ce2786adfe0decf8c926a3d846-C001-56", "intents": ["@SIM@"], "paper_id": "ABC_1527ce2786adfe0decf8c926a3d846_25", "text": "To retain only rules of the form complex word → simple word we calculate the corpus complexity, C (Biran et al., 2011) of each word w as the ratio between the frequency (f ) in the scientific versus general corpus: C w = f w,scientif ic /f w,general ."}
{"sent_id": "1527ce2786adfe0decf8c926a3d846-C001-96", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_1527ce2786adfe0decf8c926a3d846_25", "text": "Our SimpleScience approach outperforms the original approach by Biran et al. (2011) applied to the Wikipedia and SEW corpus as well as to the scientific corpus (Table 1) ."}
{"sent_id": "1527ce2786adfe0decf8c926a3d846-C001-114", "intents": ["@FUT@"], "paper_id": "ABC_1527ce2786adfe0decf8c926a3d846_25", "text": "Adding techniques to filter antonym rules, such as using co-reference chains (Adel and Schütze, 2014) , is important in future work."}
{"sent_id": "05d1ecc230c7907d9a14d3351070c3-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_05d1ecc230c7907d9a14d3351070c3_25", "text": "The introduction of pre-trained language models, such as BERT [2] and Open-GPT [3] , among many others, has brought tremendous progress to the NLP research and industrial communities."}
{"sent_id": "05d1ecc230c7907d9a14d3351070c3-C001-63", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_05d1ecc230c7907d9a14d3351070c3_25", "text": "Stickland and Murray [22] invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of Devlin et al. [2] ."}
{"sent_id": "05d1ecc230c7907d9a14d3351070c3-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_05d1ecc230c7907d9a14d3351070c3_25", "text": "In this line of work, Liu et al. [21] investigated the linguistic knowledge and transferability of contextual representations by comparing BERT [2] with ELMo [15] , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models."}
{"sent_id": "05d1ecc230c7907d9a14d3351070c3-C001-61", "intents": ["@MOT@"], "paper_id": "ABC_05d1ecc230c7907d9a14d3351070c3_25", "text": "In the presence of the success of pre-trained language models, especially BERT [2] , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results."}
{"sent_id": "05d1ecc230c7907d9a14d3351070c3-C001-101", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_05d1ecc230c7907d9a14d3351070c3_25", "text": "Since BERT-Adam [2] has excellent performance, in our experiments, we use it as an optimizer with β 1 = 0.9, β 2 = 0.999,L 2 -weight decay of 0.01.We apply a dropout trick on all layers and set the dropout probability as 0.1."}
{"sent_id": "05d1ecc230c7907d9a14d3351070c3-C001-104", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_05d1ecc230c7907d9a14d3351070c3_25", "text": "In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset [6] , which is a public available used in many studies to test the accuracy of their proposed methods [30, 31, 32, 33, 2] ."}
{"sent_id": "13d3d973a4be832f66b049b364fea5-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_13d3d973a4be832f66b049b364fea5_25", "text": "Alikaniotis et al. (2016) assessed the same dataset by building a bidirectional double-layer LSTM which outperformed Distributed Memory Model of Paragraph Vectors (PV-DM) (Le and Mikolov, 2014) and Support Vector Machines (SVM) baselines."}
{"sent_id": "13d3d973a4be832f66b049b364fea5-C001-26", "intents": ["@MOT@"], "paper_id": "ABC_13d3d973a4be832f66b049b364fea5_25", "text": "For instance, Alikaniotis et al. (2016) developed score-specific word embeddings (SSWE) to address the AA task on the ASAP dataset."}
{"sent_id": "13d3d973a4be832f66b049b364fea5-C001-36", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_13d3d973a4be832f66b049b364fea5_25", "text": "We implement a CNN as the AA model and compare its performance when initialized with our embeddings, tuned based on natural writing errors, to the one obtained when bootstrapped with the SSWE, proposed by Alikaniotis et al. (2016) , that relies on random noisy contexts and script scores."}
{"sent_id": "13d3d973a4be832f66b049b364fea5-C001-152", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_13d3d973a4be832f66b049b364fea5_25", "text": "Table 2 demonstrates that learning from the er-9 Using the same parameters as Alikaniotis et al. (2016) ."}
{"sent_id": "70d41cad40091bcc30a1fd544c277d-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_70d41cad40091bcc30a1fd544c277d_26", "text": "Bidirectional Encoder Representations from Transformers (BERT; Devlin et al., 2018) represents one of the latest developments in this line of work."}
{"sent_id": "70d41cad40091bcc30a1fd544c277d-C001-85", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_70d41cad40091bcc30a1fd544c277d_26", "text": "Trending with Devlin et al. (2018) , BERT large achieves state-of-the-art results on all four datasets, followed by BERT base (see Table 2 , rows 11 and 12)."}
{"sent_id": "70d41cad40091bcc30a1fd544c277d-C001-65", "intents": ["@SIM@"], "paper_id": "ABC_70d41cad40091bcc30a1fd544c277d_26", "text": "As is the case with Devlin et al. (2018) , we find that choosing a batch size of 16, learning rate of 2×10 −5 , and MSL of 512 tokens yields optimal performance on the validation sets of all datasets."}
{"sent_id": "70d41cad40091bcc30a1fd544c277d-C001-103", "intents": ["@DIF@"], "paper_id": "ABC_70d41cad40091bcc30a1fd544c277d_26", "text": "Contrary to Devlin et al. (2018) , who achieve state of the art on small datasets with only a few epochs of fine-tuning, we find that smaller datasets require many more epochs to converge."}
{"sent_id": "c067711a58722737ef8b7ea987bcf3-C001-19", "intents": ["@USE@"], "paper_id": "ABC_c067711a58722737ef8b7ea987bcf3_26", "text": "Since we will compare our results mainly to Van Noord, Abzianidze, Toral, and Bos (2018) , we will only employ the gold and silver data."}
{"sent_id": "c067711a58722737ef8b7ea987bcf3-C001-24", "intents": ["@USE@"], "paper_id": "ABC_c067711a58722737ef8b7ea987bcf3_26", "text": "We represent the source and target data in the same way as Van Noord, Abzianidze, Toral, and Bos (2018) , who represent the source sentence as a sequence of characters, with a special character indicating uppercase characters."}
{"sent_id": "c067711a58722737ef8b7ea987bcf3-C001-74", "intents": ["@USE@"], "paper_id": "ABC_c067711a58722737ef8b7ea987bcf3_26", "text": "We now compare our best models to previous parsers 4 (Bos, 2015; Van Noord, Abzianidze, Toral, and Bos, 2018) and two baseline systems, SPAR and SIM-SPAR."}
{"sent_id": "c067711a58722737ef8b7ea987bcf3-C001-75", "intents": ["@USE@", "@DIF@", "@SIM@"], "paper_id": "ABC_c067711a58722737ef8b7ea987bcf3_26", "text": "As previously indicated, Van Noord, Abzianidze, Toral, and Bos (2018) used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features."}
{"sent_id": "ab5788da3f24e01b0ec40fba0bdbec-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_ab5788da3f24e01b0ec40fba0bdbec_26", "text": "For that purpose, state-of-the-art approaches rely on either a separately trained unsupervised Statistical Machine Translation (SMT) system, which is used for warmup during the initial back-translation iterations (Marie and Fujita, 2018; Artetxe et al., 2019) , or large-scale pre-training through masked denoising, which is used to initialize the weights of the underlying encoder-decoder (Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020) ."}
{"sent_id": "ab5788da3f24e01b0ec40fba0bdbec-C001-19", "intents": ["@USE@"], "paper_id": "ABC_ab5788da3f24e01b0ec40fba0bdbec_26", "text": "For that purpose, we mimic the experimental settings of Artetxe et al. (2019) , and measure the effect of using different initial systems for warmup: the unsupervised SMT system proposed by Artetxe et al. (2019) themselves, supervised NMT and SMT systems trained on both small and large parallel corpora, and a commercial Rule-Based Machine Translation (RBMT) system."}
{"sent_id": "ab5788da3f24e01b0ec40fba0bdbec-C001-44", "intents": ["@USE@"], "paper_id": "ABC_ab5788da3f24e01b0ec40fba0bdbec_26", "text": "• Unsupervised: We use the unsupervised SMT system proposed by Artetxe et al. (2019) , which induces an initial phrase-table using cross-lingual word embedding mappings, combines it with an n-gram language model, and further improves the resulting model through unsupervised tuning and joint refinement."}
{"sent_id": "d92e92b9a375914f3dd74868f463fc-C001-49", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_d92e92b9a375914f3dd74868f463fc_26", "text": "4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in Cífka and Bojar (2018) as well as the average of all 10 SentEval downstream tasks."}
{"sent_id": "d92e92b9a375914f3dd74868f463fc-C001-51", "intents": ["@DIF@"], "paper_id": "ABC_d92e92b9a375914f3dd74868f463fc_26", "text": "(1) In contrast with the results from Cífka and Bojar (2018), our scores demonstrate that an increasing number of attention heads is beneficial for classification-based downstream tasks."}
{"sent_id": "d92e92b9a375914f3dd74868f463fc-C001-28", "intents": ["@USE@"], "paper_id": "ABC_d92e92b9a375914f3dd74868f463fc_26", "text": "Due to the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is an adaptation from the model proposed by Cífka and Bojar (2018) ."}
{"sent_id": "d42e2a9175e024e3ae44118e12fb58-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_d42e2a9175e024e3ae44118e12fb58_26", "text": "In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; Yang et al., 2015) , humor recognition was modeled as a binary classification task"}
{"sent_id": "d42e2a9175e024e3ae44118e12fb58-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_d42e2a9175e024e3ae44118e12fb58_26", "text": "In a recent work (Yang et al., 2015) , a new corpus was constructed from a Pun of the Day website."}
{"sent_id": "d42e2a9175e024e3ae44118e12fb58-C001-45", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_d42e2a9175e024e3ae44118e12fb58_26", "text": "Following (Mihalcea and Strapparava, 2005; Yang et al., 2015) , we selected the same sizes (n = 4726) of humorous and non-humorous sentences."}
{"sent_id": "d42e2a9175e024e3ae44118e12fb58-C001-33", "intents": ["@MOT@"], "paper_id": "ABC_d42e2a9175e024e3ae44118e12fb58_26", "text": "1 CNN-based text categorization methods have been applied for humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in Yang et al. (2015) is missing; (b) CNN's performance in the previous research is not quite clear 2 ; and (c) some important techniques that can improve CNN performance (e.g., using variedsized filters and dropout regularization (Hinton et al., 2012) ) were missing."}
{"sent_id": "d42e2a9175e024e3ae44118e12fb58-C001-52", "intents": ["@USE@"], "paper_id": "ABC_d42e2a9175e024e3ae44118e12fb58_26", "text": "Following Yang et al. (2015) , we applied Random Forest (Breiman, 2001 ) to do humor recognition by using the following two groups of features."}
{"sent_id": "d42e2a9175e024e3ae44118e12fb58-C001-73", "intents": ["@SIM@"], "paper_id": "ABC_d42e2a9175e024e3ae44118e12fb58_26", "text": "The Pun data allows us to verify that our implementation is consistent with the work reported in Yang et al. (2015) ."}
{"sent_id": "3d84cf97f48dad66b4d8de0baf79b1-C001-31", "intents": ["@USE@", "@EXT@", "@DIF@"], "paper_id": "ABC_3d84cf97f48dad66b4d8de0baf79b1_26", "text": "Finally, the work that most resembles ours is that of Fagarasan et al. (2015) , who use Partial Least Squares Regression (PLSR) to learn a mapping from a word embedding model onto specific conceptual properties."}
{"sent_id": "3d84cf97f48dad66b4d8de0baf79b1-C001-86", "intents": ["@USE@"], "paper_id": "ABC_3d84cf97f48dad66b4d8de0baf79b1_26", "text": "We first evaluate how well the baseline PLSR model performs on the feature vector reconstruction task used by Fagarasan et al. (2015) ."}
{"sent_id": "3d84cf97f48dad66b4d8de0baf79b1-C001-125", "intents": ["@MOT@"], "paper_id": "ABC_3d84cf97f48dad66b4d8de0baf79b1_26", "text": "As discussed by Fagarasan et al. (2015) and others, it is clear that property norm datasets provide only a semi-complete picture of human conceptual knowledge, and more extensive surveys may provide additional useful property knowledge information."}
{"sent_id": "c437e447603ecdbe4053651169770a-C001-12", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_c437e447603ecdbe4053651169770a_26", "text": "Identifying partisan preferences in news, based only on text content, has been shown to be a challenging task (Potthast et al., 2018) ."}
{"sent_id": "c437e447603ecdbe4053651169770a-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_c437e447603ecdbe4053651169770a_26", "text": "A recent paper (Potthast et al., 2018) claims that stylometric features are a key factor to tackle this task."}
{"sent_id": "c437e447603ecdbe4053651169770a-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_c437e447603ecdbe4053651169770a_26", "text": "They achieve state-of-the-art results on a publicly available collection (Potthast et al., 2018) , showing that neural models can effectively address the task of hyperpartisan detection without including stylometric features."}
{"sent_id": "c437e447603ecdbe4053651169770a-C001-62", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_c437e447603ecdbe4053651169770a_26", "text": "As our results are not directly comparable with the values reported in Potthast et al. (2018) , we re-evaluated their approach on this single fold."}
{"sent_id": "c437e447603ecdbe4053651169770a-C001-84", "intents": ["@USE@"], "paper_id": "ABC_c437e447603ecdbe4053651169770a_26", "text": "This deserves a set of extra experiments to better understand the real contribution of stylometric features when combined with strong representations/classifiers to validate the work of Potthast et al. (2018) ."}
{"sent_id": "d9877fc29c2e4f20805076392a70d0-C001-78", "intents": ["@USE@"], "paper_id": "ABC_d9877fc29c2e4f20805076392a70d0_26", "text": "The averaged cosine values between word embeddings before and after an epoch are used as a convergence measure c (Kim et al., 2014; Kulkarni et al., 2015) ."}
{"sent_id": "d9877fc29c2e4f20805076392a70d0-C001-114", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_d9877fc29c2e4f20805076392a70d0_26", "text": "SVD PPMI , which are conceptually not bothered by the reliability problems we discussed here, were not a good fit for the hyperparameters we adopted from Kulkarni et al. (2015) ."}
{"sent_id": "cee22bd0384d3d3fd4e45833341e77-C001-48", "intents": ["@USE@"], "paper_id": "ABC_cee22bd0384d3d3fd4e45833341e77_26", "text": "For T3, we regress over all Shekhar et al. (2017) ."}
{"sent_id": "cee22bd0384d3d3fd4e45833341e77-C001-56", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_cee22bd0384d3d3fd4e45833341e77_26", "text": "Following Shekhar et al. (2017) , we aim at creating a dataset of images associated with both correct and foil captions, where the latter are obtained by replacing one word in the original text."}
{"sent_id": "cee22bd0384d3d3fd4e45833341e77-C001-75", "intents": ["@USE@"], "paper_id": "ABC_cee22bd0384d3d3fd4e45833341e77_26", "text": "In order to obtain a balanced dataset across the various PoS, we only use a subset of the FOIL-COCO dataset of Shekhar et al. (2017) ."}
{"sent_id": "74db2b52e81969742f8f7e5681bd2b-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_74db2b52e81969742f8f7e5681bd2b_26", "text": "It's remarkable success is also embodied in machine translation tasks (Bahdanau et al., 2014; Vaswani et al., 2017) ."}
{"sent_id": "74db2b52e81969742f8f7e5681bd2b-C001-81", "intents": ["@BACK@"], "paper_id": "ABC_74db2b52e81969742f8f7e5681bd2b_26", "text": "Attention: Multi-head self-attention has demonstrated its capacity in neural transduction models (Vaswani et al., 2017) , language model pre-training (Devlin et al., 2018; Radford et al., 2018) and speech synthesis (Yang et al., 2019c) ."}
{"sent_id": "74db2b52e81969742f8f7e5681bd2b-C001-33", "intents": ["@USE@"], "paper_id": "ABC_74db2b52e81969742f8f7e5681bd2b_26", "text": "then the non-local operation degrades to the multihead self-attention as is described in (Vaswani et al., 2017) (formula 2 describes only one attention head):"}
{"sent_id": "013f0e54384a8a4662a746eb4c30d9-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_013f0e54384a8a4662a746eb4c30d9_26", "text": "Recent work has moved away from the original \"one word, one embedding\" paradigm to investigate contextualized embedding models (Peters et al., 2017 (Peters et al., , 2018 Akbik et al., 2018) ."}
{"sent_id": "013f0e54384a8a4662a746eb4c30d9-C001-88", "intents": ["@BACK@", "@USE@", "@SIM@"], "paper_id": "ABC_013f0e54384a8a4662a746eb4c30d9_26", "text": "The default setup of Akbik et al. (2018) recommends contextual string embeddings to be used in combination with standard word embeddings."}
{"sent_id": "013f0e54384a8a4662a746eb4c30d9-C001-61", "intents": ["@USE@"], "paper_id": "ABC_013f0e54384a8a4662a746eb4c30d9_26", "text": "It requires an embed() function that produces a contextualized embedding for a given word in a 1 https://github.com/zalandoresearch/flair sentence context (see Akbik et al. (2018) )."}
{"sent_id": "013f0e54384a8a4662a746eb4c30d9-C001-91", "intents": ["@USE@"], "paper_id": "ABC_013f0e54384a8a4662a746eb4c30d9_26", "text": "Our baseline are contextual string embeddings without pooling, i.e. the original setup proposed in Akbik et al. (2018) 2 ."}
{"sent_id": "d52dfb30158deae64a1c3d787d9b95-C001-22", "intents": ["@MOT@"], "paper_id": "ABC_d52dfb30158deae64a1c3d787d9b95_26", "text": "Defining precise rules for morphologically complex texts, especially for the purpose of infix removal is sometimes impossible [5] ."}
{"sent_id": "d52dfb30158deae64a1c3d787d9b95-C001-64", "intents": ["@USE@"], "paper_id": "ABC_d52dfb30158deae64a1c3d787d9b95_26", "text": "We opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [9, 3, 5] ."}
{"sent_id": "d52dfb30158deae64a1c3d787d9b95-C001-69", "intents": ["@USE@"], "paper_id": "ABC_d52dfb30158deae64a1c3d787d9b95_26", "text": "To avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary (c i,j and c i ,j ) or a pair of a candidate from the dictionary and an inflection from the collection (c i,j and c i ,j ) [5] ."}
{"sent_id": "eebf1edb6dbd3e58a904eff309f548-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_eebf1edb6dbd3e58a904eff309f548_26", "text": "Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a) , and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014) ."}
{"sent_id": "eebf1edb6dbd3e58a904eff309f548-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_eebf1edb6dbd3e58a904eff309f548_26", "text": "Both Berant et al. (2013) and Yao and Van Durme (2014) tested their systems on the WEBQUESTIONS dataset, which contains 3778 training questions and 2032 test questions collected from the Google Suggest API."}
{"sent_id": "eebf1edb6dbd3e58a904eff309f548-C001-15", "intents": ["@USE@"], "paper_id": "ABC_eebf1edb6dbd3e58a904eff309f548_26", "text": "While our conclusions should hold generally for similar KBs, we will focus on Freebase, such as explored by Krishnamurthy and Mitchell (2012) , and then others such as Cai and Yates (2013a) and Berant et al. (2013) ."}
{"sent_id": "499580e888a1598681a8d877b07866-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_499580e888a1598681a8d877b07866_26", "text": "1 Kim and Lee (2016) modify the NMT model to 1) use source and target A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output"}
{"sent_id": "499580e888a1598681a8d877b07866-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_499580e888a1598681a8d877b07866_26", "text": "In RNN based sentence-level QE model (Figure 2) , HTER (human-targeted translation edit rate) (Snover et al., 2006) in [0,1] for target sentence is predicted by using a logistic sigmoid func-A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output (Kim and Lee, 2016) tion such that"}
{"sent_id": "499580e888a1598681a8d877b07866-C001-30", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_499580e888a1598681a8d877b07866_26", "text": "Kim and Lee (2016) apply RNN based model to sentence-level QE and we extend this model to word and phraselevel QE."}
{"sent_id": "35ef3eba487c3cd97d32210670678a-C001-13", "intents": ["@MOT@"], "paper_id": "ABC_35ef3eba487c3cd97d32210670678a_26", "text": "Recently, Glockner et al. (2018) have shown that state-of-the-art NLI systems break considerably easily when instead of tested on the original SNLI test set, they are tested on a test set which Preprint."}
{"sent_id": "35ef3eba487c3cd97d32210670678a-C001-112", "intents": ["@SIM@", "@FUT@"], "paper_id": "ABC_35ef3eba487c3cd97d32210670678a_26", "text": "Our findings, together with the previous negative findings e.g. by Glockner et al. (2018) and Gururangan et al. (2018) , indicate that the current state-of-the-art neural network models fail to capture the semantics of NLI in a way that will enable them to generalize across different NLI situations."}
{"sent_id": "35ef3eba487c3cd97d32210670678a-C001-105", "intents": ["@DIF@"], "paper_id": "ABC_35ef3eba487c3cd97d32210670678a_26", "text": "In contrast to the findings of Glockner et al. (2018) , utilizing external knowledge did not improve the model's generalization capability, as KIM performed equally poorly across all dataset combinations."}
{"sent_id": "35ef3eba487c3cd97d32210670678a-C001-78", "intents": ["@BACK@"], "paper_id": "ABC_35ef3eba487c3cd97d32210670678a_26", "text": "KIM is particularly interesting in this context as it performed significantly better than other models in the Breaking NLI experiment conducted by Glockner et al. (2018) ."}
{"sent_id": "0a55859a36d0887ba4febc98762715-C001-23", "intents": ["@USE@"], "paper_id": "ABC_0a55859a36d0887ba4febc98762715_26", "text": "In this paper, we propose a new encoder, by improving GLAD architecture (Zhong et al., 2018) ."}
{"sent_id": "0a55859a36d0887ba4febc98762715-C001-62", "intents": ["@USE@"], "paper_id": "ABC_0a55859a36d0887ba4febc98762715_26", "text": "Scoring Model: We follow the proposed architecture in GLAD (Zhong et al., 2018) for computing score of each slot-value pair, in the user utterance and previous system actions."}
{"sent_id": "0a55859a36d0887ba4febc98762715-C001-73", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_0a55859a36d0887ba4febc98762715_26", "text": "The joint goal is the accumulation of turn goals as described in Zhong et al. (2018) ."}
{"sent_id": "0a55859a36d0887ba4febc98762715-C001-21", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_0a55859a36d0887ba4febc98762715_26", "text": "Recently, Zhong et al. (2018) proposed a model based on training a binary classifier for each slot-value, Global-Locally Self Attentive encoder (GLAD, by employing recurrent and self attention for each utterance and previous system actions, and measuring similaity of these computed representation to each slot-value, which achieve state of the art results on WoZ and DSTC2 (Williams et al., 2013) datasets."}
{"sent_id": "58b423c4ea2a3530d0c469fc0f5528-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_58b423c4ea2a3530d0c469fc0f5528_26", "text": "Kushman and Barzilay (2013) proposed a model that learns to perform the task from a parallel corpus of regular expressions and the text descriptions."}
{"sent_id": "58b423c4ea2a3530d0c469fc0f5528-C001-62", "intents": ["@USE@"], "paper_id": "ABC_58b423c4ea2a3530d0c469fc0f5528_26", "text": "We identify these via frequency analysis of smaller datasets from previous work (Kushman and Barzilay, 2013) ."}
{"sent_id": "58b423c4ea2a3530d0c469fc0f5528-C001-85", "intents": ["@USE@"], "paper_id": "ABC_58b423c4ea2a3530d0c469fc0f5528_26", "text": "We report DFA-Equal accuracy as our model's evaluation metric, using Kushman and Barzilay (2013) 's implementation to directly compare our results."}
{"sent_id": "58b423c4ea2a3530d0c469fc0f5528-C001-90", "intents": ["@USE@"], "paper_id": "ABC_58b423c4ea2a3530d0c469fc0f5528_26", "text": "Semantic-Unify: Our second baseline, SemanticUnify, is the previous state-of-the-art model from (Kushman and Barzilay, 2013) , explained above."}
{"sent_id": "58b423c4ea2a3530d0c469fc0f5528-C001-29", "intents": ["@SIM@"], "paper_id": "ABC_58b423c4ea2a3530d0c469fc0f5528_26", "text": "Our work, however, is closest to Kushman and Barzilay (2013) ."}
{"sent_id": "28900a293048fdb0c40dc540985cf1-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_28900a293048fdb0c40dc540985cf1_26", "text": "The model Kann et al. (2017) use and we explore in more detail here is an encoder-decoder recurrent neural network (RNN) with attention (Bahdanau et al., 2015) ."}
{"sent_id": "28900a293048fdb0c40dc540985cf1-C001-116", "intents": ["@BACK@"], "paper_id": "ABC_28900a293048fdb0c40dc540985cf1_26", "text": "SIGMORPHON hosted two shared tasks on paradigm completion (Cotterell et al., 2016 (Cotterell et al., , 2017 , in order to encourage the development of systems for the task."}
{"sent_id": "ce8997b630e9544b0f5812be319a59-C001-46", "intents": ["@USE@"], "paper_id": "ABC_ce8997b630e9544b0f5812be319a59_26", "text": "Following prior work (Lau et al., 2018), we generate words in each line in reverse order (i.e. right to left), and begin generation with the last line first."}
{"sent_id": "ce8997b630e9544b0f5812be319a59-C001-75", "intents": ["@USE@"], "paper_id": "ABC_ce8997b630e9544b0f5812be319a59_26", "text": "We work with the Shakespeare SONNET dataset (Lau et al., 2018 ) and a new LIMERICK corpus."}
{"sent_id": "ce8997b630e9544b0f5812be319a59-C001-108", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_ce8997b630e9544b0f5812be319a59_26", "text": "Our generator implementation is largely based on that of Lau et al. (2018) ."}
{"sent_id": "ce8997b630e9544b0f5812be319a59-C001-137", "intents": ["@USE@"], "paper_id": "ABC_ce8997b630e9544b0f5812be319a59_26", "text": "Following prior work (Lau et al., 2018) , we requested human annotators to identify the humanwritten poem when presented with two samples at a time -a quatrain from the Sonnet corpus and a machine-generated quatrain, and report the annotator accuracy on this task."}
{"sent_id": "cb81d56412d1e800074777687fb45a-C001-62", "intents": ["@USE@"], "paper_id": "ABC_cb81d56412d1e800074777687fb45a_26", "text": "We use the neural semantic parsing method of Yu et al. (2018a) as the baseline model, which can be regarded as a sequence-to-tree model."}
{"sent_id": "cb81d56412d1e800074777687fb45a-C001-85", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_cb81d56412d1e800074777687fb45a_26", "text": "Our hyperparameters are mostly taken from Yu et al. (2018a) , but tuned on the Chinese Spider development set."}
{"sent_id": "cb81d56412d1e800074777687fb45a-C001-38", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_cb81d56412d1e800074777687fb45a_26", "text": "There has been a line of work improving the model of Yu et al. (2018a) since the release of the Spider dataset (Guo et al., 2019; Lin et al., 2019) ."}
{"sent_id": "cb81d56412d1e800074777687fb45a-C001-98", "intents": ["@DIF@"], "paper_id": "ABC_cb81d56412d1e800074777687fb45a_26", "text": "In this table, ENG represents the results of Yu et al. (2018a) 's model on their English dataset but under our split."}
{"sent_id": "9227b5afd1ef18ecf83400dc402459-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_9227b5afd1ef18ecf83400dc402459_27", "text": "Interestingly but also expected in conversational dialogues, the distribution of token frequency across all subjects is highly symmetric (Tseng 2001) ."}
{"sent_id": "9227b5afd1ef18ecf83400dc402459-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_9227b5afd1ef18ecf83400dc402459_27", "text": "Falling tones may not show falling tendency anymore, when the associated words are used for specific discourse functions such as for indicating hesitation or the beginning of a turn (Tseng 2001) ."}
{"sent_id": "9227b5afd1ef18ecf83400dc402459-C001-37", "intents": ["@MOT@"], "paper_id": "ABC_9227b5afd1ef18ecf83400dc402459_27", "text": "Regarding the small size of data used in Tseng (2001) , it is one of the reasons why the ongoing project is necessary for research of Mandarin spontaneous conversations."}
{"sent_id": "6f4dc72277119f0df3d4a7155c61fc-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_6f4dc72277119f0df3d4a7155c61fc_27", "text": "Bayesian sparsification techniques [14, 16, 8, 9, 2] treat weights of an RNN as random variables and approximate posterior distribution over them given sparsity-inducing prior distribution."}
{"sent_id": "6f4dc72277119f0df3d4a7155c61fc-C001-37", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_6f4dc72277119f0df3d4a7155c61fc_27", "text": "In [2] SparseVD is adapted to RNNs."}
{"sent_id": "6f4dc72277119f0df3d4a7155c61fc-C001-43", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_6f4dc72277119f0df3d4a7155c61fc_27", "text": "To sparsify individual weights, we apply SparseVD [14] to all weights of the RNN, taking into account recurrent specifics underlined in [2] ."}
{"sent_id": "6f4dc72277119f0df3d4a7155c61fc-C001-66", "intents": ["@USE@"], "paper_id": "ABC_6f4dc72277119f0df3d4a7155c61fc_27", "text": "Since in text classification tasks usually only a small number of input words are important, we use additional multiplicative weights to sparsify the input vocabulary in case of group sparsification (W+N, W+G+N) following [2] ."}
{"sent_id": "5a039a2af7e07cffff76d3470f32f1-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_5a039a2af7e07cffff76d3470f32f1_27", "text": "Many techniques have been proposed to learn such embeddings (Pennington et al., 2014; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013) ."}
{"sent_id": "5a039a2af7e07cffff76d3470f32f1-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_5a039a2af7e07cffff76d3470f32f1_27", "text": "Zahran et al. (Zahran et al., 2015) translated the English benchmark in (Mikolov et al., 2013) and used it to evaluate different embedding techniques when applied on a large Arabic corpus."}
{"sent_id": "5a039a2af7e07cffff76d3470f32f1-C001-76", "intents": ["@USE@"], "paper_id": "ABC_5a039a2af7e07cffff76d3470f32f1_27", "text": "Using this corpus, the authors generated three different word embeddings using three different techniques, namely the Continuous Bagof-Words (CBOW) model (Mikolov et al., 2013) , the Skip-gram model (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) ."}
{"sent_id": "167511f278a8596aed0124c3a4242b-C001-57", "intents": ["@USE@"], "paper_id": "ABC_167511f278a8596aed0124c3a4242b_27", "text": "The delay reward is smoothed using a Target Delay which is a scalar constant denoted by d ⇤ (Gu et al., 2017) :"}
{"sent_id": "e92c6b44f4482ca868221bff551d67-C001-15", "intents": ["@EXT@"], "paper_id": "ABC_e92c6b44f4482ca868221bff551d67_27", "text": "Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003) , whose architecture and properties make it particularly adaptive to new tasks."}
{"sent_id": "e92c6b44f4482ca868221bff551d67-C001-43", "intents": ["@USE@"], "paper_id": "ABC_e92c6b44f4482ca868221bff551d67_27", "text": "We use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003) , which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations."}
{"sent_id": "e92c6b44f4482ca868221bff551d67-C001-127", "intents": ["@USE@"], "paper_id": "ABC_e92c6b44f4482ca868221bff551d67_27", "text": "H03 indicates the model illustrated in (Henderson, 2003) ."}
{"sent_id": "e92c6b44f4482ca868221bff551d67-C001-148", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_e92c6b44f4482ca868221bff551d67_27", "text": "(Henderson, 2003) tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs."}
{"sent_id": "e92c6b44f4482ca868221bff551d67-C001-131", "intents": ["@SIM@"], "paper_id": "ABC_e92c6b44f4482ca868221bff551d67_27", "text": "All our models, as well as the parser described in (Henderson, 2003) , are run only once."}
{"sent_id": "fca75d394e9f7007e1f674c7b99794-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_fca75d394e9f7007e1f674c7b99794_27", "text": "Litman et al. (2016) found significant group-level differences in pitch, jitter and shimmer between first and second halves of conversation."}
{"sent_id": "fca75d394e9f7007e1f674c7b99794-C001-35", "intents": ["@EXT@"], "paper_id": "ABC_fca75d394e9f7007e1f674c7b99794_27", "text": "Finally, to support our studies, we have developed an innovative representation of multi-party entrainment by extending the measurement from Litman et al. (2016) and adapting it to study the feature of linguistic style from Pennebaker and King (1999) ."}
{"sent_id": "fca75d394e9f7007e1f674c7b99794-C001-40", "intents": ["@USE@"], "paper_id": "ABC_fca75d394e9f7007e1f674c7b99794_27", "text": "The freely available Teams Corpus (Litman et al. 2016) The corpus also includes survey data."}
{"sent_id": "fca75d394e9f7007e1f674c7b99794-C001-100", "intents": ["@USE@"], "paper_id": "ABC_fca75d394e9f7007e1f674c7b99794_27", "text": "Since Litman et al. (2016) previously found that in the Teams corpus the highest acoustic-prosodic convergence occurred within the first and last three minutes, we used this finding to define our n. We evenly divided each game, which was limited to 30 minutes, into ten intervals, so each interval is less than three minutes."}
{"sent_id": "fca75d394e9f7007e1f674c7b99794-C001-85", "intents": ["@SIM@"], "paper_id": "ABC_fca75d394e9f7007e1f674c7b99794_27", "text": "Litman et al. (2016) then define convergence, a type of entrainment measuring increase in feature similarity, by comparing the T Dif f of two non-overlapping temporal intervals of a game as in Equation 4."}
{"sent_id": "f3c2c538019b1d9daa8e6c932d9826-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_f3c2c538019b1d9daa8e6c932d9826_27", "text": "The task of labeling topics consists of two main components: (1) a candidate generation component where candidate labels are obtained for a given topic (usually using information retrieval techniques and knowledge bases [11, 3] ), and (2) a ranking (or label selection) component that scores the candidates according to their relevance to the topic."}
{"sent_id": "f3c2c538019b1d9daa8e6c932d9826-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_f3c2c538019b1d9daa8e6c932d9826_27", "text": "The method presented by [3] generates a graph where the candidate images are its nodes."}
{"sent_id": "f3c2c538019b1d9daa8e6c932d9826-C001-43", "intents": ["@USE@"], "paper_id": "ABC_f3c2c538019b1d9daa8e6c932d9826_27", "text": "That is the output of the publicly available 16-layer VGG-net [13] trained over the ImageNet dataset [9] ."}
{"sent_id": "f3c2c538019b1d9daa8e6c932d9826-C001-64", "intents": ["@USE@"], "paper_id": "ABC_f3c2c538019b1d9daa8e6c932d9826_27", "text": "The 20 candidate image labels per topic are collected by [3] using an information retrieval engine (Google)."}
{"sent_id": "27ee0fbed3a88854ebe945dfffefd8-C001-91", "intents": ["@USE@"], "paper_id": "ABC_27ee0fbed3a88854ebe945dfffefd8_27", "text": "A review of the methods in the article [35] about the recognition of timexes for English and Spanish has shown a certain shift within the most popular solutions."}
{"sent_id": "27ee0fbed3a88854ebe945dfffefd8-C001-93", "intents": ["@USE@"], "paper_id": "ABC_27ee0fbed3a88854ebe945dfffefd8_27", "text": "The best systems listed in [35] , called TIPSem [16] and ClearTK [1] , use CRFs for recognition, so initially, we decided to apply the CRF-based approach for this task."}
{"sent_id": "27ee0fbed3a88854ebe945dfffefd8-C001-118", "intents": ["@USE@"], "paper_id": "ABC_27ee0fbed3a88854ebe945dfffefd8_27", "text": "Then we evaluated these results using more detailed measures for timexes, presented in [35] ."}
{"sent_id": "74420437db295ca874d5c946891f69-C001-30", "intents": ["@EXT@"], "paper_id": "ABC_74420437db295ca874d5c946891f69_27", "text": "We adopt the BN model of Tsvetkov and Wintner (2014) , but extend it with language-specific as well as semantically motivated features."}
{"sent_id": "74420437db295ca874d5c946891f69-C001-113", "intents": ["@EXT@"], "paper_id": "ABC_74420437db295ca874d5c946891f69_27", "text": "We adopted the Bayesian network model of Tsvetkov and Wintner (2014) and extended it with new features and manually-designed feature interactions, inspired by an analysis of Croatian MWEs."}
{"sent_id": "91723cf7f22ba6405c85a929ac2d8e-C001-3", "intents": ["@USE@"], "paper_id": "ABC_91723cf7f22ba6405c85a929ac2d8e_27", "text": "Evaluated on a recent large scale dataset (Hermann et al., 2015) , our model exhibits better results than previous research, and we find that max-pooling is suited for modeling the accumulation of information on entities."}
{"sent_id": "91723cf7f22ba6405c85a929ac2d8e-C001-68", "intents": ["@USE@"], "paper_id": "ABC_91723cf7f22ba6405c85a929ac2d8e_27", "text": "We use the CNN-QA dataset (Hermann et al., 2015) for evaluating our model's ability to answer questions about named entities."}
{"sent_id": "91723cf7f22ba6405c85a929ac2d8e-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_91723cf7f22ba6405c85a929ac2d8e_27", "text": "Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries (Hermann et al., 2015) , by replacing named entities in the summaries with placeholders to form Cloze (Taylor, 1953 ) style questions ( Figure 1 )."}
{"sent_id": "91723cf7f22ba6405c85a929ac2d8e-C001-34", "intents": ["@DIF@"], "paper_id": "ABC_91723cf7f22ba6405c85a929ac2d8e_27", "text": "in which u(q) is the learned meaning for the query and v(e; D, q) the dynamically constructed meaning for an entity, depending on the document D and the query q. We note that (1) is in contrast to the factorization used by Hermann et al. (2015):"}
{"sent_id": "3881903212a2d0fea039c8967ab553-C001-15", "intents": ["@MOT@"], "paper_id": "ABC_3881903212a2d0fea039c8967ab553_27", "text": "The motivation for this paper stems from prior work done by the first author in collaboration with other researchers (Prabhakaran et al., 2013a; Prabhakaran et al., 2013b) ."}
{"sent_id": "3881903212a2d0fea039c8967ab553-C001-105", "intents": ["@USE@"], "paper_id": "ABC_3881903212a2d0fea039c8967ab553_27", "text": "We use the best performing feature set of (Prabhakaran et al., 2013b) posted the overall best system obtaining a Tau of 0.60, NDCG of 0.970, and NDCG@3 of 0.937."}
{"sent_id": "0753a2be70f9844d353ec54c04fd53-C001-26", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_0753a2be70f9844d353ec54c04fd53_27", "text": "Many recent discourse relation classification approaches have focused on cross-lingual data augmentation , training models to better represent the relational arguments by using various neural network models, including feed-forward network (Rutherford et al., 2017) , convolutional neural networks (Zhang et al., 2015) , recurrent neural network (Ji et al., 2016; Bai and Zhao, 2018) , character-based (Qin et al., 2016) or formulating relation classification as an adversarial task (Qin et al., 2017) ."}
{"sent_id": "0753a2be70f9844d353ec54c04fd53-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_0753a2be70f9844d353ec54c04fd53_27", "text": "The current best performance was achieved by Bai and Zhao (2018) , who combined representations from different grained em-beddings including contextualized word vectors from ELMo (Peters et al., 2018) , which has been proved very helpful."}
{"sent_id": "0753a2be70f9844d353ec54c04fd53-C001-78", "intents": ["@USE@"], "paper_id": "ABC_0753a2be70f9844d353ec54c04fd53_27", "text": "We also tested the state of the art model of implicit discourse relation classification proposed by Bai and Zhao (2018) on BioDRB."}
{"sent_id": "0753a2be70f9844d353ec54c04fd53-C001-65", "intents": ["@DIF@"], "paper_id": "ABC_0753a2be70f9844d353ec54c04fd53_27", "text": "It obtained improvements of 7.3% points on PDTB-Lin, 5.5% points on PDTB-Ji, compared with the ELMobased method proposed in (Bai and Zhao, 2018) ."}
{"sent_id": "0753a2be70f9844d353ec54c04fd53-C001-79", "intents": ["@DIF@"], "paper_id": "ABC_0753a2be70f9844d353ec54c04fd53_27", "text": "From Table 2 , we can see that the BERT base model achieved almost 12% points improvement over the Bi-LSTM baseline and 15% points over Bai and Zhao (2018) ."}
{"sent_id": "3e5c070a6966361b54f069248438ec-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_3e5c070a6966361b54f069248438ec_27", "text": "Recently, Chen and Manning (2014) have showed that fast and accurate parsing can be achieved using neural network based parsers."}
{"sent_id": "3e5c070a6966361b54f069248438ec-C001-74", "intents": ["@BACK@"], "paper_id": "ABC_3e5c070a6966361b54f069248438ec_27", "text": "Chen and Manning (2014)'s parser is a greedy parser and it is not straight forward to add a beam during training into their parser."}
{"sent_id": "3e5c070a6966361b54f069248438ec-C001-42", "intents": ["@SIM@"], "paper_id": "ABC_3e5c070a6966361b54f069248438ec_27", "text": "The architecture of our neural network based shift-reduce CCG parser is similar to that of Chen and Manning (2014) ."}
{"sent_id": "3e5c070a6966361b54f069248438ec-C001-52", "intents": ["@USE@"], "paper_id": "ABC_3e5c070a6966361b54f069248438ec_27", "text": "Following Chen and Manning (2014), we use a cube activation function and softmax for output layer."}
{"sent_id": "98d8ea63896cc80f0989130e7cbbf1-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_98d8ea63896cc80f0989130e7cbbf1_27", "text": "Akiva and Koppel (2013) followed that work with an expanded method, and Aldebei et al. (2015) have since presented an improved technique in the 'multi-author document' context by exploiting posterior probabilities of a Naive-Bayesian Model."}
{"sent_id": "98d8ea63896cc80f0989130e7cbbf1-C001-13", "intents": ["@DIF@"], "paper_id": "ABC_98d8ea63896cc80f0989130e7cbbf1_27", "text": "In this paper, we argue that the biblical clustering done by Koppel et al. (2011) and by Aldebei et al. (2015) do not represent a grouping around true authorship within the Bible, but rather around common topics or shared style."}
{"sent_id": "98d8ea63896cc80f0989130e7cbbf1-C001-40", "intents": ["@SIM@"], "paper_id": "ABC_98d8ea63896cc80f0989130e7cbbf1_27", "text": "The NYT cor- pus is used both because the author of each document is known with certainty and because it is a canonical dataset that has served as a benchmark for both Akiva and Koppel (2013) and Aldebei et al. (2015) ."}
{"sent_id": "98d8ea63896cc80f0989130e7cbbf1-C001-79", "intents": ["@SIM@"], "paper_id": "ABC_98d8ea63896cc80f0989130e7cbbf1_27", "text": "Indeed, we were further surprised to discover that by adjusting our framework to be similar to that presented in Akiva and Koppel (2013) and Aldebei et al. (2015) -by replacing POS n-grams with ordinary word occurrences in step one -our framework performed very well, clustering at 95.3%."}
{"sent_id": "684349c06be7ff51c0944b25be10ce-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_684349c06be7ff51c0944b25be10ce_27", "text": "Two approaches mined social interaction net-works without relying on dialogue, unlike the methods of Elson et al. (2010) and He et al. (2013) ."}
{"sent_id": "684349c06be7ff51c0944b25be10ce-C001-15", "intents": ["@USE@"], "paper_id": "ABC_684349c06be7ff51c0944b25be10ce_27", "text": "Third, as practical applications, we analyze literary trends in character density over 20 decades and revisit the character-based literary hypothesis tested by Elson et al. (2010) ."}
{"sent_id": "684349c06be7ff51c0944b25be10ce-C001-82", "intents": ["@USE@"], "paper_id": "ABC_684349c06be7ff51c0944b25be10ce_27", "text": "Therefore, three state-of-the-art systems for social network extraction were selected: the method described in Elson et al. (2010) , BookNLP (Bamman et al., 2014) , and the method described in Ardanuy and Sporleder (2014) ."}
{"sent_id": "684349c06be7ff51c0944b25be10ce-C001-110", "intents": ["@USE@"], "paper_id": "ABC_684349c06be7ff51c0944b25be10ce_27", "text": "Character detection was run on the same novels from Elson et al. (2010) and we found no statistically-significant difference in the mean number of characters in urban and rural settings, even when accounting for text size."}
{"sent_id": "2e967f8560ffdb216135ae387776eb-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_2e967f8560ffdb216135ae387776eb_27", "text": "Later work by Hancke et al. (2012) also combines traditional readability formula measures, such as text or word length, with more sophisticated lexical, syntactic, and language model, and morphological features to assess German readability, but they employ an overall broader and more diverse feature set than DeLite."}
{"sent_id": "2e967f8560ffdb216135ae387776eb-C001-17", "intents": ["@EXT@"], "paper_id": "ABC_2e967f8560ffdb216135ae387776eb_27", "text": "To address these issues, we first present two new data sets for German readability assessment in Section 3: a set of German news broadcast subtitles based on the primary German TV news outlet Tagesschau and the children's counterpart Logo!, and a GEO/GEOlino corpus crawled from the educational GEO magazine's web site, a source first identified by Hancke et al. (2012) , but double in size."}
{"sent_id": "2e967f8560ffdb216135ae387776eb-C001-151", "intents": ["@SIM@"], "paper_id": "ABC_2e967f8560ffdb216135ae387776eb_27", "text": "On GEO/GEOlino S , the performance is comparable to the performance observed by Hancke et al. (2012) on the original GEO/GEOlino data."}
{"sent_id": "99b26d9151c7c0a1df1df1300fc764-C001-3", "intents": ["@EXT@"], "paper_id": "ABC_99b26d9151c7c0a1df1df1300fc764_27", "text": "The proposed model can be viewed as a speech version of Word2Vec [1]."}
{"sent_id": "99b26d9151c7c0a1df1df1300fc764-C001-13", "intents": ["@MOT@"], "paper_id": "ABC_99b26d9151c7c0a1df1df1300fc764_27", "text": "Our work, highly inspired by Word2Vec [1] , uses a skipgrams or continuous bag-of-words formulation to focus on neighboring acoustic regions, rather than the acoustic segment associated with the word itself."}
{"sent_id": "99b26d9151c7c0a1df1df1300fc764-C001-119", "intents": ["@SIM@"], "paper_id": "ABC_99b26d9151c7c0a1df1df1300fc764_27", "text": "This result aligns with the empirical fact that skipgrams Word2Vec is likely to work better than cbow Word2Vec with small training corpus size [1] ."}
{"sent_id": "bdeffcf02a86d06f57dbfae979b098-C001-35", "intents": ["@USE@"], "paper_id": "ABC_bdeffcf02a86d06f57dbfae979b098_27", "text": "In this paper, we focus on the Polylingual Topic Model, introduced by Mimno et al. (2009) ."}
{"sent_id": "bdeffcf02a86d06f57dbfae979b098-C001-128", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_bdeffcf02a86d06f57dbfae979b098_27", "text": "That paper used the Europarl (Koehn, 2005) (Mimno et al., 2009) , these performance comparisons are not done on the same training and test sets-a gap that we fill below."}
{"sent_id": "5c5abc2773143af41d49087e17310e-C001-4", "intents": ["@EXT@"], "paper_id": "ABC_5c5abc2773143af41d49087e17310e_27", "text": "We modify an existing system, HIER-SUM (Haghighi & Vanderwende, 2009) , to use our objective, which significantly outperforms the original HIERSUM in pairwise user evaluation."}
{"sent_id": "5c5abc2773143af41d49087e17310e-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_5c5abc2773143af41d49087e17310e_27", "text": "Haghighi and Vanderwende (2009) demonstrated that these models can improve the quality of generic multi-document summaries over simpler surface models."}
{"sent_id": "5c5abc2773143af41d49087e17310e-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_5c5abc2773143af41d49087e17310e_27", "text": "However, the raw (Haghighi & Vanderwende, 2009)."}
{"sent_id": "5c5abc2773143af41d49087e17310e-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_5c5abc2773143af41d49087e17310e_27", "text": "This quantity is used for summary sentence selection in several systems including Lerman and McDonald (2009) and Haghighi and Vanderwende (2009) , and was used as a feature in the discrimitive sentence ranking of Daumé and Marcu (2006) ."}
{"sent_id": "5c5abc2773143af41d49087e17310e-C001-96", "intents": ["@USE@"], "paper_id": "ABC_5c5abc2773143af41d49087e17310e_27", "text": "We asked users to select which summary was better for the following ques-5 Haghighi and Vanderwende (2009) Q1 Which was better in terms of overall content?"}
{"sent_id": "fe443d5e13b525cbdfa58dafb83162-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_fe443d5e13b525cbdfa58dafb83162_28", "text": "Yang and Xue (2010) simply count unlabeled empty elements: items are (i, i) for each empty element, where i is its position."}
{"sent_id": "fe443d5e13b525cbdfa58dafb83162-C001-81", "intents": ["@DIF@"], "paper_id": "ABC_fe443d5e13b525cbdfa58dafb83162_28", "text": "The unlabeled empty elements column shows that our system outperforms the baseline system of Yang and Xue (2010) ."}
{"sent_id": "622bd6f16d55ab5853389286cdda56-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_622bd6f16d55ab5853389286cdda56_28", "text": "In student papers, identifying revision purposes with respect to argument structure has been used to predict the grade improvement in the paper after revision (Zhang and Litman, 2015) ."}
{"sent_id": "622bd6f16d55ab5853389286cdda56-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_622bd6f16d55ab5853389286cdda56_28", "text": "There are multiple works on the classification of revisions (Adler et al., 2011; Javanmardi et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013; Zhang and Litman, 2015) ."}
{"sent_id": "622bd6f16d55ab5853389286cdda56-C001-42", "intents": ["@MOT@"], "paper_id": "ABC_622bd6f16d55ab5853389286cdda56_28", "text": "As Zhang and Litman (2015) reported that both Rebuttals and multiple labels for a single revision were rare, we merge Rebuttal and Warrant into one Warrant category 1 and allow only a single (primary) label per revision."}
{"sent_id": "318487ac270ca272ec11a3de6c0685-C001-18", "intents": ["@DIF@"], "paper_id": "ABC_318487ac270ca272ec11a3de6c0685_28", "text": "On a standard paraphrase identification task (Dolan et al., 2004) , this method improves on both traditional TF-IDF and Weighted Textual Matrix Factorization (WTMF; Guo and Diab, 2012) ."}
{"sent_id": "318487ac270ca272ec11a3de6c0685-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_318487ac270ca272ec11a3de6c0685_28", "text": "While previous work has performed paraphrase classification using distance or similarity in the latent space (Guo and Diab, 2012; Socher et al., 2011) , more direct supervision can be applied."}
{"sent_id": "318487ac270ca272ec11a3de6c0685-C001-85", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_318487ac270ca272ec11a3de6c0685_28", "text": "As in prior work (Guo and Diab, 2012) , the threshold is tuned on held-out training data."}
{"sent_id": "318487ac270ca272ec11a3de6c0685-C001-87", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_318487ac270ca272ec11a3de6c0685_28", "text": "To compare with Guo and Diab (2012) , we set the latent dimensionality to K = 100, which was the same in their paper."}
{"sent_id": "49b42346795d541dbcac9e2b9ad00a-C001-45", "intents": ["@EXT@"], "paper_id": "ABC_49b42346795d541dbcac9e2b9ad00a_28", "text": "Summarizing, our approach was to enrich the model of (Makarov et al., 2017 ) with the language model component."}
{"sent_id": "49b42346795d541dbcac9e2b9ad00a-C001-54", "intents": ["@USE@"], "paper_id": "ABC_49b42346795d541dbcac9e2b9ad00a_28", "text": "As the state-of-the-art baseline we choose the model of Makarov et al. (Makarov et al., 2017) , the winner of previous Sigmorphon Shared Task."}
{"sent_id": "49b42346795d541dbcac9e2b9ad00a-C001-115", "intents": ["@USE@"], "paper_id": "ABC_49b42346795d541dbcac9e2b9ad00a_28", "text": "We submitted three systems, one replicating the algorithm of (Makarov et al., 2017) , the second equipped with language models."}
{"sent_id": "afee292717afe1b0dcd77e155a5121-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_afee292717afe1b0dcd77e155a5121_28", "text": "Dakka and Cucerzan (2008) explored the use of NB and SVM classifiers for categorising Wikipedia."}
{"sent_id": "afee292717afe1b0dcd77e155a5121-C001-36", "intents": ["@USE@"], "paper_id": "ABC_afee292717afe1b0dcd77e155a5121_28", "text": "We experimented with a combination of the classification techniques used by Dakka and Cucerzan (2008) and the feature extraction methods used by Nothman et al. (2009) and others (Ponzetto and Strube, 2007; Hu et al., 2008; Biadsy et al., 2008) , focusing on the extraction of features from Wikipedia's rich metadata."}
{"sent_id": "afee292717afe1b0dcd77e155a5121-C001-69", "intents": ["@EXT@"], "paper_id": "ABC_afee292717afe1b0dcd77e155a5121_28", "text": "Tokens in the first paragraph were identified by Dakka and Cucerzan (2008) as useful features for a machine learner, an idea stemming from the fact that most human annotators will recognise an article's category after reading just the first paragraph."}
{"sent_id": "afee292717afe1b0dcd77e155a5121-C001-83", "intents": ["@UNSURE@"], "paper_id": "ABC_afee292717afe1b0dcd77e155a5121_28", "text": "We compared our two classifiers against the heuristic-based system described by Nothman et al. (2009) and the classifiers described by Dakka and Cucerzan (2008) ."}
{"sent_id": "afee292717afe1b0dcd77e155a5121-C001-90", "intents": ["@DIF@"], "paper_id": "ABC_afee292717afe1b0dcd77e155a5121_28", "text": "There were also a number of complications when comparing our system with the system described by Dakka and Cucerzan (2008) : they used a different, and substantially smaller, hand-labelled data set; they did not specify how they handled disambiguation pages; they provided no results for experiments using only hand-labelled data, instead incorporating training data produced via their semi-automated approach into the final results; and they neglected to report the final size of the training data produced by their semi-automated annotation."}
{"sent_id": "c594df62c01bef2ffb1a7ee9c5ea28-C001-12", "intents": ["@MOT@"], "paper_id": "ABC_c594df62c01bef2ffb1a7ee9c5ea28_28", "text": "This approach has achieved promising initial results [6] [7] [8] [9] 14] , but many questions remain."}
{"sent_id": "c594df62c01bef2ffb1a7ee9c5ea28-C001-15", "intents": ["@UNSURE@"], "paper_id": "ABC_c594df62c01bef2ffb1a7ee9c5ea28_28", "text": "It compares two methods for learning verb representations, the distributional model of [7] in which positive examples of subject-object pairs for a given verb are structurally mixed, and the regression model of [14] in which positive and negative examples of subject-object pairs for a given verb are mapped into a plausibility space."}
{"sent_id": "c594df62c01bef2ffb1a7ee9c5ea28-C001-58", "intents": ["@UNSURE@"], "paper_id": "ABC_c594df62c01bef2ffb1a7ee9c5ea28_28", "text": "Relational: from [7, 10] , the meaning of a transitive sentence is a matrix, obtained by the following formula, where ⊗ is outer product, ⊙ is elementwise product, and × is matrix multiplication:"}
{"sent_id": "c594df62c01bef2ffb1a7ee9c5ea28-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_c594df62c01bef2ffb1a7ee9c5ea28_28", "text": "The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6, 7] or defined a new space for sentence meaning, particularly plausibility space [11, 14] ."}
{"sent_id": "c594df62c01bef2ffb1a7ee9c5ea28-C001-69", "intents": ["@USE@"], "paper_id": "ABC_c594df62c01bef2ffb1a7ee9c5ea28_28", "text": "For the verb disambiguation task we use the GS2011 dataset [7] ."}
{"sent_id": "4ad830d8377d2584798a30bed65254-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_4ad830d8377d2584798a30bed65254_28", "text": "This is shown when training word embeddings, a vector representation of words, in news sets with crowd-sourcing evaluation to quantify the presence of biases, such as gender bias, in those representation (Bolukbasi et al., 2016) ."}
{"sent_id": "4ad830d8377d2584798a30bed65254-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_4ad830d8377d2584798a30bed65254_28", "text": "More specifically, gender stereotypes are learned from human generated corpora as shown by (Bolukbasi et al., 2016) ."}
{"sent_id": "4ad830d8377d2584798a30bed65254-C001-136", "intents": ["@BACK@"], "paper_id": "ABC_4ad830d8377d2584798a30bed65254_28", "text": "Debiaswe (Bolukbasi et al., 2016 ) is a debiasing post-process performed on trained embeddings."}
{"sent_id": "188f10a5b78a5e691e10d180dfde6f-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_188f10a5b78a5e691e10d180dfde6f_28", "text": "Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (Blitzer et al., 2006; Huang and Yates, 2010a) , chunking (Daumé III, 2007; Huang and Yates, 2009) , named entity recognition (Guo et al., 2009; Turian et al., 2010) , dependency parsing (Dredze et al., 2007; Sagae and Tsujii, 2007) and semantic role labeling (Dahlmeier and Ng, 2010; Huang and Yates, 2010b) ."}
{"sent_id": "188f10a5b78a5e691e10d180dfde6f-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_188f10a5b78a5e691e10d180dfde6f_28", "text": "For example, Huang and Yates (2009) used the discrete hidden state of a word under HMMs as augmenting features for cross-domain POS tagging and NP chunking."}
{"sent_id": "188f10a5b78a5e691e10d180dfde6f-C001-21", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_188f10a5b78a5e691e10d180dfde6f_28", "text": "The proposed approach is closely related to the clustering based method (Huang and Yates, 2009 ) as we both use latent state representations as generalizable features."}
{"sent_id": "188f10a5b78a5e691e10d180dfde6f-C001-86", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_188f10a5b78a5e691e10d180dfde6f_28", "text": "We used the same experimental datasets as in (Huang and Yates, 2009 ) for cross-domain POS tagging from Wall Street Journal (WSJ) domain (Marcus et al., 1993) to MED-LINE domain (PennBioIE, 2005) and for crossdomain NP chunking from CoNLL shared task dataset (Tjong et al., 2000) to Open American National Corpus (OANC) (Reppen et al., 2005) ."}
{"sent_id": "188f10a5b78a5e691e10d180dfde6f-C001-97", "intents": ["@UNSURE@"], "paper_id": "ABC_188f10a5b78a5e691e10d180dfde6f_28", "text": "We compared with the following systems: a Baseline system without representation learning, a SGM based word embedding system (Hovy et al., 2015) , and a discrete hidden state based clustering system (Huang and Yates, 2009) ."}
{"sent_id": "9666fd26c7e9a02505ff26a687076d-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_9666fd26c7e9a02505ff26a687076d_28", "text": "However, using the reparameterization in (Dyer et al., 2013) would leave the model simple enough even with a relatively large amount of word classes."}
{"sent_id": "9666fd26c7e9a02505ff26a687076d-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_9666fd26c7e9a02505ff26a687076d_28", "text": "As the partition function (Z(·)) used in (Dyer et al., 2013) consists of 2 calculations for each target position i, one for above and one for below the diagonal, we can simply substitute γ for the geometric series calculations in order to use different parameters for each:"}
{"sent_id": "9666fd26c7e9a02505ff26a687076d-C001-69", "intents": ["@SIM@"], "paper_id": "ABC_9666fd26c7e9a02505ff26a687076d_28", "text": "We use similar corpora as used in (Dyer et al., 2013) : a French-English corpus made up of Europarl version 7 and news-commentary corpora, the ArabicEnglish parallel data consisting of the non-UN portions of the NIST training corpora, and the FBIS Chinese-English corpora."}
{"sent_id": "0d1fb27d847ca44af36862cf78744e-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_0d1fb27d847ca44af36862cf78744e_28", "text": "As observed by Kahane et al. (1998) , any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc w j → w k by a projective arc w i → w k such that w i → * w j holds in the original graph."}
{"sent_id": "0d1fb27d847ca44af36862cf78744e-C001-42", "intents": ["@UNSURE@"], "paper_id": "ABC_0d1fb27d847ca44af36862cf78744e_28", "text": "The dependency graph in Figure 1 satisfies all the defining conditions above, but it fails to satisfy the condition of projectivity (Kahane et al., 1998): 1."}
{"sent_id": "0d1fb27d847ca44af36862cf78744e-C001-53", "intents": ["@USE@"], "paper_id": "ABC_0d1fb27d847ca44af36862cf78744e_28", "text": "Using the terminology of Kahane et al. (1998) , we say that jedna is the syntactic head of Z, while je is its linear head in the projectivized representation."}
{"sent_id": "0d1fb27d847ca44af36862cf78744e-C001-54", "intents": ["@DIF@"], "paper_id": "ABC_0d1fb27d847ca44af36862cf78744e_28", "text": "Unlike Kahane et al. (1998) , we do not regard a projectivized representation as the final target of the parsing process."}
{"sent_id": "fe30705e03f0475f9ab9d044a3c9ca-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_fe30705e03f0475f9ab9d044a3c9ca_28", "text": "This makes standard similarity calculations as proposed in (Lin, 1998; Curran, 2002; Lund and Burgess, 1996; Weeds et al., 2004) computationally infeasible."}
{"sent_id": "fe30705e03f0475f9ab9d044a3c9ca-C001-62", "intents": ["@DIF@"], "paper_id": "ABC_fe30705e03f0475f9ab9d044a3c9ca_28", "text": "In contrast to the best measures proposed by Lin (1998; Curran (2002; Pantel et al. (2009; Goyal et al. (2010) we do not calculate any information measure using frequencies of features and terms (we use significance ranking instead), as shown in Table 1 ."}
{"sent_id": "fe30705e03f0475f9ab9d044a3c9ca-C001-79", "intents": ["@UNSURE@"], "paper_id": "ABC_fe30705e03f0475f9ab9d044a3c9ca_28", "text": "We compare our results against DTs calculated using Lin's (Lin, 1998) measure and the best measure proposed by Curran (2002) (see Table 1)."}
{"sent_id": "3351b13fc0c9d4d2de16d897c78aee-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_3351b13fc0c9d4d2de16d897c78aee_28", "text": "As long as a labeled training set is available, string kernels can reach state-of-the-art results in various languages including English (Ionescu et al., 2014; Giménez-Pérez et al., 2017; Cozma et al., 2018) , Arabic (Ionescu, 2015; Ionescu et al., 2016; Ionescu and Butnaru, 2017; , Chinese and Norwegian (Ionescu et al., 2016) ."}
{"sent_id": "3351b13fc0c9d4d2de16d897c78aee-C001-96", "intents": ["@BACK@"], "paper_id": "ABC_3351b13fc0c9d4d2de16d897c78aee_28", "text": "Ionescu and Butnaru (2017) combined four kernels into a sum, and used Kernel Ridge Regression for training."}
{"sent_id": "3351b13fc0c9d4d2de16d897c78aee-C001-59", "intents": ["@DIF@"], "paper_id": "ABC_3351b13fc0c9d4d2de16d897c78aee_28", "text": "Nevertheless, we present empirical results indicating that our approach can obtain significantly better accuracy rates in cross-domain polarity classification and Arabic dialect identification compared to state-of-the-art methods based on string kernels (Giménez-Pérez et al., 2017; Ionescu and Butnaru, 2017) ."}
{"sent_id": "3351b13fc0c9d4d2de16d897c78aee-C001-107", "intents": ["@DIF@"], "paper_id": "ABC_3351b13fc0c9d4d2de16d897c78aee_28", "text": "The domain-adapted sum of kernels obtains improvements above 0.8% over the stateof-the-art sum of kernels (Ionescu and Butnaru, 2017) ."}
{"sent_id": "3351b13fc0c9d4d2de16d897c78aee-C001-100", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_3351b13fc0c9d4d2de16d897c78aee_28", "text": "In our experiments, we employ the exact same kernels as Ionescu and Butnaru (2017) to ensure an unbiased comparison with their ap- Butnaru, 2017) and the first runner up ."}
{"sent_id": "5e6d5bb4fb5be2b18ce3256302bf28-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_5e6d5bb4fb5be2b18ce3256302bf28_28", "text": "It always generates shortest possible referring description to identify an object. But Reiter and Dale (1995) later proved that Full Brevity requirement is an NP-Hard task, thus computationally intractable and offered an alternative polynomial time Incremental Algorithm."}
{"sent_id": "5e6d5bb4fb5be2b18ce3256302bf28-C001-34", "intents": ["@USE@"], "paper_id": "ABC_5e6d5bb4fb5be2b18ce3256302bf28_28", "text": "The scheme is based on Incremental algorithm (Reiter and Dale 1995) and incorporates the attractive properties (e.g. speed, simplicity etc) of that algorithm."}
{"sent_id": "5e6d5bb4fb5be2b18ce3256302bf28-C001-36", "intents": ["@USE@"], "paper_id": "ABC_5e6d5bb4fb5be2b18ce3256302bf28_28", "text": "Reiter and Dale (1995) pointed out the notion of 'PreferredAttributes' (e.g. Type, Size, Color etc) which is a sequence of attributes of an object that human speakers generally use to identify that object from the contrast set."}
{"sent_id": "a6f32017135e984fbe59f2171c50f4-C001-24", "intents": ["@USE@"], "paper_id": "ABC_a6f32017135e984fbe59f2171c50f4_28", "text": "(ii) We integrate the HAN in a very competitive NMT ar-chitecture (Vaswani et al., 2017) and show significant improvement over two strong baselines on multiple data sets."}
{"sent_id": "a6f32017135e984fbe59f2171c50f4-C001-76", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_a6f32017135e984fbe59f2171c50f4_28", "text": "The configuration is the same as the model called \"base model\" in the original paper (Vaswani et al., 2017) ."}
{"sent_id": "a6f32017135e984fbe59f2171c50f4-C001-80", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_a6f32017135e984fbe59f2171c50f4_28", "text": "The optimization and regularization methods were the same as proposed by Vaswani et al. (2017) ."}
{"sent_id": "d3f5f9b1ef8bda3d33c563d252d58a-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_d3f5f9b1ef8bda3d33c563d252d58a_28", "text": "In (Mikolov et al., 2013b) , the linear relation is extended to the bilingual scenario, where a linear transform is learned to project semantically identical words from one language to another."}
{"sent_id": "d3f5f9b1ef8bda3d33c563d252d58a-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_d3f5f9b1ef8bda3d33c563d252d58a_28", "text": "The bilingual word translation provided by (Mikolov et al., 2013b ) learns a linear transform from the source language to the target language by the linear regression."}
{"sent_id": "d3f5f9b1ef8bda3d33c563d252d58a-C001-28", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_d3f5f9b1ef8bda3d33c563d252d58a_28", "text": "This work largely follows the methodology and experimental settings of (Mikolov et al., 2013b) , while we normalize the embedding and use an orthogonal transform to conduct bilingual translation."}
{"sent_id": "d3f5f9b1ef8bda3d33c563d252d58a-C001-89", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_d3f5f9b1ef8bda3d33c563d252d58a_28", "text": "For an easy comparison, we largely follow Mikolov's settings in (Mikolov et al., 2013b) and set English and Spanish as the source and target language, respectively."}
{"sent_id": "d3f5f9b1ef8bda3d33c563d252d58a-C001-31", "intents": ["@SIM@"], "paper_id": "ABC_d3f5f9b1ef8bda3d33c563d252d58a_28", "text": "Our method in this paper and the linear projection method in (Mikolov et al., 2013b ) both belong to this category."}
{"sent_id": "d3f5f9b1ef8bda3d33c563d252d58a-C001-119", "intents": ["@SIM@"], "paper_id": "ABC_d3f5f9b1ef8bda3d33c563d252d58a_28", "text": "These results are comparable with the results reported in (Mikolov et al., 2013b"}
{"sent_id": "2db9f6c8540d8d2b7a5946c3c132e9-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_2db9f6c8540d8d2b7a5946c3c132e9_28", "text": "As SVD PPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities (Hamilton et al., 2016; Hellrich and Hahn, 2016a) ."}
{"sent_id": "2db9f6c8540d8d2b7a5946c3c132e9-C001-61", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_2db9f6c8540d8d2b7a5946c3c132e9_28", "text": "In accordance with this limit, we also discarded slices with less than 10k (5k for RSC) 7 Parameters were chosen in accordance with Levy et al. (2015) and Hamilton et al. (2016 words above the minimum frequency threshold used during PPMI and χ 2 calculation, e.g., the 1810s and 1820s COHA slices."}
{"sent_id": "2db9f6c8540d8d2b7a5946c3c132e9-C001-93", "intents": ["@FUT@"], "paper_id": "ABC_2db9f6c8540d8d2b7a5946c3c132e9_28", "text": "Future technical work will add functionality to compare words across corpora which might require a mapping between embeddings (Kulkarni et al., 2015; Hamilton et al., 2016) and provide optional stemming routines."}
{"sent_id": "9a52e0ea1f12e3455fca48ac8f8936-C001-18", "intents": ["@USE@"], "paper_id": "ABC_9a52e0ea1f12e3455fca48ac8f8936_28", "text": "In this work we assess the performance of the AWD-LSTM model [4] for language modeling to better understand how relevant the published hyper parameters may be for a codemixed corpus and to isolate which hyper parameters could be further tuned to improve performance."}
{"sent_id": "9a52e0ea1f12e3455fca48ac8f8936-C001-19", "intents": ["@UNSURE@"], "paper_id": "ABC_9a52e0ea1f12e3455fca48ac8f8936_28", "text": "Our results show that as a whole, the set of hyperparameters considered the best [4] are reasonably good, however ther are better sets hyperparamers for the code-mixed corpora."}
{"sent_id": "9a52e0ea1f12e3455fca48ac8f8936-C001-91", "intents": ["@UNSURE@"], "paper_id": "ABC_9a52e0ea1f12e3455fca48ac8f8936_28", "text": "We use the term default value when refering to an individual hyper parameter value that makes up the configuration with the best result for the AWD-LSTM model as reported in [4] ."}
{"sent_id": "9a52e0ea1f12e3455fca48ac8f8936-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_9a52e0ea1f12e3455fca48ac8f8936_28", "text": "The current state of the art on modeling both PTB and WikiText 2 [8] datasets as reported in [4] shows little sensitivity to hyper parameters; sharing almost all hyper parameters values between both datasets."}
{"sent_id": "9a52e0ea1f12e3455fca48ac8f8936-C001-86", "intents": ["@SIM@"], "paper_id": "ABC_9a52e0ea1f12e3455fca48ac8f8936_28", "text": "Both the population based and sequential search space were manually initialized with four (4) values of each hyperparameter in the neighbourhood of the best values reported in [4] as shown in Table I ."}
{"sent_id": "ffcefdc73338187d4a6b2dc2f0bb47-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_ffcefdc73338187d4a6b2dc2f0bb47_28", "text": "Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree (Dyer et al., 2016) :"}
{"sent_id": "ffcefdc73338187d4a6b2dc2f0bb47-C001-71", "intents": ["@SIM@"], "paper_id": "ABC_ffcefdc73338187d4a6b2dc2f0bb47_28", "text": "Given x, we produce Y (x), 50-best trees, with Charniak parser and find y with LSTM-LM as Dyer et al. (2016) do with their discriminative and generative models."}
{"sent_id": "ffcefdc73338187d4a6b2dc2f0bb47-C001-111", "intents": ["@DIF@"], "paper_id": "ABC_ffcefdc73338187d4a6b2dc2f0bb47_28", "text": "In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models (Dyer et al., 2016) ."}
{"sent_id": "fe0f9312caccf41def06e4311d15fb-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_fe0f9312caccf41def06e4311d15fb_28", "text": "More recently, automatic induction of script knowledge from text have started to attract attention: these methods exploit either natural texts (Chambers & Jurafsky, 2008; 2009) or crowdsourced data (Regneri et al., 2010) , and, consequently, do not require expensive expert annotation."}
{"sent_id": "fe0f9312caccf41def06e4311d15fb-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_fe0f9312caccf41def06e4311d15fb_28", "text": "Regneri et al. (2010) collected short textual descriptions (called event sequence descriptions, ESDs) of various types of human activities (e.g., going to a restaurant, ironing clothes) using crowdsourcing (Amazon Mechanical Turk), this dataset was also complemented by descriptions provided in the OMICS corpus (Gupta & Kochenderfer, 2004) ."}
{"sent_id": "fe0f9312caccf41def06e4311d15fb-C001-24", "intents": ["@USE@"], "paper_id": "ABC_fe0f9312caccf41def06e4311d15fb_28", "text": "The approach is evaluated on crowdsourced dataset of Regneri et al. (2010) and we demonstrate that using our model results in the 13.5% absolute improvement in F 1 on event ordering with respect to their graph induction method (84% vs. 71%)."}
{"sent_id": "fe0f9312caccf41def06e4311d15fb-C001-80", "intents": ["@UNSURE@"], "paper_id": "ABC_fe0f9312caccf41def06e4311d15fb_28", "text": "In our experiments, we compared our event embedding model (EE) against three baseline systems (BL , MSA) and BSMSA is the system of Regneri et al. (2010) ."}
{"sent_id": "7d89a96743d9db5667d90cbd3ebd30-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_7d89a96743d9db5667d90cbd3ebd30_28", "text": "This task is often considered as one of the simplest in NLP because basic machine learning techniques can yield strong baselines (Wang & Manning, 2012) , often beating much more intricate approaches (Socher et al., 2011) ."}
{"sent_id": "7d89a96743d9db5667d90cbd3ebd30-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_7d89a96743d9db5667d90cbd3ebd30_28", "text": "One can use advanced machine learning techniques such as recurrent neural networks and their variations (Mikolov et al., 2010; Socher et al., 2011) , however it is not clear if these provide any significant gain over simple bag-of-words and bag-of-ngram techniques (Pang & Lee, 2008; Wang & Manning, 2012) ."}
{"sent_id": "7d89a96743d9db5667d90cbd3ebd30-C001-27", "intents": ["@SIM@"], "paper_id": "ABC_7d89a96743d9db5667d90cbd3ebd30_28", "text": "The large pool of diverse models is a) simple to implement (in line with previous work by Wang and Manning (Wang & Manning, 2012) ) and b) it yields state of the art performance on one of the largest publicly available benchmarks of movie reviews, the Stanford IMDB dataset of reviews."}
{"sent_id": "7d89a96743d9db5667d90cbd3ebd30-C001-64", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_7d89a96743d9db5667d90cbd3ebd30_28", "text": "In our ensemble, we used a supervised reweighing of the counts as in the Naive Bayes Support Vector Machine (NB-SVM) approach (Wang & Manning, 2012) ."}
{"sent_id": "7d89a96743d9db5667d90cbd3ebd30-C001-68", "intents": ["@DIF@"], "paper_id": "ABC_7d89a96743d9db5667d90cbd3ebd30_28", "text": "Our implementation 1 slightly improved the performance reported in (Wang & Manning, 2012) by adding tri-grams (improvement of +0.6%), as shown in Table 1 ."}
{"sent_id": "7d89a96743d9db5667d90cbd3ebd30-C001-97", "intents": ["@DIF@"], "paper_id": "ABC_7d89a96743d9db5667d90cbd3ebd30_28", "text": "When we interpolate the scores of RNN, sentence vectors and NB-SVM, we achieve a new state-of-the-art performance of 92.57%, to be compared to 91.22% reported by (Wang & Manning, 2012) ."}
{"sent_id": "591e2873606d6171e48fd34a731fc7-C001-54", "intents": ["@MOT@"], "paper_id": "ABC_591e2873606d6171e48fd34a731fc7_29", "text": "This is the case of the studies of Han et al. (2012) and Han et al. (2014) who based their predictions on the so-called Location-Indicative Words (LIW)."}
{"sent_id": "591e2873606d6171e48fd34a731fc7-C001-58", "intents": ["@USE@"], "paper_id": "ABC_591e2873606d6171e48fd34a731fc7_29", "text": "Data sets We apply our method to two widely used data sets for geolocation: TWITTER-US (Roller et al., 2012) , and TWITTER-WORLD (Han et al., 2012) ."}
{"sent_id": "591e2873606d6171e48fd34a731fc7-C001-100", "intents": ["@USE@"], "paper_id": "ABC_591e2873606d6171e48fd34a731fc7_29", "text": "Following Han et al. (2012) , we further filter the vocabulary via Information Gain Ratio (IGR), selecting the terms with the highest values until we reach a computationally feasible vocabulary size: here, 750K and 460K for TWITTER-US and TWITTER-WORLD."}
{"sent_id": "2bb115f1c3e753e9dc66735887a52d-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_2bb115f1c3e753e9dc66735887a52d_29", "text": "Prior work (Soon et al., 2001; Denis and Baldridge, 2007) has generated training data for pairwise classifiers in the following manner."}
{"sent_id": "2bb115f1c3e753e9dc66735887a52d-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_2bb115f1c3e753e9dc66735887a52d_29", "text": "The COREF-ILP model of Denis and Baldridge (2007) took a different approach at test time: for each mention they would work backwards and add a link for all previous mentions which the classifier deemed coreferent."}
{"sent_id": "2bb115f1c3e753e9dc66735887a52d-C001-78", "intents": ["@USE@"], "paper_id": "ABC_2bb115f1c3e753e9dc66735887a52d_29", "text": "For comparison, we also give the results of the COREF-ILP system of Denis and Baldridge (2007) , which was also based on a naïve pairwise classifier."}
{"sent_id": "18ef4e4fafdf62839d6797d62eb76b-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_18ef4e4fafdf62839d6797d62eb76b_29", "text": "Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources (Rubin et al., 2016) ."}
{"sent_id": "18ef4e4fafdf62839d6797d62eb76b-C001-22", "intents": ["@USE@"], "paper_id": "ABC_18ef4e4fafdf62839d6797d62eb76b_29", "text": "We present a series of experiments on News Corpus with Varying Reliability dataset (Rashkin et al., 2017) and Satirical Legitimate News dataset (Rubin et al., 2016) ."}
{"sent_id": "e2705ae777acffc894f7aa18d42771-C001-10", "intents": ["@MOT@"], "paper_id": "ABC_e2705ae777acffc894f7aa18d42771_29", "text": "However, it still lags behind other statistical methods on very lowresource language pairs (Zoph et al., 2016; Koehn and Knowles, 2017) ."}
{"sent_id": "e2705ae777acffc894f7aa18d42771-C001-17", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_e2705ae777acffc894f7aa18d42771_29", "text": "We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of Zoph et al. (2016) does not always work, but it is still possible to use the parent model to considerably improve the child model."}
{"sent_id": "e2705ae777acffc894f7aa18d42771-C001-50", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_e2705ae777acffc894f7aa18d42771_29", "text": "The basic idea of our method is to extend the transfer method of Zoph et al. (2016) to share the parent and child's source vocabularies, so that when source word embeddings are transferred, a word that appears in both vocabularies keeps its embedding."}
{"sent_id": "e2705ae777acffc894f7aa18d42771-C001-101", "intents": ["@USE@"], "paper_id": "ABC_e2705ae777acffc894f7aa18d42771_29", "text": "We also compared against a word-based baseline (without transfer) and two word-based systems using transfer without vocabulary-sharing, corresponding with the method of Zoph et al. (2016) ( §2.2): one where the target word embeddings are fine-tuned, and one where they are frozen."}
{"sent_id": "e2705ae777acffc894f7aa18d42771-C001-115", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_e2705ae777acffc894f7aa18d42771_29", "text": "In this paper, we have shown that the transfer learning method of Zoph et al. (2016) , while appealing, might not always work in a low-resource context."}
{"sent_id": "eab79e8aa2cbe6f3aeef0018129208-C001-2", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_eab79e8aa2cbe6f3aeef0018129208_29", "text": "We propose solutions to enhance the Inside-Outside Recursive Neural Network (IORNN) reranker of Le and Zuidema (2014) ."}
{"sent_id": "eab79e8aa2cbe6f3aeef0018129208-C001-15", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_eab79e8aa2cbe6f3aeef0018129208_29", "text": "We focus on how to enhance the IORNN reranker of Le and Zuidema (2014) by both reducing its computational complexity and increasing its accuracy."}
{"sent_id": "eab79e8aa2cbe6f3aeef0018129208-C001-25", "intents": ["@USE@"], "paper_id": "ABC_eab79e8aa2cbe6f3aeef0018129208_29", "text": "We firstly introduce the IORNN reranker (Le and Zuidema, 2014) ."}
{"sent_id": "eab79e8aa2cbe6f3aeef0018129208-C001-114", "intents": ["@EXT@"], "paper_id": "ABC_eab79e8aa2cbe6f3aeef0018129208_29", "text": "Solutions to enhance the IORNN reranker of Le and Zuidema (2014) were proposed."}
{"sent_id": "eab79e8aa2cbe6f3aeef0018129208-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_eab79e8aa2cbe6f3aeef0018129208_29", "text": "For dependency parsing, the inside-outside recursive neural net (IORNN) reranker proposed by Le and Zuidema (2014) is among the top systems, including the Chen and Manning (2014)'s extremely fast transition-based parser employing a traditional feed-forward neural network."}
{"sent_id": "eab79e8aa2cbe6f3aeef0018129208-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_eab79e8aa2cbe6f3aeef0018129208_29", "text": "Second, by comparing a countbased model with their neural-net-based model on perplexity, Le and Zuidema (2014) suggested that predicting with neural nets is an effective solution for the problem of data sparsity."}
{"sent_id": "17d44521cfdd351d29b4e5f80d41cd-C001-44", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_17d44521cfdd351d29b4e5f80d41cd_29", "text": "These features are mostly taken from Zhang and Clark (2008) and Huang and Sagae (2010) , and our parser reproduces the same accuracies as reported by both papers."}
{"sent_id": "17d44521cfdd351d29b4e5f80d41cd-C001-90", "intents": ["@USE@"], "paper_id": "ABC_17d44521cfdd351d29b4e5f80d41cd_29", "text": "We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row 'H&S10'), and graphbased models including MSTParser (McDonald and Pereira, 2006) , the baseline feature parser of Koo et al. (2008) (row 'K08 baeline') , and the two models of Koo and Collins (2010 ing the highest attachment score reported for a transition-based parser, comparable to those of the best graph-based parsers."}
{"sent_id": "17d44521cfdd351d29b4e5f80d41cd-C001-110", "intents": ["@DIF@"], "paper_id": "ABC_17d44521cfdd351d29b4e5f80d41cd_29", "text": "The effect of the new features appears to outweigh the effect of combining transition-based and graph-based models, reported by Zhang and Clark (2008) , as well as the effect of using dynamic programming, as in- Huang and Sagae (2010) ."}
{"sent_id": "1c86f563ababf5ec3c67cbf259252b-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_1c86f563ababf5ec3c67cbf259252b_29", "text": "In recent years, a number of successful approaches have been proposed for both extractive (Nallapati et al., 2017; Narayan et al., 2018) and abstractive (See et al., 2017; Chen and Bansal, 2018) summarization paradigms."}
{"sent_id": "1c86f563ababf5ec3c67cbf259252b-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_1c86f563ababf5ec3c67cbf259252b_29", "text": "State-ofthe-art approaches are typically trained to generate summaries either in a fully end-to-end fashion (See et al., 2017) , processing the entire article at once; or hierarchically, first extracting content and then paraphrasing it sentence-by-sentence (Chen and Bansal, 2018) ."}
{"sent_id": "1c86f563ababf5ec3c67cbf259252b-C001-92", "intents": ["@BACK@", "@SIM@", "@DIF@", "@USE@"], "paper_id": "ABC_1c86f563ababf5ec3c67cbf259252b_29", "text": "EXT-ABS is the hierarchical model from (Chen and Bansal, 2018) , consisting of a supervised LSTM extractor and separate abstractor, both of which are individually trained on the CNN/DM dataset by aligning summary to article sentences."}
{"sent_id": "1c86f563ababf5ec3c67cbf259252b-C001-77", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_1c86f563ababf5ec3c67cbf259252b_29", "text": "We pick this model size to be comparable to recent work (See et al., 2017; Chen and Bansal, 2018) ."}
{"sent_id": "d0fa481abaf6d1b5529e40ff73f00a-C001-50", "intents": ["@BACK@", "@DIF@", "@MOT@"], "paper_id": "ABC_d0fa481abaf6d1b5529e40ff73f00a_29", "text": "While this can lead to much faster convergence, the resulting models are shown to slightly underperform compared to annealing SGD (Wu et al., 2016) ."}
{"sent_id": "d0fa481abaf6d1b5529e40ff73f00a-C001-68", "intents": ["@USE@"], "paper_id": "ABC_d0fa481abaf6d1b5529e40ff73f00a_29", "text": "7 Learning rates of 0.5 for SGD and 0.0002 for Adam or very similar are shown to work well in NMT implementations including GNMT (Wu et al., 2016) , Nematus, Marian, and OpenNMT (http://opennmt.net)."}
{"sent_id": "d0fa481abaf6d1b5529e40ff73f00a-C001-109", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_d0fa481abaf6d1b5529e40ff73f00a_29", "text": "As Wu et al. (2016) show different levels of effectiveness for different sub-word vocabulary sizes, we evaluate running BPE with 16K and 32K merge operations."}
{"sent_id": "d0fa481abaf6d1b5529e40ff73f00a-C001-185", "intents": ["@SIM@"], "paper_id": "ABC_d0fa481abaf6d1b5529e40ff73f00a_29", "text": "This trend follows previous work showing that dropout combats overfitting of small data, though the point of inflection is worth noting (Sennrich et al., 2016a; Wu et al., 2016) ."}
{"sent_id": "7176d3dd72e781dca42f8c146d062d-C001-4", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_7176d3dd72e781dca42f8c146d062d_29", "text": "We also report a comparison of our approach with that of (Munteanu and Marcu, 2005) using exactly the same corpora and show performance gain by using much lesser data."}
{"sent_id": "7176d3dd72e781dca42f8c146d062d-C001-50", "intents": ["@SIM@"], "paper_id": "ABC_7176d3dd72e781dca42f8c146d062d_29", "text": "Our use of full SMT sentences, gives us an added advantage of being able to detect one of the major errors of these approaches, also identified by (Munteanu and Marcu, 2005) , i.e, the cases where the initial sentences are identical but the retrieved sentence has a tail of extra words at sentence end."}
{"sent_id": "7176d3dd72e781dca42f8c146d062d-C001-174", "intents": ["@SIM@"], "paper_id": "ABC_7176d3dd72e781dca42f8c146d062d_29", "text": "LDC provides extracted parallel texts extracted with the algorithm published by (Munteanu and Marcu, 2005) ."}
{"sent_id": "7176d3dd72e781dca42f8c146d062d-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_7176d3dd72e781dca42f8c146d062d_29", "text": "Recent work by Munteanu and Marcu (2005) uses a bilingual lexicon to translate some of the words of the source sentence."}
{"sent_id": "0ae49d1618e18eb794666543d924ed-C001-21", "intents": ["@SIM@"], "paper_id": "ABC_0ae49d1618e18eb794666543d924ed_29", "text": "By adding very simple CLM-based features to the system, our scores approach those of a state-of-the-art NER system (Lample et al., 2016) across multiple languages, demonstrating both the unique importance and the broad utility of this approach."}
{"sent_id": "0ae49d1618e18eb794666543d924ed-C001-63", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_0ae49d1618e18eb794666543d924ed_29", "text": "4 We compare the CLM's Entity Identification against two state-of-the-art NER systems: CogCompNER (Khashabi et al., 2018) and LSTM-CRF (Lample et al., 2016) ."}
{"sent_id": "0ae49d1618e18eb794666543d924ed-C001-82", "intents": ["@USE@"], "paper_id": "ABC_0ae49d1618e18eb794666543d924ed_29", "text": "CogCompNER is run with standard features, including Brown clusters; (Lample et al., 2016) is run with default parameters and pre-trained embeddings."}
{"sent_id": "0ae49d1618e18eb794666543d924ed-C001-92", "intents": ["@DIF@"], "paper_id": "ABC_0ae49d1618e18eb794666543d924ed_29", "text": "The results in Table 3 show that for six of the eight languages we studied, the baseline NER can be significantly improved by adding simple CLM features; for English and Arabic, it performs better even than the neural NER model of (Lample et al., 2016) ."}
{"sent_id": "0ae49d1618e18eb794666543d924ed-C001-112", "intents": ["@BACK@"], "paper_id": "ABC_0ae49d1618e18eb794666543d924ed_29", "text": "Lample et al. (2016) use character embeddings in an LSTM-CRF model."}
{"sent_id": "b8d0e66901698d201b9fb1f362b8c6-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_b8d0e66901698d201b9fb1f362b8c6_29", "text": "Such representations, often extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono-or bi-directional, word-level or characterlevel long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016) ."}
{"sent_id": "b8d0e66901698d201b9fb1f362b8c6-C001-31", "intents": ["@MOT@"], "paper_id": "ABC_b8d0e66901698d201b9fb1f362b8c6_29", "text": "We refer to Plank et al. (2016) for further details."}
{"sent_id": "b8d0e66901698d201b9fb1f362b8c6-C001-69", "intents": ["@USE@"], "paper_id": "ABC_b8d0e66901698d201b9fb1f362b8c6_29", "text": "PoS accuracy scores are given for each language in the baseline configuration (the same as Plank et al., 2016) and in the lexicon-enabled configuration."}
{"sent_id": "603f49fc6ecf90da67a9a55986f217-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_603f49fc6ecf90da67a9a55986f217_29", "text": "Distributed word representations, also known as word embeddings, are low-dimensional vector representations for words that capture semantic aspects (Bengio et al., 2003; Pennington et al., 2014; Mikolov et al., 2013a) ."}
{"sent_id": "603f49fc6ecf90da67a9a55986f217-C001-31", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_603f49fc6ecf90da67a9a55986f217_29", "text": "Two variants were proposed in (Mikolov et al., 2013a ) -SKIP-GRAM, which maximizes the log likelihood of the local context words given the target word, and CBOW, which maximizes the log likelihood of the target word given its local context."}
{"sent_id": "603f49fc6ecf90da67a9a55986f217-C001-101", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_603f49fc6ecf90da67a9a55986f217_29", "text": "Google word analogy data (Mikolov et al., 2013a) contains 19544 analogy relations (14 relation types -5 semantic, 9 syntactic) of the form a:b::c:d constructed from 550 unique relation triplets."}
{"sent_id": "603f49fc6ecf90da67a9a55986f217-C001-29", "intents": ["@USE@"], "paper_id": "ABC_603f49fc6ecf90da67a9a55986f217_29", "text": "2 Subspace-regularized word embedding Although our proposed framework for relational modeling is general enough to use with any existing word embedding method, we work with Word2Vec model (Mikolov et al., 2013a) in this paper for illustrating our ideas and later for empirical evaluations."}
{"sent_id": "603f49fc6ecf90da67a9a55986f217-C001-130", "intents": ["@USE@"], "paper_id": "ABC_603f49fc6ecf90da67a9a55986f217_29", "text": "We use the Google word-analogy data (Mikolov et al., 2013a) for this evaluation."}
{"sent_id": "685b0b0da37b81765bb78f0f87505b-C001-75", "intents": ["@USE@"], "paper_id": "ABC_685b0b0da37b81765bb78f0f87505b_29", "text": "Following previous work (Nallapati et al., 2016; Chopra et al., 2016; Rush et al., 2015) , we use the full-length F1 variant of Rouge (Lin, 2004) for the Gigaword results, and the 75-bytes length limited Recall variant of Rouge for DUC."}
{"sent_id": "685b0b0da37b81765bb78f0f87505b-C001-112", "intents": ["@FUT@"], "paper_id": "ABC_685b0b0da37b81765bb78f0f87505b_29", "text": "Our next steps to this workshop paper include: (1) stronger summarization baselines, e.g., using pointer copy mechanism (See et al., 2017; Nallapati et al., 2016) , and also adding this capability to the entailment generation model; (2) results on CNN/Daily Mail corpora (Nallapati et al., 2016) ; (3) incorporating entailment knowledge from other news-style domains such as the new Multi-NLI corpus (Williams et al., 2017) , and (4) demonstrating mutual improvements on the entailment generation task."}
{"sent_id": "09f627b9a70966dc7b63316c56a2a0-C001-13", "intents": ["@DIF@"], "paper_id": "ABC_09f627b9a70966dc7b63316c56a2a0_29", "text": "1 Henceforth we will use terms like \"RNN\" and \"LSTM\" with the understanding that we are referring to language models that use these formalisms have outperformed their count-based counterparts (Chelba et al., 2013; Zaremba et al., 2014; Mikolov, 2012) ."}
{"sent_id": "09f627b9a70966dc7b63316c56a2a0-C001-22", "intents": ["@DIF@"], "paper_id": "ABC_09f627b9a70966dc7b63316c56a2a0_29", "text": "We achieve significantly lower perplexities with a single model, while using only a sixth of the parameters of a very strong baseline model (Chelba et al., 2013) ."}
{"sent_id": "09f627b9a70966dc7b63316c56a2a0-C001-26", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_09f627b9a70966dc7b63316c56a2a0_29", "text": "• Significantly improved perplexities (43.2) on the One Billion Word benchmark over Chelba et al. (2013) • Extrinsic machine translation improvement over a strong baseline."}
{"sent_id": "09f627b9a70966dc7b63316c56a2a0-C001-115", "intents": ["@DIF@"], "paper_id": "ABC_09f627b9a70966dc7b63316c56a2a0_29", "text": "Parameters Perplexity Chelba et al. (2013) 20m 51.3 NCE (ours) 3.4m 43.2 Recently, (Józefowicz et al., 2016) achieved stateof-the-art language modeling perplexities (30.0) on the billion word dataset with a single model, using importance sampling to approximate the normalization constant, Z(u)."}
{"sent_id": "09f627b9a70966dc7b63316c56a2a0-C001-21", "intents": ["@USE@"], "paper_id": "ABC_09f627b9a70966dc7b63316c56a2a0_29", "text": "Using our new objective, we train large multi-layer LSTMs on the One Billion Word benchmark (Chelba et al., 2013) , with its full 780k word vocabulary."}
{"sent_id": "09f627b9a70966dc7b63316c56a2a0-C001-87", "intents": ["@USE@"], "paper_id": "ABC_09f627b9a70966dc7b63316c56a2a0_29", "text": "For our language modeling experiment we use the One Billion Word benchmark proposed by Chelba et al. (2013) ."}
{"sent_id": "68d41bee7361b6680103c9951a6570-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_68d41bee7361b6680103c9951a6570_29", "text": "Deep learning's recent success in speech recognition is based on learning feature hierarchies atop these representations [1, 2] ."}
{"sent_id": "68d41bee7361b6680103c9951a6570-C001-46", "intents": ["@USE@"], "paper_id": "ABC_68d41bee7361b6680103c9951a6570_29", "text": "The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech [2] ."}
{"sent_id": "68d41bee7361b6680103c9951a6570-C001-57", "intents": ["@USE@"], "paper_id": "ABC_68d41bee7361b6680103c9951a6570_29", "text": "Following [2] , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances."}
{"sent_id": "68d41bee7361b6680103c9951a6570-C001-62", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_68d41bee7361b6680103c9951a6570_29", "text": "As previously observed [2] , deep neural networks trained on sufficient data perform better as the model size grows."}
{"sent_id": "68d41bee7361b6680103c9951a6570-C001-119", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_68d41bee7361b6680103c9951a6570_29", "text": "While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs [2, 16] ."}
{"sent_id": "40d370558d873499e493a83f106f17-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_40d370558d873499e493a83f106f17_29", "text": "Large-scale distributional thesauri created automatically from corpora (Grefenstette, 1994; Lin, 1998; Weeds et al., 2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998 ) are unavailable or lack coverage."}
{"sent_id": "40d370558d873499e493a83f106f17-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_40d370558d873499e493a83f106f17_29", "text": "The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004) ."}
{"sent_id": "40d370558d873499e493a83f106f17-C001-29", "intents": ["@MOT@"], "paper_id": "ABC_40d370558d873499e493a83f106f17_29", "text": "For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (Lin, 1998) , and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004) ."}
{"sent_id": "3d99ad1ba1696c8ef743f233530601-C001-21", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_3d99ad1ba1696c8ef743f233530601_29", "text": "Many approaches have been used in DP, the majority of them using machine learning algorithms, such as probabilistic models (Soricut and Marcu, 2003) , SVMs (Reitter, 2003; duVerle and Prendinger, 2009; Hernault et al., 2010; Feng and Hirst, 2012) and dynamic conditional random field (Joty et al., 2012) ."}
{"sent_id": "3d99ad1ba1696c8ef743f233530601-C001-73", "intents": ["@BACK@", "@USE@", "@SIM@"], "paper_id": "ABC_3d99ad1ba1696c8ef743f233530601_29", "text": "This work focuses on the identification of rhetorical relations at the sentence level, and as is common since the work of Soricut and Marcu (2003) , fine-grained relations were grouped: 29 sentence-level rhetorical relations were found and grouped into 16 groups."}
{"sent_id": "3d99ad1ba1696c8ef743f233530601-C001-151", "intents": ["@BACK@"], "paper_id": "ABC_3d99ad1ba1696c8ef743f233530601_29", "text": "Using separated test data, we tried to avoid possible overfitting on training data, but the size of test data may not lead to a fair evaluation Soricut and Marcu (2003) or Joty et al. (2012) , since HILDA-PT used different corpora (RST-DT-PT instead of RST-DT), and some reported results are for the complete DP."}
{"sent_id": "84ae490de92cb9d064993be751b3e0-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_84ae490de92cb9d064993be751b3e0_29", "text": "Experimental results are reported on the ARPA Wall Street Journal (WSJ) [19] and BREF [14] corpora, using for both corpora over 37k utterances for acoustic training and more than 37 million words of newspaper text for language model training."}
{"sent_id": "84ae490de92cb9d064993be751b3e0-C001-38", "intents": ["@USE@"], "paper_id": "ABC_84ae490de92cb9d064993be751b3e0_29", "text": "This property can be used in the first bigram decod1While we have built n-gram-backoff LMs directly from the 37M-word standardized WSJ training text material, in these experiments all results are reported using the 5k or 20k, bigram and tfigram backoff LMs provided by Lincoln Labs [ 19] as required by ARPA so as to be compatible with the other sites participating in the tests."}
{"sent_id": "84ae490de92cb9d064993be751b3e0-C001-41", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_84ae490de92cb9d064993be751b3e0_29", "text": "In order to be able to constnact LMs for BREF, it was necessary to normalize the text material of Le Monde newpaper, which entailed a pre-treatment rather different from that used to normalize the WSJ texts [19] ."}
{"sent_id": "84ae490de92cb9d064993be751b3e0-C001-140", "intents": ["@USE@"], "paper_id": "ABC_84ae490de92cb9d064993be751b3e0_29", "text": "Except when explicitly stated otherwise, all of the results reported for WSJ use the standard language models [19] ."}
{"sent_id": "1deb67be8226867fe6b9514cdecdec-C001-6", "intents": ["@BACK@"], "paper_id": "ABC_1deb67be8226867fe6b9514cdecdec_29", "text": "Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000; Henderson, 2003) have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence."}
{"sent_id": "1deb67be8226867fe6b9514cdecdec-C001-20", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_1deb67be8226867fe6b9514cdecdec_29", "text": "We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics."}
{"sent_id": "1deb67be8226867fe6b9514cdecdec-C001-38", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_1deb67be8226867fe6b9514cdecdec_29", "text": "To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003) , which do not make any explicit independence assumptions, and are therefore likely to adapt without much modification to the current problem."}
{"sent_id": "1deb67be8226867fe6b9514cdecdec-C001-123", "intents": ["@SIM@"], "paper_id": "ABC_1deb67be8226867fe6b9514cdecdec_29", "text": "The third line of Table 1 gives the performance on the simpler PTB parsing task of the original SSN parser (Henderson, 2003) , that was trained on the PTB data sets contrary to our SSN model trained on the PropBank data sets."}
{"sent_id": "b335178d833e26190b7056469d3fa7-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_b335178d833e26190b7056469d3fa7_30", "text": "One recent exception is Neubig and Hu (2018) which trained many-to-one models from 58 languages into English."}
{"sent_id": "b335178d833e26190b7056469d3fa7-C001-26", "intents": ["@USE@"], "paper_id": "ABC_b335178d833e26190b7056469d3fa7_30", "text": "Our experiments on the publicly available TED talks dataset (Qi et al., 2018) show that massively multilingual many-to-many models with up to 58 languages to-and-from English are very effective in low resource settings, allowing to use high-capacity models while avoiding overfitting and achieving superior results to the current stateof-the-art on this dataset (Neubig and Hu, 2018; Wang et al., 2019) when translating into English."}
{"sent_id": "b335178d833e26190b7056469d3fa7-C001-44", "intents": ["@USE@"], "paper_id": "ABC_b335178d833e26190b7056469d3fa7_30", "text": "Regarding the languages we evaluate on, we begin with the same four languages as Neubig and Hu (2018) -Azerbeijani (Az), Belarusian (Be), Galician (Gl) and Slovak (Sk)."}
{"sent_id": "b335178d833e26190b7056469d3fa7-C001-68", "intents": ["@USE@"], "paper_id": "ABC_b335178d833e26190b7056469d3fa7_30", "text": "We use tokenized BLEU in order to be comparable with Neubig and Hu (2018) ."}
{"sent_id": "35522a080b41f716723d2a619f59c4-C001-65", "intents": ["@SIM@"], "paper_id": "ABC_35522a080b41f716723d2a619f59c4_30", "text": "The difficulty we are facing is, due to noise in the translations, the conditional probabilities p(y|x s ) and the one in the translated texts p(y|x s ) may be quite different. Consider the following two straightforward strategies of using automatic machine translations: one can translate the original English labeled data (y, x s ) into (y, x t ) in Chinese and train a classifier, or one can train a classifier on (y, x s ) and translate x t in Chinese into x s in English so as to use the classifier. But as the conditional distribution can be quite different for the original language and the pseudo language produced by the machine translators, these two strategies give poor performance as reported in (Wan, 2009) ."}
{"sent_id": "35522a080b41f716723d2a619f59c4-C001-110", "intents": ["@USE@"], "paper_id": "ABC_35522a080b41f716723d2a619f59c4_30", "text": "The features we used are bigrams and unigrams in the two languages as in (Wan, 2009) ."}
{"sent_id": "35522a080b41f716723d2a619f59c4-C001-118", "intents": ["@UNSURE@"], "paper_id": "ABC_35522a080b41f716723d2a619f59c4_30", "text": "The method with the best performance in (Wan, 2009) ."}
{"sent_id": "fe3e71020dfb32927f5c348a6fdcfc-C001-19", "intents": ["@USE@"], "paper_id": "ABC_fe3e71020dfb32927f5c348a6fdcfc_30", "text": "We bring together two strands of research: one strand uses Reinforcement Learning to automatically optimise dialogue strategies, e.g. (Singh et al., 2002) , (Henderson et al., 2008) , (Rieser and Lemon, 2008a; Rieser and Lemon, 2008b) ; the other other focuses on automatic evaluation of dialogue strategies, e.g. the PARADISE framework (Walker et al., 1997) , and meta-evaluation of dialogue metrics, e.g. (Engelbrecht and Möller, 2007; Paek, 2007) ."}
{"sent_id": "fe3e71020dfb32927f5c348a6fdcfc-C001-36", "intents": ["@USE@"], "paper_id": "ABC_fe3e71020dfb32927f5c348a6fdcfc_30", "text": "We therefore formulate dialogue learning as a hierarchical optimisation problem (Rieser and Lemon, 2008b) ."}
{"sent_id": "fe3e71020dfb32927f5c348a6fdcfc-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_fe3e71020dfb32927f5c348a6fdcfc_30", "text": "In the following the overall method is shortly summarised. Please see (Rieser and Lemon, 2008b; Rieser, 2008) for details."}
{"sent_id": "fe3e71020dfb32927f5c348a6fdcfc-C001-123", "intents": ["@DIF@"], "paper_id": "ABC_fe3e71020dfb32927f5c348a6fdcfc_30", "text": "The SL policy, in contrast, did not learn an upper boundary for when to show items on the screen (since the wizards did not follow a specific pattern, (Rieser and Lemon, 2008b) )."}
{"sent_id": "56d1812bec8abbdb31a2346d96e5ca-C001-19", "intents": ["@USE@"], "paper_id": "ABC_56d1812bec8abbdb31a2346d96e5ca_30", "text": "Figure 1: An English sentence re-ordered into Japanese order using the rule-based method of Isozaki et al. (2010b) , and its reference Japanese translation."}
{"sent_id": "56d1812bec8abbdb31a2346d96e5ca-C001-38", "intents": ["@USE@"], "paper_id": "ABC_56d1812bec8abbdb31a2346d96e5ca_30", "text": "Reordering can be done either using rules based on linguistic knowledge (Isozaki et al., 2010b; Collins et al., 2005) or learning from aligned parallel data (Xia and McCord, 2004; Habash, 2007) , and in principle our pseudo-corpus creation paradigm is compatible with any of these methods."}
{"sent_id": "56d1812bec8abbdb31a2346d96e5ca-C001-54", "intents": ["@USE@"], "paper_id": "ABC_56d1812bec8abbdb31a2346d96e5ca_30", "text": "As noted above, we use HF (Isozaki et al., 2010b) as our re-ordering rule."}
{"sent_id": "56d1812bec8abbdb31a2346d96e5ca-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_56d1812bec8abbdb31a2346d96e5ca_30", "text": "Importantly, even simple heuristic reordering methods with a few handcreated rules have been shown to be highly effective in closing syntactic gaps (Collins et al. (2005) ; Isozaki et al. (2010b) ; Fig. 1 )."}
{"sent_id": "56d1812bec8abbdb31a2346d96e5ca-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_56d1812bec8abbdb31a2346d96e5ca_30", "text": "Examples of rule-based methods include those to reorder English into German (Navratil et al., 2012) , Arabic (Badr et al., 2009 ), or Japanese (Isozaki et al., 2010b) ."}
{"sent_id": "bd3663405d2d68f943acc73720b42d-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_bd3663405d2d68f943acc73720b42d_30", "text": "State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model (Horn et al., 2014; Bingel and Søgaard, 2016; Specia, 2016a, 2017) ."}
{"sent_id": "bd3663405d2d68f943acc73720b42d-C001-84", "intents": ["@USE@"], "paper_id": "ABC_bd3663405d2d68f943acc73720b42d_30", "text": "Since both datasets contain instances from the LexMturk dataset (Horn et al., 2014) , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 ."}
{"sent_id": "bd3663405d2d68f943acc73720b42d-C001-86", "intents": ["@USE@"], "paper_id": "ABC_bd3663405d2d68f943acc73720b42d_30", "text": "We adopt the same evaluation metrics featured in Glavaš andŠtajner (2015) and Horn et al. (2014) : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system."}
{"sent_id": "bd3663405d2d68f943acc73720b42d-C001-98", "intents": ["@SIM@"], "paper_id": "ABC_bd3663405d2d68f943acc73720b42d_30", "text": "with default parameters) for ranking substitution candidates, similar to the method described in (Horn et al., 2014) ."}
{"sent_id": "80e3aec943c37927050f97459360b4-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_80e3aec943c37927050f97459360b4_30", "text": "Hence, the Connectionist Temporal Classification (CTC) approach [15] [16] [17] [18] was introduced to map the speech input frames into an output label sequence."}
{"sent_id": "80e3aec943c37927050f97459360b4-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_80e3aec943c37927050f97459360b4_30", "text": "In [17] , the CTC with up to 27 thousand (k) word output targets was explored but the ASR accuracy is not very good, partially due to the high out-of-vocabulary (OOV) rate when using only around 3k hours training data."}
{"sent_id": "80e3aec943c37927050f97459360b4-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_80e3aec943c37927050f97459360b4_30", "text": "The CTC output labels can be phonemes [16] [17] [18] , characters [19] [20] [21] [22] [36] or even words [17] [23] [24] ."}
{"sent_id": "80e3aec943c37927050f97459360b4-C001-143", "intents": ["@SIM@"], "paper_id": "ABC_80e3aec943c37927050f97459360b4_30", "text": "The WER gap is consistent with what has been observed in [17] [24] ."}
{"sent_id": "fb75198b7c9e569932dfd486ba6c0a-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_fb75198b7c9e569932dfd486ba6c0a_30", "text": "Koeling et al. (2005) present a corpus were the examples are drawn from the balanced BNC corpus (Leech, 1992) and the SPORTS and FINANCES sections of the newswire Reuters corpus (Rose et al., 2002) , comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns."}
{"sent_id": "fb75198b7c9e569932dfd486ba6c0a-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_fb75198b7c9e569932dfd486ba6c0a_30", "text": "In contrast, ) reimplemented this method and showed that the improvement on WSD in the (Koeling et al., 2005) data was marginal."}
{"sent_id": "332e252e09d28763deb1ded2171c90-C001-3", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_332e252e09d28763deb1ded2171c90_30", "text": "The trees may be learned directly from parallel corpora (Wu, 1997), or provided by a parser trained on hand-annotated treebanks (Yamada and Knight, 2001) ."}
{"sent_id": "332e252e09d28763deb1ded2171c90-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_332e252e09d28763deb1ded2171c90_30", "text": "Wu (1997) modeled the reordering process with binary branching trees, where each production could be either in the same or in reverse order going from source to target language."}
{"sent_id": "332e252e09d28763deb1ded2171c90-C001-140", "intents": ["@BACK@"], "paper_id": "ABC_332e252e09d28763deb1ded2171c90_30", "text": "Zens and Ney (2003) compute the viterbi alignments for German-English and French-English sentences pairs using IBM Model 5, and then measure how many of the resulting alignments fall within the hard constraints of both Wu (1997) and Berger et al. (1996) ."}
{"sent_id": "332e252e09d28763deb1ded2171c90-C001-18", "intents": ["@USE@"], "paper_id": "ABC_332e252e09d28763deb1ded2171c90_30", "text": "In this paper, we make a direct comparison of a syntactically unsupervised alignment model, based on Wu (1997) , with a syntactically supervised model, based on Yamada and Knight (2001) ."}
{"sent_id": "4a7fecf3b80c274739e9c83be9a36b-C001-5", "intents": ["@USE@"], "paper_id": "ABC_4a7fecf3b80c274739e9c83be9a36b_30", "text": "On the ISNotes corpus (Markert et al., 2012) , our model with the contextually-encoded word representations (BERT) (Devlin et al., 2018) achieves new state-of-the-art performances on fine-grained IS classification, obtaining a 4.1% absolute overall accuracy improvement compared to Hou et al. (2013a) ."}
{"sent_id": "4a7fecf3b80c274739e9c83be9a36b-C001-13", "intents": ["@USE@"], "paper_id": "ABC_4a7fecf3b80c274739e9c83be9a36b_30", "text": "In this paper, we follow the IS scheme proposed by Markert et al. (2012) and focus on learning finegrained IS on written texts."}
{"sent_id": "4a7fecf3b80c274739e9c83be9a36b-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_4a7fecf3b80c274739e9c83be9a36b_30", "text": "Markert et al. (2012) et al. (2013a) regarding the overall IS classificiation accuracy but the result on bridging anaphora recognition is much worse than Hou et al. (2013a) ."}
{"sent_id": "89b2b492b4319636ff2f28a4ba0d95-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_89b2b492b4319636ff2f28a4ba0d95_30", "text": "One interesting aspect of using a global model with beam-search is that it narrows down the contrast between \"local, greedy, transition-based parsing\" and \"global, exhaustive, graph-based parsing\" as exemplified by McDonald and Nivre (2007) ."}
{"sent_id": "89b2b492b4319636ff2f28a4ba0d95-C001-30", "intents": ["@USE@"], "paper_id": "ABC_89b2b492b4319636ff2f28a4ba0d95_30", "text": "We follow McDonald and Nivre (2007) and perform a comparative error analysis of ZPar, MSTParser and MaltParser using the CoNLL-X shared task data."}
{"sent_id": "89b2b492b4319636ff2f28a4ba0d95-C001-94", "intents": ["@USE@"], "paper_id": "ABC_89b2b492b4319636ff2f28a4ba0d95_30", "text": "Following McDonald and Nivre (2007) we evaluate the parsers on the CoNLL-X Shared Task data (Buchholz and Marsi, 2006) , which include training and test sentences for 13 different languages."}
{"sent_id": "370da04cbb2a6ab807428f7e058110-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_370da04cbb2a6ab807428f7e058110_30", "text": "It is crucial for practical applications such as text classification [25] , copyright resolution [28] , identification of terrorist messages [29] and of plagiarism [26] ."}
{"sent_id": "370da04cbb2a6ab807428f7e058110-C001-26", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_370da04cbb2a6ab807428f7e058110_30", "text": "The adequacy of co-occurrence networks for the task was confirmed for the first time with the correlation between network topology and authors' styles [25] ."}
{"sent_id": "370da04cbb2a6ab807428f7e058110-C001-45", "intents": ["@USE@"], "paper_id": "ABC_370da04cbb2a6ab807428f7e058110_30", "text": "The co-occurrence networks were constructed with each distinct word becoming a node and two words being linked if they were adjacent in the pre-processed text [25] ."}
{"sent_id": "370da04cbb2a6ab807428f7e058110-C001-160", "intents": ["@SIM@"], "paper_id": "ABC_370da04cbb2a6ab807428f7e058110_30", "text": "A similar study for the same task [25] analyzed 40 texts from 8 authors in English reaching a success score of 65%."}
{"sent_id": "460a83a07ca3aa4d56deabad4f9831-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_460a83a07ca3aa4d56deabad4f9831_30", "text": "Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (Rohrbach et al., 2016; Pan et al., 2016; Lu and Grauman, 2013; Huang et al., 2016) ."}
{"sent_id": "460a83a07ca3aa4d56deabad4f9831-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_460a83a07ca3aa4d56deabad4f9831_30", "text": "While (Park and collects data by mining Blog Posts, (Huang et al., 2016) collects stories using Mechanical Turk, providing more directly relevant stories."}
{"sent_id": "460a83a07ca3aa4d56deabad4f9831-C001-23", "intents": ["@USE@"], "paper_id": "ABC_460a83a07ca3aa4d56deabad4f9831_30", "text": "In this paper, we make use of the Visual Storytelling Dataset (Huang et al., 2016) ."}
{"sent_id": "616e8732490f0fa87d35998f769196-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_616e8732490f0fa87d35998f769196_30", "text": "In addition, in the semantics domain, the use of a new TAG operation, flexible composition, is used to perform certain semantic operations that seemingly cannot be modeled with TL-MCTAG alone (Chiang and Scheffler, 2008) and in work in synchronous TAG semantics, constructions such as nested quantifiers require a set-local MCTAG (SL-MCTAG) analysis (Nesson and Shieber, 2006) ."}
{"sent_id": "616e8732490f0fa87d35998f769196-C001-107", "intents": ["@BACK@"], "paper_id": "ABC_616e8732490f0fa87d35998f769196_30", "text": "7 Delayed TL-MCTAG Chiang and Scheffler (2008) introduce the delayed TL-MCTAG formalism which makes use of a derivational distance restriction in a somewhat different way."}
{"sent_id": "616e8732490f0fa87d35998f769196-C001-20", "intents": ["@USE@"], "paper_id": "ABC_616e8732490f0fa87d35998f769196_30", "text": "In Section 7 we recall the delayed TL-MCTAG formalism introduced by Chiang and Scheffler (2008) and define a CKY-style parser for it as well."}
{"sent_id": "dcfd8cb0179ab156a6ffcab3358a45-C001-27", "intents": ["@EXT@"], "paper_id": "ABC_dcfd8cb0179ab156a6ffcab3358a45_30", "text": "Our FSVQA dataset, derived from (Antol et al. 2015) , minimizes such limitation by converting the answers to full-sentences, thus widely expanding the set of answers."}
{"sent_id": "dcfd8cb0179ab156a6ffcab3358a45-C001-43", "intents": ["@EXT@"], "paper_id": "ABC_dcfd8cb0179ab156a6ffcab3358a45_30", "text": "We circumvent this financial cost by converting the answers in the original VQA dataset (Antol et al. 2015) to full-sentence answers by applying a number of linguistic rules using natural language processing techniques."}
{"sent_id": "dcfd8cb0179ab156a6ffcab3358a45-C001-105", "intents": ["@USE@"], "paper_id": "ABC_dcfd8cb0179ab156a6ffcab3358a45_30", "text": "Following (Antol et al. 2015) , we examined the effect of Conversely, we also examined an approach where only image features are concerned."}
{"sent_id": "dcfd8cb0179ab156a6ffcab3358a45-C001-144", "intents": ["@SIM@"], "paper_id": "ABC_dcfd8cb0179ab156a6ffcab3358a45_30", "text": "This tendency is consistent with the results reported in (Antol et al. 2015) ."}
{"sent_id": "dcfd8cb0179ab156a6ffcab3358a45-C001-145", "intents": ["@SIM@"], "paper_id": "ABC_dcfd8cb0179ab156a6ffcab3358a45_30", "text": "It must nevertheless be reminded that the best performances in both (Antol et al. 2015) and our experiment were achieved with the presence of both visual and textual clues."}
{"sent_id": "b4d7e9b7942698ef0678d3b4a0ad7d-C001-23", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_b4d7e9b7942698ef0678d3b4a0ad7d_30", "text": "Early NLP-based approaches such as Bramsen et al. (2011) and Gilbert (2012) built systems to predict hierarchical power relations between people in the Enron email corpus using lexical features from all the messages exchanged between them."}
{"sent_id": "b4d7e9b7942698ef0678d3b4a0ad7d-C001-125", "intents": ["@BACK@"], "paper_id": "ABC_b4d7e9b7942698ef0678d3b4a0ad7d_30", "text": "Lexical features have already been shown to be valuable in predicting power relations (Bramsen et al., 2011; Gilbert, 2012) ."}
{"sent_id": "b4d7e9b7942698ef0678d3b4a0ad7d-C001-25", "intents": ["@MOT@"], "paper_id": "ABC_b4d7e9b7942698ef0678d3b4a0ad7d_30", "text": "For example, Bramsen et al. (2011) excluded sender-recipient pairs who exchanged fewer than 500 words from their evaluation set, since they found smaller text samples are harder to classify."}
{"sent_id": "b4d7e9b7942698ef0678d3b4a0ad7d-C001-34", "intents": ["@DIF@"], "paper_id": "ABC_b4d7e9b7942698ef0678d3b4a0ad7d_30", "text": "From (Bramsen et al., 2011) we retain the idea that we want to predict the power relation between pairs of people. But in contrast to their formulation, we retain the goal from ) that we want to study communication in the context of an interaction, and that we want to be able to make predictions using only the emails exchanged in a single thread."}
{"sent_id": "b4d7e9b7942698ef0678d3b4a0ad7d-C001-61", "intents": ["@SIM@"], "paper_id": "ABC_b4d7e9b7942698ef0678d3b4a0ad7d_30", "text": "This problem formulation is similar to the ones in (Bramsen et al., 2011) and (Gilbert, 2012) ."}
{"sent_id": "280affafa32147a63e7eeda8d5f763-C001-38", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_280affafa32147a63e7eeda8d5f763_30", "text": "The test data of Elliott and Keller (2013) contains 101 images paired with three reference descriptions."}
{"sent_id": "280affafa32147a63e7eeda8d5f763-C001-40", "intents": ["@BACK@", "@USE@", "@DIF@"], "paper_id": "ABC_280affafa32147a63e7eeda8d5f763_30", "text": "Elliott and Keller (2013) generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1-5 for each imagedescription pair, resulting in a total of 2,042 human judgement-description pairings."}
{"sent_id": "280affafa32147a63e7eeda8d5f763-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_280affafa32147a63e7eeda8d5f763_30", "text": "Unigram BLEU without a brevity penalty has been reported by ), Ordonez et al. (2011 , and Kuznetsova et al. (2012) ; to the best of our knowledge, the only image description work to use higher-order n-grams with BLEU is Elliott and Keller (2013) ."}
{"sent_id": "280affafa32147a63e7eeda8d5f763-C001-33", "intents": ["@USE@"], "paper_id": "ABC_280affafa32147a63e7eeda8d5f763_30", "text": "We perform the correlation analysis on the Flickr8K data set of Hodosh et al. (2013) , and the data set of Elliott and Keller (2013) ."}
{"sent_id": "280affafa32147a63e7eeda8d5f763-C001-107", "intents": ["@USE@"], "paper_id": "ABC_280affafa32147a63e7eeda8d5f763_30", "text": "We failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the Elliott and Keller (2013) data."}
{"sent_id": "280affafa32147a63e7eeda8d5f763-C001-82", "intents": ["@SIM@"], "paper_id": "ABC_280affafa32147a63e7eeda8d5f763_30", "text": "A similar pattern is observed in the Elliott and Keller (2013) data set, though the correlations are lower across all measures."}
{"sent_id": "3e86788379f2c0074ff16687d68fc9-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_3e86788379f2c0074ff16687d68fc9_30", "text": "Since Ramshaw and Marcus approached NP chunking using a machine learning method, many researchers have used various machine learning techniques [2, 4, 5, 6, 10, 11, 13, 14] ."}
{"sent_id": "3e86788379f2c0074ff16687d68fc9-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_3e86788379f2c0074ff16687d68fc9_30", "text": "Park et al. proposed a hybrid of the rule-based and the machine learning method to resolve this problem [5, 6] ."}
{"sent_id": "a800862f17f7a8c13ed13fc6e9433f-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_a800862f17f7a8c13ed13fc6e9433f_30", "text": "SpecAugment masks blocks of frequency channels and blocks of time steps [4] and also warps the spectrogram along the time axis to perform data augmentation."}
{"sent_id": "a800862f17f7a8c13ed13fc6e9433f-C001-54", "intents": ["@USE@"], "paper_id": "ABC_a800862f17f7a8c13ed13fc6e9433f_30", "text": "The final features presented to the network are also processed through a SpecAugment block that uses the SM policy [4] with p = 0.3 and no time warping."}
{"sent_id": "a800862f17f7a8c13ed13fc6e9433f-C001-130", "intents": ["@DIF@"], "paper_id": "ABC_a800862f17f7a8c13ed13fc6e9433f_30", "text": "We note that this model already outperforms the best published attention based seq2seq model [4] , with roughly"}
{"sent_id": "472b7c9f53b3130d7e5bba772e5b88-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_472b7c9f53b3130d7e5bba772e5b88_30", "text": "Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss."}
{"sent_id": "472b7c9f53b3130d7e5bba772e5b88-C001-102", "intents": ["@USE@"], "paper_id": "ABC_472b7c9f53b3130d7e5bba772e5b88_30", "text": "The proposed approaches are mainly compared with APC [6] representations, as they also experiment on phone classification and speaker verification."}
{"sent_id": "4590b1a4a0566915a6f2d6439a4e8a-C001-32", "intents": ["@USE@"], "paper_id": "ABC_4590b1a4a0566915a6f2d6439a4e8a_30", "text": "The second task was introduced by Bowman et al. (2015b) to compare tree-based recursive neural networks against sequence-based recurrent networks with respect to their ability to exploit hierarchical structures to make accurate inferences."}
{"sent_id": "4590b1a4a0566915a6f2d6439a4e8a-C001-63", "intents": ["@USE@"], "paper_id": "ABC_4590b1a4a0566915a6f2d6439a4e8a_30", "text": "In this task, we choose the artificial language introduced by Bowman et al. (2015b) ."}
{"sent_id": "4590b1a4a0566915a6f2d6439a4e8a-C001-88", "intents": ["@USE@"], "paper_id": "ABC_4590b1a4a0566915a6f2d6439a4e8a_30", "text": "Following the experimental protocol of Bowman et al. (2015b) , the data is divided into 13 bins based on the number of logical operators."}
{"sent_id": "4590b1a4a0566915a6f2d6439a4e8a-C001-80", "intents": ["@SIM@"], "paper_id": "ABC_4590b1a4a0566915a6f2d6439a4e8a_30", "text": "The LSTM architecture used in this experiment is similar to that of Bowman et al. (2015b) ."}
{"sent_id": "1265a336e56a4535f0a904ca89b220-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_1265a336e56a4535f0a904ca89b220_31", "text": "SGNS is preferred in general, yet SGHS showed slight benefits in some reliability scenarios in our prior investigations (Hellrich and Hahn, 2016a) ."}
{"sent_id": "1265a336e56a4535f0a904ca89b220-C001-75", "intents": ["@MOT@"], "paper_id": "ABC_1265a336e56a4535f0a904ca89b220_31", "text": "We processed the full subcorpora for each time span, due to the extremely low reliability values between samples we observed in previous investigations (Hellrich and Hahn, 2016a) ."}
{"sent_id": "1265a336e56a4535f0a904ca89b220-C001-88", "intents": ["@SIM@"], "paper_id": "ABC_1265a336e56a4535f0a904ca89b220_31", "text": "Reliability at different top-n cut-offs is very similar for all languages and time spans under scrutiny, confirming previous observations in Hellrich and Hahn (2016a) and strengthening the suggestion to use only top-1 reliability for evaluation."}
{"sent_id": "48def208400142f043a07be5d83713-C001-10", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_48def208400142f043a07be5d83713_31", "text": "We improve our RTM models (Biçici and Way, 2014):"}
{"sent_id": "48def208400142f043a07be5d83713-C001-14", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_48def208400142f043a07be5d83713_31", "text": "We present top results with Referential Translation Machines (Biçici, 2015; Biçici and Way, 2014) at quality estimation task (QET15) in WMT15 (Bojar et al., 2015) ."}
{"sent_id": "48def208400142f043a07be5d83713-C001-88", "intents": ["@USE@"], "paper_id": "ABC_48def208400142f043a07be5d83713_31", "text": "In Table 6 , we list the RTM test results for tasks and subtasks that predict HTER or METEOR from QET15, QET14 (Biçici and Way, 2014) , and QET13 (Biçici, 2013) ."}
{"sent_id": "365171603fb13c6534ef4abab092d6-C001-13", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_365171603fb13c6534ef4abab092d6_31", "text": "Our contributions are as follows: (1) we achieve state-of-the-art results on benchmark Twitter geolocation datasets; (2) we show that the model is less sensitive to the specific location discretisation method; (3) we release the first broad-coverage dataset for evaluation of lexical dialectology models; (4) we incorporate our text-based model into a network-based model (Rahimi et al., 2015a) and improve the performance utilising both network and text; and (5) we use the model's embeddings for extraction of local terms and show that it outperforms two baselines."}
{"sent_id": "365171603fb13c6534ef4abab092d6-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_365171603fb13c6534ef4abab092d6_31", "text": "Network-based methods also use either real-valued coordinates (Jurgens et al., 2015) or discretised regions (Rahimi et al., 2015a) as labels, and use label propagation over the interaction graph (e.g. @-mentions)."}
{"sent_id": "365171603fb13c6534ef4abab092d6-C001-56", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_365171603fb13c6534ef4abab092d6_31", "text": "While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user (Rahimi et al., 2015a) ."}
{"sent_id": "365171603fb13c6534ef4abab092d6-C001-55", "intents": ["@DIF@"], "paper_id": "ABC_365171603fb13c6534ef4abab092d6_31", "text": "4 The results reported in Rahimi et al. (2015b; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset."}
{"sent_id": "a7e49bec53a2bfd7795b9c770f5d0c-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_a7e49bec53a2bfd7795b9c770f5d0c_31", "text": "There is an accumulation of evidence that the use of dense distributional lexical representations, known as word embeddings, often supports better performance on a range of NLP tasks (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Mikolov et al., 2013a; Mikolov et al., 2013b; Levy et al., 2015) ."}
{"sent_id": "a7e49bec53a2bfd7795b9c770f5d0c-C001-25", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_a7e49bec53a2bfd7795b9c770f5d0c_31", "text": "In fact, word embedding generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability (Bengio et al., 2003; Mikolov et al., 2013a; Mikolov et al., 2013b) ."}
{"sent_id": "a7e49bec53a2bfd7795b9c770f5d0c-C001-83", "intents": ["@USE@"], "paper_id": "ABC_a7e49bec53a2bfd7795b9c770f5d0c_31", "text": "We ran both CBOW and skipgram with negative sampling (Mikolov et al., 2013a; Mikolov et al., 2013b) on the Wikipedia dump data, and set the window size of context to be five."}
{"sent_id": "021e5dbe22bf0f4ebda4d37040d0a6-C001-86", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_021e5dbe22bf0f4ebda4d37040d0a6_31", "text": "To test this, we use a cross-lingual transfer parser similar to that of McDonald et al. (2011) ."}
{"sent_id": "021e5dbe22bf0f4ebda4d37040d0a6-C001-59", "intents": ["@USE@"], "paper_id": "ABC_021e5dbe22bf0f4ebda4d37040d0a6_31", "text": "The selected sentences were pre-processed using cross-lingual taggers (Das and Petrov, 2011) and parsers (McDonald et al., 2011) ."}
{"sent_id": "062e9348de5fda68e61fff3ca4f186-C001-24", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_062e9348de5fda68e61fff3ca4f186_31", "text": "The third type of projection, P Acc , integrates syntactic information in the edge weights calculated by the following formula: While the entity grid (Barzilay and Lapata, 2008) uses information about sentences which do not share entities by means of the \"--\" transition, the entity graph cannot employ this negative information."}
{"sent_id": "062e9348de5fda68e61fff3ca4f186-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_062e9348de5fda68e61fff3ca4f186_31", "text": "Results for Guinaudeau and Strube (2013) , G&S, are reproduced, results for Barzilay and Lapata (2008) , B&L, and Elsner and Charniak (2011) , E&C, were reproduced by Guinaudeau and Strube (2013) ."}
{"sent_id": "062e9348de5fda68e61fff3ca4f186-C001-94", "intents": ["@DIF@"], "paper_id": "ABC_062e9348de5fda68e61fff3ca4f186_31", "text": "We compared our model to the entity graph and to the entity grid (Barzilay and Lapata, 2008) and showed that normalization improves the results significantly in most tasks."}
{"sent_id": "42ca932eaa96c174cdfb815bee82cb-C001-23", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_42ca932eaa96c174cdfb815bee82cb_31", "text": "From that single parameter we have shown that it is possible to infer head-final placement within verb arguments in SOV, head-first placement within verb arguments in SVO, head-first placement within verb arguments of VSO/VOS (Ferrer-i-Cancho, 2015) and also head first for the placement of inflected auxiliaries in SOV and head-final for their placement in VSO (Ferrer-i-Cancho, 2008 )."}
{"sent_id": "42ca932eaa96c174cdfb815bee82cb-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_42ca932eaa96c174cdfb815bee82cb_31", "text": "Fourth, an explanation for the relative placement of verbal auxiliaries given the placement of the main verb (Ferrer-i-Cancho, 2015 , 2008 would be lost."}
{"sent_id": "42ca932eaa96c174cdfb815bee82cb-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_42ca932eaa96c174cdfb815bee82cb_31", "text": "Third, notice that Ferrer-i-Cancho (2015) assumes that the cost of a dependency of length d is g(d)."}
{"sent_id": "c608567abe72c75bbbc8eb917ab5d3-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_c608567abe72c75bbbc8eb917ab5d3_31", "text": "Many neural network methods have recently been exploited in various natural language processing (NLP) tasks, such as parsing , POS tagging (Lample et al., 2016) , relation extraction (dos Santos et al., 2015) , translation (Bahdanau et al., 2015) , and joint tasks (Miwa and Bansal, 2016) ."}
{"sent_id": "c608567abe72c75bbbc8eb917ab5d3-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_c608567abe72c75bbbc8eb917ab5d3_31", "text": "Specifically, Miwa and Bansal (2016) as well as Li et al. (2017) apply bidirectional tree-structured RNNs for different contexts (i.e., news, biomedical) to capture syntactic information (using external dependency parsers)."}
{"sent_id": "c608567abe72c75bbbc8eb917ab5d3-C001-79", "intents": ["@USE@"], "paper_id": "ABC_c608567abe72c75bbbc8eb917ab5d3_31", "text": "1 Specifically, we follow the 5-fold crossvalidation defined by Miwa and Bansal (2016) for the ACE04 (Doddington et al., 2004) dataset."}
{"sent_id": "307c18e2928c4a45f574a9c3a36b76-C001-11", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_307c18e2928c4a45f574a9c3a36b76_31", "text": "This research has been applied to a wide array of domains such as Wikipedia talk pages (Strzalkowski et al., 2010; Taylor et al., 2012; Danescu-Niculescu-Mizil et al., 2012; Swayamdipta and Rambow, 2012) , blogs (Rosenthal, 2014) as well as workplace interactions (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran, 2015) ."}
{"sent_id": "307c18e2928c4a45f574a9c3a36b76-C001-55", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_307c18e2928c4a45f574a9c3a36b76_31", "text": "We use support vector machine (SVM) based approaches as our baseline, since they are the state-of-the art in this problem (Prabhakaran and Rambow, 2014; Bramsen et al., 2011; Gilbert, 2012) ."}
{"sent_id": "307c18e2928c4a45f574a9c3a36b76-C001-35", "intents": ["@DIF@", "@EXT@", "@SIM@"], "paper_id": "ABC_307c18e2928c4a45f574a9c3a36b76_31", "text": "Grouped: Here, we group all emails A sent to B across all threads in the corpus, and vice versa, and use these sets of emails to predict the power relation between A and B. This formulation is similar those in (Bramsen et al., 2011; Gilbert, 2012) , but our results are not directly comparable since, unlike them, we rely on the ground truth of power relations from (Agarwal et al., 2012) ; however, we created an SVM model that uses word-ngram features similar to theirs as a baseline to our proposed neural architectures."}
{"sent_id": "307c18e2928c4a45f574a9c3a36b76-C001-56", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_307c18e2928c4a45f574a9c3a36b76_31", "text": "We use the performance reported by (Prabhakaran and Rambow, 2014) using SVM as baseline for the Per-Thread formulation (using the same train-dev-test splits) and implemented an SVM baseline for the Grouped formulation (not directly comparable to performance reported by (Bramsen et al., 2011; Gilbert, 2012) )."}
{"sent_id": "a869bebe1744e3a7c71cb0f6fed12c-C001-85", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_a869bebe1744e3a7c71cb0f6fed12c_31", "text": "However, Strapparava and Mihalcea (2007) employ a rather weak notion of human performance which is-broadly speaking-based on the reliability of a single human rater."}
{"sent_id": "a869bebe1744e3a7c71cb0f6fed12c-C001-31", "intents": ["@USE@"], "paper_id": "ABC_a869bebe1744e3a7c71cb0f6fed12c_31", "text": "SE07: The test set of SemEval 2007 Task 14 (Strapparava and Mihalcea, 2007) comprises 1000 English news headlines which are annotated according to six Basic Emotions, joy, anger, sadness, fear, disgust, and surprise on a [0; 100]-scale (BE6 annotation format)."}
{"sent_id": "43422cf92d8cbb280c0b4c590632f1-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_43422cf92d8cbb280c0b4c590632f1_31", "text": "Furthermore, the WTA principle is applied in several studies [13, 14, 15] for increase activation sparseness in neural networks."}
{"sent_id": "43422cf92d8cbb280c0b4c590632f1-C001-32", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_43422cf92d8cbb280c0b4c590632f1_31", "text": "Recent work has started to address as well the problematic of memory footprint when learning word embeddings [20, 15] ."}
{"sent_id": "61f88b86c451fb6a5e5893c8c42a24-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_61f88b86c451fb6a5e5893c8c42a24_31", "text": "Suggestions can either be expressed explicitly (Brun, 2013) , or by expressing wishes regarding new features and improvements (Ramanand et al., 2010) (Table 1) ."}
{"sent_id": "61f88b86c451fb6a5e5893c8c42a24-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_61f88b86c451fb6a5e5893c8c42a24_31", "text": "Suggestion Detection: Ramanand et al. (2010) pointed out that wish is a broader category, which might not bear suggestions every time."}
{"sent_id": "61f88b86c451fb6a5e5893c8c42a24-C001-84", "intents": ["@SIM@"], "paper_id": "ABC_61f88b86c451fb6a5e5893c8c42a24_31", "text": "Ramanand et al. (2010) also used a similar but much smaller subset {love, like, prefer and suggest} in their rules."}
{"sent_id": "ee66681690f2c92fe705a09bf7015d-C001-38", "intents": ["@DIF@", "@BACK@"], "paper_id": "ABC_ee66681690f2c92fe705a09bf7015d_31", "text": "Our results demonstrate a significant improvement in accuracy of 7.2% over a statistical machine translation (SMT) system (Zens et al., 2005) and of 2.2% over a perceptron-based edit model (Freitag and Khadivi, 2007) ."}
{"sent_id": "ee66681690f2c92fe705a09bf7015d-C001-57", "intents": ["@USE@"], "paper_id": "ABC_ee66681690f2c92fe705a09bf7015d_31", "text": ". In this paper, we employ the same features as those used by Freitag and Khadivi (2007) ."}
{"sent_id": "ee66681690f2c92fe705a09bf7015d-C001-92", "intents": ["@USE@"], "paper_id": "ABC_ee66681690f2c92fe705a09bf7015d_31", "text": "We compare our model against a state-of-the-art statistical machine translation (SMT) system (Zens et al., 2005) and an averaged perceptron edit model (PTEM) with identical features (Freitag and Khadivi, 2007) ."}
{"sent_id": "5b98a80237182b2d506ea4c9d71aa1-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5b98a80237182b2d506ea4c9d71aa1_31", "text": "Most empirical approaches currently employed in NER task make decision only on local context for extract inference, which is based on the data independent assumption (Krishnan and Manning, 2006) ."}
{"sent_id": "5b98a80237182b2d506ea4c9d71aa1-C001-39", "intents": ["@SIM@"], "paper_id": "ABC_5b98a80237182b2d506ea4c9d71aa1_31", "text": "Similar to (Krishnan and Manning, 2006) , we employ two-stage architecture under conditional random fields (CRFs) framework."}
{"sent_id": "4a63ef4085639a66d1c7f6344f7548-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_4a63ef4085639a66d1c7f6344f7548_31", "text": "For example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (Liu et al., 2006; Liu et al., 2007) ."}
{"sent_id": "45d804ec30d20bd7e484c3bbd8399f-C001-17", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_45d804ec30d20bd7e484c3bbd8399f_31", "text": "The first method-maximum attention weight (MAX)-designates the word with the highest incoming attention weight as the parent, and is meant to identify specialist heads that track specific dependencies like obj (in the style of Clark et al., 2019) ."}
{"sent_id": "45d804ec30d20bd7e484c3bbd8399f-C001-21", "intents": ["@BACK@", "@MOT@", "@EXT@"], "paper_id": "ABC_45d804ec30d20bd7e484c3bbd8399f_31", "text": "In prior work, Clark et al. (2019) find that some heads of BERT exhibit the behavior of some dependency relation types, though they do not perform well at all types of relations in general."}
{"sent_id": "45d804ec30d20bd7e484c3bbd8399f-C001-66", "intents": ["@EXT@", "@SIM@"], "paper_id": "ABC_45d804ec30d20bd7e484c3bbd8399f_31", "text": "This method is similar to Clark et al. (2019) , and attempts to recover individual arcs between words; the relations extracted using this method need not form a valid tree, or even be fully connected, and the resulting edge directions may or may not match the canonical directions."}
{"sent_id": "45d804ec30d20bd7e484c3bbd8399f-C001-93", "intents": ["@SIM@"], "paper_id": "ABC_45d804ec30d20bd7e484c3bbd8399f_31", "text": "These results are consistent with the findings of Clark et al. (2019) ."}
{"sent_id": "4c8e83eb213879e68285e9cd09be47-C001-12", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_4c8e83eb213879e68285e9cd09be47_31", "text": "Over the years, researchers have proposed normalization methods based on rules and/or edit distances (Baron and Rayson, 2008; Bollmann, 2012; Hauser and Schulz, 2007; Bollmann et al., 2011; Pettersson et al., 2013a; Mitankin et al., 2014; Pettersson et al., 2014) , statistical machine translation (Pettersson et al., 2013b; Scherrer and Erjavec, 2013) , and most recently neural network models (Bollmann and Søgaard, 2016; Bollmann et al., 2017; Korchagina, 2017) ."}
{"sent_id": "4c8e83eb213879e68285e9cd09be47-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_4c8e83eb213879e68285e9cd09be47_31", "text": "Since historical spelling normalization is typically a low-resource task, systems should also ideally be tested with varying amounts of training data to assess how much annotation might be required for a new corpus (Pettersson et al., 2014; Bollmann and Søgaard, 2016; Korchagina, 2017) ."}
{"sent_id": "4c8e83eb213879e68285e9cd09be47-C001-46", "intents": ["@UNSURE@"], "paper_id": "ABC_4c8e83eb213879e68285e9cd09be47_31", "text": "We focus on two neural encoder-decoder models for spelling normalization, comparing them against the memorization baseline and to previous results from Pettersson et al. (2014) ."}
{"sent_id": "4c8e83eb213879e68285e9cd09be47-C001-62", "intents": ["@USE@"], "paper_id": "ABC_4c8e83eb213879e68285e9cd09be47_31", "text": "We use the same datasets as Pettersson et al. (2014) , with data from five languages over a range of historical periods."}
{"sent_id": "6bf17a793eaee0593596df0c2249b5-C001-67", "intents": ["@USE@"], "paper_id": "ABC_6bf17a793eaee0593596df0c2249b5_32", "text": "For all our experiment, unless specified otherwise, we use the open domain corpus 4 released by Yang et al. (2018) which contains over 5.23 million Wikipedia abstracts (introductory paragraphs)."}
{"sent_id": "6bf17a793eaee0593596df0c2249b5-C001-103", "intents": ["@DIF@"], "paper_id": "ABC_6bf17a793eaee0593596df0c2249b5_32", "text": "We were able to achieve better scores than reported in the baseline reader model of Yang et al. (2018) by using Adam (Kingma and Ba, 2014) instead of standard SGD (our re-implementation)."}
{"sent_id": "15df1d107fb349f78c313b0c3342b8-C001-29", "intents": ["@EXT@"], "paper_id": "ABC_15df1d107fb349f78c313b0c3342b8_32", "text": "The system that we propose builds on top of one of the latest neural MT architectures called the Transformer (Vaswani et al., 2017) ."}
{"sent_id": "15df1d107fb349f78c313b0c3342b8-C001-34", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_15df1d107fb349f78c313b0c3342b8_32", "text": "This section provides a brief high-level explanation of the neural MT approach that we are using as a baseline system, which is one of the strongest systems presented recently (Vaswani et al., 2017) , as well as a glance of its differences with other popular neural machine translation architectures."}
{"sent_id": "15df1d107fb349f78c313b0c3342b8-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_15df1d107fb349f78c313b0c3342b8_32", "text": "Equations and details about the transformer system can be found in the original paper (Vaswani et al., 2017) and are out of the scope of this paper."}
{"sent_id": "15df1d107fb349f78c313b0c3342b8-C001-37", "intents": ["@USE@"], "paper_id": "ABC_15df1d107fb349f78c313b0c3342b8_32", "text": "In this paper we make use of the third paradigm for neural machine translation, proposed in (Vaswani et al., 2017) , namely the Transformer architecture, which is based on a feed-forward encoder-decoder scheme with attention mechanisms."}
{"sent_id": "15df1d107fb349f78c313b0c3342b8-C001-71", "intents": ["@UNSURE@"], "paper_id": "ABC_15df1d107fb349f78c313b0c3342b8_32", "text": "The results of each of the pivotal translation systems as well as the combined cascaded translation are summarized in table 4, which shows the high quality of the translations of the attentional architecture from (Vaswani et al., 2017) ."}
{"sent_id": "8905d5936a5b2a839bfd56783ff55d-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_8905d5936a5b2a839bfd56783ff55d_32", "text": "In subsequent work, we showed that we could augment the E2E training data with synthetically generated stylistic variants and train a neural generator to reproduce these variants, however the models can still only generate what they have seen in training [5] ."}
{"sent_id": "8905d5936a5b2a839bfd56783ff55d-C001-28", "intents": ["@USE@"], "paper_id": "ABC_8905d5936a5b2a839bfd56783ff55d_32", "text": "The PERSONAGE corpus [5] provides a controlled environment for testing different models of neural generation and style generation."}
{"sent_id": "10acbeba830b2f8b3feb30de542c56-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_10acbeba830b2f8b3feb30de542c56_32", "text": "To address this, recent tasks based on simulated environments include photo-realistic visual input, such as Room-to-Room (R2R; Anderson et al., 2018) , Talk-the-Walk (de Vries et al., 2018) and Touchdown (Chen et al., 2019) , all of which rely on panorama photos."}
{"sent_id": "10acbeba830b2f8b3feb30de542c56-C001-90", "intents": ["@BACK@"], "paper_id": "ABC_10acbeba830b2f8b3feb30de542c56_32", "text": "TC, SPD, and SED are defined in Chen et al. (2019) and nDTW and SDTW are defined in Ilharco et al. (2019) ."}
{"sent_id": "10acbeba830b2f8b3feb30de542c56-C001-71", "intents": ["@USE@"], "paper_id": "ABC_10acbeba830b2f8b3feb30de542c56_32", "text": "We re-implement the best-reported models on the navigation and spatial description resolution tasks from Chen et al. (2019) to compare performance with our data release to the original Touchdown paper."}
{"sent_id": "10acbeba830b2f8b3feb30de542c56-C001-80", "intents": ["@UNSURE@"], "paper_id": "ABC_10acbeba830b2f8b3feb30de542c56_32", "text": "As such, the Touchdown panoramas available through StreetLearn can be reliably used as direct replacement for those used in Chen et al. (2019) ."}
{"sent_id": "10acbeba830b2f8b3feb30de542c56-C001-92", "intents": ["@UNSURE@"], "paper_id": "ABC_10acbeba830b2f8b3feb30de542c56_32", "text": "Our Retouchdown reimplementation of the RCONCAT model improves over the results given in Chen et al. (2019) for all metrics."}
{"sent_id": "55e429045af4434f9cb27ae8c6db66-C001-46", "intents": ["@USE@"], "paper_id": "ABC_55e429045af4434f9cb27ae8c6db66_32", "text": "Feature templates follow (Phandi et al., 2015) , extracted by EASE 1 , which are briefly listed in Table 1 . \"Useful n-grams\" are determined using the Fisher test to separate the good scoring essays and bad scoring essays."}
{"sent_id": "55e429045af4434f9cb27ae8c6db66-C001-80", "intents": ["@USE@"], "paper_id": "ABC_55e429045af4434f9cb27ae8c6db66_32", "text": "The settings of data preparation follow (Phandi et al., 2015) ."}
{"sent_id": "55e429045af4434f9cb27ae8c6db66-C001-82", "intents": ["@USE@"], "paper_id": "ABC_55e429045af4434f9cb27ae8c6db66_32", "text": "For domainadaptation (cross-domain) experiments, we follow (Phandi et al., 2015) , picking four pairs of essay prompts, namely, 1→2, 3→4, 5→6 and 7→8, where 1→2 denotes prompt 1 as source domain and prompt Hyper-parameters We use Adagrad for optimization."}
{"sent_id": "06276db79ed5aa04bb24a31c10d3a9-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_06276db79ed5aa04bb24a31c10d3a9_32", "text": "Out of the four systems that made these scores available (all scores reported in van Noord and Bos (2017) ), the reentrancy metric obtained the lowest F-score for three of them."}
{"sent_id": "06276db79ed5aa04bb24a31c10d3a9-C001-20", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_06276db79ed5aa04bb24a31c10d3a9_32", "text": "Foland and Martin (2017) and van Noord and Bos (2017) use the same input transformation as Konstas et al. (2017) , but do try to restore co-referring nodes by merging all equal concepts into a single concept in a post-processing step."}
{"sent_id": "06276db79ed5aa04bb24a31c10d3a9-C001-108", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_06276db79ed5aa04bb24a31c10d3a9_32", "text": "The second approach also employs the postprocessing methods Wikification and pruning, as explained in van Noord and Bos (2017)."}
{"sent_id": "06276db79ed5aa04bb24a31c10d3a9-C001-97", "intents": ["@USE@"], "paper_id": "ABC_06276db79ed5aa04bb24a31c10d3a9_32", "text": "The parameter settings are the same as in van Noord and Bos (2017) and are shown in Table 2 ."}
{"sent_id": "672d4299e60752e866293d72f97905-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_672d4299e60752e866293d72f97905_32", "text": "To the best of our knowledge there are only two studies that propose regression methods to automatically estimate missing psycholinguistic properties in the MRC Database [4, 12] ."}
{"sent_id": "672d4299e60752e866293d72f97905-C001-19", "intents": ["@SIM@"], "paper_id": "ABC_672d4299e60752e866293d72f97905_32", "text": "As for the automatic inference, this work is strongly based on the results of [12] which proposed an automatic bootstrapping method for regression to populate the MRC Database."}
{"sent_id": "672d4299e60752e866293d72f97905-C001-78", "intents": ["@USE@"], "paper_id": "ABC_672d4299e60752e866293d72f97905_32", "text": "We choose this regression method due to the promising results reported by [12] ."}
{"sent_id": "6da7dcbcb7f52f31ec23c8131d438d-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_6da7dcbcb7f52f31ec23c8131d438d_32", "text": "Recently, pretrained language representations with self-supervised objectives (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2018) have further pushed forward the state-of-the-art on many English tasks."}
{"sent_id": "6da7dcbcb7f52f31ec23c8131d438d-C001-107", "intents": ["@BACK@"], "paper_id": "ABC_6da7dcbcb7f52f31ec23c8131d438d_32", "text": "With models such as ELMo (Peters et al., 2018) , GPT-2 (Radford et al., 2018) , and BERT (Devlin et al., 2018) , important progress has been made in learning improved sentence representations with context-specific encodings of words via a language modeling objective."}
{"sent_id": "43d670e583caab9b38ddce999b8872-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_43d670e583caab9b38ddce999b8872_32", "text": "This task has gained some popularity recently for evaluating dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015) ."}
{"sent_id": "43d670e583caab9b38ddce999b8872-C001-70", "intents": ["@USE@"], "paper_id": "ABC_43d670e583caab9b38ddce999b8872_32", "text": "The number of utterances in the context were sampled according to the procedure in (Lowe et al., 2015a) , with a maximum context length of 6 turns -this was done for both the human trials and ANN model."}
{"sent_id": "3ea1f4acd7e2812e68eca54600fc5c-C001-22", "intents": ["@EXT@"], "paper_id": "ABC_3ea1f4acd7e2812e68eca54600fc5c_32", "text": "In our approach, we adopt Eisner (1996) 's bottomup chart-parsing algorithm in McDonald et al. (2005) 's formulation, which finds the best projective dependency tree for an input string"}
{"sent_id": "3ea1f4acd7e2812e68eca54600fc5c-C001-29", "intents": ["@USE@"], "paper_id": "ABC_3ea1f4acd7e2812e68eca54600fc5c_32", "text": "We essentially employ the same set of features as McDonald et al. (2005) :"}
{"sent_id": "3ea1f4acd7e2812e68eca54600fc5c-C001-38", "intents": ["@DIF@"], "paper_id": "ABC_3ea1f4acd7e2812e68eca54600fc5c_32", "text": "Having a closed-form solution, OPAL is easier to implement and more efficient than the MIRA algorithm used by McDonald et al. (2005) , although it achieves a performance comparable to MIRA's on many problems (Crammer et al., 2006) ."}
{"sent_id": "3ea1f4acd7e2812e68eca54600fc5c-C001-41", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_3ea1f4acd7e2812e68eca54600fc5c_32", "text": "So far, the presented system, which follows closely the approach of McDonald et al. (2005) , only predicts unlabelled dependency trees."}
{"sent_id": "4924d721b50abe1c8d883a7efd0205-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_4924d721b50abe1c8d883a7efd0205_32", "text": "UKB is a collection of programs which was first released for performing graph-based Word Sense Disambiguation using a preexisting knowledge base such as WordNet, and attained state-of-the-art results among knowledge-based systems when evaluated on standard benchmarks Agirre et al., 2014) ."}
{"sent_id": "4924d721b50abe1c8d883a7efd0205-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_4924d721b50abe1c8d883a7efd0205_32", "text": "The use of sense frequencies with UKB was introduced in (Agirre et al., 2014) ."}
{"sent_id": "4924d721b50abe1c8d883a7efd0205-C001-40", "intents": ["@USE@"], "paper_id": "ABC_4924d721b50abe1c8d883a7efd0205_32", "text": "For each of those we mention the best options and the associated UKB parameter when relevant (in italics), as taken from Agirre et al., 2014 ):"}
{"sent_id": "4924d721b50abe1c8d883a7efd0205-C001-91", "intents": ["@EXT@"], "paper_id": "ABC_4924d721b50abe1c8d883a7efd0205_32", "text": "In addition to the results of UKB using the setting in Agirre et al., 2014) as specified in Section 3, we checked whether some reasonable settings would obtain better results."}
{"sent_id": "91685660d3d689c50e7436be46f37e-C001-16", "intents": ["@USE@"], "paper_id": "ABC_91685660d3d689c50e7436be46f37e_32", "text": "We create a new GEC corpus for German along with the models needed for the neural GEC approach presented in Chollampatt and Ng (2018) ."}
{"sent_id": "91685660d3d689c50e7436be46f37e-C001-76", "intents": ["@USE@"], "paper_id": "ABC_91685660d3d689c50e7436be46f37e_32", "text": "We evaluate the effect of extending the Falko-MERLIN GEC Corpus with Wikipedia edits for a German GEC system using the multilayer convolutional encoder-decoder neural network approach from Chollampatt and Ng (2018) , using the same parameters as for English."}
{"sent_id": "0fd87fbdbe64e7d002ca31783448fb-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_0fd87fbdbe64e7d002ca31783448fb_32", "text": "Many researches have been conducted to involve AI into poem generation [Zhang and Lapata, 2014; Cheng et al., 2018] , creation of classical or pop music [Manzelli et al., 2018; Hadjeres et al., 2017] and automatic images generation [van den Oord et al., 2016; Yan et al., 2016; Xu et al., 2018] ."}
{"sent_id": "0fd87fbdbe64e7d002ca31783448fb-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_0fd87fbdbe64e7d002ca31783448fb_32", "text": "As imagination is the soul for artistic Mind Map, Mappa Mundi employs several features to increase information variety [Liu et al., 2019] during topic expansion."}
{"sent_id": "81dd7a27479f0cec3a01337c57ca95-C001-60", "intents": ["@USE@"], "paper_id": "ABC_81dd7a27479f0cec3a01337c57ca95_32", "text": "The Gigaword dataset (Napoles et al., 2012) with 1.02 million examples, where the first 200 are labeled for extractive Sentence Compression by two annotators (Zhao et al., 2018) ."}
{"sent_id": "81dd7a27479f0cec3a01337c57ca95-C001-63", "intents": ["@USE@"], "paper_id": "ABC_81dd7a27479f0cec3a01337c57ca95_32", "text": "Thus to faithfully evaluate the quality of the compressions generated by our model, we follow Zhao et al. (2018) and conducted human studies on the Gigaword dataset with Amazon Mechanical Turk (MTurk)."}
{"sent_id": "81dd7a27479f0cec3a01337c57ca95-C001-91", "intents": ["@SIM@"], "paper_id": "ABC_81dd7a27479f0cec3a01337c57ca95_32", "text": "Similar to our AvgPPL, Zhao et al. (2018) also employed average Perplexity (though without our length correction terms) as the reward to a policy network trained with reinforcement learning."}
{"sent_id": "1f463f2f87bc2d572299d96481084f-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_1f463f2f87bc2d572299d96481084f_32", "text": "Empirical tests detailed in Koehn (2004) show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large."}
{"sent_id": "1f463f2f87bc2d572299d96481084f-C001-33", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_1f463f2f87bc2d572299d96481084f_32", "text": "We compare this method with approximate randomization and also paired bootstrap resampling (Koehn, 2004) , which is widely used in MT evaluation."}
{"sent_id": "1f463f2f87bc2d572299d96481084f-C001-65", "intents": ["@UNSURE@"], "paper_id": "ABC_1f463f2f87bc2d572299d96481084f_32", "text": "Paired bootstrap resampling (Koehn, 2004 ) is shown in Figure 4 ."}
{"sent_id": "1f463f2f87bc2d572299d96481084f-C001-85", "intents": ["@UNSURE@"], "paper_id": "ABC_1f463f2f87bc2d572299d96481084f_32", "text": "We evaluate paired bootstrap resampling (Koehn, 2004) and bootstrap resampling as shown in Figure 3 and approximate randomization as shown in Figure 2 , each in combination with four automatic MT metrics: BLEU (Papineni et al., 2002) , NIST (NIST, 2002) , METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) ."}
{"sent_id": "b31acd3535cd740e609d45986fbf33-C001-3", "intents": ["@USE@"], "paper_id": "ABC_b31acd3535cd740e609d45986fbf33_32", "text": "We derive character-level contextual embeddings from Flair (Akbik et al., 2018) , and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes."}
{"sent_id": "b31acd3535cd740e609d45986fbf33-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_b31acd3535cd740e609d45986fbf33_32", "text": "Flair achieves state-of-the-art or competitive results on part-of-speech tagging and named entity tagging (Akbik et al., 2018) ."}
{"sent_id": "b31acd3535cd740e609d45986fbf33-C001-18", "intents": ["@MOT@"], "paper_id": "ABC_b31acd3535cd740e609d45986fbf33_32", "text": "However, both Akbik et al. (2018) and Bohnet et al. (2018) discard all other contextual character embeddings, and no analyses of the models are performed at the character-level."}
{"sent_id": "b31acd3535cd740e609d45986fbf33-C001-80", "intents": ["@UNSURE@"], "paper_id": "ABC_b31acd3535cd740e609d45986fbf33_32", "text": "We perform a feature ablation to see if pre-trained contextual character embeddings capture basic syntax (e.g., part-of-speech) like pre-trained contextual word embeddings do (Peters et al., 2018; Akbik et al., 2018) ."}
{"sent_id": "46cc0df5c6ed25f735cc0afd301ec8-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_46cc0df5c6ed25f735cc0afd301ec8_32", "text": "Recent research has started to investigate models with self-explaining capability, i.e. extracting evidence to support their final predictions (Li et al., 2015; Lei et al., 2016; Lin et al., 2017; Mullenbach et al., 2018) ."}
{"sent_id": "46cc0df5c6ed25f735cc0afd301ec8-C001-27", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_46cc0df5c6ed25f735cc0afd301ec8_32", "text": "Another issue of Lin et al. (2017) is that their attention model is applied on the representation vectors produced by an LSTM."}
{"sent_id": "46cc0df5c6ed25f735cc0afd301ec8-C001-44", "intents": ["@USE@"], "paper_id": "ABC_46cc0df5c6ed25f735cc0afd301ec8_32", "text": "We compare two types of baseline text encoders in Figure 1 . (1) Lin et al. (2017) (BiLSTM), which formulates single word positions as basic units, and computes the vector h i for the i-th word position with a BiLSTM; (2) Extension of Mullenbach et al. (2018) (CNN) ."}
{"sent_id": "8a8670fd7cfb8db9ddd3f546ce4534-C001-19", "intents": ["@SIM@"], "paper_id": "ABC_8a8670fd7cfb8db9ddd3f546ce4534_32", "text": "Furthermore, we adopt the goal of Erk et al. (2009) , which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity."}
{"sent_id": "8a8670fd7cfb8db9ddd3f546ce4534-C001-50", "intents": ["@USE@"], "paper_id": "ABC_8a8670fd7cfb8db9ddd3f546ce4534_32", "text": "MaxDiff MaxDiff is an alternative to scale-based ratings in which Turkers are presented with a only subset of all of a word's senses and then asked to select (1) the sense option that best matches the mean-add.v ask.v win.v argument.n interest.n paper.n different.a important.a Erk et al. (2009) ing in the example context and (2) the sense option that least matches (Louviere, 1991) ."}
{"sent_id": "8a8670fd7cfb8db9ddd3f546ce4534-C001-93", "intents": ["@DIF@"], "paper_id": "ABC_8a8670fd7cfb8db9ddd3f546ce4534_32", "text": "Though Erk et al. (2009) reported a pair-wise IAA of the GWS annotators between 0.466 and 0.506 using Spearman's ρ, the IAA varies considerably between words for both Turkers and GWS annotators when measured using Krippendorff's α."}
{"sent_id": "d0c12613f09b36e071b9a842a4d844-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_d0c12613f09b36e071b9a842a4d844_32", "text": "For instance, the method presented in [Lavie et al. 2008 ] assigns a prime number to each pair of aligned leaf nodes in source and target trees based on the lexical alignment."}
{"sent_id": "d0c12613f09b36e071b9a842a4d844-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_d0c12613f09b36e071b9a842a4d844_32", "text": "In this case, the product of the probabilities of lexical alignment (not prime numbers as [Lavie et al. 2008] ) is assigned to parent nodes."}
{"sent_id": "d0c12613f09b36e071b9a842a4d844-C001-13", "intents": ["@EXT@", "@MOT@"], "paper_id": "ABC_d0c12613f09b36e071b9a842a4d844_32", "text": "This paper, therefore, proposes the combination of two syntactic tree alignment methods - [Lavie et al. 2008 ] (a bottom-up approach) and [Tinsley et al. 2007 ] (a topdown approach) -aiming at improving their performance evaluated on Brazilian Portuguese (pt) and English (en) pair of languages."}
{"sent_id": "d0c12613f09b36e071b9a842a4d844-C001-29", "intents": ["@USE@"], "paper_id": "ABC_d0c12613f09b36e071b9a842a4d844_32", "text": "For the experiments presented in this paper, the baseline models were implemented based on [Lavie et al. 2008] and [Tinsley et al. 2007 ] mainly because they do not require rich resources such as [Marecek et al. 2008] neither use manually created composition rules as [Menezes and Richardson 2001] and [Groves et al. 2004 ]."}
{"sent_id": "d0c12613f09b36e071b9a842a4d844-C001-33", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_d0c12613f09b36e071b9a842a4d844_32", "text": "Model 1 -Based on [Lavie et al. 2008] Following an idea similar to that described in [Lavie et al. 2008] , our implementation (model 1) assigns prime numbers to each pair of aligned terminal nodes 1 ."}
{"sent_id": "5c63296c36cbd95e07f05f2563a2a1-C001-70", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5c63296c36cbd95e07f05f2563a2a1_32", "text": "In this study, we specifically examine BLSTMCNNs-CRF (Ma and Hovy, 2016) because it achieves state-of-the-art performance in the CoNLL 2003 corpus."}
{"sent_id": "5c63296c36cbd95e07f05f2563a2a1-C001-125", "intents": ["@UNSURE@"], "paper_id": "ABC_5c63296c36cbd95e07f05f2563a2a1_32", "text": "Here, the CNN layer and the CRF layer improve by 2.36 pt and 1.75 pt in English (Ma and Hovy, 2016) ."}
{"sent_id": "5c63296c36cbd95e07f05f2563a2a1-C001-109", "intents": ["@USE@"], "paper_id": "ABC_5c63296c36cbd95e07f05f2563a2a1_32", "text": "Other conditions are the same as those reported for an earlier study (Ma and Hovy, 2016) ."}
{"sent_id": "0fed8b9e785426880fa8e5641116a4-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_0fed8b9e785426880fa8e5641116a4_33", "text": "AMBER is a machine translation evaluation metric first described in (Chen and Kuhn, 2011) ."}
{"sent_id": "0fed8b9e785426880fa8e5641116a4-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_0fed8b9e785426880fa8e5641116a4_33", "text": "8 types were tried in (Chen and Kuhn, 2011) ."}
{"sent_id": "0fed8b9e785426880fa8e5641116a4-C001-65", "intents": ["@DIF@"], "paper_id": "ABC_0fed8b9e785426880fa8e5641116a4_33", "text": "In (Chen and Kuhn, 2011) , we manually set the 17 free parameters of AMBER (see section 3.2 of that paper)."}
{"sent_id": "c54a1aba5845a52f468cde916c970b-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_c54a1aba5845a52f468cde916c970b_33", "text": "Other work stores dialogue context in a memory module and repeatedly queries and reasons about this context to select an adequate system response (Bordes and Weston, 2016) ."}
{"sent_id": "c54a1aba5845a52f468cde916c970b-C001-20", "intents": ["@DIF@"], "paper_id": "ABC_c54a1aba5845a52f468cde916c970b_33", "text": "The result is a simple, intuitive, and highly competitive model, which outperforms the more complex model of Bordes and Weston (2016) by 6.9%."}
{"sent_id": "c54a1aba5845a52f468cde916c970b-C001-74", "intents": ["@DIF@"], "paper_id": "ABC_c54a1aba5845a52f468cde916c970b_33", "text": "Evaluating using this metric on our model is therefore significantly more stringent a test than for the model of Bordes and Weston (2016) ."}
{"sent_id": "c54a1aba5845a52f468cde916c970b-C001-75", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_c54a1aba5845a52f468cde916c970b_33", "text": "• Per-Dialogue Accuracy: Bordes and Weston (2016) also report a per-dialogue accuracy, which assesses their model's ability to produce every system response of the dialogue correctly."}
{"sent_id": "c54a1aba5845a52f468cde916c970b-C001-87", "intents": ["@MOT@", "@UNSURE@"], "paper_id": "ABC_c54a1aba5845a52f468cde916c970b_33", "text": "In Table 2 , we present the results of our models compared to the reported performance of the best performing model of (Bordes and Weston, 2016) , which is a variant of an end-to-end memory network (Sukhbaatar et al., 2015) ."}
{"sent_id": "45c4e9bc90d28cd1a5a61393625062-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_45c4e9bc90d28cd1a5a61393625062_33", "text": "More recently, and departing from such traditional approaches, we have proposed in (Li and Gaussier, 2010) an approach based on improving the comparability of the corpus under consideration, prior to extracting bilingual lexicons."}
{"sent_id": "45c4e9bc90d28cd1a5a61393625062-C001-24", "intents": ["@USE@"], "paper_id": "ABC_45c4e9bc90d28cd1a5a61393625062_33", "text": "In order to measure the degree of comparability of bilingual corpora, we make use of the measure M developed in (Li and Gaussier, 2010) : Given a comparable corpus P consisting of an English part P e and a French part P f , the degree of comparability of P is defined as the expectation of finding the translation of any given source/target word in the target/source corpus vocabulary."}
{"sent_id": "45c4e9bc90d28cd1a5a61393625062-C001-68", "intents": ["@USE@"], "paper_id": "ABC_45c4e9bc90d28cd1a5a61393625062_33", "text": "In our experiments, we use the method described in this paper, as well as the one in (Li and Gaussier, 2010) which is the only alternative method to enhance corpus comparability."}
{"sent_id": "45c4e9bc90d28cd1a5a61393625062-C001-124", "intents": ["@USE@"], "paper_id": "ABC_45c4e9bc90d28cd1a5a61393625062_33", "text": "We have followed here the general approach in (Li and Gaussier, 2010) which consists in enhancing the quality of a comparable corpus prior to extracting information from it."}
{"sent_id": "d61f75366022f043d4c3a005b5a73d-C001-42", "intents": ["@SIM@", "@EXT@"], "paper_id": "ABC_d61f75366022f043d4c3a005b5a73d_33", "text": "Our model form is a generalization of the fastText model (Bojanowski et al., 2016) , which in turn extends the skip-gram model of Mikolov et al (2013) ."}
{"sent_id": "d61f75366022f043d4c3a005b5a73d-C001-48", "intents": ["@EXT@"], "paper_id": "ABC_d61f75366022f043d4c3a005b5a73d_33", "text": "We generalize Bojanowski et al (2016) by replacing the set of ngrams G(w) with a set P(w) of explicit linguistic properties."}
{"sent_id": "d61f75366022f043d4c3a005b5a73d-C001-56", "intents": ["@EXT@"], "paper_id": "ABC_d61f75366022f043d4c3a005b5a73d_33", "text": "Our implementation is based on the fastText 2 library (Bojanowski et al., 2016) , which we modify as described above."}
{"sent_id": "b208c7180bc3f973b8616937b2801c-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_b208c7180bc3f973b8616937b2801c_33", "text": "The main paragraph captioning dataset is the Visual Genome corpus, introduced by Krause et al. (2016) ."}
{"sent_id": "b208c7180bc3f973b8616937b2801c-C001-83", "intents": ["@DIF@"], "paper_id": "ABC_b208c7180bc3f973b8616937b2801c_33", "text": "Our encoder is a convolutional network pretrained for object detection (as opposed to dense captioning, as in Krause et al. (2016) and Liang et al. (2017) )."}
{"sent_id": "201aa2a740b5d45f273ee298595f5a-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_201aa2a740b5d45f273ee298595f5a_33", "text": "Recently, multiple studies have focussed on providing a fine-grained analysis of the nature of concrete vs. abstract words from a corpus-based perspective (Bhaskar et al., 2017; Frassinelli et al., 2017; Naumann et al., 2018) ."}
{"sent_id": "201aa2a740b5d45f273ee298595f5a-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_201aa2a740b5d45f273ee298595f5a_33", "text": "Specifically, Naumann et al. (2018) performed their analyses across parts-of-speech by comparing the behaviour of nouns, verbs and adjectives in large-scale corpora."}
{"sent_id": "201aa2a740b5d45f273ee298595f5a-C001-21", "intents": ["@UNSURE@"], "paper_id": "ABC_201aa2a740b5d45f273ee298595f5a_33", "text": "First of all, we expect to replicate the main results from Naumann et al. (2018) : in general, concrete nouns should co-occur more frequently with concrete verbs and abstract nouns with abstract verbs."}
{"sent_id": "201aa2a740b5d45f273ee298595f5a-C001-53", "intents": ["@SIM@"], "paper_id": "ABC_201aa2a740b5d45f273ee298595f5a_33", "text": "This result is perfectly in line with the more general analysis by Naumann et al. (2018) ."}
{"sent_id": "201aa2a740b5d45f273ee298595f5a-C001-91", "intents": ["@SIM@"], "paper_id": "ABC_201aa2a740b5d45f273ee298595f5a_33", "text": "The general pattern already described in Naumann et al. (2018) is confirmed by our quantitative analysis: overall, concrete verbs predominantly subcategorise concrete nouns as subjects and direct objects, while abstract verbs predominantly subcategorise abstract nouns as subjects and direct objects."}
{"sent_id": "4a19f17c00e904a595c1703ab9318d-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_4a19f17c00e904a595c1703ab9318d_33", "text": "Thanks to recent advances in deep learning over the last two decades, machine translation (MT) has shown steady improvements (Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) ."}
{"sent_id": "4a19f17c00e904a595c1703ab9318d-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_4a19f17c00e904a595c1703ab9318d_33", "text": "As described in Vaswani et al. (2017) , each stacked layer is composed of multi-head attention networks and position-wise fully connected feed-forward networks."}
{"sent_id": "4a19f17c00e904a595c1703ab9318d-C001-48", "intents": ["@USE@"], "paper_id": "ABC_4a19f17c00e904a595c1703ab9318d_33", "text": "In addition to shared embedding described in Vaswani et al. (2017) , we also utilize weight sharing across the embedding and output layers in a manner similar to Junczys-Dowmunt and Grundkiewicz (2018)."}
{"sent_id": "4a19f17c00e904a595c1703ab9318d-C001-52", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_4a19f17c00e904a595c1703ab9318d_33", "text": "As described in Vaswani et al. (2017) , we utilize the same multihead attention with h-heads based on scaled dotproduct attention to get matrix C composed of context vectors as follows:"}
{"sent_id": "cbed76ac8086637fe1d2e30f39c585-C001-18", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_cbed76ac8086637fe1d2e30f39c585_33", "text": "We show in this paper that performance continues to improve further after adding neural network joint models (NNJMs), as introduced in (Chollampatt et al., 2016b) ."}
{"sent_id": "cbed76ac8086637fe1d2e30f39c585-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_cbed76ac8086637fe1d2e30f39c585_33", "text": "It later became the most widely used approach and was used in state-of-the-art GEC systems Chollampatt et al., 2016b; JunczysDowmunt and Grundkiewicz, 2016; Rozovskaya and Roth, 2016) ."}
{"sent_id": "cbed76ac8086637fe1d2e30f39c585-C001-52", "intents": ["@USE@"], "paper_id": "ABC_cbed76ac8086637fe1d2e30f39c585_33", "text": "Following Chollampatt et al. (2016b) , we add a neural network joint model (NNJM) feature to further improve the SMT component."}
{"sent_id": "cbe9e36f371c072432ca25800c96d3-C001-92", "intents": ["@BACK@"], "paper_id": "ABC_cbe9e36f371c072432ca25800c96d3_33", "text": "Other models capable of disentangling phonetic and domain information have recently been shown to learn acoustic features with a greater degree of domain invariance than traditional acoustic features [16, 7, 17] ."}
{"sent_id": "cbe9e36f371c072432ca25800c96d3-C001-116", "intents": ["@BACK@"], "paper_id": "ABC_cbe9e36f371c072432ca25800c96d3_33", "text": "While previous work investigated usage of FHVAE for ASR by training FHVAE models on all domains of the target task (e.g., Aurora-4 with all four conditions) [17, 8] , we also evaluate FHVAE models trained on PlacesAudCap to test cross-dataset transferability, and on the subset of domains used for ASR training."}
{"sent_id": "cbe9e36f371c072432ca25800c96d3-C001-79", "intents": ["@SIM@"], "paper_id": "ABC_cbe9e36f371c072432ca25800c96d3_33", "text": "The first two are evaluated using a protocol similar to [17] , where an ASR model is trained on a set of domains, and evaluated on both in-domain and out-of-domain speech (relative to the training data)."}
{"sent_id": "cbe9e36f371c072432ca25800c96d3-C001-105", "intents": ["@SIM@"], "paper_id": "ABC_cbe9e36f371c072432ca25800c96d3_33", "text": "Similar to [17] , we use the clean set (A) for training ASR systems, and test on the four groups separately."}
{"sent_id": "0a538968f0cd121a1ef63b58a0c9f7-C001-147", "intents": ["@UNSURE@"], "paper_id": "ABC_0a538968f0cd121a1ef63b58a0c9f7_33", "text": "We also compare our model with Kalchbrenner and Blunsom (2013) who used the contextbased learning approach achieving 73.9%."}
{"sent_id": "52b9efca757fe5a376ca6b548d77ce-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_52b9efca757fe5a376ca6b548d77ce_33", "text": "There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009) ."}
{"sent_id": "52b9efca757fe5a376ca6b548d77ce-C001-60", "intents": ["@USE@"], "paper_id": "ABC_52b9efca757fe5a376ca6b548d77ce_33", "text": "We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space."}
{"sent_id": "52b9efca757fe5a376ca6b548d77ce-C001-74", "intents": ["@DIF@"], "paper_id": "ABC_52b9efca757fe5a376ca6b548d77ce_33", "text": "The result of F3 is 6.4% higher than the result (F=63.28) reported in Sun and Korhonen (2009) using the F1 feature."}
{"sent_id": "cb57b8886be9ea4f0c50fd2c3a178a-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_cb57b8886be9ea4f0c50fd2c3a178a_33", "text": "Recently, there has been much work designing ranking architectures to effectively score query-document pairs, with encouraging results [5, 6, 20] ."}
{"sent_id": "cb57b8886be9ea4f0c50fd2c3a178a-C001-30", "intents": ["@UNSURE@"], "paper_id": "ABC_cb57b8886be9ea4f0c50fd2c3a178a_33", "text": "In summary, our contributions are as follows: -We are the first to demonstrate that contextualized word representations can be successfully incorporated into existing neural architectures (PACRR [6] , KNRM [20] , and DRMM [5] ), allowing them to leverage contextual information to improve ad-hoc document ranking."}
{"sent_id": "cb57b8886be9ea4f0c50fd2c3a178a-C001-76", "intents": ["@USE@"], "paper_id": "ABC_cb57b8886be9ea4f0c50fd2c3a178a_33", "text": "We evaluate our methods on three neural relevance matching methods: PACRR [6] , KNRM [20] , and DRMM [5] ."}
{"sent_id": "8b2bb6753dc72a048ec28958e943fb-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_8b2bb6753dc72a048ec28958e943fb_33", "text": "Yüret and Türe (2006) proposed a decision list learning algorithm for extraction of Finally, hybrid models which combine statistical and rule based approaches are also proposed (Oflazer and Tur 1996; Kutlu and Cicekli 2013) ."}
{"sent_id": "8b2bb6753dc72a048ec28958e943fb-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_8b2bb6753dc72a048ec28958e943fb_33", "text": "Yüret and Türe (2006) observe that more than ten thousand tag types exists in a corpus comprised of a million Turkish words."}
{"sent_id": "8b2bb6753dc72a048ec28958e943fb-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_8b2bb6753dc72a048ec28958e943fb_33", "text": "Yüret and Türe (2006) extract Turkish morphological disambiguation rules using a decision list learner, Greedy Prepend Algorithm (GPA), and they achieve 95.8% accuracy on manually disambiguated data consisting of around 1K words."}
{"sent_id": "8b2bb6753dc72a048ec28958e943fb-C001-172", "intents": ["@BACK@"], "paper_id": "ABC_8b2bb6753dc72a048ec28958e943fb_33", "text": "Yüret and Türe (2006) report that the accuracy of the training data is below 95%."}
{"sent_id": "8b2bb6753dc72a048ec28958e943fb-C001-143", "intents": ["@USE@"], "paper_id": "ABC_8b2bb6753dc72a048ec28958e943fb_33", "text": "For Turkish, we used a semi-automatically disambiguated corpus containing 1M tokens (Yüret and Türe 2006) ."}
{"sent_id": "41d56f3f962fa3b0e0563544a6de40-C001-2", "intents": ["@BACK@"], "paper_id": "ABC_41d56f3f962fa3b0e0563544a6de40_33", "text": "Left-to-right (LR) decoding (Watanabe et al., 2006 ) is promising decoding algorithm for hierarchical phrase-based translation (Hiero) that visits input spans in arbitrary order producing the output translation in left to right order."}
{"sent_id": "41d56f3f962fa3b0e0563544a6de40-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_41d56f3f962fa3b0e0563544a6de40_33", "text": "Watanabe et al. (2006) first proposed left-toright (LR) decoding for Hiero (LR-Hiero henceforth) which uses beam search and runs in O(n 2 b) in practice where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010) ."}
{"sent_id": "41d56f3f962fa3b0e0563544a6de40-C001-56", "intents": ["@USE@"], "paper_id": "ABC_41d56f3f962fa3b0e0563544a6de40_33", "text": "Left side shows the rules used in the derivation (G indicates glue rules as defined in (Watanabe et al., 2006) )."}
{"sent_id": "fe2f22d3d25358b23d0b75a6edee57-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_fe2f22d3d25358b23d0b75a6edee57_33", "text": "Zhou et al. (2017) utilized the answer-position, and linguistic features such as named entity recognition (NER) and parts of speech (POS) information to further improve the QG performance as the model is aware that for which answer a question need to be generated."}
{"sent_id": "fe2f22d3d25358b23d0b75a6edee57-C001-38", "intents": ["@UNSURE@"], "paper_id": "ABC_fe2f22d3d25358b23d0b75a6edee57_33", "text": "Inspired by the success of using linguistic features in (Zhou et al., 2017; Harrison and Walker, 2018) , we exploit word knowledge in the form of entity linking and fine-grained entity typing in the encoder of the network."}
{"sent_id": "fe2f22d3d25358b23d0b75a6edee57-C001-98", "intents": ["@UNSURE@"], "paper_id": "ABC_fe2f22d3d25358b23d0b75a6edee57_33", "text": "In order to compare our models with the existing coarse-grained entity features (NER) being used in literature (Zhou et al., 2017; Harrison and Walker, 2018) , we also report the following experiments."}
{"sent_id": "fe2f22d3d25358b23d0b75a6edee57-C001-43", "intents": ["@MOT@"], "paper_id": "ABC_fe2f22d3d25358b23d0b75a6edee57_33", "text": "In previous works (Zhou et al., 2017; Harrison and Walker, 2018) , named entity type features have been used."}
{"sent_id": "fe2f22d3d25358b23d0b75a6edee57-C001-76", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_fe2f22d3d25358b23d0b75a6edee57_33", "text": "We used the same split as (Zhou et al., 2017) ."}
{"sent_id": "8622616ffd4db96058a9b8aff54212-C001-43", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_8622616ffd4db96058a9b8aff54212_33", "text": "For these experiments, the same machine learning system-RDS-is used as for the experiments presented by Hulth (2003a) ."}
{"sent_id": "8622616ffd4db96058a9b8aff54212-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_8622616ffd4db96058a9b8aff54212_33", "text": "In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented."}
{"sent_id": "8622616ffd4db96058a9b8aff54212-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_8622616ffd4db96058a9b8aff54212_33", "text": "In the experiments presented in Hulth (2003a) , only the documents present in the training, validation, and test set respectively are used for calculating the collection frequency."}
{"sent_id": "8622616ffd4db96058a9b8aff54212-C001-84", "intents": ["@BACK@"], "paper_id": "ABC_8622616ffd4db96058a9b8aff54212_33", "text": "In the experiments presented in Hulth (2003a) , the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword."}
{"sent_id": "a8743bb89abd16f75bec9e72e446b3-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_a8743bb89abd16f75bec9e72e446b3_33", "text": "Bartlett et al. (2008) observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification."}
{"sent_id": "a8743bb89abd16f75bec9e72e446b3-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_a8743bb89abd16f75bec9e72e446b3_33", "text": "Bartlett et al. (2008) present a discriminative approach to automatic syllabification."}
{"sent_id": "a8743bb89abd16f75bec9e72e446b3-C001-22", "intents": ["@EXT@"], "paper_id": "ABC_a8743bb89abd16f75bec9e72e446b3_33", "text": "We augment the syllabification approach of Bartlett et al. (2008) , with features encoding morphological segmentation of words."}
{"sent_id": "a8743bb89abd16f75bec9e72e446b3-C001-93", "intents": ["@EXT@", "@USE@"], "paper_id": "ABC_a8743bb89abd16f75bec9e72e446b3_33", "text": "As a baseline, we replicate the experiments of Bartlett et al. (2008) , and extend them to lowresource settings."}
{"sent_id": "a8743bb89abd16f75bec9e72e446b3-C001-30", "intents": ["@USE@"], "paper_id": "ABC_a8743bb89abd16f75bec9e72e446b3_33", "text": "In this section, we describe the original syllabification method of Bartlett et al. (2008) , which serves as our baseline system, and discuss various approaches to incorporating morphological information."}
{"sent_id": "a8743bb89abd16f75bec9e72e446b3-C001-73", "intents": ["@UNSURE@"], "paper_id": "ABC_a8743bb89abd16f75bec9e72e446b3_33", "text": "We investigate the quality of the morphological segmentations of produced by various methods, and replicate the syllabification results of Bartlett et al. (2008) ."}
{"sent_id": "4be5a47b5fd900c3578330b352b24c-C001-12", "intents": ["@EXT@"], "paper_id": "ABC_4be5a47b5fd900c3578330b352b24c_33", "text": "To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of Agarwal et al (2009) , but adapt it for the language of social media, in particular Twitter."}
{"sent_id": "4be5a47b5fd900c3578330b352b24c-C001-14", "intents": ["@EXT@"], "paper_id": "ABC_4be5a47b5fd900c3578330b352b24c_33", "text": "In this rest of this paper, we discuss related work, including the state of the art sentiment system (Agarwal et al., 2009) our method is based on, the lexicons we used, our method, and experiments and results."}
{"sent_id": "4be5a47b5fd900c3578330b352b24c-C001-26", "intents": ["@EXT@", "@SIM@", "@USE@"], "paper_id": "ABC_4be5a47b5fd900c3578330b352b24c_33", "text": "We use the DAL and expand it with WordNet, as it was used in the original work (Agarwal et al., 2009) , and expand it further to use Wiktionary and an emoticon lexicon."}
{"sent_id": "4be5a47b5fd900c3578330b352b24c-C001-36", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4be5a47b5fd900c3578330b352b24c_33", "text": "We compute the polarity of a chunk in the same manner as the original work (Agarwal et al., 2009) , using the sum of the AE Space Score's (| √ ee 2 + aa 2 |) of each word within the chunk."}
{"sent_id": "4be5a47b5fd900c3578330b352b24c-C001-71", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4be5a47b5fd900c3578330b352b24c_33", "text": "We include all the features described in the original system (Agarwal et al., 2009 )."}
{"sent_id": "4be5a47b5fd900c3578330b352b24c-C001-70", "intents": ["@USE@"], "paper_id": "ABC_4be5a47b5fd900c3578330b352b24c_33", "text": "The DAL and other dictionaries are used along with a negation state machine (Agarwal et al., 2009) to determine the polarity for each word in the sentence."}
{"sent_id": "e9f7d339ccda101000b53d89da4e49-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_e9f7d339ccda101000b53d89da4e49_33", "text": "The previous method for AMR parsing takes a Train Dev Test  3504 463 398   Table 1 : Statistics of the extracted NP data two-step approach: first identifying distinct concepts (nodes) in the AMR graph, then defining the dependency relations between those concepts (Flanigan et al., 2014) ."}
{"sent_id": "e9f7d339ccda101000b53d89da4e49-C001-34", "intents": ["@USE@"], "paper_id": "ABC_e9f7d339ccda101000b53d89da4e49_33", "text": "We obtain this alignment by using the rule-based alignment tool by Flanigan et al. (2014) ."}
{"sent_id": "e9f7d339ccda101000b53d89da4e49-C001-84", "intents": ["@DIF@"], "paper_id": "ABC_e9f7d339ccda101000b53d89da4e49_33", "text": "The method by Flanigan et al. (2014) can only generate the concepts that appear in the training data."}
{"sent_id": "3452953ac579f1c05870442456a49c-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_3452953ac579f1c05870442456a49c_33", "text": "In recent studies, various classes of features are explored to capture lexical and semantic regularities for identifying the sense of implicit relations, including linguistically informed features like polarity tags, Levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010; Zhang et al., 2015; Chen et al., 2016) ."}
{"sent_id": "3452953ac579f1c05870442456a49c-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_3452953ac579f1c05870442456a49c_33", "text": "The test set that is currently most often used for 11 way classification is section 23 (Lin et al., 2009; Ji and Eisenstein, 2015; Rutherford et al., 2017) , which contains only about 761 implicit relations."}
{"sent_id": "3452953ac579f1c05870442456a49c-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_3452953ac579f1c05870442456a49c_33", "text": "Previous work in this task has been done over two schemes of evaluation: first-level 4-ways classification (Pitler et al., 2009; Rutherford and Xue, 2014; Chen et al., 2016) , second-level 11-way classification (Lin et al., 2009; Ji and Eisenstein, 2015) ."}
{"sent_id": "3452953ac579f1c05870442456a49c-C001-65", "intents": ["@USE@"], "paper_id": "ABC_3452953ac579f1c05870442456a49c_33", "text": "We follow the preprocessing method in (Lin et al., 2009; Rutherford et al., 2017) ."}
{"sent_id": "3452953ac579f1c05870442456a49c-C001-80", "intents": ["@USE@"], "paper_id": "ABC_3452953ac579f1c05870442456a49c_33", "text": "As a label set, we use 11-way distinction as proposed in Lin et al., (2009); Ji and Eisenstein (2015) ."}
{"sent_id": "7a1a1593a9480b6ee246ff4248668e-C001-93", "intents": ["@USE@"], "paper_id": "ABC_7a1a1593a9480b6ee246ff4248668e_34", "text": "Baseline Results and Previous Work Our baseline is a strong encoder-attention-decoder model based on Luong et al. (2015) and presented by Chopra et al. (2016) ."}
{"sent_id": "6c872be6b2fbe83890e28ddc1098a3-C001-72", "intents": ["@USE@"], "paper_id": "ABC_6c872be6b2fbe83890e28ddc1098a3_34", "text": "Scores for the NLI annotations were calculated when the original dataset was collected and are reproduced here (Lalor et al., 2016) ."}
{"sent_id": "6c872be6b2fbe83890e28ddc1098a3-C001-51", "intents": ["@SIM@"], "paper_id": "ABC_6c872be6b2fbe83890e28ddc1098a3_34", "text": "For SA, we collected a new data set of labels for 134 examples randomly selected from the Stanford Sentiment Treebank (SSTB) (Socher et al., 2013) , using a similar AMT setup as Lalor et al. (2016) ."}
{"sent_id": "c67297dc1c4376dc715bf5c1c9132f-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_c67297dc1c4376dc715bf5c1c9132f_34", "text": "One could expect considerable performance boosts by simply substituting distributional word embeddings with Flair (Akbik et al., 2018) , ELMo (Peters et al., 2018) , and BERT (Devlin et al., 2019) embeddings."}
{"sent_id": "c67297dc1c4376dc715bf5c1c9132f-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_c67297dc1c4376dc715bf5c1c9132f_34", "text": "ELMo is a deep word-level bidirectional LSTM language model with character level convolution networks along with a final linear projection output layer (Peters et al., 2018) ."}
{"sent_id": "c67297dc1c4376dc715bf5c1c9132f-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_c67297dc1c4376dc715bf5c1c9132f_34", "text": "Melamud et al. (2016) and Peters et al. (2018) also adopt bi-LSTM networks with KNN classifiers."}
{"sent_id": "c67297dc1c4376dc715bf5c1c9132f-C001-81", "intents": ["@USE@"], "paper_id": "ABC_c67297dc1c4376dc715bf5c1c9132f_34", "text": "K-Nearest Neighbor (KNN) approach is adopted from both ELMo (Peters et al., 2018) and con-text2vec (Melamud et al., 2016) to establish strong baseline approaches."}
{"sent_id": "2e636754342e9bb857068922519dbc-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_2e636754342e9bb857068922519dbc_34", "text": "More specifically, these techniques enhanced NLP algorithms through the use of contextualized text embeddings at word, sentence, and paragraph levels (Mikolov et al., 2013; Le and Mikolov, 2014; Peters et al., 2017; Devlin et al., 2018; Logeswaran and Lee, 2018; Radford et al., 2018) ."}
{"sent_id": "2e636754342e9bb857068922519dbc-C001-32", "intents": ["@DIF@"], "paper_id": "ABC_2e636754342e9bb857068922519dbc_34", "text": "This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model (Devlin et al., 2018) ."}
{"sent_id": "2e636754342e9bb857068922519dbc-C001-74", "intents": ["@USE@"], "paper_id": "ABC_2e636754342e9bb857068922519dbc_34", "text": "The model has 12 attention layers and all texts are converted to lowercase by the tokenizer (Devlin et al., 2018) ."}
{"sent_id": "2e636754342e9bb857068922519dbc-C001-80", "intents": ["@UNSURE@"], "paper_id": "ABC_2e636754342e9bb857068922519dbc_34", "text": "The advantage of this approach is that few parameters need to be learned from scratch (Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018) ."}
{"sent_id": "c6ae69051a6d9111dea1a6e8405ac9-C001-15", "intents": ["@DIF@"], "paper_id": "ABC_c6ae69051a6d9111dea1a6e8405ac9_34", "text": "An important contribution of our approach is that unlike previous approaches such as forced alignment (Wuebker et al., 2010) , reordering and language models can also be re-estimated."}
{"sent_id": "c6ae69051a6d9111dea1a6e8405ac9-C001-97", "intents": ["@DIF@"], "paper_id": "ABC_c6ae69051a6d9111dea1a6e8405ac9_34", "text": "Surprisingly, this is in contrast with forced decoding as discussed in Wuebker et al. (2010) , where the best improvements are obtained for n = 100."}
{"sent_id": "c6ae69051a6d9111dea1a6e8405ac9-C001-18", "intents": ["@MOT@"], "paper_id": "ABC_c6ae69051a6d9111dea1a6e8405ac9_34", "text": "The forced alignment technique of Wuebker et al. (2010) forms the main motivation for our work."}
{"sent_id": "c6ae69051a6d9111dea1a6e8405ac9-C001-26", "intents": ["@SIM@"], "paper_id": "ABC_c6ae69051a6d9111dea1a6e8405ac9_34", "text": "Another relevant line of research relates tuning (weight optimisation), where our work lies between forced decoding (Wuebker et al., 2010) and the bold updating approach of (Liang et al., 2006) ."}
{"sent_id": "4edb60770ebeafc56446aeca9a3b2e-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_4edb60770ebeafc56446aeca9a3b2e_34", "text": "Figure 1: The two paths above consist of the same relations (locatedIn → locatedIn) and, hence, the model of Neelakantan (2015) will assign them the same score for the relation AirportServesPlace without considering the fact that Yankee Stadium is not an airport."}
{"sent_id": "4edb60770ebeafc56446aeca9a3b2e-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_4edb60770ebeafc56446aeca9a3b2e_34", "text": "Another line of work in relation extraction performs reasoning on the paths (multi-hop reasoning on paths of length ≥ 1) connecting an entity pair (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014; Neelakantan et al., 2015; Guu et al., 2015) ."}
{"sent_id": "4edb60770ebeafc56446aeca9a3b2e-C001-48", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_4edb60770ebeafc56446aeca9a3b2e_34", "text": "In the following section, we first briefly describe the model proposed by Neelakantan (2015) (RNN model henceforth) followed by our extensions to it."}
{"sent_id": "4edb60770ebeafc56446aeca9a3b2e-C001-22", "intents": ["@EXT@"], "paper_id": "ABC_4edb60770ebeafc56446aeca9a3b2e_34", "text": "In this work, we extend the method of Neelakantan (2015) by incorporating entity type information."}
{"sent_id": "4edb60770ebeafc56446aeca9a3b2e-C001-31", "intents": ["@EXT@"], "paper_id": "ABC_4edb60770ebeafc56446aeca9a3b2e_34", "text": "This paper extends the Recurrent Neural Network model of Neelakantan (2015) by jointly reasoning over the relations and entity types occurring in the paths between an entity pair."}
{"sent_id": "4edb60770ebeafc56446aeca9a3b2e-C001-96", "intents": ["@USE@"], "paper_id": "ABC_4edb60770ebeafc56446aeca9a3b2e_34", "text": "We rank the entity pairs in the test set based on their scores and calculate the Mean Average Precision (MAP) score for the ranking following previous work Neelakantan et al., 2015) ."}
{"sent_id": "4c615cb5a496c1e225a8457a4f55d4-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_4c615cb5a496c1e225a8457a4f55d4_34", "text": "For automatic evaluation of a simplification output, it is common practice to use machine translation (MT) metrics (e.g. BLEU (Papineni et al., 2002) ), simplicity metrics (e.g. SARI (Xu et al., 2016) ), and readability metrics (e.g. FKGL (Kincaid et al., 1975) )."}
{"sent_id": "4c615cb5a496c1e225a8457a4f55d4-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_4c615cb5a496c1e225a8457a4f55d4_34", "text": "Previous work (Xu et al., 2016) has shown that BLEU correlates fairly well with human judgements of grammaticality and meaning preservation."}
{"sent_id": "4c615cb5a496c1e225a8457a4f55d4-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_4c615cb5a496c1e225a8457a4f55d4_34", "text": "EASSE provides access to three publicly available datasets for automatic SS evaluation (Table 1): PWKP (Zhu et al., 2010) , TurkCorpus (Xu et al., 2016) , and HSplit (Sulem et al., 2018a) ."}
{"sent_id": "4c615cb5a496c1e225a8457a4f55d4-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_4c615cb5a496c1e225a8457a4f55d4_34", "text": "TurkCorpus Xu et al. (2016) asked crowdworkers to simplify 2,359 original sentences extracted from PWKP to collect eight simplification references for each one."}
{"sent_id": "4c615cb5a496c1e225a8457a4f55d4-C001-107", "intents": ["@BACK@"], "paper_id": "ABC_4c615cb5a496c1e225a8457a4f55d4_34", "text": "According to Xu et al. (2016) , the annotators in TurkCorpus were instructed to mainly produce paraphrases, i.e. mostly replacements with virtually no deletions."}
{"sent_id": "4c615cb5a496c1e225a8457a4f55d4-C001-34", "intents": ["@DIF@"], "paper_id": "ABC_4c615cb5a496c1e225a8457a4f55d4_34", "text": "Although Xu et al. (2016) indicate that only precision should be considered for the deletion operation, we follow the Java implementation that uses F1 score for all operations in corpus-level SARI."}
{"sent_id": "4c615cb5a496c1e225a8457a4f55d4-C001-88", "intents": ["@USE@"], "paper_id": "ABC_4c615cb5a496c1e225a8457a4f55d4_34", "text": "We also included SBSMT-SARI (Xu et al., 2016) , which relies on syntaxbased statistical MT; DRESS-LS (Zhang and Lapata, 2017 ), a neural model using the standard encoder-decoder architecture with attention combined with reinforcement learning; and DMASS-DCSS (Zhao et al., 2018) , the current state-of-theart in the TurkCorpus, which is based on the Transformer architecture (Vaswani et al., 2017) ."}
{"sent_id": "033ce75c882764e08fb3871656a8d1-C001-20", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_033ce75c882764e08fb3871656a8d1_34", "text": "In contrast, Zilio et al. (2011) make a study involving training a model but use it only on English and use extra lexical resources to complement the machine learning method, so their study does not focus just on classifier evaluation."}
{"sent_id": "033ce75c882764e08fb3871656a8d1-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_033ce75c882764e08fb3871656a8d1_34", "text": "Zilio et al. (2011) provide experiments with this tool as well."}
{"sent_id": "033ce75c882764e08fb3871656a8d1-C001-57", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_033ce75c882764e08fb3871656a8d1_34", "text": "That is the reason why we will run three experiments close to the one of Zilio et al. (2011) but were the only changing parameter is the pattern that we train our classifiers on."}
{"sent_id": "033ce75c882764e08fb3871656a8d1-C001-62", "intents": ["@DIF@"], "paper_id": "ABC_033ce75c882764e08fb3871656a8d1_34", "text": "In contrast to Zilio et al. (2011) we run our experiment on French."}
{"sent_id": "6869f08e826aa434471c51c010ef28-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_6869f08e826aa434471c51c010ef28_34", "text": "In a more recent attempt [19] , we further proposed a framework of discovering multi-level acoustic patterns with varying model granularity."}
{"sent_id": "6869f08e826aa434471c51c010ef28-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_6869f08e826aa434471c51c010ef28_34", "text": "Note that in our previous work [19] , the effect of the third dimension, the acoustic granularity which is the number of Gaussians in each state, was shown to be negligible, thus here we simply set the number of Gaussians in each state to be 4 in all cases."}
{"sent_id": "6869f08e826aa434471c51c010ef28-C001-72", "intents": ["@BACK@"], "paper_id": "ABC_6869f08e826aa434471c51c010ef28_34", "text": "In this section we summarize the way to perform spoken term detection [19] ."}
{"sent_id": "6869f08e826aa434471c51c010ef28-C001-87", "intents": ["@BACK@"], "paper_id": "ABC_6869f08e826aa434471c51c010ef28_34", "text": "However, previous experiments showed that the extra improvements brought in this way is almost negligible, probably because here we have jointly considered the M × N different pattern sequences based on the M × N different pattern sets (e.g. including longer /shorter patterns), so the different time-warped matching and insertion/deletion between d and q is already automatically included [19] ."}
{"sent_id": "6869f08e826aa434471c51c010ef28-C001-90", "intents": ["@UNSURE@"], "paper_id": "ABC_6869f08e826aa434471c51c010ef28_34", "text": "But here we simply assume the detection is completely unsupervised without any annotation, and all pattern sets are equally weighted [19] ."}
{"sent_id": "b9e9f358ace19da43bfe9e5bc380c5-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_b9e9f358ace19da43bfe9e5bc380c5_34", "text": "Recently, Yu et al. (2018b) released a manually labelled dataset for parsing natural language questions into complex SQL, which facilitates related research."}
{"sent_id": "b9e9f358ace19da43bfe9e5bc380c5-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_b9e9f358ace19da43bfe9e5bc380c5_34", "text": "The second and dominant category of datasets uses SQL, which includes Restaurants (Tang and Mooney, 2001; Popescu et al., 2003) , Academic (Iyer et al., 2017) , Yelp and IMDB (Yaghmazadeh et al., 2017) , Ad-vising (Finegan-Dollak et al., 2018) and the recently proposed WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) ."}
{"sent_id": "b9e9f358ace19da43bfe9e5bc380c5-C001-22", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_b9e9f358ace19da43bfe9e5bc380c5_34", "text": "In particular, we translate the Spider (Yu et al., 2018b) dataset into Chinese."}
{"sent_id": "b9e9f358ace19da43bfe9e5bc380c5-C001-51", "intents": ["@USE@"], "paper_id": "ABC_b9e9f358ace19da43bfe9e5bc380c5_34", "text": "Following the database split setting of Yu et al. (2018b) , we make training, development and test sets split in a way that no database overlaps in them as shown in Table 1 ."}
{"sent_id": "8b5e14bdf3f415725333de672be114-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_8b5e14bdf3f415725333de672be114_34", "text": "More advanced machine learning techniques such as classification and regression have been applied to the task of reading level prediction (Collins- Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009; Feng et al., 2010) ; such works are described in further detail in the next Section 2."}
{"sent_id": "8b5e14bdf3f415725333de672be114-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_8b5e14bdf3f415725333de672be114_34", "text": "A detailed analysis of various features for automatic readability assessment has been done by Feng et al. (2010) ."}
{"sent_id": "8b5e14bdf3f415725333de672be114-C001-20", "intents": ["@USE@"], "paper_id": "ABC_8b5e14bdf3f415725333de672be114_34", "text": "At the same time, we have also extended our previous feature set by introducing a richer set of automatically derived textbased features, proposed by Feng et al. (2010) , which capture deeper syntactic complexities of the text."}
{"sent_id": "8b5e14bdf3f415725333de672be114-C001-91", "intents": ["@USE@"], "paper_id": "ABC_8b5e14bdf3f415725333de672be114_34", "text": "We utilize the Stanford Parser (Klein and Manning, 2003) to extract the following features from the XML files based on those used in (Feng et al., 2010) :"}
{"sent_id": "d5d81a4c7759f9a4ab81195819c6d9-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_d5d81a4c7759f9a4ab81195819c6d9_34", "text": "Variants of network structures have been applied in NMT such as LSTM (Wu et al., 2016) , CNN (Gehring et al., 2017) and Transformer (Vaswani et al., 2017) ."}
{"sent_id": "d5d81a4c7759f9a4ab81195819c6d9-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_d5d81a4c7759f9a4ab81195819c6d9_34", "text": "For example, the LSTM-based models are usually stacked for 4 (Stahlberg et al., 2018 ) or 6 (Chen et al., 2018 blocks, and the state-of-the-art Transformer models are equipped with a 6-block encoder and decoder (Vaswani et al., 2017; JunczysDowmunt, 2018; Edunov et al., 2018) ."}
{"sent_id": "d5d81a4c7759f9a4ab81195819c6d9-C001-11", "intents": ["@USE@"], "paper_id": "ABC_d5d81a4c7759f9a4ab81195819c6d9_34", "text": "† denotes the result reported in (Vaswani et al., 2017) ."}
{"sent_id": "d5d81a4c7759f9a4ab81195819c6d9-C001-68", "intents": ["@USE@"], "paper_id": "ABC_d5d81a4c7759f9a4ab81195819c6d9_34", "text": "Training We use Adam (Kingma and Ba, 2015) optimizer following the optimization settings and default learning rate schedule in Vaswani et al. (2017) for model training."}
{"sent_id": "aa87225d7d326adfb4a8b2702b8f25-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_aa87225d7d326adfb4a8b2702b8f25_34", "text": "The objective can be to predict either the next word given the initial words of a sentence [4, 14, 8] , or simply a nearby word given a single cue word [13, 15] ."}
{"sent_id": "aa87225d7d326adfb4a8b2702b8f25-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_aa87225d7d326adfb4a8b2702b8f25_34", "text": "More recent work has shown that high quality word embeddings can be learned via models with no nonlinear hidden layer [13, 15] ."}
{"sent_id": "aa87225d7d326adfb4a8b2702b8f25-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_aa87225d7d326adfb4a8b2702b8f25_34", "text": "For instance, in the skipgram approach [13] , for each 'cue word' w the 'context words' c are sampled from windows either side of tokens of w in the corpus (with c more likely to be sampled if it occurs closer to w)."}
{"sent_id": "aa87225d7d326adfb4a8b2702b8f25-C001-76", "intents": ["@BACK@"], "paper_id": "ABC_aa87225d7d326adfb4a8b2702b8f25_34", "text": "Lexical analogy questions are an alternative way of evaluating word representations [13, 15] ."}
{"sent_id": "aa87225d7d326adfb4a8b2702b8f25-C001-78", "intents": ["@BACK@"], "paper_id": "ABC_aa87225d7d326adfb4a8b2702b8f25_34", "text": "For skipgram-style embeddings, it has been shown that if m, b and w are the embeddings for man, boy and woman respectively, the correct answer is often the nearest neighbour in the vocabulary (by cosine distance) to the vector v = w + b − m [13] ."}
{"sent_id": "0f0e13e275c4bc4021b1b0d26f3e0c-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_0f0e13e275c4bc4021b1b0d26f3e0c_34", "text": "For the task of fact extraction from billions of Web pages the method of Open Information Extraction (OIE) (Fader et al., 2011) trains domainindependent extractors."}
{"sent_id": "0f0e13e275c4bc4021b1b0d26f3e0c-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_0f0e13e275c4bc4021b1b0d26f3e0c_34", "text": "Existing approaches for OIE, such as REVERB (Fader et al., 2011) , WOE (Wu and Weld, 2010) or WANDER-LUST (Akbik and Bross, 2009 ) focus on the extraction of binary facts, e.g. facts that consist of only two arguments, as well as a fact phrase which denotes the nature of the relationship between the arguments."}
{"sent_id": "0f0e13e275c4bc4021b1b0d26f3e0c-C001-12", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_0f0e13e275c4bc4021b1b0d26f3e0c_34", "text": "Worse, the analyses performed in (Fader et al., 2011) and (Akbik and Bross, 2009) show that incorrect handling of N-ary facts leads to extraction errors, such as incomplete, uninformative or erroneous facts."}
{"sent_id": "0f0e13e275c4bc4021b1b0d26f3e0c-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_0f0e13e275c4bc4021b1b0d26f3e0c_34", "text": "The OIE system REVERB (Fader et al., 2011) by contrast uses a fast shallow syntax parser for labeling sentences and applies syntactic and a lexical constraints for identifying binary facts."}
{"sent_id": "0f0e13e275c4bc4021b1b0d26f3e0c-C001-29", "intents": ["@USE@"], "paper_id": "ABC_0f0e13e275c4bc4021b1b0d26f3e0c_34", "text": "We examine intra sentence fact correctness (true/false) and fact completeness for KRAKEN and REVERB on the corpus of (Fader et al., 2011) ."}
{"sent_id": "5b9a6590d2e7c49f9a9788abe6dc1b-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_5b9a6590d2e7c49f9a9788abe6dc1b_34", "text": "Recent work on neural constituency parsing (Dyer et al., 2016; Choe and Charniak, 2016) has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler."}
{"sent_id": "5b9a6590d2e7c49f9a9788abe6dc1b-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_5b9a6590d2e7c49f9a9788abe6dc1b_34", "text": "Past work on discriminative neural constituency parsers has shown the effectiveness of beam search with a small beam (Vinyals et al., 2015) or even greedy search, as in the case of RD (Dyer et al., 2016) ."}
{"sent_id": "5b9a6590d2e7c49f9a9788abe6dc1b-C001-20", "intents": ["@USE@"], "paper_id": "ABC_5b9a6590d2e7c49f9a9788abe6dc1b_34", "text": "In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of Dyer et al. (2016) , and the LSTM language modeling generative parser (LM) of Choe and Charniak (2016) ."}
{"sent_id": "5b9a6590d2e7c49f9a9788abe6dc1b-C001-22", "intents": ["@USE@"], "paper_id": "ABC_5b9a6590d2e7c49f9a9788abe6dc1b_34", "text": "Our findings suggest the presence of model combination effects in both generative parsers: when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser (the RNNG discriminative parser, RD (Dyer et al., 2016) ), performance decreases when compared to using just candidates from the base parser, i.e., B ∪ A → A has lower evaluation performance than B → A (Section 3.1)."}
{"sent_id": "5b9a6590d2e7c49f9a9788abe6dc1b-C001-65", "intents": ["@USE@"], "paper_id": "ABC_5b9a6590d2e7c49f9a9788abe6dc1b_34", "text": "We train RNNG discriminative (RD) and generative (RG) models, following Dyer et al. (2016) by using the same hyperparameter settings, and using pretrained word embeddings from Ling et al. (2015) for the discriminative model."}
{"sent_id": "ada92083e8c012c328d5b6172b76ad-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_ada92083e8c012c328d5b6172b76ad_34", "text": "Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community (Kim and Hovy, 2006; Breck et al., 2007; Ruppenhofer et al., 2008; Wilson et al., 2009; Qiu et al., 2011; Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016) ."}
{"sent_id": "ada92083e8c012c328d5b6172b76ad-C001-107", "intents": ["@SIM@"], "paper_id": "ABC_ada92083e8c012c328d5b6172b76ad_34", "text": "The results show that SRL information is very helpful for ORL, which is consistent with previous studies (Kim and Hovy, 2006; Ruppenhofer et al., 2008; Marasović and Frank, 2018) ."}
{"sent_id": "eeada4aedbb43b575365a15d75f2ac-C001-4", "intents": ["@USE@"], "paper_id": "ABC_eeada4aedbb43b575365a15d75f2ac_34", "text": "Using the sense-tagged corpus of 192,800 word occurrences reported in (Ng and Lee, 1996) , I examine the effect of the number of training examples on the accuracy of an exemplar-based classifier versus the base-line, most-frequent-sense classitier."}
{"sent_id": "eeada4aedbb43b575365a15d75f2ac-C001-59", "intents": ["@USE@"], "paper_id": "ABC_eeada4aedbb43b575365a15d75f2ac_34", "text": "To overcome this data sparseness problem of WSD, I initiated a mini-project in sense tagging and collected a corpus in which 192,800 occurrences of 191 words have been manually tagged with senses of WORDNET (Ng and Lee, 1996) ."}
{"sent_id": "eeada4aedbb43b575365a15d75f2ac-C001-76", "intents": ["@DIF@"], "paper_id": "ABC_eeada4aedbb43b575365a15d75f2ac_34", "text": "The performance figures of LEXAS in Table 1 are higher than those reported in (Ng and Lee, 1996) ."}
{"sent_id": "8e59c2c48e27b2abd5f63d6b4ce23d-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_8e59c2c48e27b2abd5f63d6b4ce23d_34", "text": "Recently, (Cherry et al., 2018) extended the approach of NMT based on subword units to implement the translation model directly at the level of characters, which could reach comparable performance to the subword-based model, although this would require much larger networks which may be more difficult to train."}
{"sent_id": "8e59c2c48e27b2abd5f63d6b4ce23d-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_8e59c2c48e27b2abd5f63d6b4ce23d_34", "text": "Although character-level NMT models have shown the potential to obtain comparable performance with subwordbased NMT models, this would require increasing the computational cost of the model, defined by the network parameters (Kreutzer and Sokolov, 2018; Cherry et al., 2018) ."}
{"sent_id": "8e59c2c48e27b2abd5f63d6b4ce23d-C001-113", "intents": ["@BACK@"], "paper_id": "ABC_8e59c2c48e27b2abd5f63d6b4ce23d_34", "text": "Studies have shown that character-level NMT models could potentially reach the same performance with the subword-based NMT models (Cherry et al., 2018) , although this might require increasing the capacity of the network."}
{"sent_id": "8e59c2c48e27b2abd5f63d6b4ce23d-C001-128", "intents": ["@USE@"], "paper_id": "ABC_8e59c2c48e27b2abd5f63d6b4ce23d_34", "text": "In this paper, we limit the evaluation to recurrent architectures for comparison to previous work, including (Luong and Manning, 2016) , (Sennrich et al., 2016) and (Cherry et al., 2018) , and leave implementation of hierarchical decoding with feed-forward architectures to future work."}
{"sent_id": "d1bff202991116a6a957aa61c05770-C001-3", "intents": ["@EXT@"], "paper_id": "ABC_d1bff202991116a6a957aa61c05770_35", "text": "In order to better understand the components that lead to effective representations, we propose a lightweight version of InferSent (Conneau et al., 2017) , called InferLite, that does not use any recurrent layers and operates on a collection of pre-trained word embeddings."}
{"sent_id": "d1bff202991116a6a957aa61c05770-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_d1bff202991116a6a957aa61c05770_35", "text": "Recently, Conneau et al. (2017) showed that a bidirectional LSTM with max pooling trained to perform Natural Language Inference (NLI), called InferSent, outperforms several other encoding functions on a suite of downstream prediction tasks."}
{"sent_id": "d1bff202991116a6a957aa61c05770-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_d1bff202991116a6a957aa61c05770_35", "text": "Conneau et al. (2017) showed that similar or improved performance can be obtained using NLI datasets as a source of supervisory information."}
{"sent_id": "d1bff202991116a6a957aa61c05770-C001-20", "intents": ["@SIM@"], "paper_id": "ABC_d1bff202991116a6a957aa61c05770_35", "text": "Despite its simplicity, our method obtains performances on par with InferSent (Conneau et al., 2017) when using Glove representations (Pennington et al., 2014) as the source of pre-trained word vectors."}
{"sent_id": "d1bff202991116a6a957aa61c05770-C001-89", "intents": ["@USE@"], "paper_id": "ABC_d1bff202991116a6a957aa61c05770_35", "text": "For training on NLI, we follow existing work and compute the concatenation of the embeddings of premise and hypothesis sentences along with their componentwise and absolute difference (Conneau et al., 2017) ."}
{"sent_id": "7b69c68a602e90c7ee7b9fa8a8facf-C001-10", "intents": ["@USE@"], "paper_id": "ABC_7b69c68a602e90c7ee7b9fa8a8facf_35", "text": "We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007) , as opposed to formally syntactic systems such as Hiero (Chiang, 2007) ."}
{"sent_id": "7b69c68a602e90c7ee7b9fa8a8facf-C001-98", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_7b69c68a602e90c7ee7b9fa8a8facf_35", "text": "LHSN, as shown by Galley et al. (2006) , cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus."}
{"sent_id": "7b69c68a602e90c7ee7b9fa8a8facf-C001-50", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_7b69c68a602e90c7ee7b9fa8a8facf_35", "text": "The first is synchronous parsing (Galley et al., 2006; May and Knight, 2007) , where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up."}
{"sent_id": "7b69c68a602e90c7ee7b9fa8a8facf-C001-94", "intents": ["@BACK@"], "paper_id": "ABC_7b69c68a602e90c7ee7b9fa8a8facf_35", "text": "The other one is normalization based on the root of the LHS (ROOTN) (Galley et al., 2006) , corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously."}
{"sent_id": "7b69c68a602e90c7ee7b9fa8a8facf-C001-58", "intents": ["@SIM@"], "paper_id": "ABC_7b69c68a602e90c7ee7b9fa8a8facf_35", "text": "This algorithm is referred to as GHKM (Galley et al., 2004) and is widely used in SSMT systems (Galley et al., 2006; Liu et al., 2006; Huang et al., 2006) ."}
{"sent_id": "cc3d38692097020ee7f4f17cf9247d-C001-27", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_cc3d38692097020ee7f4f17cf9247d_35", "text": "Recent studies on relation extraction have shown that by combining kernels with Support-vector Machines (SVM), one can obtain results superior to feature-based methods (Bunescu and Mooney, 2005b; Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004; Cumby and Roth, 2003; Zelenko et al., 2003; Zhang et al., 2006a; Zhang et al., 2006b; Zhao and Grishman, 2005) ."}
{"sent_id": "cc3d38692097020ee7f4f17cf9247d-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_cc3d38692097020ee7f4f17cf9247d_35", "text": "In a later work also done by Bunescu & Mooney (2005a) , they proposed a kernel that computes similarities between nodes on the shortest dependency paths that connect the entities."}
{"sent_id": "cc3d38692097020ee7f4f17cf9247d-C001-100", "intents": ["@BACK@"], "paper_id": "ABC_cc3d38692097020ee7f4f17cf9247d_35", "text": "The shortest path dependency kernel proposed by Bunescu & Mooney (2005a) also works with dependency parse trees."}
{"sent_id": "cc3d38692097020ee7f4f17cf9247d-C001-66", "intents": ["@USE@"], "paper_id": "ABC_cc3d38692097020ee7f4f17cf9247d_35", "text": "We then present the algorithms behind three kernels that we are particularly interested in: subsequence kernel (Bunescu and Mooney, 2005b) , dependency tree kernel (Culotta and Sorensen, 2004) and shortest path dependency kernel (Bunescu and Mooney, 2005a) ."}
{"sent_id": "a789aea59eebfefb990dfc6367d323-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_a789aea59eebfefb990dfc6367d323_35", "text": "Despite considerable prior work in temporal information extraction, to date state-of-the-art resources are designed for extracting temporally scoped facts about public figures/organizations from newswire or Wikipedia articles McClosky and Manning, 2012; Garrido et [11/15/2008] I have noticed some pulling recently and I won't start rads until March."}
{"sent_id": "a789aea59eebfefb990dfc6367d323-C001-107", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_a789aea59eebfefb990dfc6367d323_35", "text": "Previous work only retrieves time-rich sentences (i.e., date sentences) (Ling and Weld, 2010; McClosky and Manning, 2012; Garrido et al., 2012) ."}
{"sent_id": "a789aea59eebfefb990dfc6367d323-C001-143", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_a789aea59eebfefb990dfc6367d323_35", "text": "In previous work (McClosky and Manning, 2012; , the evaluation metric score is defined as 1/((1 + |d|)) where d is the difference between the values in years."}
{"sent_id": "a789aea59eebfefb990dfc6367d323-C001-45", "intents": ["@SIM@"], "paper_id": "ABC_a789aea59eebfefb990dfc6367d323_35", "text": "Our task is closest to the temporal slot filling track in the TAC-KBP 2011 shared task and timelining task (McClosky and Manning, 2012) ."}
{"sent_id": "a789aea59eebfefb990dfc6367d323-C001-94", "intents": ["@SIM@"], "paper_id": "ABC_a789aea59eebfefb990dfc6367d323_35", "text": "Features for the classifier include many of those in (McClosky and Manning, 2012; Yoshikawa et al., 2009 ): namely, event keyword and its dominant verb, verb and preposition that dominate TE, dependency path between TE and keyword and its length, unigram and bigram word and POS features."}
{"sent_id": "289ea9be270f68e23ca1809f997be9-C001-32", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_289ea9be270f68e23ca1809f997be9_35", "text": "Recently Deep Neural Network Based (DNN) models have also been applied for detection of cyberbullying [10] , [11] ."}
{"sent_id": "289ea9be270f68e23ca1809f997be9-C001-33", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_289ea9be270f68e23ca1809f997be9_35", "text": "In [11] , authors have used DNN models for detection of cyberbullying and have expanded their models across multiple social media platforms."}
{"sent_id": "289ea9be270f68e23ca1809f997be9-C001-44", "intents": ["@USE@"], "paper_id": "ABC_289ea9be270f68e23ca1809f997be9_35", "text": "In this study, we have first reproduced the experiments conducted in [11] on the datasets used by the authors namely, Formspring [12] , Wikipedia [14] , and Twitter [13] ."}
{"sent_id": "289ea9be270f68e23ca1809f997be9-C001-158", "intents": ["@USE@"], "paper_id": "ABC_289ea9be270f68e23ca1809f997be9_35", "text": "Following [11] we also implemented the transfer learning procedure to evaluate to what extent the DNN models trained on a social network, here Twitter, Formspring, and Wiki, can successfully detect cyberbullying posts in another social network, i.e., YouTube."}
{"sent_id": "b6afd492c60af7ab1ba0be3cd654b2-C001-15", "intents": ["@DIF@"], "paper_id": "ABC_b6afd492c60af7ab1ba0be3cd654b2_35", "text": "It is able to surpass n-gram-based scores achieved previously by Dušek and Jurčíček (2015) , offering a simpler setup and more relevant outputs."}
{"sent_id": "b6afd492c60af7ab1ba0be3cd654b2-C001-96", "intents": ["@DIF@"], "paper_id": "ABC_b6afd492c60af7ab1ba0be3cd654b2_35", "text": "The best results of both setups surpass the best results on this dataset using training data without manual alignments (Dušek and Jurčíček, 2015) in both automatic metrics 12 and the number of semantic errors."}
{"sent_id": "b6afd492c60af7ab1ba0be3cd654b2-C001-114", "intents": ["@DIF@"], "paper_id": "ABC_b6afd492c60af7ab1ba0be3cd654b2_35", "text": "Our generator was able to surpass the best BLEU/NIST scores on the same dataset previously achieved by a perceptron-based generator of Dušek and Jurčíček (2015) while reducing the amount of irrelevant information on the output."}
{"sent_id": "b6afd492c60af7ab1ba0be3cd654b2-C001-87", "intents": ["@USE@"], "paper_id": "ABC_b6afd492c60af7ab1ba0be3cd654b2_35", "text": "10 The surface realizer works almost flawlessly on this lim-ited domain (Dušek and Jurčíček, 2015) , leaving the seq2seq generator as the major error source."}
{"sent_id": "c7e304499654516cce43c550256eae-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_c7e304499654516cce43c550256eae_35", "text": "Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (Yarowsky and Ngai, 2001; Täckström et al., 2013; Fang and Cohn, 2016; Zhang et al., 2016) ."}
{"sent_id": "c7e304499654516cce43c550256eae-C001-59", "intents": ["@DIF@"], "paper_id": "ABC_c7e304499654516cce43c550256eae_35", "text": "This component allows for a more expressive label mapping than Fang and Cohn (2016)'s linear matrix translation."}
{"sent_id": "c7e304499654516cce43c550256eae-C001-88", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_c7e304499654516cce43c550256eae_35", "text": "For a more direct comparison, we include BILSTM-DEBIAS (Fang and Cohn, 2016) , applied using our proposed cross-lingual supervision based on dictionaries, instead of parallel corpora; accordingly the key difference is their linear transformation for the distant data, versus our non-linear transformation to the gold data."}
{"sent_id": "c7e304499654516cce43c550256eae-C001-92", "intents": ["@DIF@"], "paper_id": "ABC_c7e304499654516cce43c550256eae_35", "text": "BILSTM-DEBIAS (Fang and Cohn, 2016) performs worse than our proposed method, indicating that a linear transformation is insufficient for modelling distant supervision."}
{"sent_id": "c7e304499654516cce43c550256eae-C001-43", "intents": ["@EXT@"], "paper_id": "ABC_c7e304499654516cce43c550256eae_35", "text": "Our approach extends the work of Fang and Cohn (2016) , who present a model based on distant supervision in the form of cross-lingual projection and use projected tags generated from parallel corpora as distant annotations."}
{"sent_id": "b624e8d3ad3d351d4cb27ea4b5c616-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_b624e8d3ad3d351d4cb27ea4b5c616_35", "text": "The semi-supervised approach circumvents the need to include word n-gram features from any tweets, and builds upon the successful usage of word representations (Collobert et al., 2011) , and word clusters (Lin & Wu, 2009; Miller et al., 2004; Ratinov & Roth, 2009; Turian et al., 2010) for NER by utilizing large amounts of unlabelled data or models pre-trained on a large vocabulary."}
{"sent_id": "b624e8d3ad3d351d4cb27ea4b5c616-C001-83", "intents": ["@BACK@"], "paper_id": "ABC_b624e8d3ad3d351d4cb27ea4b5c616_35", "text": "Distributed word representations have been shown to improve the accuracy of NER systems (Collobert et al., 2011; Turian et al., 2010) ."}
{"sent_id": "b624e8d3ad3d351d4cb27ea4b5c616-C001-88", "intents": ["@BACK@"], "paper_id": "ABC_b624e8d3ad3d351d4cb27ea4b5c616_35", "text": "Word clusters are word groupings that get generated in an unsupervised fashion, and they have been successfully used as features for NER tasks (Lin & Wu, 2009; Miller et al., 2004; Ratinov & Roth, 2009; Turian et al., 2010) ."}
{"sent_id": "b624e8d3ad3d351d4cb27ea4b5c616-C001-183", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_b624e8d3ad3d351d4cb27ea4b5c616_35", "text": "Prior work has shown that semi-supervised algorithms can perform decently for NER tasks with sparse labelled data (Blum, 1998; Carlson et al., 2010; Chapelle et al., 2009; Liang, 2005; Turian et al., 2010; Zhu & Goldberg, 2009 )."}
{"sent_id": "b624e8d3ad3d351d4cb27ea4b5c616-C001-188", "intents": ["@BACK@"], "paper_id": "ABC_b624e8d3ad3d351d4cb27ea4b5c616_35", "text": "As identified by Turian et al. (2010) , the importance of word representations and word clusters increases as the availability of unlabelled data increases."}
{"sent_id": "b624e8d3ad3d351d4cb27ea4b5c616-C001-137", "intents": ["@SIM@"], "paper_id": "ABC_b624e8d3ad3d351d4cb27ea4b5c616_35", "text": "Although it might appear that our classifier has access to the unlabelled test data sequences while learning, it rather is the case that we resemble an online setting where we continuously update our unsupervised features using the new batch of unlabelled test data, and then retrain our model on the original training data (Blum, 1998; Blum & Mitchell, 1998; Carlson et al., 2010; Chapelle et al., 2009; Liang, 2005; Turian et al., 2010; Zhu & Goldberg, 2009) ."}
{"sent_id": "7f1723c42fc577fdfd7144b7991db1-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_7f1723c42fc577fdfd7144b7991db1_35", "text": "Contrastive Predictive Coding (CPC) [5] and wav2vec [7] use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task."}
{"sent_id": "3e344c590b4d5270a29054ac15efa5-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_3e344c590b4d5270a29054ac15efa5_35", "text": "Character CNN was first introduced by Zhang [2] for the text classification task."}
{"sent_id": "3e344c590b4d5270a29054ac15efa5-C001-82", "intents": ["@BACK@"], "paper_id": "ABC_3e344c590b4d5270a29054ac15efa5_35", "text": "Pure Character level classification was first explored using CNN architecture [2] ."}
{"sent_id": "3e344c590b4d5270a29054ac15efa5-C001-197", "intents": ["@BACK@"], "paper_id": "ABC_3e344c590b4d5270a29054ac15efa5_35", "text": "As stated in the paper [2] , ConvNets with character embedding can completely replace words and work even without any semantic meanings."}
{"sent_id": "3e344c590b4d5270a29054ac15efa5-C001-208", "intents": ["@SIM@"], "paper_id": "ABC_3e344c590b4d5270a29054ac15efa5_35", "text": "Considering the small size of our datasets, we hope to have improved performance with larger datasets, as is the case for character level ConvNets [2] ."}
{"sent_id": "3ebfa05038431571701a7199163832-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_3ebfa05038431571701a7199163832_35", "text": "Previous work has explored learnable alternatives to speech features that rely on a similar computation to spectral representations [19, 20, 21, 22, 5] ."}
{"sent_id": "3ebfa05038431571701a7199163832-C001-42", "intents": ["@USE@"], "paper_id": "ABC_3ebfa05038431571701a7199163832_35", "text": "As the first step of our computational pipeline, we use TimeDomain filterbanks from [22] ."}
{"sent_id": "fd9122d20c390ea115c27092170739-C001-8", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_fd9122d20c390ea115c27092170739_35", "text": "The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; Wu and Jiang, 2000; Goh, 2003) ."}
{"sent_id": "fd9122d20c390ea115c27092170739-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_fd9122d20c390ea115c27092170739_35", "text": "The rule-based approach was rejected with the claim that rules are bound to overgenerate (Wu and Jiang, 2000) ."}
{"sent_id": "fd9122d20c390ea115c27092170739-C001-48", "intents": ["@USE@"], "paper_id": "ABC_fd9122d20c390ea115c27092170739_35", "text": "The models we will consider are a rule-based model, the trigram model, and the statistical model developed by Wu and Jiang (2000) ."}
{"sent_id": "30718e751f18432c2478442530267e-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_30718e751f18432c2478442530267e_35", "text": "However, Jia and Liang (2017) show that these systems are very vulnerable to paragraphs with adversarial sentences."}
{"sent_id": "30718e751f18432c2478442530267e-C001-16", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_30718e751f18432c2478442530267e_35", "text": "Besides the single BiDAF, the single Match LSTM, the ensemble Match LSTM, and the ensemble BiDAF achieve an F1 of 7.6, 11.7, and 2.7 respectively in question answering on ADDANY adversarial dataset (Jia and Liang, 2017) ."}
{"sent_id": "30718e751f18432c2478442530267e-C001-73", "intents": ["@USE@"], "paper_id": "ABC_30718e751f18432c2478442530267e_35", "text": "The performance of question answering is evaluated by the Macro-averaged F1 score (Rajpurkar Jia and Liang, 2017) ."}
{"sent_id": "30718e751f18432c2478442530267e-C001-111", "intents": ["@SIM@"], "paper_id": "ABC_30718e751f18432c2478442530267e_35", "text": "However, Jia and Liang (2017) also present the deterioration of QA systems on another dataset, ADDSENT adversarial dataset."}
{"sent_id": "05eecafea7684dc8de13c29a76b767-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_05eecafea7684dc8de13c29a76b767_35", "text": "The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Wing and Baldridge, 2014) or dialectology Eisenstein, 2015) ."}
{"sent_id": "05eecafea7684dc8de13c29a76b767-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_05eecafea7684dc8de13c29a76b767_35", "text": "Three main text-based approaches are: (1) the use of gazetteers Quercini et al., 2010) ; (2) unsupervised text clustering based on topic models or similar (Eisenstein et al., 2010; Hong et al., 2012; Ahmed et al., 2013) ; and (3) supervised classification (Ding et al., 2000; Backstrom et al., 2008; Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Wing and Baldridge, 2011; Han et al., 2012; Rout et al., 2013) , which unlike gazetteers can be applied to informal text and compared to topic models, scales better."}
{"sent_id": "05eecafea7684dc8de13c29a76b767-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_05eecafea7684dc8de13c29a76b767_35", "text": "There have also been attempts to automatically identify such words from geotagged documents (Eisenstein et al., 2010; Ahmed et al., 2013; Eisenstein, 2015) ."}
{"sent_id": "05eecafea7684dc8de13c29a76b767-C001-53", "intents": ["@USE@"], "paper_id": "ABC_05eecafea7684dc8de13c29a76b767_35", "text": "Following Cheng (2010) and Eisenstein (2010) , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\")."}
{"sent_id": "6781a5d131e68f7a7ac2fe239cc3d0-C001-6", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_6781a5d131e68f7a7ac2fe239cc3d0_35", "text": "In previous work (Lucena & Paraboni, 2008) we presented a frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008."}
{"sent_id": "6781a5d131e68f7a7ac2fe239cc3d0-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_6781a5d131e68f7a7ac2fe239cc3d0_35", "text": "In Lucena & Paraboni (2008) we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation."}
{"sent_id": "6781a5d131e68f7a7ac2fe239cc3d0-C001-18", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_6781a5d131e68f7a7ac2fe239cc3d0_35", "text": "The above approach performed fairly well (at least considering its simplicity) as reported in Lucena & Paraboni (2008) ."}
{"sent_id": "6781a5d131e68f7a7ac2fe239cc3d0-C001-24", "intents": ["@EXT@"], "paper_id": "ABC_6781a5d131e68f7a7ac2fe239cc3d0_35", "text": "The present work is a refined version of the original frequency-based greedy attribute selection strategy submitted to the TUNA Challenge 2008 (Lucena & Paraboni, 2008) , now taking also the trial condition (+/-LOC) into account."}
{"sent_id": "6781a5d131e68f7a7ac2fe239cc3d0-C001-35", "intents": ["@DIF@"], "paper_id": "ABC_6781a5d131e68f7a7ac2fe239cc3d0_35", "text": "The most relevant comparison with our previous work is observed in the overall string-edit distance values in Figure 1 : considering that in Lucena & Paraboni (2008) we reported 6.12 editdistance for Furniture and 7.38 for People, the overall improvement (driven by the descriptions in the Furniture domain) may be explained by the fact that the current version makes more accurate decisions as to when to use these attributes according to the instructions given to the participants of the TUNA trials (the trial condition +/-LOC. )"}
{"sent_id": "b7e0879c4cac85054870146e61aa6f-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_b7e0879c4cac85054870146e61aa6f_35", "text": "In a later work, Das et al. [6] introduce Neural Modular Control (NMC) which is a hierarchical policy network that operates over expert sub-policy sketches."}
{"sent_id": "0751f2ced4f7ced37cf206fea051fa-C001-39", "intents": ["@USE@"], "paper_id": "ABC_0751f2ced4f7ced37cf206fea051fa_35", "text": "First, we built an initial SITG by following the method described in (Sánchez and Benedí, 2006b )."}
{"sent_id": "2b7ba7f7aa2a03ad0de84e007c1f64-C001-18", "intents": ["@USE@"], "paper_id": "ABC_2b7ba7f7aa2a03ad0de84e007c1f64_35", "text": "We focus on buyer-seller negotiations (He et al., 2018) where two individuals negotiate the price of a given product."}
{"sent_id": "2b7ba7f7aa2a03ad0de84e007c1f64-C001-30", "intents": ["@USE@"], "paper_id": "ABC_2b7ba7f7aa2a03ad0de84e007c1f64_35", "text": "Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by He et al. (2018) ."}
{"sent_id": "5eb321d3c63642a4b148e1276eab20-C001-33", "intents": ["@USE@"], "paper_id": "ABC_5eb321d3c63642a4b148e1276eab20_35", "text": "Firstly PAN employs LSTM to generate representations of sentences s i following (Shimaoka et al. 2016) , where s i ∈ R d is the semantic representation of s i , i ∈ {1, 2, ..., n}. Afterwards, we build path-based attention α i,t over sentences s i for each type t ∈ T e , which is expected to focus on relevant sentences to type t. Then, the representation of sentence set S e for type t, denoted by s e,t ∈ R d , is calculated through weighted sum of vectors of sentences."}
{"sent_id": "5eb321d3c63642a4b148e1276eab20-C001-54", "intents": ["@DIF@"], "paper_id": "ABC_5eb321d3c63642a4b148e1276eab20_35", "text": "Experiments are carried on two widely used datasets OntoNotes and FIGER(GOLD), and the training dataset of OntoNotes is noisy compared to FIGER(GOLD) (Shimaoka et al. 2016) ."}
{"sent_id": "6f3d6ad1f09c55a1a006abbeddb4ab-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_6f3d6ad1f09c55a1a006abbeddb4ab_35", "text": "A breakthrough has come in the form of research by McClosky et al. (2006a; 2006b ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005) ."}
{"sent_id": "6f3d6ad1f09c55a1a006abbeddb4ab-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_6f3d6ad1f09c55a1a006abbeddb4ab_35", "text": "McClosky et al. (2006a; 2006b ) proceed as follows: sentences * Now affiliated to Lalic, Université Paris 4 La Sorbonne."}
{"sent_id": "6f3d6ad1f09c55a1a006abbeddb4ab-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_6f3d6ad1f09c55a1a006abbeddb4ab_35", "text": "In the experiments of McClosky et al. (2006a; 2006b) , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material."}
{"sent_id": "6f3d6ad1f09c55a1a006abbeddb4ab-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_6f3d6ad1f09c55a1a006abbeddb4ab_35", "text": "However, McCloskey et al. (2006b) report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training."}
{"sent_id": "6f3d6ad1f09c55a1a006abbeddb4ab-C001-18", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_6f3d6ad1f09c55a1a006abbeddb4ab_35", "text": "In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a; 2006b) ."}
{"sent_id": "6f3d6ad1f09c55a1a006abbeddb4ab-C001-45", "intents": ["@DIF@"], "paper_id": "ABC_6f3d6ad1f09c55a1a006abbeddb4ab_35", "text": "The f-score of 83.7% is lower than the f-score of 85.2% reported by McClosky et al. (2006b) for the same parser on Brown corpus data."}
{"sent_id": "0c2f7cea9f27b4799736fbcba48192-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_0c2f7cea9f27b4799736fbcba48192_36", "text": "It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017) ."}
{"sent_id": "0c2f7cea9f27b4799736fbcba48192-C001-37", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_0c2f7cea9f27b4799736fbcba48192_36", "text": "In Felbo et al. (2017) , attention is computed as follows:"}
{"sent_id": "0c2f7cea9f27b4799736fbcba48192-C001-29", "intents": ["@DIF@"], "paper_id": "ABC_0c2f7cea9f27b4799736fbcba48192_36", "text": "We observed a performance improvement over competitive baselines such as FastText (FT) (Joulin et al., 2017) and Deepmoji (Felbo et al., 2017) , which is most noticeable in the case of infrequent emojis."}
{"sent_id": "0c2f7cea9f27b4799736fbcba48192-C001-33", "intents": ["@USE@"], "paper_id": "ABC_0c2f7cea9f27b4799736fbcba48192_36", "text": "Our base architecture is the Deepmoji model (Felbo et al., 2017) , which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM."}
{"sent_id": "4625b603beb2308b8b306dd4c01823-C001-8", "intents": ["@DIF@"], "paper_id": "ABC_4625b603beb2308b8b306dd4c01823_36", "text": "For example, we show that training our best model for only one epoch with < 40% of the data enables better performance than the baseline reported by Klinger et al. (2018) for the task."}
{"sent_id": "4625b603beb2308b8b306dd4c01823-C001-99", "intents": ["@DIF@"], "paper_id": "ABC_4625b603beb2308b8b306dd4c01823_36", "text": "As the Table shows, all our models achieve sizable gains over the logistic regression model introduced by (Klinger et al., 2018) as a baseline for the competition (F-score = 60%)."}
{"sent_id": "4625b603beb2308b8b306dd4c01823-C001-127", "intents": ["@DIF@"], "paper_id": "ABC_4625b603beb2308b8b306dd4c01823_36", "text": "Interestingly, as Figure 3 shows, the model exceeds the baseline model reported by the task organizers (Klinger et al., 2018) when trained on only 10% of the training data."}
{"sent_id": "4625b603beb2308b8b306dd4c01823-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_4625b603beb2308b8b306dd4c01823_36", "text": "Klinger et al. (2018) propose yet a fourth method for collecting emotion data that depends on the existence of the expression \"emotion-word + one of the following words (when, that or because)\" in a tweet, regardless of the position of the emotion word."}
{"sent_id": "4625b603beb2308b8b306dd4c01823-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_4625b603beb2308b8b306dd4c01823_36", "text": "The full details of the dataset can be found in Klinger et al. (2018) ."}
{"sent_id": "4625b603beb2308b8b306dd4c01823-C001-51", "intents": ["@USE@"], "paper_id": "ABC_4625b603beb2308b8b306dd4c01823_36", "text": "As an additional baseline, we compare to Klinger et al. (2018) who propose a model based on Logistic Regression with a bag of word unigrams (BOW)."}
{"sent_id": "b2a6ec11403fe73b9bae7742c1c5a2-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_b2a6ec11403fe73b9bae7742c1c5a2_36", "text": "Prior natural language processing (NLP) approaches to student revision analysis have focused on identifying revisions during argumentative writing and classifying their purposes and other properties [12, 11, 7, 1] ."}
{"sent_id": "b2a6ec11403fe73b9bae7742c1c5a2-C001-32", "intents": ["@EXT@"], "paper_id": "ABC_b2a6ec11403fe73b9bae7742c1c5a2_36", "text": "Nonidentical aligned sentences were extracted as the revisions, resulting in three types of revision operations -Add, Delete, M odif y. Each extracted revision was manually annotated with a purpose following the revision schema shown in Figure 1 (modified compared to [12] by adding the Precision category)."}
{"sent_id": "fc58a9813b80afc9811b8ee27679b7-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_fc58a9813b80afc9811b8ee27679b7_36", "text": "Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000) , weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006) , fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009 ) and other variations."}
{"sent_id": "fc58a9813b80afc9811b8ee27679b7-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_fc58a9813b80afc9811b8ee27679b7_36", "text": "An important step forwards is TIER light (Huang and Riloff, 2012a ) that targeted the minimization of human supervision with a bootstrapping technique for event roles detection."}
{"sent_id": "fc58a9813b80afc9811b8ee27679b7-C001-28", "intents": ["@DIF@"], "paper_id": "ABC_fc58a9813b80afc9811b8ee27679b7_36", "text": "We then propose to rely only on these word representations to detect the event roles whereas, in most works (Riloff, 1996; Patwardhan and Riloff, 2007; Huang and Riloff, 2012a; Huang and Riloff, 2012b) , the role fillers are represented by a set of different features (raw words, their parts-ofspeech, syntactic or semantic roles in the sentence)."}
{"sent_id": "fc58a9813b80afc9811b8ee27679b7-C001-83", "intents": ["@DIF@"], "paper_id": "ABC_fc58a9813b80afc9811b8ee27679b7_36", "text": "Figure 1: F1-score results for event role labeling on MUC-4 data, for different size of training data, of \"String Slots\" on the TST3+TST4 with different parameters, compared to the learning curve of TIER (Huang and Riloff, 2012a) ."}
{"sent_id": "fc58a9813b80afc9811b8ee27679b7-C001-71", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_fc58a9813b80afc9811b8ee27679b7_36", "text": "Following previous works (Huang and Riloff, 2011; Huang and Riloff, 2012a) , we only consider the \"String Slots\" in this work (other slots need different treatments) and we group certain slots to finally consider the five slot types PerpInd (individual perpetrator), PerpOrg (organizational perpetrator), Target (physical target), Victim (human target name or description) and Weapon (instrument id or type)."}
{"sent_id": "196e7ca5ccd6754ac986137ec55cd3-C001-9", "intents": ["@USE@"], "paper_id": "ABC_196e7ca5ccd6754ac986137ec55cd3_36", "text": "Our work is similar in spirit to e.g. (Roy, 2002; Skocaj et al., 2011) but advances it in several aspects (Yu et al., 2016) ."}
{"sent_id": "196e7ca5ccd6754ac986137ec55cd3-C001-20", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_196e7ca5ccd6754ac986137ec55cd3_36", "text": "We developed a multimodal framework in support of building an interactive learning system, which loosely follows that of Yu et al. (2016) ."}
{"sent_id": "196e7ca5ccd6754ac986137ec55cd3-C001-41", "intents": ["@DIF@", "@MOT@", "@EXT@"], "paper_id": "ABC_196e7ca5ccd6754ac986137ec55cd3_36", "text": "For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as Yu et al. (2016) point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent."}
{"sent_id": "a67f40381e82afaf249f097a208555-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_a67f40381e82afaf249f097a208555_36", "text": "Previous work addressing the problem can be roughly classified into three categories: (1) learning word embeddings from large collections of text using variants of neural networks (Mikolov et al. (2013a) ; Mikolov et al. (2013b) ; Mikolov et al. (2013c) ; Levy and Goldberg (2014) ) or global matrix factorization (Deerwester et al. (1990) ; Turney (2012) ); (2) extracting knowledge from existing semantic networks, such as WordNet (Yang and Powers (2005) ; Alvarez and Lim (2007) ; Hughes and Ramage (2007) ) and ConceptNet (Boteanu and Chernova (2015) ); (3) combining the above two models by various ways (Agirre et al. (2009) ; Zhila et al. (2013) ; Iacobacci et al. (2015) ; Summers-Stay et al. (2016) )."}
{"sent_id": "a67f40381e82afaf249f097a208555-C001-20", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_a67f40381e82afaf249f097a208555_36", "text": "(2) The two existing models for measuring relational similarity are: the directional similarity model (Zhila et al. (2013) ) and the dual-space model consisting of domain and function space (Turney (2012) )."}
{"sent_id": "07cee5aa02b518d48e41b1d6010c2f-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_07cee5aa02b518d48e41b1d6010c2f_36", "text": "The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; Huang, 2008) can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one grammar rule as non-local features."}
{"sent_id": "07cee5aa02b518d48e41b1d6010c2f-C001-70", "intents": ["@USE@"], "paper_id": "ABC_07cee5aa02b518d48e41b1d6010c2f_36", "text": "The parameters θ of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008) ."}
{"sent_id": "07cee5aa02b518d48e41b1d6010c2f-C001-49", "intents": ["@DIF@"], "paper_id": "ABC_07cee5aa02b518d48e41b1d6010c2f_36", "text": "This algorithm is more complex than the approximate decoding algorithm of Huang (2008) ."}
{"sent_id": "b9e1a6bb7b6f2d55c6ea3bb2d3897d-C001-7", "intents": ["@MOT@"], "paper_id": "ABC_b9e1a6bb7b6f2d55c6ea3bb2d3897d_36", "text": "The automatic identification of information status (Prince, 1981; 1992) , i.e. categorizing discourse entities into different classes on the given-new scale, has recently been identified as an important issue in natural language processing (Nissim, 2006; Rahman and Ng, 2011; ."}
{"sent_id": "b9e1a6bb7b6f2d55c6ea3bb2d3897d-C001-65", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_b9e1a6bb7b6f2d55c6ea3bb2d3897d_36", "text": "The fact that Rahman and Ng (2011) report the highest confusion rate between NEW and MEDIATED entities may have its roots in this issue."}
{"sent_id": "b9e1a6bb7b6f2d55c6ea3bb2d3897d-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_b9e1a6bb7b6f2d55c6ea3bb2d3897d_36", "text": "Nissim (2006) and Rahman and Ng (2011) developed methods to automatically identify three different classes: OLD, MEDIATED and NEW expressions."}
{"sent_id": "b9e1a6bb7b6f2d55c6ea3bb2d3897d-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_b9e1a6bb7b6f2d55c6ea3bb2d3897d_36", "text": "Based on work described in Nissim (2006) , Rahman and Ng (2011) develop a machine learning approach to information-status determination."}
{"sent_id": "b9e1a6bb7b6f2d55c6ea3bb2d3897d-C001-81", "intents": ["@SIM@"], "paper_id": "ABC_b9e1a6bb7b6f2d55c6ea3bb2d3897d_36", "text": "We also include a basic \"coreference\" feature, similar to the lexical features of Rahman and Ng (2011) , that fires if there is some lexical overlap of nouns (or compound nouns) in the preceding 10 sentences."}
{"sent_id": "ff5122ce817d506fbcb269b7ae41fe-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_ff5122ce817d506fbcb269b7ae41fe_36", "text": "While AWDB is designed for bracketing NPs of any length, we first experiment with bracketing 3NCs, the largest class of 3 + NCs (93.8% on the basic dataset of Ziering and Van der Plas (2014)), for which bracketing is a binary classification (i.e., LEFT or RIGHT)."}
{"sent_id": "ff5122ce817d506fbcb269b7ae41fe-C001-122", "intents": ["@BACK@"], "paper_id": "ABC_ff5122ce817d506fbcb269b7ae41fe_36", "text": "The harmonic mean numbers for the system of Ziering and Van der Plas (2014) illustrate that coverage gain of types outweighs a higher accuracy of tokens."}
{"sent_id": "ff5122ce817d506fbcb269b7ae41fe-C001-90", "intents": ["@MOT@"], "paper_id": "ABC_ff5122ce817d506fbcb269b7ae41fe_36", "text": "The test set used by Ziering and Van der Plas (2014) is very small and the labeling is less fine-grained."}
{"sent_id": "ff5122ce817d506fbcb269b7ae41fe-C001-36", "intents": ["@DIF@"], "paper_id": "ABC_ff5122ce817d506fbcb269b7ae41fe_36", "text": "Using less restricted forms of cross-lingual supervision, we achieve a much higher coverage than Ziering and Van der Plas (2014) ."}
{"sent_id": "ff5122ce817d506fbcb269b7ae41fe-C001-106", "intents": ["@DIF@"], "paper_id": "ABC_ff5122ce817d506fbcb269b7ae41fe_36", "text": "As expected, the system of Ziering and Van der Plas (2014) does not cover more than 48.1%."}
{"sent_id": "ff5122ce817d506fbcb269b7ae41fe-C001-116", "intents": ["@DIF@"], "paper_id": "ABC_ff5122ce817d506fbcb269b7ae41fe_36", "text": "AWDB outperforms Ziering and Van der Plas (2014) significantly 7 ."}
{"sent_id": "ff5122ce817d506fbcb269b7ae41fe-C001-67", "intents": ["@USE@"], "paper_id": "ABC_ff5122ce817d506fbcb269b7ae41fe_36", "text": "Although AWDB can also process compounds including adjectives (e.g., active inclusion policy aligned to the Dutch beleid voor actieve insluiting (policy for active inclusion)), for a direct comparison with the system of Ziering and Van der Plas (2014) , that analyses 3NCs, we restrict ourselves to noun sequences."}
{"sent_id": "ff5122ce817d506fbcb269b7ae41fe-C001-81", "intents": ["@USE@"], "paper_id": "ABC_ff5122ce817d506fbcb269b7ae41fe_36", "text": "We compare AWDB with the bracketing approach of Ziering and Van der Plas (2014)."}
{"sent_id": "9770f647c0b406462f4b941f136748-C001-27", "intents": ["@MOT@"], "paper_id": "ABC_9770f647c0b406462f4b941f136748_36", "text": "The main limitation of their algorithm is that it was developed specifically for adjectives and that the question of its application to other grammatical categories has not been solved (Turney & Littman, 2003) ."}
{"sent_id": "9770f647c0b406462f4b941f136748-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_9770f647c0b406462f4b941f136748_36", "text": "The technique proposed by Turney and Littman (2003) tries to infer semantic orientation from semantic association in a corpus."}
{"sent_id": "9770f647c0b406462f4b941f136748-C001-87", "intents": ["@SIM@"], "paper_id": "ABC_9770f647c0b406462f4b941f136748_36", "text": "The fourteen SO-LSA benchmarks chosen by Turney and Littman (2003) were translated into 2 Each sentence was automatically modified so as to replace the name and the description of the function of every individual by a generic first name of adequate sex (Mary, John, etc.) in order to prevent the judges being influenced by their prior positive or negative opinion about these people."}
{"sent_id": "3a7625a0f38424fe922ad095e07e68-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_3a7625a0f38424fe922ad095e07e68_36", "text": "COCO-QA (Ren et al., 2015) also uses images from COCO, with the questions generated by an NLP algorithm that uses COCO's captions."}
{"sent_id": "3a7625a0f38424fe922ad095e07e68-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_3a7625a0f38424fe922ad095e07e68_36", "text": "These baseline methods predict the answer using a vector of image features concatenated to a vector of question features (Ren et al., 2015; Zhou et al., 2015; Kafle and Kanan, 2016) ."}
{"sent_id": "3a7625a0f38424fe922ad095e07e68-C001-47", "intents": ["@USE@"], "paper_id": "ABC_3a7625a0f38424fe922ad095e07e68_36", "text": "We conduct experiments on two of the most popular VQA datasets: 'The VQA Dataset' (Antol et al., 2015) and COCO-QA (Ren et al., 2015) ."}
{"sent_id": "8ace0627a085efd0cf0ccc211c556f-C001-111", "intents": ["@USE@"], "paper_id": "ABC_8ace0627a085efd0cf0ccc211c556f_36", "text": "It was previously trained on BooksCorpus [15] and English Wikipedia which have 13GB of plain text combined [11] ."}
{"sent_id": "8ace0627a085efd0cf0ccc211c556f-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_8ace0627a085efd0cf0ccc211c556f_36", "text": "XLNet is a generalized auto-regressive model that can be used for language modeling based on the transformer-XL architecture [11] ."}
{"sent_id": "8ace0627a085efd0cf0ccc211c556f-C001-60", "intents": ["@MOT@"], "paper_id": "ABC_8ace0627a085efd0cf0ccc211c556f_36", "text": "masking the input introduces a few disadvantages mentioned in [11] ."}
{"sent_id": "0a226accf1fa8b471176916a76f1c6-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_0a226accf1fa8b471176916a76f1c6_36", "text": "Syntactic analysis of search queries is important for a variety of tasks including better query refinement, improved matching and better ad targeting (Barr et al., 2008) ."}
{"sent_id": "0a226accf1fa8b471176916a76f1c6-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_0a226accf1fa8b471176916a76f1c6_36", "text": "5 Related Work Barr et al. (2008) manually annotate a corpus of 2722 queries with 19 POS tags and use it to train and evaluate POS taggers, and also describe the linguistic structures they find."}
{"sent_id": "0a226accf1fa8b471176916a76f1c6-C001-10", "intents": ["@MOT@"], "paper_id": "ABC_0a226accf1fa8b471176916a76f1c6_36", "text": "However, search queries differ substantially from traditional forms of written language (e.g., no capitalization, few function words, fairly free word order, etc.), and are therefore difficult to process with natural language processing tools trained on standard corpora (Barr et al., 2008) ."}
{"sent_id": "0a226accf1fa8b471176916a76f1c6-C001-65", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_0a226accf1fa8b471176916a76f1c6_36", "text": "As a reference, Barr et al. (2008) report 79.3% when annotating queries with 19 POS tags."}
{"sent_id": "0a226accf1fa8b471176916a76f1c6-C001-97", "intents": ["@SIM@"], "paper_id": "ABC_0a226accf1fa8b471176916a76f1c6_36", "text": "This is consistent with the analysis in Barr et al. (2008) ."}
{"sent_id": "8e0dcaec15a3b9c4947946a4e885c8-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_8e0dcaec15a3b9c4947946a4e885c8_36", "text": "Most recent work (Bao and Bollegala, 2018 ) has focused on the use of an autoencoder (AE) to encode a set of N pretrained embeddings using 3 different variants: (1) Decoupled Autoencoded Meta Embeddings (DAEME) that keep activations separated for each respective embedding input during encoding and uses a reconstruction loss for both predicted embeddings while minimizing the loss for each respective decoded output, (2) Coupled Autoencoded Meta Embeddings (CAEME) which instead learn to predict from a shared encoding and (3) Averaged Autoencoded Meta-Embedding (AAME) is simply an averaging of the embedding set as input instead of using a concatenation."}
{"sent_id": "8e0dcaec15a3b9c4947946a4e885c8-C001-41", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_8e0dcaec15a3b9c4947946a4e885c8_36", "text": "Figure 2 shows a comparison of the previous autoencoder approaches (Bao and Bollegala, 2018) (left) and the alternative AE (right), where dashed lines indicate connections during training and bold lines indicate prediction."}
{"sent_id": "8e0dcaec15a3b9c4947946a4e885c8-C001-50", "intents": ["@USE@"], "paper_id": "ABC_8e0dcaec15a3b9c4947946a4e885c8_36", "text": "As stated, we compare against previous methods (Yin and Schütze, 2015; Bao and Bollegala, 2018 ) that use 2 distance, as shown in Equation 1)."}
{"sent_id": "56fa13128027f7c37d504d97cfcc45-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_56fa13128027f7c37d504d97cfcc45_36", "text": "Recent work seeks to derive valuable information from SynData while filtering noise, via domain adaptation (Braud and Denis, 2014; , classifying connectives (Rutherford and Xue, 2015) or multi-task learning (Lan et al., 2013; Liu et al., 2016) , and shows promising results."}
{"sent_id": "56fa13128027f7c37d504d97cfcc45-C001-99", "intents": ["@BACK@"], "paper_id": "ABC_56fa13128027f7c37d504d97cfcc45_36", "text": "Liu et al. (2016) use a multi-task model with three auxiliary tasks: 1) conn: connective classification on explicit instances, 2) exp: relation classification on the labeled explicit instances in the PDTB, and 3) rst: relation classification on the labeled RST corpus (William and Thompson, 1988), which defines different discourse relations with that in the PDTB."}
{"sent_id": "56fa13128027f7c37d504d97cfcc45-C001-101", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_56fa13128027f7c37d504d97cfcc45_36", "text": "Although Liu et al. (2016) achieve the stateof-the-art performance (Line 5), they use two additional labeled corpora."}
{"sent_id": "56fa13128027f7c37d504d97cfcc45-C001-115", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_56fa13128027f7c37d504d97cfcc45_36", "text": "Lan et al. (2013) and Liu et al. (2016) combine explicit and implicit data using multi-task learning models and gain improvements."}
{"sent_id": "56fa13128027f7c37d504d97cfcc45-C001-65", "intents": ["@USE@"], "paper_id": "ABC_56fa13128027f7c37d504d97cfcc45_36", "text": "Following Liu et al. (2016) , we alternately use two tasks to train the model, one task per epoch."}
{"sent_id": "291a6ac3f0c2d27ca69ee8f5f266f5-C001-2", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_291a6ac3f0c2d27ca69ee8f5f266f5_36", "text": "This paper proposes an expansion of set of primitive constraints available within the Primitive Optimality Theory framework (Eisner, 1997a) ."}
{"sent_id": "291a6ac3f0c2d27ca69ee8f5f266f5-C001-10", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_291a6ac3f0c2d27ca69ee8f5f266f5_36", "text": "The same is true for the implementation described in Eisner (1997a) , although a proposal is given there for a method that might improve the situation."}
{"sent_id": "291a6ac3f0c2d27ca69ee8f5f266f5-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_291a6ac3f0c2d27ca69ee8f5f266f5_36", "text": "primitives of OTP without reference to ad hoc tiers, and proposes a formalization of these constraints that is compatible with the finite state model described in Eisner (1997a) and Albro (1998) ."}
{"sent_id": "291a6ac3f0c2d27ca69ee8f5f266f5-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_291a6ac3f0c2d27ca69ee8f5f266f5_36", "text": "2 Existential Implication 2.1 Motivation OWP as described in Eisner (1997a) provides some support for correspondence constraints (input-output only)."}
{"sent_id": "0ab60c5c9ace058a5fbe3bc2643cba-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_0ab60c5c9ace058a5fbe3bc2643cba_36", "text": "Apart from its application to machine translation, the encoder-decoder or sequence-to-sequence (seq2seq) paradigm has been successfully applied to monolingual text-to-text tasks including simplification (Nisioi et al., 2017) , paraphrasing (Mallinson et al., 2017) , style transfer (Jhamtani et al., 2017) , sarcasm interpretation (Peled and Reichart, 2017) , automated lyric annotation (Sterckx et al., 2017) and dialogue systems (Serban et al., 2016) ."}
{"sent_id": "0ab60c5c9ace058a5fbe3bc2643cba-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_0ab60c5c9ace058a5fbe3bc2643cba_36", "text": "Wikilarge is a collection of 296,402 automatically aligned complex and simple sentences from the ordinary and simple English Wikipedia corpora, used extensively in previous work (Wubben et al., 2012; Woodsend and Lapata, 2011; Zhang and Lapata, 2017; Nisioi et al., 2017) ."}
{"sent_id": "0ab60c5c9ace058a5fbe3bc2643cba-C001-86", "intents": ["@USE@"], "paper_id": "ABC_0ab60c5c9ace058a5fbe3bc2643cba_36", "text": "For comparison, we include the SMT-based model by (Wubben et al., 2012) , the NTS model by (Nisioi et al., 2017) and the EncDecA by (Zhang and Lapata, 2017) ."}
{"sent_id": "0ab60c5c9ace058a5fbe3bc2643cba-C001-82", "intents": ["@SIM@"], "paper_id": "ABC_0ab60c5c9ace058a5fbe3bc2643cba_36", "text": "work on neural text simplification (Zhang and Lapata, 2017; Nisioi et al., 2017) ."}
{"sent_id": "59a7c1fffdd45f8e152d060a4b9f50-C001-65", "intents": ["@USE@"], "paper_id": "ABC_59a7c1fffdd45f8e152d060a4b9f50_36", "text": "We use mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), and correlation (r) as well as relative MAE (MAER) and relative RAE (MRAER) to evaluate (Biçici, 2015; Biçici, 2013) ."}
{"sent_id": "97fd0f1ce3d4f510c1566d642e9d2c-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_97fd0f1ce3d4f510c1566d642e9d2c_37", "text": "Neural Machine Translation (Luong et al., 2015; Bahdanau et al., 2014; Johnson et al., 2017; Vaswani et al., 2017) has been receiving considerable attention in the recent years, given its superior performance without the demand of heavily hand crafted engineering efforts."}
{"sent_id": "97fd0f1ce3d4f510c1566d642e9d2c-C001-30", "intents": ["@USE@"], "paper_id": "ABC_97fd0f1ce3d4f510c1566d642e9d2c_37", "text": "where c t is the context vector, h enc and h dec are the hidden vectors generated by the encoder and decoder respectively, AttentionFunction(. , .) is the attention mechanism as shown in (Luong et al., 2015) and [. ; .] is the concatenation of two vectors."}
{"sent_id": "877a0b5b5d25b3849ca44ed42b8d6d-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_877a0b5b5d25b3849ca44ed42b8d6d_37", "text": "The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT) (Wu, 1997; Eisner, 2003; Galley et al., 2006; Chiang, 2007; Zhang et al., 2008) ."}
{"sent_id": "877a0b5b5d25b3849ca44ed42b8d6d-C001-26", "intents": ["@USE@"], "paper_id": "ABC_877a0b5b5d25b3849ca44ed42b8d6d_37", "text": "The rule extraction in current implementation can be considered as a combination of the ones in (Chiang, 2007) and (Zhang et al., 2008) ."}
{"sent_id": "877a0b5b5d25b3849ca44ed42b8d6d-C001-66", "intents": ["@USE@"], "paper_id": "ABC_877a0b5b5d25b3849ca44ed42b8d6d_37", "text": "For comparisons, we used the following three baseline systems: LSTSSG An in-house implementation of linguistically motivated STSSG based model similar to (Zhang et al., 2008) ."}
{"sent_id": "9340338e7cf8ff8de4db84b462dfe5-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_9340338e7cf8ff8de4db84b462dfe5_37", "text": "Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims (Vlachos and Riedel, 2015) ."}
{"sent_id": "9340338e7cf8ff8de4db84b462dfe5-C001-81", "intents": ["@USE@"], "paper_id": "ABC_9340338e7cf8ff8de4db84b462dfe5_37", "text": "We further validate the system by evaluating the ability of this fact checking system to make veracity assessments on simple numerical claims from the data set collected by (Vlachos and Riedel, 2015) ."}
{"sent_id": "9340338e7cf8ff8de4db84b462dfe5-C001-28", "intents": ["@EXT@"], "paper_id": "ABC_9340338e7cf8ff8de4db84b462dfe5_37", "text": "To learn a model to perform the KB look up (essentially a semantic parsing task), we extend the work of Vlachos and Riedel (2015) who used distant supervision (Mintz et al., 2009 ) to generate training data, obviating the need for manual labeling."}
{"sent_id": "24b38363d53468175e0274ac0b4fd3-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_24b38363d53468175e0274ac0b4fd3_37", "text": "Excitingly, the state of the art has recently shifted toward novel semi-supervised techniques such as the incorporation of word embeddings to represent the context of words and concepts (Tang et al., 2014b) ."}
{"sent_id": "24b38363d53468175e0274ac0b4fd3-C001-22", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_24b38363d53468175e0274ac0b4fd3_37", "text": "In previous work (Tang et al., 2014a; Tang et al., 2014b) sentiment-specific word embeddings have been used as features for identification of tweet-level sentiment but not phrase-level sentiment."}
{"sent_id": "24b38363d53468175e0274ac0b4fd3-C001-41", "intents": ["@USE@"], "paper_id": "ABC_24b38363d53468175e0274ac0b4fd3_37", "text": "For each strategy, class and dimension, we used the functions suggested by (Tang et al., 2014b ) (average, maximum and minimum), resulting in 2,400 features."}
{"sent_id": "24b38363d53468175e0274ac0b4fd3-C001-76", "intents": ["@DIF@"], "paper_id": "ABC_24b38363d53468175e0274ac0b4fd3_37", "text": "Contrary to the approach by (Tang et al., 2014b) , we didn't integrate the sentiment information in the word embeddings training process, but rather the sentiment-specific nature of the embeddings was reflected in the choice of different training datasets, yielding different word embedding features for positive and negative tweets."}
{"sent_id": "dcc866dcfb5f9233170d633d052e8b-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_dcc866dcfb5f9233170d633d052e8b_37", "text": "For example, (Chiang, 2007) adopts a CKY style span-based decoding while (Liu et al., 2006 ) applies a linguistically syntax node based bottom-up decoding, which are difficult to integrate."}
{"sent_id": "dcc866dcfb5f9233170d633d052e8b-C001-67", "intents": ["@SIM@"], "paper_id": "ABC_dcc866dcfb5f9233170d633d052e8b_37", "text": "FSCFG An in-house implementation of purely formally SCFG based model similar to (Chiang, 2007) ."}
{"sent_id": "e29c7551ea78cb425054963489e1b9-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_e29c7551ea78cb425054963489e1b9_37", "text": "Salesky et al. (2018) introduced a set of fluent references 1 for Fisher Spanish-English, enabling a new task: end-to-end training and evaluation against fluent references."}
{"sent_id": "e29c7551ea78cb425054963489e1b9-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_e29c7551ea78cb425054963489e1b9_37", "text": "Further, corpora can have different translation and annotation schemes: for example for Fisher Spanish-English, translated using Mechanical Turk, Salesky et al. (2018) found 268 unique filler words due to spelling and casing."}
{"sent_id": "e29c7551ea78cb425054963489e1b9-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_e29c7551ea78cb425054963489e1b9_37", "text": "Salesky et al. (2018) introduced a new set of fluent reference translations collected on Mechanical Turk."}
{"sent_id": "e29c7551ea78cb425054963489e1b9-C001-26", "intents": ["@USE@"], "paper_id": "ABC_e29c7551ea78cb425054963489e1b9_37", "text": "For our experiments, we use Fisher Spanish speech (Graff et al.) and with two sets of English translations (Salesky et al., 2018; Post et al., 2013) ."}
{"sent_id": "e29c7551ea78cb425054963489e1b9-C001-126", "intents": ["@EXT@"], "paper_id": "ABC_e29c7551ea78cb425054963489e1b9_37", "text": "Using clean references for disfluent data collected by Salesky et al. (2018) , we extend their text baseline to speech input and provide first results for direct generation of fluent text from noisy disfluent speech."}
{"sent_id": "32ca78f6e01b7732a4bf573b91fbfe-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_32ca78f6e01b7732a4bf573b91fbfe_37", "text": "An important advance in this area is the development of the word2vec technique [4] , which has proved to be an effective approach in Twitter sentiment classification [5] ."}
{"sent_id": "32ca78f6e01b7732a4bf573b91fbfe-C001-43", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_32ca78f6e01b7732a4bf573b91fbfe_37", "text": "Word embeddings proved to be effective representations in the tasks of sentiment analysis [5, 8, 9 ] and text classification [10] ."}
{"sent_id": "32ca78f6e01b7732a4bf573b91fbfe-C001-68", "intents": ["@USE@"], "paper_id": "ABC_32ca78f6e01b7732a4bf573b91fbfe_37", "text": "To improve sentiment citation classification results, I trained polarity specific word embeddings (PS-Embeddings), which were inspired by the Sentiment-Specific Word Embedding [5] ."}
{"sent_id": "32ca78f6e01b7732a4bf573b91fbfe-C001-113", "intents": ["@DIF@"], "paper_id": "ABC_32ca78f6e01b7732a4bf573b91fbfe_37", "text": "However, unlike the outcomes in the paper of [5] , where they concluded that sentiment specific word embeddings performed best, integrating polarity information did not improve the result in this experiment."}
{"sent_id": "6bb7d5f16861470214626c1cc497bb-C001-46", "intents": ["@USE@"], "paper_id": "ABC_6bb7d5f16861470214626c1cc497bb_37", "text": "The score on UW with MAE was obtained by Stanovsky et al. (2017) , while the other scores were obtained by ."}
{"sent_id": "6bb7d5f16861470214626c1cc497bb-C001-59", "intents": ["@USE@"], "paper_id": "ABC_6bb7d5f16861470214626c1cc497bb_37", "text": "Focusing on the restricted set, we perform detailed error analysis of the outputs of the rule-based and hybrid biLSTM models, which achieved the best Figure 3: Pearson r correlation and Mean Absolute Error (MAE) on All -2.0 baseline, Rule-based annotator (Stanovsky et al., 2017) , and three biLSTM models in ."}
{"sent_id": "6bb7d5f16861470214626c1cc497bb-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_6bb7d5f16861470214626c1cc497bb_37", "text": "Rule-based model Stanovsky et al. (2017) proposed a rule-based model based on a deterministic algorithm based on TruthTeller (Lotan et al., 2013) , which uses a top-down approach on a de- pendency tree and predicts speaker commitment score in [−3, 3] according to the implicative signatures (Karttunen, 2012) of the predicates, and whether the predicates are under the scope of negation and uncertainty modifiers."}
{"sent_id": "db1fd6f10a3ee22e22093d50395217-C001-39", "intents": ["@DIF@"], "paper_id": "ABC_db1fd6f10a3ee22e22093d50395217_37", "text": "Please note that the way we categorised an abstract as structured or unstructured might be a bit different from previous approaches by Kim et al. (2011) and Verbeke et al. 2012 ."}
{"sent_id": "db1fd6f10a3ee22e22093d50395217-C001-54", "intents": ["@DIF@"], "paper_id": "ABC_db1fd6f10a3ee22e22093d50395217_37", "text": "Using sentence ordering labels for unstructured abstracts is the main difference compared to earlier methods (Kim et al., 2011; Verbeke et al., 2012) ."}
{"sent_id": "db1fd6f10a3ee22e22093d50395217-C001-100", "intents": ["@DIF@"], "paper_id": "ABC_db1fd6f10a3ee22e22093d50395217_37", "text": "Our system outperformed earlier existing state-of-art systems (Kim et al., 2011; Verbeke et al., 2012) ."}
{"sent_id": "6fd0c2fbbe0c7fb669f2618f4d01f7-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_6fd0c2fbbe0c7fb669f2618f4d01f7_37", "text": "Wikipedia is an abundant resource for generation of different types of multilingual, cross lingual, cross script and language independent corpus, etc., its markup has been used extensively to automatically generate NER annotated corpus for training machine learning models [4] [5] [6] [11] [12] [13] [14] 19] ."}
{"sent_id": "6fd0c2fbbe0c7fb669f2618f4d01f7-C001-66", "intents": ["@SIM@"], "paper_id": "ABC_6fd0c2fbbe0c7fb669f2618f4d01f7_37", "text": "This approach is similar to Nothman et al (2008) [4] to generate the NER data from wikilinks."}
{"sent_id": "6fd0c2fbbe0c7fb669f2618f4d01f7-C001-106", "intents": ["@SIM@"], "paper_id": "ABC_6fd0c2fbbe0c7fb669f2618f4d01f7_37", "text": "The MISC F-score is expectedly low, in agreement with the results of Nothman et al (2008) [4] ."}
{"sent_id": "782517ae7688cf18b4bca37a8087dd-C001-8", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_782517ae7688cf18b4bca37a8087dd_37", "text": "Rao et al. (2018) identified short document length, informality of language, and heterogeneous relevance signals as main challenges in relevance modeling, and proposed a model specifically designed to handle these characteristics."}
{"sent_id": "782517ae7688cf18b4bca37a8087dd-C001-84", "intents": ["@BACK@"], "paper_id": "ABC_782517ae7688cf18b4bca37a8087dd_37", "text": "To the best of our knowledge, Rao et al. (2018) is the best neural model to date, and there are no neural models from TREC evaluations for further comparison."}
{"sent_id": "782517ae7688cf18b4bca37a8087dd-C001-73", "intents": ["@USE@"], "paper_id": "ABC_782517ae7688cf18b4bca37a8087dd_37", "text": "MP-HCNN (Rao et al., 2018) is the first neural model that captures the characteristics of social media domain."}
{"sent_id": "28805bfa8f8b847110664d7e05b1b3-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_28805bfa8f8b847110664d7e05b1b3_37", "text": "Previously, a graph based framework has been proposed that models word semantic similarity from parsed text (Minkov and Cohen, 2008) ."}
{"sent_id": "28805bfa8f8b847110664d7e05b1b3-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_28805bfa8f8b847110664d7e05b1b3_37", "text": "PCW is a graph walk variant proposed recently that is intended to bias the random walk process to follow meaningful edge sequences (paths) (Minkov and Cohen, 2008) ."}
{"sent_id": "28805bfa8f8b847110664d7e05b1b3-C001-78", "intents": ["@USE@"], "paper_id": "ABC_28805bfa8f8b847110664d7e05b1b3_37", "text": "In conducting the constrained walk, we applied a threshold of 0.5 to truncate paths associated with lower probability of reaching a relevant response, following on previous work (Minkov and Cohen, 2008) ."}
{"sent_id": "f326a3e2a5e349ce84b0a759f8e0b2-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_f326a3e2a5e349ce84b0a759f8e0b2_37", "text": "This approach is well suited for non-interactive, static contexts, but recently, there has been increased interest in generation for situated dialog (Stoia, 2007; ."}
{"sent_id": "f326a3e2a5e349ce84b0a759f8e0b2-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_f326a3e2a5e349ce84b0a759f8e0b2_37", "text": "Stoia (2007) studies instruction giving in a virtual environment and finds that references to target objects are often not made when they first become visible."}
{"sent_id": "f326a3e2a5e349ce84b0a759f8e0b2-C001-104", "intents": ["@DIF@"], "paper_id": "ABC_f326a3e2a5e349ce84b0a759f8e0b2_37", "text": "Stoia (2007) as well as have addressed this question, but their approaches only make a choice between generating an instruction to move or a uniquely identifying referring expression."}
{"sent_id": "f326a3e2a5e349ce84b0a759f8e0b2-C001-111", "intents": ["@FUT@", "@EXT@"], "paper_id": "ABC_f326a3e2a5e349ce84b0a759f8e0b2_37", "text": "We are planning on building on the work by Stoia (2007) on using machine learning techniques to develop a model that takes into account various contextual factors and on the work by Thompson (2009) on generating references in installments."}
{"sent_id": "193d388c3f4c346cb62711f3f04c0f-C001-96", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_193d388c3f4c346cb62711f3f04c0f_37", "text": "The AG corpus (Zhang et al., 2015; Conneau et al., 2016) contains categorized news articles from more than 2,000 news outlets on the web."}
{"sent_id": "193d388c3f4c346cb62711f3f04c0f-C001-19", "intents": ["@DIF@"], "paper_id": "ABC_193d388c3f4c346cb62711f3f04c0f_37", "text": "Compared to deep Convolutional Networks (CNN) for text (Zhang et al., 2015; Conneau et al., 2016) , the MVN strategy emphasizes network width over depth."}
{"sent_id": "193d388c3f4c346cb62711f3f04c0f-C001-101", "intents": ["@DIF@"], "paper_id": "ABC_193d388c3f4c346cb62711f3f04c0f_37", "text": "These results show that the bag-of-words MVN outperforms the state-of-theart accuracy obtained by the non-neural n-gram TFIDF approach (Zhang et al., 2015) , as well as several very deep CNNs (Conneau et al., 2016) ."}
{"sent_id": "c86271049ebbcf8eafe781a0af6a98-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_c86271049ebbcf8eafe781a0af6a98_37", "text": "While some work focused on the representation of relations on the basis of triplets belonging to the KB [3] , other work proposed to enhance the distributed representation of words for representing their underlying concepts by taking into consideration the structure of the KB graph (e.g., concepts in the same category or their relationships with other concepts) [6, 18, 19] ."}
{"sent_id": "c86271049ebbcf8eafe781a0af6a98-C001-70", "intents": ["@BACK@"], "paper_id": "ABC_c86271049ebbcf8eafe781a0af6a98_37", "text": "A first work [6] proposes a \"retrofitting\" technique consisting in a leveraging of lexicon-derived relational information, namely adjacent words of concepts, to refine their associated word embeddings."}
{"sent_id": "c86271049ebbcf8eafe781a0af6a98-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_c86271049ebbcf8eafe781a0af6a98_37", "text": "In contrast to [6] , other work [18, 19] proposes an endto-end oriented approach that rather adjusts the objective function of the neural language model."}
{"sent_id": "c86271049ebbcf8eafe781a0af6a98-C001-100", "intents": ["@BACK@"], "paper_id": "ABC_c86271049ebbcf8eafe781a0af6a98_37", "text": "On the other hand, the semantic layer could be built by the representation of concepts (and their relationships) extracted from the plain text through a concept embedding [6] or a richer embedding representation of a KB sub-graph, as suggested in [2] ."}
{"sent_id": "c86271049ebbcf8eafe781a0af6a98-C001-96", "intents": ["@DIF@"], "paper_id": "ABC_c86271049ebbcf8eafe781a0af6a98_37", "text": "While a naive approach would be to exploit the concept embeddings learned from the KB distributed representation [6, 18] as input of the deep neural network, we believe that a hybrid representation of the distributional semantic (namely, word embeddings) and the symbolic semantics (namely, concept embeddings taking into account the graph structure) would allow enhancing the document-query matching."}
{"sent_id": "1a17ae4e5c8ea9e605f129aa96a6ee-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_1a17ae4e5c8ea9e605f129aa96a6ee_37", "text": "Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (Cohn et al., 2009; Cohn and Blunsom, 2010; Post and Gildea, 2009) ."}
{"sent_id": "1a17ae4e5c8ea9e605f129aa96a6ee-C001-45", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_1a17ae4e5c8ea9e605f129aa96a6ee_37", "text": "The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (Cohn et al., 2009; Cohn and Blunsom, 2010) ."}
{"sent_id": "1a17ae4e5c8ea9e605f129aa96a6ee-C001-68", "intents": ["@USE@"], "paper_id": "ABC_1a17ae4e5c8ea9e605f129aa96a6ee_37", "text": "Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011) ."}
{"sent_id": "1a17ae4e5c8ea9e605f129aa96a6ee-C001-72", "intents": ["@USE@"], "paper_id": "ABC_1a17ae4e5c8ea9e605f129aa96a6ee_37", "text": "It is then straightforward to represent this TSG as a CFG using the Goodman transform (Goodman, 2002; Cohn and Blunsom, 2010) ."}
{"sent_id": "1a17ae4e5c8ea9e605f129aa96a6ee-C001-82", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_1a17ae4e5c8ea9e605f129aa96a6ee_37", "text": "We compare our system (referred to as TIG) to our implementation of the TSG system of (Cohn and Blunsom, 2010 ) (referred to as TSG) and the constrained TIG variant of (Shindo et al., 2011 ) (referred to as TIG 0 )."}
{"sent_id": "5f2f4087b80aa8dc3a5ccdb686983d-C001-43", "intents": ["@USE@"], "paper_id": "ABC_5f2f4087b80aa8dc3a5ccdb686983d_37", "text": "This score is .79; see DeVault et al. (2011b) ."}
{"sent_id": "9af4a895dd4b45bb3827c74bdc7f05-C001-22", "intents": ["@MOT@"], "paper_id": "ABC_9af4a895dd4b45bb3827c74bdc7f05_37", "text": "Very recently, however, Somasundaran and Chodorow (2014) and Somasundaran et al. (2015) Even if these results were extremely promising, they leave a number of questions unanswered."}
{"sent_id": "9af4a895dd4b45bb3827c74bdc7f05-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_9af4a895dd4b45bb3827c74bdc7f05_37", "text": "In order to extract more information from the distribution of the ASs in each text than the mean or the median, Durrant and Schmitt (2009) and Somasundaran et al. (2015) used a standard procedure in descriptive statistics and automatic information processing known as discretization, binning or quantization (Garcia et al., 2013) ."}
{"sent_id": "9af4a895dd4b45bb3827c74bdc7f05-C001-106", "intents": ["@DIF@"], "paper_id": "ABC_9af4a895dd4b45bb3827c74bdc7f05_37", "text": "Unlike Somasundaran et al. (2015) , I only used bigrams' collocational features."}
{"sent_id": "dcfad33f4322738906e2fdffe2e721-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_dcfad33f4322738906e2fdffe2e721_37", "text": "In previous work , we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980) ), using charalign (Church, 1993) , a method that looks for character sequences that are the same in both the source and target."}
{"sent_id": "dcfad33f4322738906e2fdffe2e721-C001-26", "intents": ["@USE@"], "paper_id": "ABC_dcfad33f4322738906e2fdffe2e721_37", "text": "These tables were computed from a small fragment of the Canadian Hansards that has been used in a number of other studies: Church (1993) and Simard et al (1992 show where the concordances were found in the texts."}
{"sent_id": "2d7e98487698b0b6ae85f052402f7c-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_2d7e98487698b0b6ae85f052402f7c_38", "text": "Prosodic Cues for DA Recognition: It has also been noted that prosodic knowledge plays a major role in DA identification for certain DA types Stolcke et al., 2000) ."}
{"sent_id": "2d7e98487698b0b6ae85f052402f7c-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_2d7e98487698b0b6ae85f052402f7c_38", "text": "Lexical, Prosodic, and Syntactic Cues: Many studies have been carried out to find out the lexical, prosodic and syntactic cues (Stolcke et al., 2000; Surendran and Levow, 2006; O'Shea et al., 2012; Yang et al., 2014) ."}
{"sent_id": "772cdd4263cf8979a54cc5196e5853-C001-7", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_772cdd4263cf8979a54cc5196e5853_38", "text": "Both phrase-structure and dependency parsers have developed a lot in the last decade (Nivre et al., 2004; McDonald et al., 2005; Charniak and Johnson, 2005; Huang, 2008) ."}
{"sent_id": "772cdd4263cf8979a54cc5196e5853-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_772cdd4263cf8979a54cc5196e5853_38", "text": "The most successful supervised phrase-structure parsers are feature-rich discriminative parsers which heavily depend on an underlying PCFG (Charniak and Johnson, 2005; Huang, 2008) ."}
{"sent_id": "772cdd4263cf8979a54cc5196e5853-C001-74", "intents": ["@BACK@"], "paper_id": "ABC_772cdd4263cf8979a54cc5196e5853_38", "text": "For a detailed description of the training and supporting algorithms please refer to Charniak and Johnson (2005) and Huang (2008) ."}
{"sent_id": "772cdd4263cf8979a54cc5196e5853-C001-73", "intents": ["@EXT@"], "paper_id": "ABC_772cdd4263cf8979a54cc5196e5853_38", "text": "Our oracle extraction method is an extension of Huang (2008)'s dynamic programing procedure which takes into consideration POS tag and grammatical function matches as well and selects hyperedges with higher posterior probability for tie-breaking."}
{"sent_id": "772cdd4263cf8979a54cc5196e5853-C001-118", "intents": ["@SIM@"], "paper_id": "ABC_772cdd4263cf8979a54cc5196e5853_38", "text": "Regarding the two discriminative approaches, our findings are similar to Huang (2008) , i.e. the packed forest-based and n-best list procedures achieved similar results by using only local features."}
{"sent_id": "73d4518e44f14a725a28e86de96963-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_73d4518e44f14a725a28e86de96963_38", "text": "Information related to affect has been also exploited (Reyes et al., 2013; Barbieri et al., 2014; Hernández Farías et al., 2015) ."}
{"sent_id": "73d4518e44f14a725a28e86de96963-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_73d4518e44f14a725a28e86de96963_38", "text": "In ) the robustness of emotIDM was assessed over different Twitter state-of-the-art corpora for irony detection (Reyes et al., 2013; Barbieri et al., 2014; Mohammad et al., 2015; Ptáček et al., 2014; Riloff et al., 2013) ."}
{"sent_id": "73d4518e44f14a725a28e86de96963-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_73d4518e44f14a725a28e86de96963_38", "text": "Furthermore, Barbieri et al. (2014) exploited a feature for alerting the existence of an URL in a tweet; such feature was ranked among the most discriminative ones according to an information gain analysis."}
{"sent_id": "73d4518e44f14a725a28e86de96963-C001-74", "intents": ["@USE@"], "paper_id": "ABC_73d4518e44f14a725a28e86de96963_38", "text": "We exploited the corpora developed by (Reyes et al., 2013) , (Barbieri et al., 2014) , (Mohammad et al., 2015) , (Ptáček et al., 2014) , (Riloff et al., 2013) , (Ghosh et al., 2015) , (Karoui et al., 2017) , and (Sulis et al., 2016) ."}
{"sent_id": "c5e1debe3fcab509737e092505a29e-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_c5e1debe3fcab509737e092505a29e_38", "text": "Koehn and Knight (2003) , Popovic and Ney (2004) and Popović et al. (2006) have demonstrated ways to handle this issue with morphological segmentation of words before training the SMT system."}
{"sent_id": "c5e1debe3fcab509737e092505a29e-C001-87", "intents": ["@BACK@"], "paper_id": "ABC_c5e1debe3fcab509737e092505a29e_38", "text": "Preordering of the source language sentence helps in the better alignment and decoding for English to Indian language (Ramanathan et al., 2008; Patel et al., 2013; Kunchukuttan et al., 2014) SMT."}
{"sent_id": "c5e1debe3fcab509737e092505a29e-C001-48", "intents": ["@USE@"], "paper_id": "ABC_c5e1debe3fcab509737e092505a29e_38", "text": "We have used source side reordering developed by Patel et al. (2013), and Ramanathan et al. (2008 We now discuss training and testing corpus from health, tourism and general domains for be-hi, mrhi, ta-hi, te-hi, and en-hi language pairs, followed by preprocessing, SMT system setup and evaluation metrics for experiments."}
{"sent_id": "a62f376adefad10c5fb8b6c08ebb63-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_a62f376adefad10c5fb8b6c08ebb63_38", "text": "Consequently, much work has been devoted to this task (Wu, 1997; Zens and Ney, 2003; Wellington et al., 2006; Macken, 2007; Søgaard and Kuhn, 2009) ."}
{"sent_id": "a62f376adefad10c5fb8b6c08ebb63-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_a62f376adefad10c5fb8b6c08ebb63_38", "text": "The task of estimating the consequences of the structural constraints imposed by a particular syntax-based formalism consists in finding what is often called \"empirical lower bounds\" on the coverage of the formalism (Wellington et al., 2006; Søgaard and Kuhn, 2009 )."}
{"sent_id": "a62f376adefad10c5fb8b6c08ebb63-C001-18", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_a62f376adefad10c5fb8b6c08ebb63_38", "text": "The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned (Wu, 1997; Zhang and Gildea, 2004; Wellington et al., 2006; Søgaard and Kuhn, 2009) ."}
{"sent_id": "a62f376adefad10c5fb8b6c08ebb63-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_a62f376adefad10c5fb8b6c08ebb63_38", "text": "Inside-out alignments were first described by Wu (1997) , and their frequency has been a matter of some debate (Lepage and Denoual, 2005; Wellington et al., 2006; Søgaard and Kuhn, 2009) ."}
{"sent_id": "fd7bae08fd3e69744a3980daa1a649-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_fd7bae08fd3e69744a3980daa1a649_38", "text": "Barkan (2017) pointed out that the skip-gram model with negative sampling, also known as word2vec (Mikolov et al., 2013b) , admits a Bayesian interpretation."}
{"sent_id": "fd7bae08fd3e69744a3980daa1a649-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_fd7bae08fd3e69744a3980daa1a649_38", "text": "The Bayesian skip-gram model (Barkan, 2017 ) is a probabilistic interpretation of word2vec (Mikolov et al., 2013b) ."}
{"sent_id": "fd7bae08fd3e69744a3980daa1a649-C001-43", "intents": ["@USE@"], "paper_id": "ABC_fd7bae08fd3e69744a3980daa1a649_38", "text": "When training the model, we resort to the heuristics proposed in (Mikolov et al., 2013b) to create artificial evidence for the negative examples (see Section 3.2 below)."}
{"sent_id": "fd7bae08fd3e69744a3980daa1a649-C001-54", "intents": ["@USE@"], "paper_id": "ABC_fd7bae08fd3e69744a3980daa1a649_38", "text": "Following Mikolov et al. (2013b) , we construct artificial evidence X − n for negative pairs by sampling from the noise distribution"}
{"sent_id": "2db25254f275303c41f1e7ab15a5e0-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_2db25254f275303c41f1e7ab15a5e0_38", "text": "Similar observations are made by Rutherford and Xue (2015) , who attempt to add automatically-labeled instances to improve supervised classification of implicit discourse relations."}
{"sent_id": "2db25254f275303c41f1e7ab15a5e0-C001-56", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_2db25254f275303c41f1e7ab15a5e0_38", "text": "Motivated by this idea, we require that sampled instances from the source domain have a cosine similarity of at least τ with at least one target domain instance (Rutherford and Xue, 2015) ."}
{"sent_id": "2db25254f275303c41f1e7ab15a5e0-C001-74", "intents": ["@SIM@"], "paper_id": "ABC_2db25254f275303c41f1e7ab15a5e0_38", "text": "In a pilot study we found that larger amounts of additional training data yielded no further improvements, which is consistent with the recent results of Rutherford and Xue (2015) ."}
{"sent_id": "2db25254f275303c41f1e7ab15a5e0-C001-109", "intents": ["@FUT@"], "paper_id": "ABC_2db25254f275303c41f1e7ab15a5e0_38", "text": "Future work will explore the combination of this approach with more sophisticated techniques for instance selection (Rutherford and Xue, 2015) and feature selection (Park and Cardie, 2012; Biran and McKeown, 2013) , while also tackling the more difficult problems of multi-class relation classification and fine-grained level-2 discourse relations."}
{"sent_id": "ad9b663ac88667c1b88767ca4b2f8f-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_ad9b663ac88667c1b88767ca4b2f8f_38", "text": "Target language side dependency structures have been successfully used in statistical machine translation (SMT) by Shen et al. (2008) and achieved state-of-the-art results as reported in the NIST 2008 Open MT Evaluation workshop and the NTCIR-9 Chinese-to-English patent translation task (Goto et al., 2011; Ma and Matsoukas, 2011) ."}
{"sent_id": "a5d7f5c5fed218149818463427d6a1-C001-9", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_a5d7f5c5fed218149818463427d6a1_38", "text": "However, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human-produced data (Bolukbasi et al., 2016; Caliskan et al., 2017) ."}
{"sent_id": "a5d7f5c5fed218149818463427d6a1-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_a5d7f5c5fed218149818463427d6a1_38", "text": "To mitigate bias from word embeddings, Bolukbasi et al. (2016) propose a post-processing method to project out the bias subspace from the pre-trained embeddings."}
{"sent_id": "a5d7f5c5fed218149818463427d6a1-C001-52", "intents": ["@DIF@"], "paper_id": "ABC_a5d7f5c5fed218149818463427d6a1_38", "text": "Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one (Bolukbasi et al., 2016) ."}
{"sent_id": "c9d9997b61974a537915a2c90af3cf-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_c9d9997b61974a537915a2c90af3cf_38", "text": "It is a challenging research area and state-of-the-art performance of automatic sentiment prediction has room for improvement compared to human performance (Zadeh et al., 2018a) ."}
{"sent_id": "c9d9997b61974a537915a2c90af3cf-C001-85", "intents": ["@USE@"], "paper_id": "ABC_c9d9997b61974a537915a2c90af3cf_38", "text": "We use the CMU Multimodal Data Software Development Kit (SDK) (Zadeh et al., 2018a) to load and pre-process the CMU-MOSI database, which splits the 2199 opinion segments into training (1283 segments), validation (229 segments), and test (686 segments) sets."}
{"sent_id": "c9d9997b61974a537915a2c90af3cf-C001-131", "intents": ["@SIM@"], "paper_id": "ABC_c9d9997b61974a537915a2c90af3cf_38", "text": "3 The verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases (e.g., Zadeh et al. (2018a) )."}
{"sent_id": "e608068f472e7045b682f979fd5295-C001-27", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_e608068f472e7045b682f979fd5295_38", "text": "Another closely related work is our own previously proposed method for leveraging on resources available for English to construct resources for a second language (Mihalcea et al., 2007) ."}
{"sent_id": "e608068f472e7045b682f979fd5295-C001-74", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_e608068f472e7045b682f979fd5295_38", "text": "More details about this data set are available in (Mihalcea et al., 2007) ."}
{"sent_id": "e608068f472e7045b682f979fd5295-C001-84", "intents": ["@DIF@"], "paper_id": "ABC_e608068f472e7045b682f979fd5295_38", "text": "Note that (Mihalcea et al., 2007 ) also proposed a corpusbased method for subjectivity classification; however that method is supervised and thus not directly comparable with the approach introduced in this paper."}
{"sent_id": "e608068f472e7045b682f979fd5295-C001-80", "intents": ["@UNSURE@"], "paper_id": "ABC_e608068f472e7045b682f979fd5295_38", "text": "In (Mihalcea et al., 2007) , a subjectivity lexicon was automatically obtained through the translation of the English subjectivity lexicon available in OpinionFinder."}
{"sent_id": "78a7ca27c5ca032116db12205af939-C001-55", "intents": ["@USE@"], "paper_id": "ABC_78a7ca27c5ca032116db12205af939_38", "text": "Similar to [16] , we apply a thresholding based bounding box (B-Box) generation method."}
{"sent_id": "484bc7c9c66bf4028eef4103beec7f-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_484bc7c9c66bf4028eef4103beec7f_38", "text": "Our previous work focused on only the segmentation part of the voice identification task (Brooke et al., 2012) ."}
{"sent_id": "484bc7c9c66bf4028eef4103beec7f-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_484bc7c9c66bf4028eef4103beec7f_38", "text": "Our work here is built on our earlier work (Brooke et al., 2012) ."}
{"sent_id": "484bc7c9c66bf4028eef4103beec7f-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_484bc7c9c66bf4028eef4103beec7f_38", "text": "Our approach to voice identification in The Waste Land consists first of identifying the boundaries of voice spans (Brooke et al., 2012) ."}
{"sent_id": "484bc7c9c66bf4028eef4103beec7f-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_484bc7c9c66bf4028eef4103beec7f_38", "text": "For a more detailed discussion of the feature set, see Brooke et al. (2012) ."}
{"sent_id": "484bc7c9c66bf4028eef4103beec7f-C001-62", "intents": ["@USE@"], "paper_id": "ABC_484bc7c9c66bf4028eef4103beec7f_38", "text": "For the automatic segmentation model, we use the settings from Brooke et al. (2012) ."}
{"sent_id": "f861e6590ff57225395e7d480c66e8-C001-80", "intents": ["@USE@"], "paper_id": "ABC_f861e6590ff57225395e7d480c66e8_38", "text": "For the CoNLL04 (Roth and Yih, 2004 ) EC task (assuming boundaries are given), we use the same splits as in Gupta et al. (2016) ; Adel and Schütze (2017) ."}
{"sent_id": "f861e6590ff57225395e7d480c66e8-C001-108", "intents": ["@UNSURE@"], "paper_id": "ABC_f861e6590ff57225395e7d480c66e8_38", "text": "Moreover, compared to the model of Gupta et al. (2016) that relies on complex features, the baseline model performs within a margin of 1% in terms of overall F 1 score."}
{"sent_id": "023a954d97b5d761b01f09bb242d19-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_023a954d97b5d761b01f09bb242d19_38", "text": "However, AMR annotation which requires a lot of human effort limits the outcome of data-driven approaches, one of which being neural network based methods [10, 3] ."}
{"sent_id": "023a954d97b5d761b01f09bb242d19-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_023a954d97b5d761b01f09bb242d19_38", "text": "NeuralAMR [10] has succeeded at both AMR parsing and sentence generation as the result of a bootstrapping training strategy on a 20-million-sentence unsupervised dataset."}
{"sent_id": "023a954d97b5d761b01f09bb242d19-C001-57", "intents": ["@EXT@"], "paper_id": "ABC_023a954d97b5d761b01f09bb242d19_38", "text": "Unlike the prior work [10] , in our model, the graphs pass through a much simpler pre-processing series which consists of variable removal, graph linearization, and infrequent word replacement."}
{"sent_id": "023a954d97b5d761b01f09bb242d19-C001-58", "intents": ["@EXT@"], "paper_id": "ABC_023a954d97b5d761b01f09bb242d19_38", "text": "For stripping the AMR text, we modified the depth-first-search traversal from the work of Kontas et al [10] in the way of marking the end of a path."}
{"sent_id": "b9a748ac201b2d8f5d52abd60aa018-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_b9a748ac201b2d8f5d52abd60aa018_38", "text": "Several studies have examined listenability for English learners (Kiyokawa 1990; Kotani et al. 2014; Kotani & Yoshimi 2016; Yoon et al. 2016) ; however, to the best of our knowledge, no previous studies on listenability for learners of Asian languages such as Chinese, Korean, and Japanese have been conducted."}
{"sent_id": "b9a748ac201b2d8f5d52abd60aa018-C001-18", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_b9a748ac201b2d8f5d52abd60aa018_38", "text": "Kotani et al. (2014) suggested the possibility of using different linguistic elements such as phonological features, and addressed this question by measuring listenability based on various linguistic features, including speech rate and the frequency of phonological modification patterns such as linking."}
{"sent_id": "b9a748ac201b2d8f5d52abd60aa018-C001-50", "intents": ["@USE@"], "paper_id": "ABC_b9a748ac201b2d8f5d52abd60aa018_38", "text": "Training/test data for a decision tree classification algorithm were constructed using the learner corpus of Kotani et al. (2014) , which includes learners' judgment of listenability."}
{"sent_id": "4d25b647a261a415d769532386265a-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_4d25b647a261a415d769532386265a_38", "text": "Recently, transformer networks have been shown to perform well for neural machine translation [11] and many other NLP tasks [12] ."}
{"sent_id": "4d25b647a261a415d769532386265a-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_4d25b647a261a415d769532386265a_38", "text": "Transformer layers [11] have the ability to learn long range relationships for many sequential classification tasks [12] ."}
{"sent_id": "4d25b647a261a415d769532386265a-C001-45", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_4d25b647a261a415d769532386265a_38", "text": "Figure(1) -left show the details of one transformer layer as proposed by [11] ."}
{"sent_id": "4d25b647a261a415d769532386265a-C001-91", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_4d25b647a261a415d769532386265a_38", "text": "Replacing the 1-D convolutional context in the decoder with sinusoidal positional embedding, as proposed in the baseline machine translation transformers [11] and adopted in [13, 15] , shows inferior WER performance."}
{"sent_id": "93a1f611592ce6aa5cde7538486f97-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_93a1f611592ce6aa5cde7538486f97_38", "text": "Meaning shift has recently been investigated with emphasis on neural language models (Kim et al., 2014; Kulkarni et al., 2015) ."}
{"sent_id": "93a1f611592ce6aa5cde7538486f97-C001-22", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_93a1f611592ce6aa5cde7538486f97_38", "text": "Neural language models for tracking semantic changes over time typically distinguish between two different training protocols-continuous training of models (Kim et al., 2014) where the model for each time span is initialized with the embeddings of its predecessor, and, alternatively, independent training with a mapping between models for different points in time (Kulkarni et al., 2015) ."}
{"sent_id": "7a5ebe06eebebabf4340f1cf583d86-C001-89", "intents": ["@DIF@"], "paper_id": "ABC_7a5ebe06eebebabf4340f1cf583d86_38", "text": "We could show that negative sampling outperforms hierarchical softmax both in terms of accuracy and reliability, especially 4 Kulkarni et al. (2015) compiled the following list based on prior work (Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014; Kim et al., 2014): card, sleep, parent, address, gay, mouse, king, checked, check, actually, supposed, guess, cell, headed, ass, mail, toilet, cock, bloody, nice and guy."}
{"sent_id": "a01a6ab7cf13c7916b1b3823a4b4de-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_a01a6ab7cf13c7916b1b3823a4b4de_38", "text": "We apply this approach on top of the same DIRECTL+ system as submitted last year (Jiampojamarn et al., 2010b) for English-to-Hindi machine transliteration."}
{"sent_id": "a01a6ab7cf13c7916b1b3823a4b4de-C001-93", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_a01a6ab7cf13c7916b1b3823a4b4de_38", "text": "For the NEWS 2009 and 2010 Shared Tasks, the discriminative DIRECTL+ system that incorporates many-to-many alignments, online maxmargin training and a phrasal decoder was shown to function well as a general string transduction tool; while originally designed for grapheme-tophoneme conversion, it produced excellent results for machine transliteration (Jiampojamarn et al., 2009; Jiampojamarn et al., 2010b) , leading us to re-use it here."}
{"sent_id": "a01a6ab7cf13c7916b1b3823a4b4de-C001-63", "intents": ["@EXT@"], "paper_id": "ABC_a01a6ab7cf13c7916b1b3823a4b4de_38", "text": "We tune the SVM's hyperparameter based on performance on the provided development data, and use the best DIRECTL+ settings established in the NEWS 2010 Shared Task (Jiampojamarn et al., 2010b) ."}
{"sent_id": "a01a6ab7cf13c7916b1b3823a4b4de-C001-80", "intents": ["@DIF@"], "paper_id": "ABC_a01a6ab7cf13c7916b1b3823a4b4de_38", "text": "This differs from the results of the successful application of romanization to Japanese (Jiampojamarn et al., 2010b) , demonstrating that it is not always possible to transfer an idea from one language to another."}
{"sent_id": "604807137ee5d9a6775821496c6af5-C001-39", "intents": ["@USE@"], "paper_id": "ABC_604807137ee5d9a6775821496c6af5_39", "text": "During generalization, it learns shared common representation (Blitzer et al., 2007; Ji et al., 2011; Pan et al., 2010) which minimizes the divergence between two collections."}
{"sent_id": "604807137ee5d9a6775821496c6af5-C001-40", "intents": ["@USE@"], "paper_id": "ABC_604807137ee5d9a6775821496c6af5_39", "text": "We leverage one of the widely used structural correspondence learning (SCL) approach (Blitzer et al., 2007) to compute shared representations."}
{"sent_id": "604807137ee5d9a6775821496c6af5-C001-92", "intents": ["@DIF@"], "paper_id": "ABC_604807137ee5d9a6775821496c6af5_39", "text": "Table 2 shows that DA module of SODA outperforms 1) a widely used domain adaptation technique , namely, structural correspondence learning (SCL) (Blitzer et al., 2007; Blitzer et al., 2006) , 2) the baseline (BL) where a classifier trained on one domain is applied on another domain, and 3) the in-domain classifier."}
{"sent_id": "f6a35ed1ec0c01d3e9faa1ec3d8478-C001-36", "intents": ["@USE@"], "paper_id": "ABC_f6a35ed1ec0c01d3e9faa1ec3d8478_39", "text": "We started with the Argument Quality (AQ) regressor from [16] , which predicts a quality score for each sentence."}
{"sent_id": "f6a35ed1ec0c01d3e9faa1ec3d8478-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_f6a35ed1ec0c01d3e9faa1ec3d8478_39", "text": "Swanson et al.(2015) created a large corpus consisting of 109,074 posts on the topics gay marriage (GM, 22425 posts), gun control (GC, 38102 posts), death penalty (DP, 5283 posts) by combining the Internet Argument Corpus(IAC) [17] , with dialogues from online debate forums 1 [16] ."}
{"sent_id": "f6a35ed1ec0c01d3e9faa1ec3d8478-C001-39", "intents": ["@EXT@"], "paper_id": "ABC_f6a35ed1ec0c01d3e9faa1ec3d8478_39", "text": "had improved upon the AQ predictor from [16] , giving a much larger and diverse corpus [12] ."}
{"sent_id": "aebac57baf260be18945feba38d6a1-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_aebac57baf260be18945feba38d6a1_39", "text": "Decompounders have been used successfully in Information Retrieval (Braschler and Ripplinger, 2004) , Machine Translation (Brown, 2002; Koehn and Knight, 2003) and Speech Recognition (Adda-Decker et al., 2000) ."}
{"sent_id": "aebac57baf260be18945feba38d6a1-C001-27", "intents": ["@USE@"], "paper_id": "ABC_aebac57baf260be18945feba38d6a1_39", "text": "By randomly sampling keywords we would get few compounds (as their frequency is small compared to that of non-compounds), so we have proceeded in the following way to ensure that the gold-standards contain a substantial amount of compounds: we started by building a very naive decompounder that splits a word in several parts using a frequency-based compound splitting method (Koehn and Knight, 2003) ."}
{"sent_id": "aebac57baf260be18945feba38d6a1-C001-47", "intents": ["@USE@"], "paper_id": "ABC_aebac57baf260be18945feba38d6a1_39", "text": "When those resources are not available, the most common methods used for compound splitting are using features such as the geometric mean of the frequencies of compound parts in a corpus, as in Koehn and Knight (2003) 's back-off method, or learning a language model from a corpus and estimating the probability of each sequence of possible compound parts (Schiller, 2005; Marek, 2006) ."}
{"sent_id": "9bab4741e6b9f132c2851bae3a3cf4-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_9bab4741e6b9f132c2851bae3a3cf4_39", "text": "An interesting study on feature selection for Chinese [10] has considered topical features as well as local collocational, syntactic, and semantic features using the maximum entropy model."}
{"sent_id": "9bab4741e6b9f132c2851bae3a3cf4-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_9bab4741e6b9f132c2851bae3a3cf4_39", "text": "In Dang's [10] work, collocational features refer to the local PoS information and bi-gram co-occurrences of words within 2 positions of the ambiguous word."}
{"sent_id": "9bab4741e6b9f132c2851bae3a3cf4-C001-67", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_9bab4741e6b9f132c2851bae3a3cf4_39", "text": "Early researches have proven that using lexical statistical information, such as bi-gram co-occurrences was sufficient to produce close to the best results [10] for Chinese WSD."}
{"sent_id": "4a093e0ca9a499c19e4721366d88f6-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_4a093e0ca9a499c19e4721366d88f6_39", "text": "Much of the focus for mitigating unintended bias in NLP is either targeted at reducing gender stereotypes in text (Bolukbasi et al., 2016b,a; Zhao et al., 2017; Zhang et al., 2018) , or inequality of sentiment or toxicity for various protected groups (Caliskan-Islam et al., 2016; Bakarov, 2018; Dixon et al.; Garg et al., 2018; Kiritchenko and Mohammad, 2018) ."}
{"sent_id": "4a093e0ca9a499c19e4721366d88f6-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_4a093e0ca9a499c19e4721366d88f6_39", "text": "(Bolukbasi et al., 2016b ) defines a useful metric for identifying gender bias and (Caliskan-Islam et al., 2016) defines a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text."}
{"sent_id": "4a093e0ca9a499c19e4721366d88f6-C001-39", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_4a093e0ca9a499c19e4721366d88f6_39", "text": "Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman (Bolukbasi et al., 2016a) , or European American names vs. African American names (Caliskan-Islam et al., 2016) ."}
{"sent_id": "4a093e0ca9a499c19e4721366d88f6-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_4a093e0ca9a499c19e4721366d88f6_39", "text": "GloVe and Word2vec embeddings have been shown to contain unintended bias in (Bolukbasi et al., 2016a; Caliskan-Islam et al., 2016) ."}
{"sent_id": "4a093e0ca9a499c19e4721366d88f6-C001-90", "intents": ["@SIM@"], "paper_id": "ABC_4a093e0ca9a499c19e4721366d88f6_39", "text": "First, we compare the RNSB metric for 3 pretrained word embeddings, showing that our metric is consistent with other word embedding analysis like WEAT (Caliskan-Islam et al., 2016) ."}
{"sent_id": "4a093e0ca9a499c19e4721366d88f6-C001-99", "intents": ["@SIM@"], "paper_id": "ABC_4a093e0ca9a499c19e4721366d88f6_39", "text": "Although the RNSB metric is not directly comparable to WEAT scores, these results are still consistent with some of the bias predicted by (Caliskan-Islam et al., 2016) ."}
{"sent_id": "4498072885df2a126e2db553cf3aca-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_4498072885df2a126e2db553cf3aca_39", "text": "An important insight from work on distributional methods is that the definition of context is often critical to the success of a system (Shwartz et al., 2017) ."}
{"sent_id": "4498072885df2a126e2db553cf3aca-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_4498072885df2a126e2db553cf3aca_39", "text": "Most unsupervised distributional approaches for hypernymy detection are based on variants of the Distributional Inclusion Hypothesis (Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Lenci and Benotto, 2012; Shwartz et al., 2017) ."}
{"sent_id": "4498072885df2a126e2db553cf3aca-C001-84", "intents": ["@USE@"], "paper_id": "ABC_4498072885df2a126e2db553cf3aca_39", "text": "This allowed us to use the same metric on all detection benchmarks, and is consistent with evaluations in Shwartz et al. (2017) ."}
{"sent_id": "4498072885df2a126e2db553cf3aca-C001-110", "intents": ["@USE@"], "paper_id": "ABC_4498072885df2a126e2db553cf3aca_39", "text": "Distributional models: For the distributional baselines, we employ the large, sparse distributional space of Shwartz et al. (2017) , which is computed from UkWaC and Wikipedia, and is known to have strong performance on several of the detection tasks."}
{"sent_id": "e59bd02bb560d80ce08dfcd6b35317-C001-38", "intents": ["@USE@"], "paper_id": "ABC_e59bd02bb560d80ce08dfcd6b35317_39", "text": "In this paper, we explore two datasets bbc and wikilow (Greene et al., 2014) with different document size and corpus size."}
{"sent_id": "e59bd02bb560d80ce08dfcd6b35317-C001-131", "intents": ["@SIM@"], "paper_id": "ABC_e59bd02bb560d80ce08dfcd6b35317_39", "text": "This complements previous work by Greene et al. (2014) who investigated how topic stability is influenced by number of topics over noisefree corpora."}
{"sent_id": "0d06c8509ebbdc61985bebcdb26e6c-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_0d06c8509ebbdc61985bebcdb26e6c_39", "text": "In a similar work, Mnih et al. [13] proposed to use Noise Contrastive Estimation (NCE) [14] to speed-up the training."}
{"sent_id": "653327ecbc925624d509c679fbe0ba-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_653327ecbc925624d509c679fbe0ba_39", "text": "Recently, the other line has studied to pre-train a language model over large corpus to learn the inherent word-level knowledge in an unsupervised way [4, 8] , which achieves very promising performance."}
{"sent_id": "653327ecbc925624d509c679fbe0ba-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_653327ecbc925624d509c679fbe0ba_39", "text": "Pre-trained language model such as BERT and GPT [4, 8] is also used as a kind of commonsense knowledge source."}
{"sent_id": "653327ecbc925624d509c679fbe0ba-C001-76", "intents": ["@USE@"], "paper_id": "ABC_653327ecbc925624d509c679fbe0ba_39", "text": "Following [4] , we first convert the concept to a set of BPE tokens tokens A and tokens B, with beginning index i and j in the input sequence respectively."}
{"sent_id": "653327ecbc925624d509c679fbe0ba-C001-97", "intents": ["@USE@"], "paper_id": "ABC_653327ecbc925624d509c679fbe0ba_39", "text": "We use the uncased BERT(base) [4] as pre-trained language model."}
{"sent_id": "7202fd7fe7e776362b126f7cbf0bf3-C001-30", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_7202fd7fe7e776362b126f7cbf0bf3_39", "text": "Background noise is used for augmentation in [8, 9] to improve performance on noisy speech."}
{"sent_id": "7202fd7fe7e776362b126f7cbf0bf3-C001-38", "intents": ["@SIM@"], "paper_id": "ABC_7202fd7fe7e776362b126f7cbf0bf3_39", "text": "The end-to-end model structure used in this work is very similar to the model architecture of Deep Speech 2 (DS2) [9] ."}
{"sent_id": "7202fd7fe7e776362b126f7cbf0bf3-C001-147", "intents": ["@SIM@"], "paper_id": "ABC_7202fd7fe7e776362b126f7cbf0bf3_39", "text": "We would like to note that our model achieved comparable results with Amodei et al. [9] on LibriSpeech dataset, although our model is only trained only on the provided training set."}
{"sent_id": "a6b450d1113e0e6d3d2813c09d12a8-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_a6b450d1113e0e6d3d2813c09d12a8_39", "text": "Gurevych (2005) conducted experiments with a German translation of an English dataset (Rubenstein and Goodenough, 1965) , but argued that the dataset (Gur65) is too small (it contains only 65 noun pairs), and does not model SR."}
{"sent_id": "a6b450d1113e0e6d3d2813c09d12a8-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_a6b450d1113e0e6d3d2813c09d12a8_39", "text": "Gurevych (2005) proposed an alternative algorithm (PG) generating surrogate glosses by using a concept's relations within the hierarchy."}
{"sent_id": "a6b450d1113e0e6d3d2813c09d12a8-C001-72", "intents": ["@SIM@"], "paper_id": "ABC_a6b450d1113e0e6d3d2813c09d12a8_39", "text": "Our results on Gur65 using GermaNet are very close to those published by Gurevych (2005) , ranging from 0.69-0.75."}
{"sent_id": "65fbdd0397473763bca35376d581be-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_65fbdd0397473763bca35376d581be_39", "text": "The performance of the state-of-the-art systems has improved significantly (Horn et al., 2014; Siddharthan and Angrosh, 2014) ."}
{"sent_id": "65fbdd0397473763bca35376d581be-C001-55", "intents": ["@USE@"], "paper_id": "ABC_65fbdd0397473763bca35376d581be_39", "text": "We evaluated the performance of our algorithm on the Mechanical Turk Lexical Simplification Data Set (Horn et al., 2014) ."}
{"sent_id": "65fbdd0397473763bca35376d581be-C001-83", "intents": ["@USE@"], "paper_id": "ABC_65fbdd0397473763bca35376d581be_39", "text": "We evaluated the quality of syntactic simplification on the first 300 sentences in the Mechanical Turk Lexical Simplification Data Set (Horn et al., 2014) ."}
{"sent_id": "9ea99bf57e9113b2f03f2285741397-C001-28", "intents": ["@DIF@"], "paper_id": "ABC_9ea99bf57e9113b2f03f2285741397_39", "text": "By adding the generated sentences and incorporating syntactic information to the training data, we achieve better performance by 10% compared to an LSTM baseline [10] and 5% to the equivalent constraint."}
{"sent_id": "9ea99bf57e9113b2f03f2285741397-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_9ea99bf57e9113b2f03f2285741397_39", "text": "A multi-task learning approach was introduced to train the syntax representation of languages by constraining the language generator [10] ."}
{"sent_id": "9ea99bf57e9113b2f03f2285741397-C001-75", "intents": ["@SIM@"], "paper_id": "ABC_9ea99bf57e9113b2f03f2285741397_39", "text": "Next, we concatenate both vectors and use it as an input [x w |x p ] to an LSTM layer similar to [10] . 4."}
{"sent_id": "9ea99bf57e9113b2f03f2285741397-C001-82", "intents": ["@SIM@"], "paper_id": "ABC_9ea99bf57e9113b2f03f2285741397_39", "text": "The split of the dataset is identical to [10] and it is showed in Table 1 ."}
{"sent_id": "6cc36fef99fb1f25370175452f30b0-C001-24", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_6cc36fef99fb1f25370175452f30b0_39", "text": "There has been much recent work in attempting to convert native parser output into alternative representations for evaluation purposes, e.g. (Clark and Curran, 2007; Matsuzaki and Tsujii, 2008 )."}
{"sent_id": "6cc36fef99fb1f25370175452f30b0-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_6cc36fef99fb1f25370175452f30b0_39", "text": "There are three types of conversion schema: schemas which introduce nodes for lexical items; schemas which insert or elide PTB nodes for unary 3 Another possible approach has been taken by Matsuzaki and Tsujii (2008) , who convert HPSG analyses from a grammar automatically extracted from the PTB back into the PTB."}
{"sent_id": "6cc36fef99fb1f25370175452f30b0-C001-81", "intents": ["@SIM@"], "paper_id": "ABC_6cc36fef99fb1f25370175452f30b0_39", "text": "In addition, we have thrown further doubt on the possible use of the PTB for cross-framework parser evaluation, as recently suggested by Matsuzaki and Tsujii (2008) ."}
{"sent_id": "65f7546e2abfd74c0daa43c25ca63f-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_65f7546e2abfd74c0daa43c25ca63f_39", "text": "A linguistics constraint-driven generation approach such as equivalent constraint [6, 7] is not restrictive to languages with distinctive grammar structure."}
{"sent_id": "65f7546e2abfd74c0daa43c25ca63f-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_65f7546e2abfd74c0daa43c25ca63f_39", "text": "The synthetic code-switching generation approach was introduced by adapting equivalence constraint on monolingual sentence pairs during the decoding step on an automatic speech recognition (ASR) model [6] ."}
{"sent_id": "65f7546e2abfd74c0daa43c25ca63f-C001-62", "intents": ["@USE@"], "paper_id": "ABC_65f7546e2abfd74c0daa43c25ca63f_39", "text": "As our baseline, we compare our proposed method with three other models: (1) We use Seq2Seq with attention; (2) We generate sequences that satisfy Equivalence Constraint [6] ."}
{"sent_id": "4235dbd05a848d934f17f35894c051-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_4235dbd05a848d934f17f35894c051_39", "text": "PredPatt 1 (White et al., 2016 ) is a pattern-based framework for predicate-argument extraction."}
{"sent_id": "4235dbd05a848d934f17f35894c051-C001-16", "intents": ["@BACK@", "@MOT@", "@EXT@"], "paper_id": "ABC_4235dbd05a848d934f17f35894c051_39", "text": "However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences (White et al., 2016) , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt."}
{"sent_id": "fa641aca676761c79c0469c195f336-C001-51", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_fa641aca676761c79c0469c195f336_39", "text": "Around the same time, Islam et al. independently reached the same conclusion [11] ."}
{"sent_id": "fa641aca676761c79c0469c195f336-C001-92", "intents": ["@DIF@"], "paper_id": "ABC_fa641aca676761c79c0469c195f336_39", "text": "We only have 30 annotated passages at our disposal, whereas Islam et al. [11] had around 300. But Islam et al.'s dataset is not annotated in as fine-grained a fashion as ours."}
{"sent_id": "f7255360eacc4e2a4e8bea2f6ab1b0-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_f7255360eacc4e2a4e8bea2f6ab1b0_39", "text": "Hearst (1997) and Nomoto and Nitta (1994) detect this coherence through patterns of lexical cooccurrence."}
{"sent_id": "f7255360eacc4e2a4e8bea2f6ab1b0-C001-61", "intents": ["@DIF@"], "paper_id": "ABC_f7255360eacc4e2a4e8bea2f6ab1b0_39", "text": "A first qualitative evaluation of the method has been done with about 20 texts but without a formal protocol as in (Hearst, 1997) ."}
{"sent_id": "932a13e179da50c9189bd0c612cb9c-C001-9", "intents": ["@DIF@"], "paper_id": "ABC_932a13e179da50c9189bd0c612cb9c_39", "text": "It differs from similar algorithms that select translation correspondences explicitly at the document level (Fung and Cheung, 2004; Resnik and Smith, 2003; Snover et al., 2008; Munteanu and Marcu, 2005; Quirk et al., 2007; Utiyama and Isahara, 2003) ."}
{"sent_id": "932a13e179da50c9189bd0c612cb9c-C001-54", "intents": ["@USE@"], "paper_id": "ABC_932a13e179da50c9189bd0c612cb9c_39", "text": "• Sentence-level filter: The word-overlap filter in (Munteanu and Marcu, 2005) has been implemented: for a sentence pair (S, T ) to be considered parallel the ratio of the lengths of the two sentences has to be smaller than two."}
{"sent_id": "932a13e179da50c9189bd0c612cb9c-C001-64", "intents": ["@USE@"], "paper_id": "ABC_932a13e179da50c9189bd0c612cb9c_39", "text": "The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005) ."}
{"sent_id": "932a13e179da50c9189bd0c612cb9c-C001-92", "intents": ["@USE@"], "paper_id": "ABC_932a13e179da50c9189bd0c612cb9c_39", "text": "Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy."}
{"sent_id": "7c2586a172dc6f061817c3a8b3ebf0-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_7c2586a172dc6f061817c3a8b3ebf0_39", "text": "Pidgin English is one of the the most widely spoken languages in West Africa with roughly 75 million speakers estimated in Nigeria; and over 5 million speakers estimated in Ghana (Ogueji & Ahia, 2019) ."}
{"sent_id": "7c2586a172dc6f061817c3a8b3ebf0-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_7c2586a172dc6f061817c3a8b3ebf0_39", "text": "Previous works on unsupervised neural machine translation for Pidgin English constructed a monolingual corpus (Ogueji & Ahia, 2019) , and achieved a BLEU score of 5.18 from English to Pidgin."}
{"sent_id": "7c2586a172dc6f061817c3a8b3ebf0-C001-31", "intents": ["@SIM@"], "paper_id": "ABC_7c2586a172dc6f061817c3a8b3ebf0_39", "text": "First phase of the approach requires training of an unsupervised NMT system similar to Ogueji & Ahia (2019) (PidginUNMT)."}
{"sent_id": "7c2586a172dc6f061817c3a8b3ebf0-C001-32", "intents": ["@SIM@"], "paper_id": "ABC_7c2586a172dc6f061817c3a8b3ebf0_39", "text": "Similar to Ogueji & Ahia (2019) , we train the cross-lingual model using FastText Bojanowski et al. (2017) on the combined Pidgin-English corpus."}
{"sent_id": "7c2586a172dc6f061817c3a8b3ebf0-C001-33", "intents": ["@SIM@"], "paper_id": "ABC_7c2586a172dc6f061817c3a8b3ebf0_39", "text": "Next, we train an unsupervised NMT similar to Lample et al. (2017) ; Artetxe et al. (2017) ; Ogueji & Ahia (2019) between them to obtain model unsup ."}
{"sent_id": "5f25b6a3bcaca2e4beb59ce0f3eb5f-C001-3", "intents": ["@EXT@"], "paper_id": "ABC_5f25b6a3bcaca2e4beb59ce0f3eb5f_40", "text": "This is an extension and further validation of the results achieved by Manion and Sainudiin (2014)."}
{"sent_id": "5f25b6a3bcaca2e4beb59ce0f3eb5f-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_5f25b6a3bcaca2e4beb59ce0f3eb5f_40", "text": "In-Degree Centrality as implemented in (Manion and Sainudiin, 2014) observes F-Score improvement (F + ∆F) by applying the iterative approach."}
{"sent_id": "5f25b6a3bcaca2e4beb59ce0f3eb5f-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_5f25b6a3bcaca2e4beb59ce0f3eb5f_40", "text": "Secondly, as shown by Manion and Sainudiin (2014) with a simple linear regression, the iterative approach increases WSD performance for documents that have a higher degree of document monosemy -the percentage of unique monosemous lemmas in a document."}
{"sent_id": "5f25b6a3bcaca2e4beb59ce0f3eb5f-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_5f25b6a3bcaca2e4beb59ce0f3eb5f_40", "text": "Formalised in (Manion and Sainudiin, 2014) , this run can act as a baseline to gauge any improvement for Run2 and Run3 that apply the iterative approach."}
{"sent_id": "0e4ca87c0e2b899bfd1f36dc5974b9-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_0e4ca87c0e2b899bfd1f36dc5974b9_40", "text": "For other languages, one could retrain a language-specific model using the BERT architecture Martin et al., 2019; de Vries et al., 2019] or employ existing pre-trained multilingual BERT-based models [Devlin et al., 2019; Conneau et al., 2019; Conneau and Lample, 2019] ."}
{"sent_id": "0e4ca87c0e2b899bfd1f36dc5974b9-C001-11", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_0e4ca87c0e2b899bfd1f36dc5974b9_40", "text": "In terms of Vietnamese language modeling, to the best of our knowledge, there are two main concerns: (i) The Vietnamese Wikipedia corpus is the only data used to train all monolingual language models , and it also is the only Vietnamese dataset included in the pre-training data used by all multilingual language models except XLM-R [Conneau et al., 2019] ."}
{"sent_id": "f60796ff05156e81c4b183cdcb05ae-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_f60796ff05156e81c4b183cdcb05ae_40", "text": "Similar experiments on using shared feature extraction layers for slot-filling across several domains have demonstrated significant performance improvements relative to single-domain baselines, especially in low data regimes [9] ."}
{"sent_id": "f60796ff05156e81c4b183cdcb05ae-C001-74", "intents": ["@USE@"], "paper_id": "ABC_f60796ff05156e81c4b183cdcb05ae_40", "text": "Both LSTM layers are shared across all domains, fol- lowed by domain specific softmax layers, following [9] ."}
{"sent_id": "5f86a4791bee14e0b1053e9b9a6fff-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_5f86a4791bee14e0b1053e9b9a6fff_40", "text": "So far, most researchers interested in co-occurrence of mutual translations have relied on bitexts where sentence boundaries (or other text unit boundaries) were easy to find (e.g. Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1995; Melamed, 1995) ."}
{"sent_id": "5f86a4791bee14e0b1053e9b9a6fff-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_5f86a4791bee14e0b1053e9b9a6fff_40", "text": "However, when authors specify their algorithms in sufficient detail to answer this question, the most common answer (given, e.g., by Brown et al., 1993; Dagan et al., 1993; Kupiec, 1993; Melamed, 1995) turns out to be unsound."}
{"sent_id": "5f86a4791bee14e0b1053e9b9a6fff-C001-74", "intents": ["@BACK@"], "paper_id": "ABC_5f86a4791bee14e0b1053e9b9a6fff_40", "text": "Other preconditions may be imposed if certain language-specific resources are available (Melamed, 1995) ."}
{"sent_id": "5f86a4791bee14e0b1053e9b9a6fff-C001-84", "intents": ["@BACK@"], "paper_id": "ABC_5f86a4791bee14e0b1053e9b9a6fff_40", "text": "It does not matter if there are dependencies among the different knowledge sources, as long as each is used as a simple filter on the co-occurrence relation (Melamed, 1995) ."}
{"sent_id": "9fdeb20207af1e8ee0c6e5374e3731-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_9fdeb20207af1e8ee0c6e5374e3731_40", "text": "The SINNET system is the result of several years of research Agarwal et al., 2012; Agarwal et al., 2013) ."}
{"sent_id": "9fdeb20207af1e8ee0c6e5374e3731-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_9fdeb20207af1e8ee0c6e5374e3731_40", "text": "In Agarwal et al. (2012) , we presented a case study on a manually extracted network from Alice in Wonderland, showing that analyzing networks based on these social events gives us insight into the roles of characters in the story."}
{"sent_id": "9fdeb20207af1e8ee0c6e5374e3731-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_9fdeb20207af1e8ee0c6e5374e3731_40", "text": "Figure 2 shows the network extracted from an abridged version of Alice in Wonderland (Agarwal et al., 2012) ."}
{"sent_id": "9fdeb20207af1e8ee0c6e5374e3731-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_9fdeb20207af1e8ee0c6e5374e3731_40", "text": "This network may be used for other In Agarwal et al. (2012) , we argued that a static network does not bring out the true nature of a network."}
{"sent_id": "27dbdd4827554df0f53013966242dc-C001-15", "intents": ["@EXT@"], "paper_id": "ABC_27dbdd4827554df0f53013966242dc_40", "text": "Our work is based on the SummaRuNNer model [5] ."}
{"sent_id": "27dbdd4827554df0f53013966242dc-C001-32", "intents": ["@DIF@"], "paper_id": "ABC_27dbdd4827554df0f53013966242dc_40", "text": "In contrast to [5] , we trained our model only on CNN articles from the CNN/Daily Mail corpus [2] ."}
{"sent_id": "27dbdd4827554df0f53013966242dc-C001-34", "intents": ["@SIM@"], "paper_id": "ABC_27dbdd4827554df0f53013966242dc_40", "text": "In a similar approach to [5] , we calculated the ROUGE-1 F1 score between each sentence and its article's abstractive summary."}
{"sent_id": "b4093db328fd6839777a6d34507b34-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_b4093db328fd6839777a6d34507b34_40", "text": "Recently, Barrett and Søgaard (2015) presented evidence that gaze features can be used to discriminate between most pairs of parts of speech (POS) ."}
{"sent_id": "b4093db328fd6839777a6d34507b34-C001-58", "intents": ["@SIM@"], "paper_id": "ABC_b4093db328fd6839777a6d34507b34_40", "text": "The features predictive of grammatical functions are similar to the features that were found to be predictive of POS (Barrett and Søgaard, 2015) , however, the probability that a word gets first and second fixation were not important features for POS classification, whereas they are contributing to dependency classification."}
{"sent_id": "b4093db328fd6839777a6d34507b34-C001-84", "intents": ["@UNSURE@"], "paper_id": "ABC_b4093db328fd6839777a6d34507b34_40", "text": "In addition to Barrett and Søgaard (2015) , our work relates to Matthies and Søgaard (2013) , who study the robustness of a fixation prediction model across readers, not domains, but our work also relates in spirit to research on using weak supervision in NLP, e.g., work on using HTML markup to improve dependency parsers (Spitkovsky, 2013) or using click-through data to improve POS taggers (Ganchev et al., 2012) ."}
{"sent_id": "60f4e3a8c8cae1b1ba2620b11ae6b0-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_60f4e3a8c8cae1b1ba2620b11ae6b0_40", "text": "To that end, researchers have investigated the linguistic knowledge that these models learn by analyzing BERT (Goldberg, 2018; Lin et al., 2019) directly or training probing classifiers on the contextualized embeddings or attention heads of BERT (Tenney et al., 2019b,a; Hewitt and Manning, 2019) ."}
{"sent_id": "60f4e3a8c8cae1b1ba2620b11ae6b0-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_60f4e3a8c8cae1b1ba2620b11ae6b0_40", "text": "Hewitt and Manning (2019) train a structural probing model that maps the hidden representations of each token to an inner-product space that corresponds to syntax tree distance."}
{"sent_id": "60f4e3a8c8cae1b1ba2620b11ae6b0-C001-74", "intents": ["@USE@"], "paper_id": "ABC_60f4e3a8c8cae1b1ba2620b11ae6b0_40", "text": "(Using the gold root as the starting point in MST may artificially improve our results slightly, but this bias is applied evenly across all the models we compare.) The resulting tree is a valid directed dependency tree, though we follow Hewitt and Manning (2019) in evaluating it as undirected, for easier comparison with our MAX method."}
{"sent_id": "60f4e3a8c8cae1b1ba2620b11ae6b0-C001-80", "intents": ["@SIM@"], "paper_id": "ABC_60f4e3a8c8cae1b1ba2620b11ae6b0_40", "text": "This approach is largely similar to that in Hewitt and Manning (2019) ."}
{"sent_id": "60f4e3a8c8cae1b1ba2620b11ae6b0-C001-108", "intents": ["@DIF@"], "paper_id": "ABC_60f4e3a8c8cae1b1ba2620b11ae6b0_40", "text": "Overall, the results of both analysis methods suggest that, although some attention heads of BERT capture specific dependency relation types, they do not reflect the full extent of the significant amount of syntactic knowledge BERT and RoBERTa are known to learn as shown in previous syntactic probing work (Tenney et al., 2019b; Hewitt and Manning, 2019) ."}
{"sent_id": "7516b533aafa8b41e7e554fa54e39c-C001-4", "intents": ["@BACK@"], "paper_id": "ABC_7516b533aafa8b41e7e554fa54e39c_40", "text": "Transformer models have come to dominate a wide variety of leader-boards in the NLU field; in the area of MRC, the current state-of-the-art model on the DREAM dataset (see [Sun et al., 2019] ) fine tunes Albert, a large pretrained Transformer-based model, and additionally combines it with an extra layer of multi-head attention between context and question-answer [Zhu et al., 2020] ."}
{"sent_id": "7516b533aafa8b41e7e554fa54e39c-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_7516b533aafa8b41e7e554fa54e39c_40", "text": "DREAM [Sun et al., 2019] is a much smaller reading comprehension dataset with more than 6,000 dialogues and over 10,000 questions."}
{"sent_id": "7516b533aafa8b41e7e554fa54e39c-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_7516b533aafa8b41e7e554fa54e39c_40", "text": "Early works on the DREAM task include feature-based GBDT [Sun et al., 2019] , and FTLM [Radford et al., 2018] which is based on the Transformer [Vaswani et al., 2017] architecture."}
{"sent_id": "373795850c8f182051214a8ee09461-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_373795850c8f182051214a8ee09461_40", "text": "In the last decade, standard DSMs using bag-of-words or syntactic cooccurrence counts have been enhanced by integration into neural networks Levy et al., 2015; Nguyen et al., 2016) , or by integrating perceptual information (Silberer and Lapata, 2014; Bruni et al., 2014; Kiela et al., 2014; Lazaridou et al., 2015) ."}
{"sent_id": "373795850c8f182051214a8ee09461-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_373795850c8f182051214a8ee09461_40", "text": "Furthermore, we zoom into factors that might influence the quality of predictions, such as lexical and empirical target properties (e.g., ambiguity, frequency, compositionality); and filters to optimise the visual space, such as dispersion and imageability filters (Kiela et al., 2014) , and a novel clustering filter."}
{"sent_id": "373795850c8f182051214a8ee09461-C001-45", "intents": ["@USE@"], "paper_id": "ABC_373795850c8f182051214a8ee09461_40", "text": "We therefore apply the dispersion-based filter suggested by Kiela et al. (2014) ."}
{"sent_id": "373795850c8f182051214a8ee09461-C001-72", "intents": ["@SIM@"], "paper_id": "ABC_373795850c8f182051214a8ee09461_40", "text": "For GS-NN, the compositionality of concrete and imaginable targets is predicted better than for abstract and less imaginable targets, as one would expect and has been shown by Kiela et al. (2014) ; for GS-PV, the opposite is the case."}
{"sent_id": "0888b30ae5dcc880761a92ffbdcd1b-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_0888b30ae5dcc880761a92ffbdcd1b_40", "text": "This principle has already been used to explain the origins of other linguistic laws: Zipf's law of abbreviation, namely, the frequency of more frequent words to be shorter [3, 4] , and Menzerath's law, the tendency of a larger linguistic construct to be made of smaller components [5] ."}
{"sent_id": "0888b30ae5dcc880761a92ffbdcd1b-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_0888b30ae5dcc880761a92ffbdcd1b_40", "text": "First, a relationship between the length of a word and its probability [4] l = a log p + b,"}
{"sent_id": "0888b30ae5dcc880761a92ffbdcd1b-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_0888b30ae5dcc880761a92ffbdcd1b_40", "text": "Third, its assumptions are far reaching: compression allows one to shed light on the origins of three linguistic laws at the same time: Zipf's law for word frequencies, Zipf's law of abbreviation and Menzerath's law with the unifying principle of compression [3, 4, 5] ."}
{"sent_id": "119d473a0a5a4c42de193e51564f1f-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_119d473a0a5a4c42de193e51564f1f_40", "text": "The example is taken from the Simple English Wikipedia corpus (Coster and Kauchak, 2011) connectives do not belong to any linguistic class and except for a few discourse connectives such as oh and well, most carry meaning."}
{"sent_id": "119d473a0a5a4c42de193e51564f1f-C001-59", "intents": ["@EXT@"], "paper_id": "ABC_119d473a0a5a4c42de193e51564f1f_40", "text": "The first data set was created from the Simple English Wikipedia corpus (Coster and Kauchak, 2011) ; the other was created from the Newsela corpus (Xu et al., 2015) ."}
{"sent_id": "119d473a0a5a4c42de193e51564f1f-C001-60", "intents": ["@EXT@", "@USE@"], "paper_id": "ABC_119d473a0a5a4c42de193e51564f1f_40", "text": "The Simple English Wikipedia (SEW) corpus (Coster and Kauchak, 2011) contains two sections: 1) article-aligned and 2) sentence-aligned."}
{"sent_id": "119d473a0a5a4c42de193e51564f1f-C001-64", "intents": ["@SIM@"], "paper_id": "ABC_119d473a0a5a4c42de193e51564f1f_40", "text": "We used this article-aligned corpus to align it at the sentence-level using an approach similar to (Coster and Kauchak, 2011) ."}
{"sent_id": "76fd2709a325366be6154d2a84b29b-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_76fd2709a325366be6154d2a84b29b_40", "text": "The original skip-gram embeddings can yield broad topical similarities, while the dependency-based word embeddings can capture more functional similarities (Levy et al., 2014) ."}
{"sent_id": "5503b8571748ea900340aead22743b-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_5503b8571748ea900340aead22743b_40", "text": "These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell'Orletta, 2009) , and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell'Orletta, 2009 )."}
{"sent_id": "5503b8571748ea900340aead22743b-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_5503b8571748ea900340aead22743b_40", "text": "One such algorithm was proposed by Attardi and Dell'Orletta (2009) ."}
{"sent_id": "5503b8571748ea900340aead22743b-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_5503b8571748ea900340aead22743b_40", "text": "Recent work has shown that the combination of base parsers at learning time, e.g., through stacking, yields considerable benefits (Nivre and McDonald, 2008; Attardi and Dell'Orletta, 2009 )."}
{"sent_id": "5f5a59f8fbf999b9eecfe7c1897b2c-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_5f5a59f8fbf999b9eecfe7c1897b2c_40", "text": "To guarantee that the resulting dependency tree is well-formed, most previous work used the dynamic programming algorithm of Eisner (1996) for reparsing (Sagae and Lavie, 2006; Hall et al., 2007) ."}
{"sent_id": "5f5a59f8fbf999b9eecfe7c1897b2c-C001-13", "intents": ["@SIM@"], "paper_id": "ABC_5f5a59f8fbf999b9eecfe7c1897b2c_40", "text": "parser variants are built by varying the parsing algorithm (we used three parsing models: Nivre's arceager (AE), Nivre's arc-standard (AS), and Covington's non-projective model (CN)), and the parsing direction (left to right (→) or right to left (←)), similar to (Hall et al., 2007) ."}
{"sent_id": "5aeb64701a6b7d9878ea5e14a87b4e-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_5aeb64701a6b7d9878ea5e14a87b4e_40", "text": "Some are \"lexical sample\" datasets, that is, only occurrences of some selected lemmas are annotated (McCarthy and Navigli, 2009; Biemann, 2013) , and some are \"all-words\", providing substitutes for all content words in the given sentences (Sinha and Mihalcea, 2014; Kremer et al., 2014) ."}
{"sent_id": "5aeb64701a6b7d9878ea5e14a87b4e-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_5aeb64701a6b7d9878ea5e14a87b4e_40", "text": "In addi- Table 2 : Analysis of lexical substitution data: Relation of the substitute to the target, in percentages by part of speech (from Kremer et al. (2014)) tion, providing substitutes is a task that seems to be well doable by untrained annotators: Both Biemann (2013) and our recent annotation (Kremer et al., 2014) used crowdsourcing to collect the substitutes."}
{"sent_id": "5aeb64701a6b7d9878ea5e14a87b4e-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_5aeb64701a6b7d9878ea5e14a87b4e_40", "text": "In a recent lexical substitution annotation effort (Kremer et al., 2014) , we collected lexical substitution annotation for all nouns, verbs, and adjectives in a mixed news and fiction corpus, using untrained annotators via crowdsourcing."}
{"sent_id": "3c74f66c209335ea33dda38c203199-C001-12", "intents": ["@USE@"], "paper_id": "ABC_3c74f66c209335ea33dda38c203199_40", "text": "To this end we use the long-distance agreement benchmark recently introduced by Gulordava et al. (2018) ."}
{"sent_id": "3c74f66c209335ea33dda38c203199-C001-38", "intents": ["@USE@"], "paper_id": "ABC_3c74f66c209335ea33dda38c203199_40", "text": "Following the setup of Gulordava et al. (2018) , we train 2-layer LSTM models with embedding and hidden layers of 650 dimensions for 40 epochs."}
{"sent_id": "3c74f66c209335ea33dda38c203199-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_3c74f66c209335ea33dda38c203199_40", "text": "Assessing the syntactic abilities of monolingual neural LMs trained without explicit supervision has been the focus of several recent studies: Linzen et al. (2016) analyzed the performance of LSTM LMs at an English subject-verb agreement task, while Gulordava et al. (2018) extended the analysis to various long-range agreement patterns in different languages."}
{"sent_id": "5f97682d8a1b78f08fd3623ee81703-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_5f97682d8a1b78f08fd3623ee81703_40", "text": "Subsequent research has focused on increasing recall -a noteworthy approach (OLLIE) uses bootstrapping for learning general language patterns (Mausam et al., 2012) ."}
{"sent_id": "5f97682d8a1b78f08fd3623ee81703-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_5f97682d8a1b78f08fd3623ee81703_40", "text": "While the focus on verbs continues to be common in these Open IE systems, some works have directed attention on noun-mediated relations such as OL-LIE (Mausam et al., 2012) , RENOUN (Yahya et al., 2014) , and RELNOUN."}
{"sent_id": "5f97682d8a1b78f08fd3623ee81703-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_5f97682d8a1b78f08fd3623ee81703_40", "text": "Probably the earliest work on Nominal Open IE is OLLIE, which is a pattern learning approach based on a bootstrapped training data using high precision verb-based extractions (Mausam et al., 2012) ."}
{"sent_id": "5f97682d8a1b78f08fd3623ee81703-C001-87", "intents": ["@USE@"], "paper_id": "ABC_5f97682d8a1b78f08fd3623ee81703_40", "text": "Following previous work (Mausam et al., 2012) , we report yield, since recall is proportional to yield and suffices for system comparisons."}
{"sent_id": "d6c8b712c8fe3dd87d23886d575098-C001-19", "intents": ["@MOT@"], "paper_id": "ABC_d6c8b712c8fe3dd87d23886d575098_40", "text": "Therefore, we trained neural language models on a large monolingual News corpus to perform data selection (Schamper et al., 2018) ."}
{"sent_id": "d6c8b712c8fe3dd87d23886d575098-C001-27", "intents": ["@EXT@"], "paper_id": "ABC_d6c8b712c8fe3dd87d23886d575098_40", "text": "Our final submission is an ensemble of both models (Schamper et al., 2018) ."}
{"sent_id": "9e8c386b7ea3b5e4e2843fb8382fd8-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_9e8c386b7ea3b5e4e2843fb8382fd8_41", "text": "1 This deficiency can be addressed by summing surprisal measures over the saccade region (see Figure 1) , and the resulting cumulative n-grams have been shown to be more predictive of reading times than the usual non-cumulative n-grams (van Schijndel and Schuler, 2015) ."}
{"sent_id": "9e8c386b7ea3b5e4e2843fb8382fd8-C001-48", "intents": ["@SIM@"], "paper_id": "ABC_9e8c386b7ea3b5e4e2843fb8382fd8_41", "text": "Accumulated PCFG surprisal (see Equation 4) did not improve reading time fit (p > 0.05), unlike n-gram surprisal, which replicates a previous result using the Dundee corpus (van Schijndel and Schuler, 2015) ."}
{"sent_id": "9e8c386b7ea3b5e4e2843fb8382fd8-C001-79", "intents": ["@SIM@"], "paper_id": "ABC_9e8c386b7ea3b5e4e2843fb8382fd8_41", "text": "This work has confirmed previous findings that cumulative n-grams provide a better model of reading times than the typical non-cumulative reading times (van Schijndel and Schuler, 2015) ."}
{"sent_id": "f59a8c650583343fe372db42fc109a-C001-53", "intents": ["@USE@"], "paper_id": "ABC_f59a8c650583343fe372db42fc109a_41", "text": "The mod- els used for back-translating monolingual data are baseline Transformers (Vaswani et al., 2017) trained on the bilingual data after data selection as described before."}
{"sent_id": "f59a8c650583343fe372db42fc109a-C001-58", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_f59a8c650583343fe372db42fc109a_41", "text": "The settings of Transformerbase is the same as the baseline Transformer in Vaswani et al. (2017) 's work."}
{"sent_id": "f59a8c650583343fe372db42fc109a-C001-45", "intents": ["@EXT@"], "paper_id": "ABC_f59a8c650583343fe372db42fc109a_41", "text": "Figure 1 (a) shows the structure of the standard Transformer translation model (Vaswani et al., 2017) and we removed the encoder and the attention layer in the decoder from the Transformer translation model to create our Transformer language model as shown in Figure 1 (b) ."}
{"sent_id": "9e491cad55265802275e9aaea9faae-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_9e491cad55265802275e9aaea9faae_41", "text": "In sentence-level QA, the task is to pick sentences that are most relevant to the question among a list of candidates (Yang et al., 2015) ."}
{"sent_id": "9e491cad55265802275e9aaea9faae-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_9e491cad55265802275e9aaea9faae_41", "text": "The recent advances in neural question answering lead to numerous datasets and successful models in these paradigms (Rajpurkar et al., 2016; Yang et al., 2015; Nguyen et al., 2016; Trischler et al., 2016 )."}
{"sent_id": "9e491cad55265802275e9aaea9faae-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_9e491cad55265802275e9aaea9faae_41", "text": "WikiQA (Yang et al., 2015) is a sentence-level QA dataset, containing 1.9k/0.3k train/dev answerable examples."}
{"sent_id": "086619bef9b4e7851bf42bf36eb14a-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_086619bef9b4e7851bf42bf36eb14a_41", "text": "Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, 28] ."}
{"sent_id": "086619bef9b4e7851bf42bf36eb14a-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_086619bef9b4e7851bf42bf36eb14a_41", "text": "In general, sentence lengths have been quantified by the number of words [24, 29, 25, 28] or characters [30, 31, 26, 27] ."}
{"sent_id": "086619bef9b4e7851bf42bf36eb14a-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_086619bef9b4e7851bf42bf36eb14a_41", "text": "However, an issue about this relationship was addressed in [37] : \"Somewhat more problematic is the relation of sentence length to the word length.\" This comment is consistent with the one in [28] asserting that the Menzerath-Altmann law does not hold if the sentence length is measured in terms of characters instead of the number of words."}
{"sent_id": "086619bef9b4e7851bf42bf36eb14a-C001-94", "intents": ["@SIM@"], "paper_id": "ABC_086619bef9b4e7851bf42bf36eb14a_41", "text": "This result is consistent with the multifractal analysis performed in [28] ."}
{"sent_id": "bd64b244756259f18f7c3cb60989a2-C001-24", "intents": ["@USE@"], "paper_id": "ABC_bd64b244756259f18f7c3cb60989a2_41", "text": "Finally, a set of experiments was performed in which the synthetic parallel corpus, obtained via back-translation, was filtered with Bicleaner 2 (Sánchez-Cartagena et al. 2018 ), a tool for misalignment detection."}
{"sent_id": "bd64b244756259f18f7c3cb60989a2-C001-112", "intents": ["@USE@"], "paper_id": "ABC_bd64b244756259f18f7c3cb60989a2_41", "text": "Finally, we presented a lightweight method for filtering synthetic sentence pairs obtained via back-translation, using a tool for misalignment detection, Bicleaner (Sánchez-Cartagena et al. 2018) ."}
{"sent_id": "bd64b244756259f18f7c3cb60989a2-C001-63", "intents": ["@UNSURE@"], "paper_id": "ABC_bd64b244756259f18f7c3cb60989a2_41", "text": "More details are provided by Sánchez-Cartagena et al. (2018) ."}
{"sent_id": "e177758a227506bbf9de48f8f35715-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_e177758a227506bbf9de48f8f35715_41", "text": "The second step is to perform dictionary induction by learning a linear projection, in the form of a matrix, between language vector spaces (Mikolov et al., 2013b; Lazaridou et al., 2015) ."}
{"sent_id": "e177758a227506bbf9de48f8f35715-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_e177758a227506bbf9de48f8f35715_41", "text": "It is one of the most competitive methods for generating word vector representations, as demonstrated by results on a various semantic tasks (Baroni et al., 2014; Mikolov et al., 2013b) ."}
{"sent_id": "aeb6a815732b36d7602a9c43c47cfa-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_aeb6a815732b36d7602a9c43c47cfa_41", "text": "Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these (Rahimi et al., 2015b,a) ."}
{"sent_id": "aeb6a815732b36d7602a9c43c47cfa-C001-75", "intents": ["@DIF@"], "paper_id": "ABC_aeb6a815732b36d7602a9c43c47cfa_41", "text": "The results are also compared with state-of-the-art text-based methods based on a flat (Rahimi et al., 2015b; Cha et al., 2015) or hierarchical (Wing and Baldridge, 2014; Melo and Martins, 2015; Liu and Inkpen, 2015) geospatial representation."}
{"sent_id": "aeb6a815732b36d7602a9c43c47cfa-C001-88", "intents": ["@DIF@"], "paper_id": "ABC_aeb6a815732b36d7602a9c43c47cfa_41", "text": "The embeddings slightly outperform the output layer of logistic regression (LR) (Rahimi et al., 2015b)"}
{"sent_id": "aeb6a815732b36d7602a9c43c47cfa-C001-61", "intents": ["@UNSURE@"], "paper_id": "ABC_aeb6a815732b36d7602a9c43c47cfa_41", "text": "We also compare the quality of the embeddings with pre-trained word2vec embeddings and the embeddings from the output layer of LR (logistic regression) (Rahimi et al., 2015b) as baselines."}
{"sent_id": "71bcd87091c4f5e2b7bc564b7bd9dd-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_71bcd87091c4f5e2b7bc564b7bd9dd_41", "text": "The encoder has 128 hidden units for each direction (forward and backward), and the decoder has 128 hidden units with soft attention mechanism (Cho et al., 2014b) ."}
{"sent_id": "71bcd87091c4f5e2b7bc564b7bd9dd-C001-42", "intents": ["@USE@"], "paper_id": "ABC_71bcd87091c4f5e2b7bc564b7bd9dd_41", "text": "We leveraged a character-based encoderdecoder model (Bojanowski et al., 2015; Chung et al., 2016) with soft attention mechanism (Cho et al., 2014b) ."}
{"sent_id": "a6296d02f21ca5887c7686a2cbe56c-C001-10", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_a6296d02f21ca5887c7686a2cbe56c_41", "text": "The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007) ."}
{"sent_id": "a6296d02f21ca5887c7686a2cbe56c-C001-20", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_a6296d02f21ca5887c7686a2cbe56c_41", "text": "As in (Rosti et al., 2007) , confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models."}
{"sent_id": "a6296d02f21ca5887c7686a2cbe56c-C001-52", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_a6296d02f21ca5887c7686a2cbe56c_41", "text": "Other scores for the word arc are set as in (Rosti et al., 2007) ."}
{"sent_id": "47e109fd12ddbeebba894cead282d2-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_47e109fd12ddbeebba894cead282d2_41", "text": "Why aren't you giving it the same treatment you do to evolution?\" Because it doesn't carry the same weight. ;P bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011) ; (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003) ; and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Rosé, 2010; Biran and Rambow, 2011) ."}
{"sent_id": "47e109fd12ddbeebba894cead282d2-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_47e109fd12ddbeebba894cead282d2_41", "text": "These properties may function to engage the audience and persuade them to form a particular opinion, but they make computational analysis of such debates challenging, with the best performance to date averaging 64% over several topics (Somasundaran and Wiebe, 2010 Second, the affordances of different online debate sites provide differential support for dialogic relations between forum participants."}
{"sent_id": "47e109fd12ddbeebba894cead282d2-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_47e109fd12ddbeebba894cead282d2_41", "text": "For example, the research of Somasundaran and Wiebe (2010) , does not explicitly model dialogue or author relations."}
{"sent_id": "12ab280d48ef6bfae0ff27a400e2ab-C001-2", "intents": ["@BACK@"], "paper_id": "ABC_12ab280d48ef6bfae0ff27a400e2ab_41", "text": "This session focused on experimental or planned approaches to human language technology evaluation and included an overview and five papers: two papers on experimental evaluation approaches [l, 2], and three about the ongoing work in new annotation and evaluation approaches for human language technology [3, 4, 5] ."}
{"sent_id": "12ab280d48ef6bfae0ff27a400e2ab-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_12ab280d48ef6bfae0ff27a400e2ab_41", "text": "The last three papers ([3, 4, 5]) take various approaches to the issue of predicate-argument 1The Penn Treebank parse annotations provide an interesting case where annotation supported evaluation."}
{"sent_id": "12ab280d48ef6bfae0ff27a400e2ab-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_12ab280d48ef6bfae0ff27a400e2ab_41", "text": "This session focused on experimental or planned approaches to human language technology evaluation and included an overview and five papers: two papers on experimental evaluation approaches [l, 2] , and three about the ongoing work in new annotation and evaluation approaches for human language technology [3, 4, 5] ."}
{"sent_id": "12ab280d48ef6bfae0ff27a400e2ab-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_12ab280d48ef6bfae0ff27a400e2ab_41", "text": "The last three papers ( [3, 4, 5] ) take various approaches to the issue of predicate-argument 1The Penn Treebank parse annotations provide an interesting case where annotation supported evaluation."}
{"sent_id": "12ab280d48ef6bfae0ff27a400e2ab-C001-75", "intents": ["@BACK@"], "paper_id": "ABC_12ab280d48ef6bfae0ff27a400e2ab_41", "text": "The last three papers [3, 4, 5] all reflect a concern to develop better evaluation methods for semantics, with a shared focus on predicate-argument evaluation."}
{"sent_id": "12ab280d48ef6bfae0ff27a400e2ab-C001-83", "intents": ["@BACK@"], "paper_id": "ABC_12ab280d48ef6bfae0ff27a400e2ab_41", "text": "Both Marcus and Grishman argued that the Treebank annotation should directly support the MUC-style predicate-argument evaluation outlined in [4] , although the Treebank annotations may be a sub-set of what is used for MUC predicate-argument evaluation."}
{"sent_id": "2915e49791d14f5b802225d10f33fb-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_2915e49791d14f5b802225d10f33fb_41", "text": "Machine Learning methods have been widely applied for sentiment analysis (Pang et al. 2008; Pang et al. 2002; Tan et al. 2008 )."}
{"sent_id": "2915e49791d14f5b802225d10f33fb-C001-104", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_2915e49791d14f5b802225d10f33fb_41", "text": "Binary weighting scheme has been identified as a better weighting scheme as compared to frequency based schemes for sentiment classification (Pang et al. 2002) ; therefore we also used binary weighting method for representing text."}
{"sent_id": "2915e49791d14f5b802225d10f33fb-C001-108", "intents": ["@BACK@"], "paper_id": "ABC_2915e49791d14f5b802225d10f33fb_41", "text": "Support Vector Machine (SVM) and Naïve Bayes (NB) classifiers are the mostly used for sentiment classification (Pang et al. 2002; Tan et al. 2008) ."}
{"sent_id": "2915e49791d14f5b802225d10f33fb-C001-102", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_2915e49791d14f5b802225d10f33fb_41", "text": "Documents are initially pre-processed as follows: (i) Negation handling is performed as Pang et al. (2002) , \"NOT_\" is added to every words occurring after the negation word (no, not, isn't, can't, never, couldn't, didn't, wouldn't, don't) and first punctuation mark in the sentence."}
{"sent_id": "4bb290aba7ee7843280a8b0e88e5a0-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_4bb290aba7ee7843280a8b0e88e5a0_41", "text": "jacana-freebase 2 (Yao and Van Durme, 2014) treats QA from a KB as a binary classification problem."}
{"sent_id": "a229630a81020951ec0be27f54885a-C001-59", "intents": ["@USE@"], "paper_id": "ABC_a229630a81020951ec0be27f54885a_41", "text": "Following (Androutsopoulos et al., 2000) , we have assigned 9 and 999 (9 and 999 times more important) penalties to the missclassification of legitimate messages as UCE."}
{"sent_id": "a229630a81020951ec0be27f54885a-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_a229630a81020951ec0be27f54885a_41", "text": "Recall and precision for the UCE class show how effective the filter is blocking UCE, and what is its effectiveness letting legitimate messages pass the filter, respectively (Androutsopoulos et al., 2000) ."}
{"sent_id": "a229630a81020951ec0be27f54885a-C001-82", "intents": ["@BACK@"], "paper_id": "ABC_a229630a81020951ec0be27f54885a_41", "text": "In (Sahami et al., 1998; Androutsopoulos et al., 2000) , the method followed is the variation of the probability threshold, which leads to a high variation of results."}
{"sent_id": "a229630a81020951ec0be27f54885a-C001-87", "intents": ["@FUT@"], "paper_id": "ABC_a229630a81020951ec0be27f54885a_41", "text": "For future experiments, we will use the collection from (Androutsopoulos et al., 2000) , which is in raw form."}
{"sent_id": "d63acda66b0c17c5c6725c0e20b2d9-C001-22", "intents": ["@USE@"], "paper_id": "ABC_d63acda66b0c17c5c6725c0e20b2d9_41", "text": "We use Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd) (Biçici, 2018) for word-and phrase-level translation performance prediction."}
{"sent_id": "d63acda66b0c17c5c6725c0e20b2d9-C001-33", "intents": ["@SIM@"], "paper_id": "ABC_d63acda66b0c17c5c6725c0e20b2d9_41", "text": "This conversion decreases the number of features and obtains close results (Biçici, 2018) ."}
{"sent_id": "1f72d18331beaef7adf4a78d1619c6-C001-31", "intents": ["@EXT@"], "paper_id": "ABC_1f72d18331beaef7adf4a78d1619c6_41", "text": "Our method is a modification of QVEC-an evaluation based on alignment of embeddings to a matrix of features extracted from a linguistic resource (Tsvetkov et al., 2015) ."}
{"sent_id": "1f72d18331beaef7adf4a78d1619c6-C001-66", "intents": ["@EXT@"], "paper_id": "ABC_1f72d18331beaef7adf4a78d1619c6_41", "text": "We extend the setup of Tsvetkov et al. (2015) with two syntactic benchmarks, and evaluate QVEC-CCA with the syntactic matrix."}
{"sent_id": "1f72d18331beaef7adf4a78d1619c6-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_1f72d18331beaef7adf4a78d1619c6_41", "text": "To evaluate the semantic content of word vectors, Tsvetkov et al. (2015) exploit supersense annotations in a WordNetannotated corpus-SemCor (Miller et al., 1993 Table 2 : Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB."}
{"sent_id": "0b2e3651610aba4bd7150eee50797f-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_0b2e3651610aba4bd7150eee50797f_41", "text": "These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010) , or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010) ."}
{"sent_id": "0b2e3651610aba4bd7150eee50797f-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_0b2e3651610aba4bd7150eee50797f_41", "text": "However, this kind of errors cannot be fixed by methods which learn new words by packing already segmented words, such as word packing (Ma et al., 2007) and Pseudo-word (Duan et al., 2010) ."}
{"sent_id": "021c423c731ecbe3e26b3ce234b390-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_021c423c731ecbe3e26b3ce234b390_41", "text": "Automatic detection of fake from legitimate news in different formats such as headlines, tweets and full news articles has been approached in recent Natural Language Processing literature (Vlachos and Riedel, 2014; Vosoughi, 2015; Jin et al., 2016; Rashkin et al., 2017; Wang, 2017; Pomerleau and Rao, 2017; Thorne et al., 2018) ."}
{"sent_id": "021c423c731ecbe3e26b3ce234b390-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_021c423c731ecbe3e26b3ce234b390_41", "text": "A few recent studies have examined full articles (i.e., actual 'fake news') to extract discriminative linguistic features of misinformation Rashkin et al., 2017; Horne and Adali, 2017) ."}
{"sent_id": "021c423c731ecbe3e26b3ce234b390-C001-12", "intents": ["@MOT@"], "paper_id": "ABC_021c423c731ecbe3e26b3ce234b390_41", "text": "Most previous systems built to identify fake news articles rely on training data labeled with respect to the general reputation of the sources, i.e., domains/user accounts (Fogg et al., 2001; Lazer et al., 2017; Rashkin et al., 2017) ."}
{"sent_id": "f3e9e5d7fb4001e3d29a171b5eb4a4-C001-21", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f3e9e5d7fb4001e3d29a171b5eb4a4_41", "text": "Similarly to Liang et al. (2016) , we employ a sequence-to-sequence model to learn query expressions and their compositions."}
{"sent_id": "f3e9e5d7fb4001e3d29a171b5eb4a4-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_f3e9e5d7fb4001e3d29a171b5eb4a4_41", "text": "Although query induction can save a considerable amount of supervision effort (Liang et al., 2016; Zhong et al., 2017) , a pseudo-gold program is not guaranteed to be correct when the same answer can be found with more than one query (e.g., as the capital is often the largest city of a country, predicates might be confused)."}
{"sent_id": "f3e9e5d7fb4001e3d29a171b5eb4a4-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_f3e9e5d7fb4001e3d29a171b5eb4a4_41", "text": "A curriculum learning (Bengio et al., 2009 ) paradigm can learn graph pattern and SPARQL operator composition, in a similar fashion of Liang et al. (2016) ."}
{"sent_id": "887864e173d7f7c164fe9f9d940727-C001-24", "intents": ["@USE@"], "paper_id": "ABC_887864e173d7f7c164fe9f9d940727_41", "text": "The embedding is produced by the Neural Attention-Based Aspect Extraction Model (ABAE) [7] ."}
{"sent_id": "887864e173d7f7c164fe9f9d940727-C001-59", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_887864e173d7f7c164fe9f9d940727_41", "text": "Following ABAE [7] , we set the aspects matrix ortho-regularization coefficient equal to 0.1."}
{"sent_id": "887864e173d7f7c164fe9f9d940727-C001-77", "intents": ["@SIM@"], "paper_id": "ABC_887864e173d7f7c164fe9f9d940727_41", "text": "We also found similar patterns in the output of the basic ABAE model [7] ."}
{"sent_id": "887864e173d7f7c164fe9f9d940727-C001-87", "intents": ["@BACK@"], "paper_id": "ABC_887864e173d7f7c164fe9f9d940727_41", "text": "He et al. [7] proposed an unsupervised neural attention-based aspect extraction (ABAE) approach that encodes word-occurrence statistics into word embeddings and applies an attention mechanism to remove irrelevant words, learning a set of aspect embeddings."}
{"sent_id": "ef6bd5e57196c013d7d0436e5b0ca5-C001-11", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_ef6bd5e57196c013d7d0436e5b0ca5_42", "text": "The Fact Extraction and VERification (FEVER) task (Thorne et al., 2018) focuses on verification of textual claims against evidence."}
{"sent_id": "ef6bd5e57196c013d7d0436e5b0ca5-C001-49", "intents": ["@USE@"], "paper_id": "ABC_ef6bd5e57196c013d7d0436e5b0ca5_42", "text": "To build the model, we utilize the NEARESTP dataset described in Thorne et al. (2018) ."}
{"sent_id": "ef6bd5e57196c013d7d0436e5b0ca5-C001-66", "intents": ["@USE@"], "paper_id": "ABC_ef6bd5e57196c013d7d0436e5b0ca5_42", "text": "For parameter tuning and performance evaluation, we used a development and test datasets used in (Thorne et al., 2018) ."}
{"sent_id": "6cb86d91918743b0e4ff27e9d2351b-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_6cb86d91918743b0e4ff27e9d2351b_42", "text": "Automatic methods for error detection in treebanks have been developed in the DECCA project 1 for several different annotation types, for example part-of-speech (Dickinson and Meurers, 2003a) , constituency syntax (Dickinson and Meurers, 2003b) , and dependency syntax (Boyd et al., 2008) ."}
{"sent_id": "6cb86d91918743b0e4ff27e9d2351b-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_6cb86d91918743b0e4ff27e9d2351b_42", "text": "The algorithm, described in Dickinson and Meurers (2003a) for POS tags, works by starting from individual tokens (the nuclei) by recording their assigned part-of-speech over an entire treebank."}
{"sent_id": "6cb86d91918743b0e4ff27e9d2351b-C001-63", "intents": ["@EXT@"], "paper_id": "ABC_6cb86d91918743b0e4ff27e9d2351b_42", "text": "In addition to the output of the algorithm by Dickinson and Meurers (2003a), the tool also provides a second view, which displays tag distributions of word forms to the user (see Figure 2) ."}
{"sent_id": "6cb86d91918743b0e4ff27e9d2351b-C001-90", "intents": ["@SIM@"], "paper_id": "ABC_6cb86d91918743b0e4ff27e9d2351b_42", "text": "These results are in line with findings by Dickinson and Meurers (2003a) for the Penn Treebank."}
{"sent_id": "6cb86d91918743b0e4ff27e9d2351b-C001-106", "intents": ["@USE@"], "paper_id": "ABC_6cb86d91918743b0e4ff27e9d2351b_42", "text": "It implements the error detection algorithms by Dickinson and Meurers (2003a) and Boyd et al. (2008) ."}
{"sent_id": "cd7bb4543828f915bc930841bb8d7c-C001-11", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_cd7bb4543828f915bc930841bb8d7c_42", "text": "While (Goyal et al., 2016) used an additional finite-state mechanism to guide the production of well-formed (and input-motivated) character sequences, the performance of their basic char2char model was already quite good."}
{"sent_id": "cd7bb4543828f915bc930841bb8d7c-C001-65", "intents": ["@BACK@"], "paper_id": "ABC_cd7bb4543828f915bc930841bb8d7c_42", "text": "There were no additions or non-words (which was one of the primary concerns for (Goyal et al., 2016) )."}
{"sent_id": "cd7bb4543828f915bc930841bb8d7c-C001-17", "intents": ["@DIF@"], "paper_id": "ABC_cd7bb4543828f915bc930841bb8d7c_42", "text": "In particular, and contrary to the findings of (Goyal et al., 2016 ) (on a different dataset), our char-based model never produced non-words."}
{"sent_id": "6d8612cfb4bf05322fed1c02f4885a-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_6d8612cfb4bf05322fed1c02f4885a_42", "text": "Such findings encourage further research in the area of measuring readability, which not only facilitates adjusting the text to the reader (Danescu-Niculescu-Mizil et al., 2011) , but can also play an important role in identifying authorial style (Pitler and Nenkova, 2008) ."}
{"sent_id": "6d8612cfb4bf05322fed1c02f4885a-C001-54", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_6d8612cfb4bf05322fed1c02f4885a_42", "text": "As shorter words are considered more readable (Gunning, 1969; Pitler and Nenkova, 2008) , we also measure the ratio of words longer than five letters."}
{"sent_id": "6d8612cfb4bf05322fed1c02f4885a-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_6d8612cfb4bf05322fed1c02f4885a_42", "text": "Syntax Researchers argue about longer sentences not necessarily being more complex in terms of syntax (Feng et al., 2009; Pitler and Nenkova, 2008) ."}
{"sent_id": "53cd860c539e20874ada4343ab788f-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_53cd860c539e20874ada4343ab788f_42", "text": "This mismatch of needs has motivated various proposals to reconstruct missing entries, in WALS and other databases, from known entries (Daumé III and Campbell, 2007; Daumé III, 2009; Coke et al., 2016; Littell et al., 2017) ."}
{"sent_id": "53cd860c539e20874ada4343ab788f-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_53cd860c539e20874ada4343ab788f_42", "text": "Baseline Feature Vectors: Several previous methods take advantage of typological implicature, the fact that some typological traits correlate strongly with others, to use known features of a language to help infer other unknown features of the language (Daumé III and Campbell, 2007; Takamura et al., 2016; Coke et al., 2016) ."}
{"sent_id": "53cd860c539e20874ada4343ab788f-C001-46", "intents": ["@EXT@"], "paper_id": "ABC_53cd860c539e20874ada4343ab788f_42", "text": "This has been demonstrated to some extent in previous work that has used specifically engineered alignment-based models (Lewis and Xia, 2008; Östling, 2015; Coke et al., 2016) , and we examine whether these results apply to neural network feature extractors and expand beyond word order and syntax to other types of typology as well."}
{"sent_id": "53cd860c539e20874ada4343ab788f-C001-77", "intents": ["@DIF@"], "paper_id": "ABC_53cd860c539e20874ada4343ab788f_42", "text": "2008; Östling, 2015; Coke et al., 2016) , our proposed method is also able to infer information about phonological or phonetic inventory features."}
{"sent_id": "1786b6c1c6532d5baa092cca40e389-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_1786b6c1c6532d5baa092cca40e389_42", "text": "Corpora combining semantic and syntactic annotations constitute the backbone for the development of probabilistic models that automatically identify the semantic relationships, or semantic roles, conveyed by sentential constituents (Gildea and Jurafsky, 2002) ."}
{"sent_id": "1786b6c1c6532d5baa092cca40e389-C001-112", "intents": ["@USE@"], "paper_id": "ABC_1786b6c1c6532d5baa092cca40e389_42", "text": "The representation of test (8), stereotype (10), and find out (11) in terms of two Notion relations, one of which is treated as more salient, reifies the concept of relative significance of Proto-Role properties in the verbal semantics."}
{"sent_id": "770368eff3410f3c2ab18b14b42243-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_770368eff3410f3c2ab18b14b42243_42", "text": "The first are generative models, which are usually trained with cross-entropy to generate responses word-by-word conditioned on a dialogue context [Ritter et al., 2011 , Vinyals and Le, 2015 , Sordoni et al., 2015 , Shang et al., 2015 , Li et al., 2016a , Serban et al., 2016b ."}
{"sent_id": "770368eff3410f3c2ab18b14b42243-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_770368eff3410f3c2ab18b14b42243_42", "text": "This task has been studied extensively in the recent literature [Ritter et al., 2011 , Sordoni et al., 2015 , Li et al., 2016a ."}
{"sent_id": "770368eff3410f3c2ab18b14b42243-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_770368eff3410f3c2ab18b14b42243_42", "text": "Li et al. [2016a] propose ranking candidate responses according to a mutual information criterion, in order to incorporate dialogue context efficiently and retrieve on-topic responses."}
{"sent_id": "770368eff3410f3c2ab18b14b42243-C001-18", "intents": ["@MOT@"], "paper_id": "ABC_770368eff3410f3c2ab18b14b42243_42", "text": "One weakness of current generative models is their limited ability to incorporate rich dialogue context and to generate meaningful and diverse responses [Serban et al., 2016b , Li et al., 2016a ."}
{"sent_id": "b1df73c14f53fea607c4c8b71740fe-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_b1df73c14f53fea607c4c8b71740fe_42", "text": "Vlachos et al. (2009) applied the basic model of this class, the Dirichlet Process Mixture Model (DPMM), to lexical-semantic verb clustering with encouraging results."}
{"sent_id": "b1df73c14f53fea607c4c8b71740fe-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_b1df73c14f53fea607c4c8b71740fe_42", "text": "Furthermore, Vlachos et al. (2009) used a constrained version of the DPMM in order to guide clustering towards some prior intuition or considerations relevant to the specific task at hand."}
{"sent_id": "b1df73c14f53fea607c4c8b71740fe-C001-36", "intents": ["@USE@"], "paper_id": "ABC_b1df73c14f53fea607c4c8b71740fe_42", "text": "Following Vlachos et al. (2009) , for each instance that does not belong to a linked-group, the sampler is restricted to choose components that do not contain instances cannot-linked with it."}
{"sent_id": "b1df73c14f53fea607c4c8b71740fe-C001-69", "intents": ["@USE@"], "paper_id": "ABC_b1df73c14f53fea607c4c8b71740fe_42", "text": "We evaluate our results using three information theoretic measures: Variation of Information (Meilȃ, 2007) , V-measure (Rosenberg and Hirschberg, 2007) and V-beta (Vlachos et al., 2009 )."}
{"sent_id": "2a01f96893f9c0630a01ecce320184-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_2a01f96893f9c0630a01ecce320184_42", "text": "Several research works have been proposed to detect propaganda on document-level (Rashkin et al., 2017; Barrón-Cedeño et al., 2019b) , sentencelevel and fragment-level (Da San Martino et al., 2019) ."}
{"sent_id": "2a01f96893f9c0630a01ecce320184-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_2a01f96893f9c0630a01ecce320184_42", "text": "A fine-grained propaganda corpus was proposed in Da San Martino et al. (2019) which includes both sentencelevel and fragment-level information."}
{"sent_id": "2a01f96893f9c0630a01ecce320184-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_2a01f96893f9c0630a01ecce320184_42", "text": "More details of the dataset could be found in Da San Martino et al. (2019) ."}
{"sent_id": "2a01f96893f9c0630a01ecce320184-C001-39", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_2a01f96893f9c0630a01ecce320184_42", "text": "As described in Da San Martino et al. (2019) , the source of the dataset that we use is news articles, and since the title is usually the summarization of a news article, we use the title as supplementary information."}
{"sent_id": "2a01f96893f9c0630a01ecce320184-C001-12", "intents": ["@DIF@"], "paper_id": "ABC_2a01f96893f9c0630a01ecce320184_42", "text": "Although Da San Martino et al. (2019) indicates that multi-task learning of both the SLC and the FLC could be beneficial for the SLC, in this paper, we only focus on the SLC task so as to better investigate whether context information could improve the performance of our system."}
{"sent_id": "2a01f96893f9c0630a01ecce320184-C001-81", "intents": ["@SIM@", "@FUT@"], "paper_id": "ABC_2a01f96893f9c0630a01ecce320184_42", "text": "In the future, we plan to apply multi-task learning to this context-dependent BERT, similar to the method mentioned in Da San Martino et al. (2019) or introducing other kinds of tasks, such as sentiment analysis or domain classification."}
{"sent_id": "7cbe7dc02bbb53fe06d9215ed88fe0-C001-22", "intents": ["@USE@"], "paper_id": "ABC_7cbe7dc02bbb53fe06d9215ed88fe0_42", "text": "Some of these features are borrowed from our previous system in the Semantic Textual Similarity (STS) task in * SEM Shared Task 2013 (Zhu and Lan, 2013) ."}
{"sent_id": "7cbe7dc02bbb53fe06d9215ed88fe0-C001-51", "intents": ["@USE@"], "paper_id": "ABC_7cbe7dc02bbb53fe06d9215ed88fe0_42", "text": "We chose the Longest Common Sequence (LCS) feature (Zhu and Lan, 2013) , the Ngram Overlap feature (n=1,2,3) and the Weighted Word Overlap feature (Šaric et al., 2012) ."}
{"sent_id": "7cbe7dc02bbb53fe06d9215ed88fe0-C001-56", "intents": ["@USE@"], "paper_id": "ABC_7cbe7dc02bbb53fe06d9215ed88fe0_42", "text": "In this work we used the knowledge based features in our previous work (Zhu and Lan, 2013) , which include four word similarity metrics based on WordNet: Path similarity (Banea et al., 2012) , WUP similarity (Wu and Palmer, 1994) , LCH similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin, 1998) ."}
{"sent_id": "7cbe7dc02bbb53fe06d9215ed88fe0-C001-64", "intents": ["@USE@"], "paper_id": "ABC_7cbe7dc02bbb53fe06d9215ed88fe0_42", "text": "In addition, we use the Co-occurrence Retrieval Model (CRM) feature from our previous work (Zhu and Lan, 2013) as another corpus-based feature."}
{"sent_id": "7cbe7dc02bbb53fe06d9215ed88fe0-C001-69", "intents": ["@USE@"], "paper_id": "ABC_7cbe7dc02bbb53fe06d9215ed88fe0_42", "text": "In this work we follow two syntactic dependency similarity features presented in our previous work (Zhu and Lan, 2013), i.e., Simple Dependency Overlap and Special Dependency Overlap."}
{"sent_id": "7cbe7dc02bbb53fe06d9215ed88fe0-C001-73", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_7cbe7dc02bbb53fe06d9215ed88fe0_42", "text": "This type of feature has been proved to be effective in our previous work (Zhu and Lan, 2013) ."}
{"sent_id": "60d39eec9573e42d7fb306b0f696c7-C001-9", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_60d39eec9573e42d7fb306b0f696c7_42", "text": "With state-of-the-art empirical results, most regard BiLSTM-CNN as a robust core module for sequence-labeling NER [1, 2, 3, 4, 5] ."}
{"sent_id": "60d39eec9573e42d7fb306b0f696c7-C001-23", "intents": ["@USE@"], "paper_id": "ABC_60d39eec9573e42d7fb306b0f696c7_42", "text": "For Baseline [2] , a CNN is used to compute character-level word features alongside word embedding and multi-layer BiLSTM is used to capture the future and the past for each time step:"}
{"sent_id": "60d39eec9573e42d7fb306b0f696c7-C001-25", "intents": ["@USE@"], "paper_id": "ABC_60d39eec9573e42d7fb306b0f696c7_42", "text": "Using OSBIE sequential labels [2] , when there are P entity types, the number of token classes d p = P × 4 + 1."}
{"sent_id": "60d39eec9573e42d7fb306b0f696c7-C001-47", "intents": ["@USE@"], "paper_id": "ABC_60d39eec9573e42d7fb306b0f696c7_42", "text": "Besides Baseline-, Cross-, and Att-BiLSTM-CNN, results of bare-bone BiLSTM-CNN [2] , CRF-BiLSTM(-BiLSTM) [11, 12] , and CRF-IDCNN [11] from the literature are also listed."}
{"sent_id": "52af1f5378194ccdb7c8f755a6ae34-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_52af1f5378194ccdb7c8f755a6ae34_42", "text": "Adel and Schütze (2017) solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer."}
{"sent_id": "4c71ac789203d50e3e428458c2f88c-C001-27", "intents": ["@DIF@"], "paper_id": "ABC_4c71ac789203d50e3e428458c2f88c_42", "text": "Our proposed mixed hierarchi- Table Summarization with fixed schema tables as input cal attention model provides an improvement of around 18 BLEU (around 30%) over the current state-of-the-art result by Mei et al. (2016) ."}
{"sent_id": "4c71ac789203d50e3e428458c2f88c-C001-67", "intents": ["@EXT@"], "paper_id": "ABC_4c71ac789203d50e3e428458c2f88c_42", "text": "Dynamic Record attention for Decoder: Our decoder is a GRU based decoder with dynamic attention mechanism similar to (Mei et al., 2016) with modifications to modulate attention weights at each time step using static record attentions."}
{"sent_id": "4c71ac789203d50e3e428458c2f88c-C001-97", "intents": ["@USE@"], "paper_id": "ABC_4c71ac789203d50e3e428458c2f88c_42", "text": "We compared the performance of our model against the state-of-the-art work of MBW (Mei et al., 2016) , as well as two other baseline models KL (Konstas and Lapata, 2013) and ALK (Angeli et al., 2010 '00010000000000' and '00001000000000' respectively."}
{"sent_id": "4c71ac789203d50e3e428458c2f88c-C001-113", "intents": ["@USE@"], "paper_id": "ABC_4c71ac789203d50e3e428458c2f88c_42", "text": "In addition to the standard BLEU (sBleu) (Papineni et al., 2002) , a customized BLEU (cBleu) (Mei et al., 2016) has also been reported."}
{"sent_id": "2292b2c0366ef12a5dd25e544f6b2d-C001-18", "intents": ["@USE@"], "paper_id": "ABC_2292b2c0366ef12a5dd25e544f6b2d_42", "text": "First, we consider Berant and Liang (2014) 's own extension of the semantic parser of Berant et al. (2013) by using paraphrases."}
{"sent_id": "2292b2c0366ef12a5dd25e544f6b2d-C001-50", "intents": ["@USE@"], "paper_id": "ABC_2292b2c0366ef12a5dd25e544f6b2d_42", "text": "Our baseline system is the parser of Berant et al. (2013) , called SEMPRE."}
{"sent_id": "2292b2c0366ef12a5dd25e544f6b2d-C001-86", "intents": ["@USE@"], "paper_id": "ABC_2292b2c0366ef12a5dd25e544f6b2d_42", "text": "For semantic parsing we use the SEMPRE and PARASEMPRE tools of Berant et al. (2013) and Berant and Liang (2014) which were trained on the training portion of the FREE917 corpus 7 ."}
{"sent_id": "2292b2c0366ef12a5dd25e544f6b2d-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_2292b2c0366ef12a5dd25e544f6b2d_42", "text": "Response-based learning has been applied in previous work to semantic parsing itself (Kwiatowski et al. (2013) , Berant et al. (2013) , Goldwasser and Roth (2013) , inter alia)."}
{"sent_id": "0984f12a6fea858c7f18263cc2fb01-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_0984f12a6fea858c7f18263cc2fb01_42", "text": "This task has only received attention in a monolingual English setting, helped by the availability of English datasets, e.g. Flickr8K (Hodosh et al., 2013) , Flickr30K (Young et al., 2014) , and MS COCO (Chen et al., 2015) ."}
{"sent_id": "0984f12a6fea858c7f18263cc2fb01-C001-28", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_0984f12a6fea858c7f18263cc2fb01_42", "text": "The Flickr30K Dataset contains 31,014 images sourced from online photo-sharing websites (Young et al., 2014) ."}
{"sent_id": "0984f12a6fea858c7f18263cc2fb01-C001-12", "intents": ["@EXT@"], "paper_id": "ABC_0984f12a6fea858c7f18263cc2fb01_42", "text": "Multi30K is an extension of the Flickr30K dataset (Young et al., 2014) with 31,014 German translations of English descriptions and 155,070 independently collected German descriptions."}
{"sent_id": "0984f12a6fea858c7f18263cc2fb01-C001-102", "intents": ["@USE@"], "paper_id": "ABC_0984f12a6fea858c7f18263cc2fb01_42", "text": "The descriptions were collected as similarly as possible to the original Flickr30K dataset by translating the instructions used by Young et al. (2014) into German."}
{"sent_id": "ecdd75533aff56771f0320694efc9a-C001-3", "intents": ["@USE@"], "paper_id": "ABC_ecdd75533aff56771f0320694efc9a_42", "text": "We submitted three systems for Hindi → Nepali direction in which we have examined the performance of a Recursive Neural Network (RNN) based Neural Machine Translation (NMT) system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by (Artetxe et al., 2017) and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data."}
{"sent_id": "ecdd75533aff56771f0320694efc9a-C001-78", "intents": ["@USE@"], "paper_id": "ABC_ecdd75533aff56771f0320694efc9a_42", "text": "We have utilized architecture proposed in (Artetxe et al., 2017) where encoder is shared and decoders are separate for each language and model is trained by alternating between denoising and back-translation."}
{"sent_id": "ecdd75533aff56771f0320694efc9a-C001-83", "intents": ["@USE@"], "paper_id": "ABC_ecdd75533aff56771f0320694efc9a_42", "text": "To train all three systems we have utilized the implementation of (Artetxe et al., 2017) ."}
{"sent_id": "ecdd75533aff56771f0320694efc9a-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_ecdd75533aff56771f0320694efc9a_42", "text": "In past, to improve the performance of NMT systems various techniques like Back-Translation (Sennrich et al., 2016a) , utilizing other similar language pairs through pivoting (Cheng et al., 2017) or transfer learning (Zoph et al., 2016) , complete unsupervised architectures (Artetxe et al., 2017) (Lample et al., 2018 ) and many others have been proposed."}
{"sent_id": "ecdd75533aff56771f0320694efc9a-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_ecdd75533aff56771f0320694efc9a_42", "text": "The Unsupervised NMT approach proposed in (Artetxe et al., 2017) follows an architecture where encoder is shared and decoder is separate for each language."}
{"sent_id": "6edf517d79f7fd2a0653a3d5fb543d-C001-11", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_6edf517d79f7fd2a0653a3d5fb543d_42", "text": "While early methods required a training dictionary to find the initial alignment (Mikolov et al., 2013) , fully unsupervised methods have managed to obtain comparable results based on either adversarial training or selflearning (Artetxe et al., 2018b) ."}
{"sent_id": "6edf517d79f7fd2a0653a3d5fb543d-C001-23", "intents": ["@USE@"], "paper_id": "ABC_6edf517d79f7fd2a0653a3d5fb543d_42", "text": "In our experiments, we use fastText embeddings (Bojanowski et al., 2017) mapped through VecMap (Artetxe et al., 2018b ), but the algorithm described next can also work with any other word embedding and cross-lingual mapping method."}
{"sent_id": "6edf517d79f7fd2a0653a3d5fb543d-C001-46", "intents": ["@USE@"], "paper_id": "ABC_6edf517d79f7fd2a0653a3d5fb543d_42", "text": "Having done that, we map these word embeddings to a cross-lingual space using the unsupervised mode in VecMap (Artetxe et al., 2018b) , which builds an initial solution based on the intralingual similarity distribution of the embeddings and iteratively improves it through self-learning."}
{"sent_id": "6edf517d79f7fd2a0653a3d5fb543d-C001-68", "intents": ["@USE@"], "paper_id": "ABC_6edf517d79f7fd2a0653a3d5fb543d_42", "text": "The amount of required supervision was later reduced through self-learning methods (Artetxe et al., 2017) , and then completely eliminated through adversarial training (Zhang et al., 2017a; or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018) ."}
{"sent_id": "ee219d599e0e0c2bbebc1849863005-C001-22", "intents": ["@USE@"], "paper_id": "ABC_ee219d599e0e0c2bbebc1849863005_42", "text": "We calculate these feature vectors using an NMT model trained on 1017 languages, and use them for typlogy prediction both on their own and in composite with feature vectors from previous work based on the genetic and geographic distance between languages (Littell et al., 2017) ."}
{"sent_id": "ee219d599e0e0c2bbebc1849863005-C001-26", "intents": ["@USE@"], "paper_id": "ABC_ee219d599e0e0c2bbebc1849863005_42", "text": "Typology Database: To perform our analysis, we use the URIEL language typology database (Littell et al., 2017) , which is a collection of binary features extracted from multiple typological, phylogenetic, and geographical databases such as WALS (World Atlas of Language Structures) (Collins and Kayne, 2011) , PHOIBLE (Moran et al., 2014) , Ethnologue (Lewis et al., 2015) , and Glottolog (Hammarström et al., 2015) ."}
{"sent_id": "db6c35071fe4e93c11acca4056e9ac-C001-26", "intents": ["@DIF@", "@BACK@", "@EXT@"], "paper_id": "ABC_db6c35071fe4e93c11acca4056e9ac_42", "text": "Finally, Bekoulis et al. (2018a) use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part."}
{"sent_id": "db6c35071fe4e93c11acca4056e9ac-C001-112", "intents": ["@DIF@"], "paper_id": "ABC_db6c35071fe4e93c11acca4056e9ac_42", "text": "In the boundaries evaluation, the baseline has an improvement of ∼3% on both tasks compared to Bekoulis et al. (2018a) , whose quadratic scoring layer complicates NER."}
{"sent_id": "db6c35071fe4e93c11acca4056e9ac-C001-82", "intents": ["@USE@"], "paper_id": "ABC_db6c35071fe4e93c11acca4056e9ac_42", "text": "For the Dutch Real Estate Classifieds, DREC (Bekoulis et al., 2017) dataset, we use train-test splits as in Bekoulis et al. (2018a) ."}
{"sent_id": "7d7895690c84fb1af46c30f858470e-C001-6", "intents": ["@EXT@"], "paper_id": "ABC_7d7895690c84fb1af46c30f858470e_42", "text": "We present extensions to the DocQA [2] model to allow incremental reading without loss of accuracy."}
{"sent_id": "7d7895690c84fb1af46c30f858470e-C001-21", "intents": ["@EXT@"], "paper_id": "ABC_7d7895690c84fb1af46c30f858470e_42", "text": "We introduce a new incremental model based on DocQA [2] , which is an RNN based model proposed for QA."}
{"sent_id": "7d7895690c84fb1af46c30f858470e-C001-122", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_7d7895690c84fb1af46c30f858470e_42", "text": "In standard question answering, we do not care how the context is presented to the model, and for the models that achieve state of the art results, e.g. [11, 2] , they process the full context before making any decisions."}
{"sent_id": "7d7895690c84fb1af46c30f858470e-C001-18", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_7d7895690c84fb1af46c30f858470e_42", "text": "However, on tasks like Question Answering, in all the existing well-performing models, RNNs are employed in a bidirectional way, or a self-attention mechanism is employed [11, 2, 4, 8] ."}
{"sent_id": "7d7895690c84fb1af46c30f858470e-C001-35", "intents": ["@USE@"], "paper_id": "ABC_7d7895690c84fb1af46c30f858470e_42", "text": "We use DocQA as the baseline model [2] ."}
{"sent_id": "3816a122d7f0847c01415fadef2d3d-C001-76", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_3816a122d7f0847c01415fadef2d3d_43", "text": "However, CFG PB (also a CFG produced by the other work (Harbusch, 1990) ) cannot avoid generating invalid parse trees that connect two lo-cal structures where adjunction takes place between them."}
{"sent_id": "6a693f9cbc6dbb3676d765eee97db7-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_6a693f9cbc6dbb3676d765eee97db7_43", "text": "Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005) ."}
{"sent_id": "6a693f9cbc6dbb3676d765eee97db7-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_6a693f9cbc6dbb3676d765eee97db7_43", "text": "For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997; Wang et al., 2005) , and yet they are not being used in current large margin training algorithms."}
{"sent_id": "6a693f9cbc6dbb3676d765eee97db7-C001-27", "intents": ["@USE@"], "paper_id": "ABC_6a693f9cbc6dbb3676d765eee97db7_43", "text": "This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al., 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al., 2005; Wang et al., 2006) ."}
{"sent_id": "6a693f9cbc6dbb3676d765eee97db7-C001-32", "intents": ["@USE@"], "paper_id": "ABC_6a693f9cbc6dbb3676d765eee97db7_43", "text": "To learn an accurate dependency parser from data, the first approach I investigated is based on a strictly lexical parsing model where all the parameters are based on words (Wang et al., 2005) ."}
{"sent_id": "cc5927700475b7abc0482a28ab209a-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_cc5927700475b7abc0482a28ab209a_43", "text": "Subsequently, there has been emphasis on post-processing the embeddings to improve their performance on downstream tasks (Mu and Viswanath, 2018) or to induce linguistic properties (Mrkšic et al.; Faruqui et al., 2015) ."}
{"sent_id": "cc5927700475b7abc0482a28ab209a-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_cc5927700475b7abc0482a28ab209a_43", "text": "In particular, the Principal Component Analysis (PCA) based post-processing algorithm proposed by (Mu and Viswanath, 2018) has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction (Raunak, 2017) ."}
{"sent_id": "cc5927700475b7abc0482a28ab209a-C001-22", "intents": ["@MOT@", "@EXT@"], "paper_id": "ABC_cc5927700475b7abc0482a28ab209a_43", "text": "4. We point out the limitations of applying variance based post-processing (Mu and Viswanath, 2018) and demonstrate that it leads to a decrease in performance in sentence classification and machine translation arXiv:1910.02211v1 [cs.CL] 5 Oct 2019 In Section 1, we provide an introduction to the problem statement."}
{"sent_id": "cc5927700475b7abc0482a28ab209a-C001-67", "intents": ["@USE@"], "paper_id": "ABC_cc5927700475b7abc0482a28ab209a_43", "text": "In this section, we first describe and then evaluate the post-processing algorithm (PPA) proposed in (Mu and Viswanath, 2018) , which achieves high scores on Word and Semantic textual similarity tasks."}
{"sent_id": "fd50c8cf386e3ce8c8dd8dc46c467f-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_fd50c8cf386e3ce8c8dd8dc46c467f_43", "text": "Later, Yang et al. (2015) formulated a classifier to distinguish between humorous and non-humorous instances, and also created computational models to discover the latent semantic structure behind humor from four perspectives: incongruity, ambiguity, interpersonal effect and phonetic style."}
{"sent_id": "fd50c8cf386e3ce8c8dd8dc46c467f-C001-42", "intents": ["@UNSURE@", "@USE@"], "paper_id": "ABC_fd50c8cf386e3ce8c8dd8dc46c467f_43", "text": "One is Pun of the Day (Yang et al., 2015) , and the other is 16000 One-Liners (Mihalcea and Strapparava, 2005) ."}
{"sent_id": "fd50c8cf386e3ce8c8dd8dc46c467f-C001-47", "intents": ["@USE@"], "paper_id": "ABC_fd50c8cf386e3ce8c8dd8dc46c467f_43", "text": "The datasets we use to construct humor recognition experiments includes four parts: Pun of the Day (Yang et al., 2015) , 16000 OneLiners (Mihalcea and Strapparava, 2005) , Short Jokes dataset and PTT jokes."}
{"sent_id": "fd50c8cf386e3ce8c8dd8dc46c467f-C001-107", "intents": ["@USE@"], "paper_id": "ABC_fd50c8cf386e3ce8c8dd8dc46c467f_43", "text": "We set the baseline on the previous works of Yang et al. (2015) by Random Forest with Word2Vec + Human Centric Feature (Word2Vec + HCF) and Chen and Lee (2017) by Convolutional Neural Networks."}
{"sent_id": "0593fb7ee345cf632e6a61f1f21e6c-C001-28", "intents": ["@USE@"], "paper_id": "ABC_0593fb7ee345cf632e6a61f1f21e6c_43", "text": "Next we present some of our computed results over the three architectures: vanilla VQA model [1] , Stacked Attention Network (SAN) [12] and Teney et al. model [14] ."}
{"sent_id": "0593fb7ee345cf632e6a61f1f21e6c-C001-91", "intents": ["@USE@"], "paper_id": "ABC_0593fb7ee345cf632e6a61f1f21e6c_43", "text": "We considered the following three models for our experiments, 1) the baseline Vanilla VQA model [1] which uses the VGG16 CNN architecture [3] and LSTMs [7] , 2) the Stacked Attention Networks [12] architecture, and 3) the 2017 VQA challenge winner Teney et al. model [14] ."}
{"sent_id": "0593fb7ee345cf632e6a61f1f21e6c-C001-96", "intents": ["@USE@"], "paper_id": "ABC_0593fb7ee345cf632e6a61f1f21e6c_43", "text": "In the experiments, we found that the Teney et al. [14] is the best performing model on both VQA and Visual7W Dataset."}
{"sent_id": "0593fb7ee345cf632e6a61f1f21e6c-C001-75", "intents": ["@SIM@"], "paper_id": "ABC_0593fb7ee345cf632e6a61f1f21e6c_43", "text": "1 . The architecture is similar to Teney et al. [14] with reduced computations with elementwise multiplication, use of GloVe vectors [23] , and ensemble of 30 models."}
{"sent_id": "88900d3533701056f6a26bf7c68670-C001-11", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_88900d3533701056f6a26bf7c68670_43", "text": "Several learning systems have been developed for semantic parsing, many of them recently (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Kate and Mooney, 2006) ."}
{"sent_id": "88900d3533701056f6a26bf7c68670-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_88900d3533701056f6a26bf7c68670_43", "text": "Learning System KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006 ) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data."}
{"sent_id": "88900d3533701056f6a26bf7c68670-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_88900d3533701056f6a26bf7c68670_43", "text": "Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data (Kate and Mooney, 2006) ."}
{"sent_id": "88900d3533701056f6a26bf7c68670-C001-17", "intents": ["@EXT@"], "paper_id": "ABC_88900d3533701056f6a26bf7c68670_43", "text": "We modify KRISP, a supervised learning system for semantic parsing presented in (Kate and Mooney, 2006) , to make a semi-supervised system we call SEMISUP-KRISP."}
{"sent_id": "966106c9e00f0333bda45d977a9f35-C001-18", "intents": ["@USE@"], "paper_id": "ABC_966106c9e00f0333bda45d977a9f35_43", "text": "We compared convolutional neural network (ConvS2S) (Gehring et al., 2017) and recurrent neural network (RNNS2S) (Bahdanau et al., 2014) based sequence to sequence learning architectures."}
{"sent_id": "966106c9e00f0333bda45d977a9f35-C001-60", "intents": ["@USE@"], "paper_id": "ABC_966106c9e00f0333bda45d977a9f35_43", "text": "The baseline model with 4 encoder layers and 3 decoder layers was trained using nag optimizer (Gehring et al., 2017 ) with a learning rate of 0.25 with 0.2 as its dropout value and gradient clipping was also applied."}
{"sent_id": "966106c9e00f0333bda45d977a9f35-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_966106c9e00f0333bda45d977a9f35_43", "text": "Recent work (Gehring et al., 2017) has shown that a purely CNN based encoder-decoder network is competitive with a RNN based network."}
{"sent_id": "966106c9e00f0333bda45d977a9f35-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_966106c9e00f0333bda45d977a9f35_43", "text": "In convolutional sequence to sequence model (Gehring et al., 2017) , the input sequence is encoded into distributional vector space using a CNN and decoded back to output sequence again using CNN instead of RNN (Sutskever et al., 2014) ."}
{"sent_id": "563476cdb64cdf7d47bc5e8e1c32c3-C001-3", "intents": ["@USE@"], "paper_id": "ABC_563476cdb64cdf7d47bc5e8e1c32c3_43", "text": "Therefore, we apply a re-implementation of a state-of-the-art, discriminative L2P system (Jiampojamarn et al., 2008) to the problem, without further modification."}
{"sent_id": "563476cdb64cdf7d47bc5e8e1c32c3-C001-15", "intents": ["@USE@"], "paper_id": "ABC_563476cdb64cdf7d47bc5e8e1c32c3_43", "text": "For our submission, we re-implement the L2P approach described by Jiampojamarn et al. (2008) as faithfully as possible, and apply it unmodified to the transliteration shared task for the English-to-Hindi (Kumaran and Kellner, 2007) and English-to-Japanese Katakana 1 tests."}
{"sent_id": "563476cdb64cdf7d47bc5e8e1c32c3-C001-24", "intents": ["@USE@"], "paper_id": "ABC_563476cdb64cdf7d47bc5e8e1c32c3_43", "text": "There are two main categories of features: context and transition features, which follow the first two feature templates described by Jiampojamarn et al. (2008) ."}
{"sent_id": "563476cdb64cdf7d47bc5e8e1c32c3-C001-36", "intents": ["@DIF@"], "paper_id": "ABC_563476cdb64cdf7d47bc5e8e1c32c3_43", "text": "Our system made two alternate design decisions (we do not claim improvements) over those made by (Jiampojamarn et al., 2008) , mostly based on the availability of software."}
{"sent_id": "60f54cf8f510affe214f63f8e23e19-C001-20", "intents": ["@USE@"], "paper_id": "ABC_60f54cf8f510affe214f63f8e23e19_43", "text": "We compare the performance of an off-the-shelf MWE identification system based on neural sequence tagging (Zampieri et al., 2018) using lemmas and surface forms as input features, encoded in the form of classical pre-initialised word2vec embeddings (Mikolov et al., 2013) or, alternatively, using new-generation FastText embeddings built from character n-grams (Bojanowski et al., 2017) ."}
{"sent_id": "60f54cf8f510affe214f63f8e23e19-C001-34", "intents": ["@USE@"], "paper_id": "ABC_60f54cf8f510affe214f63f8e23e19_43", "text": "The use of pre-trained vs. randomly initialised embeddings has been analysed in some PARSEME shared task papers (Ehren et al., 2018; Zampieri et al., 2018) ."}
{"sent_id": "60f54cf8f510affe214f63f8e23e19-C001-59", "intents": ["@USE@"], "paper_id": "ABC_60f54cf8f510affe214f63f8e23e19_43", "text": "We use our inhouse MWE identification system Veyn (Zampieri et al., 2018) , based on sequence tagging using recurrent neural networks."}
{"sent_id": "60f54cf8f510affe214f63f8e23e19-C001-35", "intents": ["@SIM@"], "paper_id": "ABC_60f54cf8f510affe214f63f8e23e19_43", "text": "The closest works to ours are the Veyn (Zampieri et al., 2018) and SHOMA (Taslimipoor and Rohanian, 2018) systems, submitted to the PARSEME shared task 1.1."}
{"sent_id": "c663b64c73f2e583ea631c054824d8-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_c663b64c73f2e583ea631c054824d8_43", "text": "A recent improved method for generating word embeddings is Glove [15] which makes efficient use of global statistics of text words and preserves the linear substructure of Skip-gram word2vec, the other popular method."}
{"sent_id": "c663b64c73f2e583ea631c054824d8-C001-31", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_c663b64c73f2e583ea631c054824d8_43", "text": "It was created by authors of [15] to evaluate Glove performance."}
{"sent_id": "c663b64c73f2e583ea631c054824d8-C001-36", "intents": ["@USE@"], "paper_id": "ABC_c663b64c73f2e583ea631c054824d8_43", "text": "Even bigger is Common Crawl 840, a huge corpus of 840 billion tokens and 2.2 million word vectors also used at [15] ."}
{"sent_id": "b7158e8478b14cd8337f2aa8ee6193-C001-17", "intents": ["@USE@"], "paper_id": "ABC_b7158e8478b14cd8337f2aa8ee6193_43", "text": "An example of the English/pidgin text in the restaurant domain (Novikova et al., 2017) is displayed in Table 1 ."}
{"sent_id": "b7158e8478b14cd8337f2aa8ee6193-C001-26", "intents": ["@USE@"], "paper_id": "ABC_b7158e8478b14cd8337f2aa8ee6193_43", "text": "We employ the publicly available parallel data-to-text corpus E2E (Novikova et al., 2017) consisting of tabulated data and English descriptions in the restaurant domain."}
{"sent_id": "b7158e8478b14cd8337f2aa8ee6193-C001-39", "intents": ["@USE@"], "paper_id": "ABC_b7158e8478b14cd8337f2aa8ee6193_43", "text": "We conduct experiments on the E2E corpus (Novikova et al., 2017) which amounts to roughly 42k samples in the training set."}
{"sent_id": "90962438b8efa0f7a14fd050365310-C001-19", "intents": ["@USE@"], "paper_id": "ABC_90962438b8efa0f7a14fd050365310_43", "text": "We parse the BNC (Burnard, 2000) in its entirety using the reranking parser of Charniak and Johnson (2005) ."}
{"sent_id": "90962438b8efa0f7a14fd050365310-C001-30", "intents": ["@USE@"], "paper_id": "ABC_90962438b8efa0f7a14fd050365310_43", "text": "The reranking parser of Charniak and Johnson (2005) was used to parse the BNC."}
{"sent_id": "460b820360d51d99b4a7ae3f0755bc-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_460b820360d51d99b4a7ae3f0755bc_43", "text": "Firstly WSD tasks before 2013 generally relied on only a lexicon, such as WordNet (Fellbaum, 1998) or an alternative equivalent, whereas SemEval 2013 Task 12 WSD and this task (Moro and Navigli, 2015) included Entity Linking (EL) using the encyclopaedia Wikipedia via BabelNet (Navigli and Ponzetto, 2012) ."}
{"sent_id": "460b820360d51d99b4a7ae3f0755bc-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_460b820360d51d99b4a7ae3f0755bc_43", "text": "Yet it is worth noting the results of the task paper (Moro and Navigli, 2015) report that SUDOKU Run2 and Run3 achieved very low F-Scores for named entity disambiguation (<28.6) in Spanish and Italian."}
{"sent_id": "460b820360d51d99b4a7ae3f0755bc-C001-70", "intents": ["@BACK@"], "paper_id": "ABC_460b820360d51d99b4a7ae3f0755bc_43", "text": "The author's baseline-independent submissions were unaffected by this, which on reviewing results in (Moro and Navigli, 2015) appears to have helped SUDOKU do best for these languages."}
{"sent_id": "abe561b75389e026a9f140280f211c-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_abe561b75389e026a9f140280f211c_43", "text": "Sentiment analysis is a type of opinion mining where affective states are represented categorically or by multi-dimensional continuous values (Yu et al., 2015) ."}
{"sent_id": "abe561b75389e026a9f140280f211c-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_abe561b75389e026a9f140280f211c_43", "text": "According to the twodimensional representation, any affective state can be represented as a point in the valence-arousal space by determining the degrees of valence and arousal of given words (Wei et al., 2011; Yu et al., 2015) or texts (Kim et al., 2010) ."}
{"sent_id": "abe561b75389e026a9f140280f211c-C001-42", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_abe561b75389e026a9f140280f211c_43", "text": "Moreover, previous research suggested that it is possible to improve the performance by aggregating the results of a number of valence-arousal methods (Yu et al., 2015) ."}
{"sent_id": "7936967a70c44890f3a61f6625c59d-C001-31", "intents": ["@DIF@"], "paper_id": "ABC_7936967a70c44890f3a61f6625c59d_43", "text": "Instead of focusing on the previously studied game environments (Asher et al., 2016; Lewis et al., 2017) , the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist 1 ."}
{"sent_id": "9ac581130218d6c68ca785e3d5ba99-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_9ac581130218d6c68ca785e3d5ba99_43", "text": "The author found in the investigations of his thesis (Manion, 2014) that the iterative approach performed best on the SemEval 2013 Multilingual WSD Task (Navigli et al., 2013) , as opposed to earlier tasks such as SensEval 2004 English All Words WSD Task (Snyder and Palmer, 2004) and the SemEval 2010 All Words WSD task on a Specific Domain (Agirre et al., 2010) ."}
{"sent_id": "9ac581130218d6c68ca785e3d5ba99-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_9ac581130218d6c68ca785e3d5ba99_43", "text": "In the previous SemEval WSD task (Navigli et al., 2013) team UMCC DLSI (Gutierrez et al., 2013) implemented this method and achieved the best performance by biasing probability mass based on SemCor (Miller et al., 1993 ) sense frequencies."}
{"sent_id": "9ac581130218d6c68ca785e3d5ba99-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_9ac581130218d6c68ca785e3d5ba99_43", "text": "The author could not improve on their superior results achieved in English, however for Spanish and Italian the BabelNet First Sense (BFS) baseline was much lower since it often resorted to lexicographic sorting in the absence of WordNet synsets -see (Navigli et al., 2013) ."}
{"sent_id": "b3ef4c176720bdc89d2f73a2560673-C001-19", "intents": ["@USE@"], "paper_id": "ABC_b3ef4c176720bdc89d2f73a2560673_43", "text": "Leveraging the recent advancements (Vaswani et al., 2017; Devlin et al., 2019) in pre-trained language encoders, we attempt to predict negotiation outcomes early on in the conversation, in a completely data-driven manner ( Figure  1 )."}
{"sent_id": "b3ef4c176720bdc89d2f73a2560673-C001-77", "intents": ["@USE@"], "paper_id": "ABC_b3ef4c176720bdc89d2f73a2560673_43", "text": "Training Details: Given the multiple segments in our model input and small data size, we use BERTbase (Devlin et al., 2019) , having output dimension of 768."}
{"sent_id": "b3ef4c176720bdc89d2f73a2560673-C001-45", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_b3ef4c176720bdc89d2f73a2560673_43", "text": "Pre-trained language models, such as BERT (Vaswani et al., 2017; Devlin et al., 2019 ) have recently gained huge success on a wide range of NLP tasks."}
{"sent_id": "3fe979e570992b79c8656ab6cb34fb-C001-99", "intents": ["@BACK@"], "paper_id": "ABC_3fe979e570992b79c8656ab6cb34fb_43", "text": "Push/Pull verbs can appear in the conative construction, which emphasizes their forceful semantic component and ability to express an attempted action where any result that might be associated with the verb is not necessarily achieved; Carry verbs (used with a goal or directional phrase) cannot take the conative alternation because this would conflict with the causation of motion which is the intrinsic meaning of the class (Dang et al., 1998) ."}
{"sent_id": "8dbc779d455ad72def6654564f9e13-C001-22", "intents": ["@USE@"], "paper_id": "ABC_8dbc779d455ad72def6654564f9e13_43", "text": "Here we investigate the unsupervised word discovery and segmentation task, using the bilingual-rooted approach from Godard et al. (2018) ."}
{"sent_id": "8dbc779d455ad72def6654564f9e13-C001-33", "intents": ["@USE@"], "paper_id": "ABC_8dbc779d455ad72def6654564f9e13_43", "text": "2 Bilingual Unsupervised Word Segmentation/Discovery Approach: We use the bilingual neuralbased Unsupervised Word Segmentation (UWS) approach from Godard et al. (2018) to discover words in Mboshi."}
{"sent_id": "8dbc779d455ad72def6654564f9e13-C001-38", "intents": ["@USE@"], "paper_id": "ABC_8dbc779d455ad72def6654564f9e13_43", "text": "Multilingual Leveraging: In this work we apply two simple methods for including multilingual information into the bilingual models from Godard et al. (2018) ."}
{"sent_id": "f3b1a39203ebf0725d8dd2b8f8c7a9-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_f3b1a39203ebf0725d8dd2b8f8c7a9_44", "text": "However, the output generated can be repetitive and generic leading to monotonous or uninteresting responses (e.g \"I don't know\") regardless of the input [2] ."}
{"sent_id": "f3b1a39203ebf0725d8dd2b8f8c7a9-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_f3b1a39203ebf0725d8dd2b8f8c7a9_44", "text": "Previous work on handling the shortcomings of MLE include length-normalizing sentence probability [6] , future cost estimation [7] , diversity-boosting objective function [8, 2] or penalizing repeating tokens [9] ."}
{"sent_id": "f3b1a39203ebf0725d8dd2b8f8c7a9-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_f3b1a39203ebf0725d8dd2b8f8c7a9_44", "text": "Li et al. [2] use a discriminator for a diversity promoting objective."}
{"sent_id": "19a62878f72c84d1c5c83a9a8cdeff-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_19a62878f72c84d1c5c83a9a8cdeff_44", "text": "For sociological reasons, these arguments started appearing in print many years later [20, 5, 21] ."}
{"sent_id": "19a62878f72c84d1c5c83a9a8cdeff-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_19a62878f72c84d1c5c83a9a8cdeff_44", "text": "For the case of a single head and its dependents, the minimization of dependency lengths yields that the head should be placed at the center of the sequence whereas the principle of predictability maximization (or uncertainty minimization) yields that the head should be placed at one of the ends of the sequence (last if the head is the target of the prediction; first otherwise) [21, 20] ."}
{"sent_id": "19a62878f72c84d1c5c83a9a8cdeff-C001-27", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_19a62878f72c84d1c5c83a9a8cdeff_44", "text": "The real challenge for psycholinguistic research is not the extent to which the theoretical framework above is supported by current results in the lab but rather to increase the precision of dependency length measurements and investigate the experimental conditions in which the following theoretical predictions are observed [20, 21] : one principle beating the other, coexistence, collaboration between principles or the very same trade-off causing the delusion that word order constraints have relaxed dramatically or even disappeared."}
{"sent_id": "8d3a20b4e50f81c94e884a0b978575-C001-23", "intents": ["@SIM@"], "paper_id": "ABC_8d3a20b4e50f81c94e884a0b978575_44", "text": "The images can be natural scenes taken by real people, or artificial scenes created with clip arts like in [1] ."}
{"sent_id": "06de9a8e72b832beea9c2f17e0862a-C001-4", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_06de9a8e72b832beea9c2f17e0862a_44", "text": "Later on, pressure from language researchers forced us to replace it with terms such as \"online memory minimization\" [5] because our initial formulation was obscure to them."}
{"sent_id": "06de9a8e72b832beea9c2f17e0862a-C001-7", "intents": ["@USE@"], "paper_id": "ABC_06de9a8e72b832beea9c2f17e0862a_44", "text": "Our position is grounded on the high predictive power of that principle per se [5] ."}
{"sent_id": "1b424cab4d7008997a31be8c2e5198-C001-33", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_1b424cab4d7008997a31be8c2e5198_44", "text": "The standard matching feature of Mou et al. (2016) uses a concatenation of u, v, |u−v| and u · v. We define the following new matching feature vector that scales the multiplicative term by a constant factor η > 0."}
{"sent_id": "1b424cab4d7008997a31be8c2e5198-C001-41", "intents": ["@USE@"], "paper_id": "ABC_1b424cab4d7008997a31be8c2e5198_44", "text": "Choosing η = 1 in w poly2 reduces it to the matching feature vector proposed by Mou et al. (2016) ."}
{"sent_id": "692f7edc151a9a833c7dd7943bb608-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_692f7edc151a9a833c7dd7943bb608_44", "text": "Several research work have been reported since 2010 in this research field of hate speech detection (Kwok and Wang, 2013; Burnap and Williams, 2015; Djuric et al., 2015; Davidson et al., 2017; Malmasi and Zampieri, 2018; Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018; ElSherief et al., 2018; Gambäck and Sikdar, 2017; Zhang et al., 2018; Mathur et al., 2018) ."}
{"sent_id": "692f7edc151a9a833c7dd7943bb608-C001-104", "intents": ["@FUT@"], "paper_id": "ABC_692f7edc151a9a833c7dd7943bb608_44", "text": "The performance may be improved further by incorporating external datasets (Kumar et al., 2018a; Davidson et al., 2017)"}
{"sent_id": "f5ad574acf9ea27c0be3129238fd92-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_f5ad574acf9ea27c0be3129238fd92_44", "text": "Recent work has shown that state-of-the-art neural models of language and translation can be successfully trained on multiple languages simultaneously without changing the model architecture (Östling and Tiedemann, 2017; Johnson et al., 2017) ."}
{"sent_id": "f5ad574acf9ea27c0be3129238fd92-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_f5ad574acf9ea27c0be3129238fd92_44", "text": "The recent advances in neural networks have opened the way to the design of architecturally simple multilingual models for various NLP tasks, such as language modeling or next word prediction (Tsvetkov et al., 2016; Östling and Tiedemann, 2017; Malaviya et al., 2017; Tiedemann, 2018) , translation (Dong et al., 2015; Zoph et al., 2016; Firat et al., 2016; Johnson et al., 2017) , morphological reinflection (Kann et al., 2017) and more (Bjerva, 2017) ."}
{"sent_id": "f5ad574acf9ea27c0be3129238fd92-C001-26", "intents": ["@USE@"], "paper_id": "ABC_f5ad574acf9ea27c0be3129238fd92_44", "text": "We consider the scenario where L1 is overresourced compared to L2 and train our bilingual models by joint training on a mixed L1/L2 corpus so that supervision is provided simultaneously in the two languages (Östling and Tiedemann, 2017; Johnson et al., 2017) ."}
{"sent_id": "3d1f3980190048625ec93517ebffdc-C001-43", "intents": ["@USE@"], "paper_id": "ABC_3d1f3980190048625ec93517ebffdc_44", "text": "We use four families of features: Word n-gram features We use tf.idf -weighted word [1, 3]-grams (Rashkin et al. 2017) ."}
{"sent_id": "3d1f3980190048625ec93517ebffdc-C001-54", "intents": ["@USE@"], "paper_id": "ABC_3d1f3980190048625ec93517ebffdc_44", "text": "We evaluated proppy on data from Rashkin et al. (2017) in a binary setup of distinguishing propaganda vs nonpropaganda."}
{"sent_id": "418e03aa7ba304c4774111e9f300ad-C001-11", "intents": ["@USE@"], "paper_id": "ABC_418e03aa7ba304c4774111e9f300ad_44", "text": "All our models are based on encoder-decoder model introduced by Faruqui et al. (2016) for the morphological inflection task."}
{"sent_id": "418e03aa7ba304c4774111e9f300ad-C001-23", "intents": ["@USE@"], "paper_id": "ABC_418e03aa7ba304c4774111e9f300ad_44", "text": "The work presented in this paper is based on the work of the simple encoder-decoder system of Faruqui et al. (2016) ."}
{"sent_id": "418e03aa7ba304c4774111e9f300ad-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_418e03aa7ba304c4774111e9f300ad_44", "text": "Most of the morphological inflection models are variants of sequence to sequence models applied by Faruqui et al. (2016) to morphological reinflection."}
{"sent_id": "3c4c0875593ed0f196f5295fbaeb37-C001-8", "intents": ["@USE@"], "paper_id": "ABC_3c4c0875593ed0f196f5295fbaeb37_44", "text": "RNNLG (Wen et al., 2015) , TGen (Dušek and Jurčíček, 2015) and LOLS (Lampouras and Vlachos, 2016) , using a large number of 21 automated metrics."}
{"sent_id": "3c4c0875593ed0f196f5295fbaeb37-C001-23", "intents": ["@DIF@"], "paper_id": "ABC_3c4c0875593ed0f196f5295fbaeb37_44", "text": "Using this framework, we collected a dataset of 50k instances in the restaurant domain, which is 10 times bigger than datasets currently used for NLG training, e.g. SFRest and SFHot (Wen et al., 2015) or Bagel (Mairesse et al., 2010) ."}
{"sent_id": "457f9916ed4d7eafacea57e208c760-C001-30", "intents": ["@USE@"], "paper_id": "ABC_457f9916ed4d7eafacea57e208c760_44", "text": "A client may connect to the server and open up a dialogue (see Figure 1 in (Busemann et al., 1997) )."}
{"sent_id": "457f9916ed4d7eafacea57e208c760-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_457f9916ed4d7eafacea57e208c760_44", "text": "The use of SMES in COSMA, semantic analysis and inference, the dialogue model mapping between human and machine dialogue structures, utterance generation, the architectural framework of the server, and the PASHA agent system are described in (Busemann et al., 1997) ."}
{"sent_id": "457f9916ed4d7eafacea57e208c760-C001-44", "intents": ["@EXT@"], "paper_id": "ABC_457f9916ed4d7eafacea57e208c760_44", "text": "We demonstrate extended versions of the systems described in (Busemann et al., 1997) ."}
{"sent_id": "8bdfc9e82e474413f29ee92f81467e-C001-19", "intents": ["@DIF@"], "paper_id": "ABC_8bdfc9e82e474413f29ee92f81467e_44", "text": "In contrast to standard sequenceto-sequence models (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) , HREDs model the dialogue context by introducing a context Recurrent Neural Network (RNN) over the encoder RNN, thus forming a hierarchical encoder."}
{"sent_id": "8bdfc9e82e474413f29ee92f81467e-C001-27", "intents": ["@USE@"], "paper_id": "ABC_8bdfc9e82e474413f29ee92f81467e_44", "text": "where f text θ ,f cxt θ and f dec θ are GRU cells (Cho et al., 2014) ."}
{"sent_id": "8bdfc9e82e474413f29ee92f81467e-C001-60", "intents": ["@USE@"], "paper_id": "ABC_8bdfc9e82e474413f29ee92f81467e_44", "text": "2 We used 512 as the word embedding size as well as hidden dimension for all the RNNs using GRUs (Cho et al., 2014) with tied embeddings for the (bidirectional) encoder and decoder."}
{"sent_id": "db6794da83b12336ab946e5777346d-C001-2", "intents": ["@USE@"], "paper_id": "ABC_db6794da83b12336ab946e5777346d_44", "text": "The title of our talk-an implicit reference to the English cliché like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on Net-like lexical resources (Fellbaum, 1998; Baker et al., 2003; Gader et al., 2012) ."}
{"sent_id": "db6794da83b12336ab946e5777346d-C001-66", "intents": ["@FUT@"], "paper_id": "ABC_db6794da83b12336ab946e5777346d_44", "text": "Work performed on the French Lexical Network (Gader et al., 2012 ) will serve to demonstrate how the lexicographic process can be made closer to actual navigation through lexical knowledge by the speaker."}
{"sent_id": "db6794da83b12336ab946e5777346d-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_db6794da83b12336ab946e5777346d_44", "text": "Computational aspects of the work on the French Lexical Network are dealt with in (Gader et al., 2012) ."}
{"sent_id": "5ad1e8b75cc6f5b627f770cced8e0f-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_5ad1e8b75cc6f5b627f770cced8e0f_44", "text": "Approaches Based on Machine Translation (MT) Evaluation Metrics: Madnani et al. (2012) conduct a study on the usefulness of automated MT evaluation metrics (e.g., BLEU, NIST and Meteor) for the task of paraphrase identification."}
{"sent_id": "5ad1e8b75cc6f5b627f770cced8e0f-C001-96", "intents": ["@SIM@"], "paper_id": "ABC_5ad1e8b75cc6f5b627f770cced8e0f_44", "text": "Similar to Madnani et al. (2012) we use these MT scores separately in a classification task to predict paraphrasticality where the respective MT score is fed into a MaxEnt classifier as only feature."}
{"sent_id": "a26260547114750ba2aa49e5f96136-C001-8", "intents": ["@USE@"], "paper_id": "ABC_a26260547114750ba2aa49e5f96136_44", "text": "For answer generation we used S-net (Tan et al., 2017) model trained on SQuAD and To evaluate our model we used Large-scale RACE (ReAding Comprehension Dataset From Examinations) (Lai et al., 2017)."}
{"sent_id": "a26260547114750ba2aa49e5f96136-C001-24", "intents": ["@USE@"], "paper_id": "ABC_a26260547114750ba2aa49e5f96136_44", "text": "Then we use answer generation using state-of-art S-Net model (Tan et al., 2017) which extract and generate answer figure 2."}
{"sent_id": "a26260547114750ba2aa49e5f96136-C001-54", "intents": ["@USE@"], "paper_id": "ABC_a26260547114750ba2aa49e5f96136_44", "text": "Answer extraction and Generation will be done using state-of-art S-NET model (Tan et al., 2017) ."}
{"sent_id": "a26260547114750ba2aa49e5f96136-C001-65", "intents": ["@USE@"], "paper_id": "ABC_a26260547114750ba2aa49e5f96136_44", "text": "Figure 4 : Answer Synthesis/Generation Model (Tan et al., 2017) The produced answer will be stored in Answer vector."}
{"sent_id": "fe1d6ca4a88c03cfb2ae94ef45030d-C001-16", "intents": ["@MOT@"], "paper_id": "ABC_fe1d6ca4a88c03cfb2ae94ef45030d_44", "text": "There has also been growing interest in deep learning models for keyphrase generation (Meng et al. 2017; Chan et al. 2019) ."}
{"sent_id": "fe1d6ca4a88c03cfb2ae94ef45030d-C001-20", "intents": ["@USE@"], "paper_id": "ABC_fe1d6ca4a88c03cfb2ae94ef45030d_44", "text": "As with most GAN architectures, our model also consists of a generator (G) and discriminator (D), which are trained in an alternating fashion (Goodfellow et al. 2014) ."}
{"sent_id": "fe1d6ca4a88c03cfb2ae94ef45030d-C001-24", "intents": ["@USE@"], "paper_id": "ABC_fe1d6ca4a88c03cfb2ae94ef45030d_44", "text": "We employ catSeq model (Yuan et al. 2018) for the generation process, which uses an encoder-decoder framework: the encoder being a bidirectional Gated Recurrent Unit (bi-GRU) and the decoder a forward GRU."}
{"sent_id": "fbd028e073459b1b4c2d8d99173e15-C001-92", "intents": ["@BACK@", "@FUT@"], "paper_id": "ABC_fbd028e073459b1b4c2d8d99173e15_44", "text": "For example, the training dataset used for the state-ofthe-art system of Hermann et al. (2014) contains only 4,458 labeled targets, which is approximately 40 times less than the number of annotated targets in Ontonotes 4.0 (Hovy et al., 2006) , a standard NLP dataset, containing PropBank-style verb annotations."}
{"sent_id": "fbd028e073459b1b4c2d8d99173e15-C001-76", "intents": ["@DIF@"], "paper_id": "ABC_fbd028e073459b1b4c2d8d99173e15_44", "text": "Subsequently, Hermann et al. (2014) used a very similar framework but presented a novel method using distributed word representations for better frame identification, outperforming the aforementioned update to SEMAFOR."}
{"sent_id": "cffa735deb802118640005a1d527ee-C001-43", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_cffa735deb802118640005a1d527ee_44", "text": "To calculate the disagreement among the committee members several metrics have been proposed including the vote entropy (Engelson and Dagan, 1996) as possibly the most well-known one."}
{"sent_id": "cffa735deb802118640005a1d527ee-C001-104", "intents": ["@USE@"], "paper_id": "ABC_cffa735deb802118640005a1d527ee_44", "text": "Disagreement is measured by vote entropy (Engelson and Dagan, 1996) ."}
{"sent_id": "2dc830dd598102ee82f1b982b88be9-C001-14", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_2dc830dd598102ee82f1b982b88be9_44", "text": "Singular value decomposition (SVD) is a common factorization technique and has been explored in feedforward networks [9, 10, 21, 22] and recurrent neural networks (RNN) [11] ."}
{"sent_id": "2dc830dd598102ee82f1b982b88be9-C001-26", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_2dc830dd598102ee82f1b982b88be9_44", "text": "Low-rank matrix factorization The factorization of weight matrices is based on the SVD compression of LSTM [11] ."}
{"sent_id": "2dc830dd598102ee82f1b982b88be9-C001-53", "intents": ["@USE@"], "paper_id": "ABC_2dc830dd598102ee82f1b982b88be9_44", "text": "As the distribution of weight matrices' eigenvalues can be different across different LSTM layers, we follow the practice of [11] to set the same threshold τ across layers as the fraction of retained singular values, defined as τ = Table 1 summarizes the results of low-rank matrix factorization compared to our baseline 3-layer LSTM."}
{"sent_id": "2e95d98d5f9d4d6fc90e3d8453f945-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_2e95d98d5f9d4d6fc90e3d8453f945_45", "text": "1 (Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability."}
{"sent_id": "2e95d98d5f9d4d6fc90e3d8453f945-C001-42", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_2e95d98d5f9d4d6fc90e3d8453f945_45", "text": "(Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task."}
{"sent_id": "2e95d98d5f9d4d6fc90e3d8453f945-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_2e95d98d5f9d4d6fc90e3d8453f945_45", "text": "For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630."}
{"sent_id": "f5bf9a833c3d46b00d70498e4f1c1b-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_f5bf9a833c3d46b00d70498e4f1c1b_45", "text": "Applications of lp and mad are varied, including video recommendation [1] and sentiment analysis over Twitter [5] ."}
{"sent_id": "3ff58556ab973a9dde640a2b74c37b-C001-28", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_3ff58556ab973a9dde640a2b74c37b_45", "text": "The two existing systems that use function labels sucessfully, either inherit Collins' modelling of the notion of complement (Gabbard, Kulick and Marcus, 2006) or model function labels directly (Musillo and Merlo, 2005) ."}
{"sent_id": "3ff58556ab973a9dde640a2b74c37b-C001-45", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_3ff58556ab973a9dde640a2b74c37b_45", "text": "For more information on this technique to capture structural domains, see (Musillo and Merlo, 2005) where the technique was applied to function parsing."}
{"sent_id": "3ff58556ab973a9dde640a2b74c37b-C001-48", "intents": ["@EXT@"], "paper_id": "ABC_3ff58556ab973a9dde640a2b74c37b_45", "text": "Extending a technique presented in (Klein and Manning, 2003 ) and adopted in (Merlo and Musillo, 2005) for function labels with stateof-the-art results, we split some part-of-speech tags into tags marked with AM-X semantic role labels."}
{"sent_id": "9c3c35343aeaae0520d92f64e118a2-C001-76", "intents": ["@BACK@"], "paper_id": "ABC_9c3c35343aeaae0520d92f64e118a2_45", "text": "At (Chen et al., 2017) , a Chinese dataset for aspectbased sentiment analysis from comments about the news with 6365 positive, 9457 neutral and 6839 negative annotated data samples was proposed."}
{"sent_id": "9c3c35343aeaae0520d92f64e118a2-C001-102", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_9c3c35343aeaae0520d92f64e118a2_45", "text": " RAM (Chen et al., 2017) : This method makes a memory from the input and with using multiple attention mechanism, it extracts important information from memory and for prediction it uses a combination of the extracted features of different attentions non-linearly."}
{"sent_id": "9c3c35343aeaae0520d92f64e118a2-C001-109", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_9c3c35343aeaae0520d92f64e118a2_45", "text": "The authors of RAM (Chen et al., 2017) claimed that their model is language insensitive, which mean it can perform on all languages and, compared to TD-LSTM (Tang et al., 2016) which might lose feature if the opinion word is far from the target, they employed the recurrent attention to solving this problem."}
{"sent_id": "929020618e8e1daa6a769f552a4655-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_929020618e8e1daa6a769f552a4655_45", "text": "Models proposed by Vajjala and Meurers [17] , Xia et al. [18] , and Mohammadi and Khasteh [19] are examples of state-of-the-art models for their target languages and target audience."}
{"sent_id": "929020618e8e1daa6a769f552a4655-C001-78", "intents": ["@BACK@"], "paper_id": "ABC_929020618e8e1daa6a769f552a4655_45", "text": "Xia et al. [18] has published a thorough study on second language text readability assessment."}
{"sent_id": "929020618e8e1daa6a769f552a4655-C001-170", "intents": ["@USE@"], "paper_id": "ABC_929020618e8e1daa6a769f552a4655_45", "text": "To examine the proposed DRL model regarding this ability, it is applied to the Cambridge Exams dataset [18] ."}
{"sent_id": "8b223f35a4685d6627d29c907e4742-C001-51", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_8b223f35a4685d6627d29c907e4742_45", "text": "Caliskan et al. proposed Word Embedding Association Test (WEAT) to computationally measure biases in any text repository [5] ."}
{"sent_id": "8b223f35a4685d6627d29c907e4742-C001-125", "intents": ["@BACK@"], "paper_id": "ABC_8b223f35a4685d6627d29c907e4742_45", "text": "Caliskan et al. designed the Word Embedding Association Test (WEAT) by tweaking the IAT test [5] ."}
{"sent_id": "8b223f35a4685d6627d29c907e4742-C001-135", "intents": ["@USE@"], "paper_id": "ABC_8b223f35a4685d6627d29c907e4742_45", "text": "We borrowed these attribute and target word sets from Caliskan et al. [5] ."}
{"sent_id": "f36b605a9088532e5f430c86ffb363-C001-6", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_f36b605a9088532e5f430c86ffb363_45", "text": "Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks (Sahlgren, 2006; Padó and Lapata, 2007) ."}
{"sent_id": "f36b605a9088532e5f430c86ffb363-C001-45", "intents": ["@USE@"], "paper_id": "ABC_f36b605a9088532e5f430c86ffb363_45", "text": "2 Following Padó and Lapata (2007) , we only consider co-occurrences where two target words are connected by certain dependency paths, namely: the top 30 most frequent preposition-mediated noun-to-noun paths (soldier+with+gun), the top 50 transitive-verbmediated noun-to-noun paths (soldier+use+gun), the top 30 direct or preposition-mediated verbnoun paths (kill+obj+victim, kill+in+school), and the modifying and predicative adjective-to-noun paths."}
{"sent_id": "f36b605a9088532e5f430c86ffb363-C001-68", "intents": ["@USE@"], "paper_id": "ABC_f36b605a9088532e5f430c86ffb363_45", "text": "Following previous simulations of this data-set (Padó and Lapata, 2007) , we measure the similarity of each related target-prime pair, and we compare it to the average similarity of the target to all the other primes instantiating the same relation, treating the latter quantity as our surrogate of an unrelated target-prime pair."}
{"sent_id": "cbfa4d71f40d8008ebd90026dc1bcd-C001-18", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_cbfa4d71f40d8008ebd90026dc1bcd_45", "text": "In this paper we focus only on the streaming shallow processing part of the SUMMA project (the dark block in Fig.1 ), where the recently developed neural machine translation techniques (Sutskerev, Vinyals & Le, 2014; Bahdanau, Cho & Bengio, 2014) enable radically new end-to-end approach to machine translation and clustering of the incoming news stories."}
{"sent_id": "cbfa4d71f40d8008ebd90026dc1bcd-C001-37", "intents": ["@USE@"], "paper_id": "ABC_cbfa4d71f40d8008ebd90026dc1bcd_45", "text": "Table 2 illustrates the character-level neural translation from English to Latvian using modified 2 TensorFlow (Abadi et al., 2015) seq2seq (Sutskerev, Vinyals & Le, 2014) neural translation model."}
{"sent_id": "cbfa4d71f40d8008ebd90026dc1bcd-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_cbfa4d71f40d8008ebd90026dc1bcd_45", "text": "The key difference of the SUMMA project is that it has been incepted after the recent paradigm-shift (Manning, 2015) in the NLP community towards neural network inspired deep learning techniques such as end-to-end automatic speech recognition (Graves & Jaitly, 2014; Hannun et al., 2014; Amodei, 2015) , end-to-end machinetranslation (Sutskerev, Vinyals & Le, 2014; Bahdanau, Cho & Bengio, 2014; Luong et al., 2015) , efficient distributed vectorspace word embeddings (Mikolov et al., 2013) , image and video captioning Venugopalan et al., 2015) , unsupervised learning of document representations by autoencoders (Li, Luong & Jurafsky, 2015) ."}
{"sent_id": "cbfa4d71f40d8008ebd90026dc1bcd-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_cbfa4d71f40d8008ebd90026dc1bcd_45", "text": "Even training of a single state-of-the-art sentencelevel translational autoencoder requires days of GPU computing (Barzdins & Gosko, 2016) ) in TensorFlow (Abadi et al., 2015) seq2seq model (Sutskerev, Vinyals & Le, 2014; Bahdanau, Cho & Bengio, 2014) ."}
{"sent_id": "c42e9d10ca8876af80eee021c969d7-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_c42e9d10ca8876af80eee021c969d7_45", "text": "Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems (Forbes-Riley et al., 2007) , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (VanLehn et al., 2007) ."}
{"sent_id": "c42e9d10ca8876af80eee021c969d7-C001-10", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_c42e9d10ca8876af80eee021c969d7_45", "text": "Although traditional top-down approaches (e.g., Cade et al., 2008) and some empirical work on analyzing the structure of tutorial dialogue (Forbes-Riley et al., 2007) have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure."}
{"sent_id": "c42e9d10ca8876af80eee021c969d7-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_c42e9d10ca8876af80eee021c969d7_45", "text": "The importance of adjacency pairs is wellestablished in natural language dialogue (e.g., Schlegoff & Sacks, 1973) , and adjacency pair analysis has illuminated important phenomena in tutoring as well (Forbes-Riley et al., 2007) ."}
{"sent_id": "253d635829c733309bb49fc1fcc1cd-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_253d635829c733309bb49fc1fcc1cd_45", "text": "The Liar dataset (Wang, 2017) is the first large dataset collected through reliable annotation, but it contains only short statements."}
{"sent_id": "b21dfcb9854b0b48af47f4f13899b0-C001-14", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_b21dfcb9854b0b48af47f4f13899b0_45", "text": "In our previous work (Zhang and Litman, 2014; Zhang and Litman, 2015) , we focused on 1) and 2), the automatic extraction and classification of revisions for argumentative writings."}
{"sent_id": "b21dfcb9854b0b48af47f4f13899b0-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_b21dfcb9854b0b48af47f4f13899b0_45", "text": "Our prior work (Zhang and Litman, 2015) demonstrates that only Text-based revisions are significantly correlated with the writing improvement."}
{"sent_id": "b21dfcb9854b0b48af47f4f13899b0-C001-37", "intents": ["@USE@"], "paper_id": "ABC_b21dfcb9854b0b48af47f4f13899b0_45", "text": "Following the argumentative revision definition in our prior work (Zhang and Litman, 2015) , revisions are first categorized to Content (Text-based) and Surface 3 according to whether the revision changed the meaning of the essay or not."}
{"sent_id": "8fc0d25eb177ea876c2b69096f0145-C001-23", "intents": ["@USE@"], "paper_id": "ABC_8fc0d25eb177ea876c2b69096f0145_45", "text": "We also used another two pretrained embeddings: polyglot embedding trained on Indonesian Wikipedia (Al-Rfou et al., 2013) and NLPL embedding trained on the Indonesian portion of CoNLL 2017 corpus (Fares et al., 2017) ."}
{"sent_id": "fddb1d19895976661babdc17d232ee-C001-24", "intents": ["@USE@"], "paper_id": "ABC_fddb1d19895976661babdc17d232ee_45", "text": "To that end, we propose to use an attention-based model and gradient-weighted class activation maps [9] , inspired by the recent successes of the attention mechanism in other domains [15, 16] ."}
{"sent_id": "fddb1d19895976661babdc17d232ee-C001-40", "intents": ["@USE@"], "paper_id": "ABC_fddb1d19895976661babdc17d232ee_45", "text": "Weights α i are computed with attention mechanism implemented as a two-layer neural network [16] : the first layer produces a hidden representation u i = tanh(W u q i + b u ) and the second layer outputs unnormalized importance a i = W a u i + b a ."}
{"sent_id": "fddb1d19895976661babdc17d232ee-C001-60", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_fddb1d19895976661babdc17d232ee_45", "text": "For visualizations in the text domain, we use attention weights β t used to compute text representation d. These weights capture relative importance of words in their context to headline popularity, as shown in [16] in the context of sentiment analysis."}
{"sent_id": "1e232f9dfa7d499d1ba39fcebf3d1a-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_1e232f9dfa7d499d1ba39fcebf3d1a_45", "text": "Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008) , are trained and tested on hand-annotated data."}
{"sent_id": "1e232f9dfa7d499d1ba39fcebf3d1a-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_1e232f9dfa7d499d1ba39fcebf3d1a_45", "text": "For example, users of the Charniak parser (Charniak, 2001) should add the AUX category to the PTB parts of speech and adjust their systems to account for the conversion of the word ain't into the tokens IS and n't."}
{"sent_id": "1e232f9dfa7d499d1ba39fcebf3d1a-C001-62", "intents": ["@USE@"], "paper_id": "ABC_1e232f9dfa7d499d1ba39fcebf3d1a_45", "text": "We use Charniak, UMD and KNP parsers (Charniak, 2001; Huang and Harper, 2009; Kurohashi and Nagao, 1998) , JET Named Entity tagger (Grishman et al., 2005; Ji and Grishman, 2006) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper."}
{"sent_id": "ab919bfae9ddf780cadcd491fe0a9b-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_ab919bfae9ddf780cadcd491fe0a9b_45", "text": "White (2014) then showed that even better results can be achieved by inducing a grammar (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013) that is directly compatible with (an enhanced version of) the SR-11 inputs."}
{"sent_id": "ab919bfae9ddf780cadcd491fe0a9b-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_ab919bfae9ddf780cadcd491fe0a9b_45", "text": "With this issue in mind, White (2014) experimented with a version of the shallow SR-11 inputs (created by Richard Johansson) which included extra dependencies for unbounded dependencies and coordination, yielding dependency graphs extending core dependency trees."}
{"sent_id": "ab919bfae9ddf780cadcd491fe0a9b-C001-56", "intents": ["@EXT@"], "paper_id": "ABC_ab919bfae9ddf780cadcd491fe0a9b_45", "text": "We have adapted and extended White's (2014) CCG induction algorithm to work with the augmented UDs that our system produces."}
{"sent_id": "8c530e0c9f7256ac44b1a2adfaf6a9-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_8c530e0c9f7256ac44b1a2adfaf6a9_45", "text": "Since, emotion detection is a classification problem, research works have been carried out by using machine learning with lexical features (Sharma et al., 2017) and deep learning with deep neural network (Phan et al., 2016) and convolutional neural network (Zahiri and Choi, 2018) to detect the emotions from text."}
{"sent_id": "8c530e0c9f7256ac44b1a2adfaf6a9-C001-41", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_8c530e0c9f7256ac44b1a2adfaf6a9_45", "text": "Instead of using basic CNN, a new recurrent sequential CNN is used by Zahiri and Choi (2018) ."}
{"sent_id": "8c530e0c9f7256ac44b1a2adfaf6a9-C001-25", "intents": ["@USE@"], "paper_id": "ABC_8c530e0c9f7256ac44b1a2adfaf6a9_45", "text": "This section reviews the research work reported for emotion detection from text / tweets (Perikos and Hatzilygeroudis, 2013; Rao, 2016; AbdulMageed and Ungar, 2017; Samy et al., 2018; AlBalooshi et al., 2018; Gaind et al., 2019 ) and text conversations (Phan et al., 2016; Sharma et al., 2017; Zahiri and Choi, 2018) ."}
{"sent_id": "2abfa447cea31af26d06d4325c94ac-C001-6", "intents": ["@BACK@"], "paper_id": "ABC_2abfa447cea31af26d06d4325c94ac_45", "text": "Recent successes in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000; Henderson, 2003) have brought the hope that the same approach could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence."}
{"sent_id": "2abfa447cea31af26d06d4325c94ac-C001-18", "intents": ["@USE@"], "paper_id": "ABC_2abfa447cea31af26d06d4325c94ac_45", "text": "We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task."}
{"sent_id": "2abfa447cea31af26d06d4325c94ac-C001-39", "intents": ["@USE@"], "paper_id": "ABC_2abfa447cea31af26d06d4325c94ac_45", "text": "To achieve the complex task of assigning semantic role labels while parsing, we use a family of state-of-the-art history-based statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003) , which use a form of left-corner parse strategy to map parse trees to sequences of derivation steps."}
{"sent_id": "ab08b0f3c8691852b3aab5e3575ebd-C001-21", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_ab08b0f3c8691852b3aab5e3575ebd_45", "text": "SODA is based on an iterative ensemble based adaptation technique (Bhatt et al., 2015) which gradually transfers knowledge from the source to the new target collection while being cognizant of similarity between the two collections."}
{"sent_id": "ab08b0f3c8691852b3aab5e3575ebd-C001-54", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_ab08b0f3c8691852b3aab5e3575ebd_45", "text": "Algorithm 1 summarizes our approach (refer (Bhatt et al., 2015) for more details)."}
{"sent_id": "7404a4e4e23eea9663b580f9959689-C001-13", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_7404a4e4e23eea9663b580f9959689_45", "text": "The edit operations include: (M)atching an element in S with an element in T; (I)nserting an element into T, and (D)eleting an element in S. 1 The generation task is part of the NEWS 2009 machine transliteration shared task (Li et al., 2009) Based on all representative symbols used for each of the two languages, emission costs for each of the edit operations and transition parameters can be estimated and used in measuring the similarity between two strings."}
{"sent_id": "7404a4e4e23eea9663b580f9959689-C001-59", "intents": ["@USE@"], "paper_id": "ABC_7404a4e4e23eea9663b580f9959689_45", "text": "The data used is divided according to the experimental runs that were specified for the NEWS 2009 shared transliteration task (Li et al., 2009 ): a standard run and non-standard runs."}
{"sent_id": "7404a4e4e23eea9663b580f9959689-C001-68", "intents": ["@USE@", "@UNSURE@"], "paper_id": "ABC_7404a4e4e23eea9663b580f9959689_45", "text": "These include (Li et al., 2009) : Accuracy (ACC), Fuzziness in Top-1 (Mean F Score), Mean Reciprocal Rank (MRR), Mean Average Precision for reference transliterations (MAP_R), Mean Average Precision in 10 best candidate transliterations (MAP_10), Mean Average Precision for the system (MAP_sys)."}
{"sent_id": "16780bd3c2b350f6d61f2f55f9f88c-C001-55", "intents": ["@USE@"], "paper_id": "ABC_16780bd3c2b350f6d61f2f55f9f88c_45", "text": "For our study, we use a small corpus of Enron email threads which has been previously annotated with dialog acts (Hu et al., 2009 )."}
{"sent_id": "16780bd3c2b350f6d61f2f55f9f88c-C001-83", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_16780bd3c2b350f6d61f2f55f9f88c_45", "text": "An utterance has one of 5 dialog acts: RequestAction, RequestInformation, Inform, Commit and Conventional (see (Hu et al., 2009 ) for details)."}
{"sent_id": "16780bd3c2b350f6d61f2f55f9f88c-C001-107", "intents": ["@USE@"], "paper_id": "ABC_16780bd3c2b350f6d61f2f55f9f88c_45", "text": "We instead use the DA tagger of Hu et al. (2009) , which we re-trained using the training sets for each of our cross validation folds, applying it to the test set of that fold."}
{"sent_id": "ec5897c392b05cb8712feadfc6d2bf-C001-26", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_ec5897c392b05cb8712feadfc6d2bf_46", "text": "Hatzivassiloglou and McKeown (1993) duster adjectives into partitions and present an interesting evaluation to compare the generated adjective classes against those provided by an expert."}
{"sent_id": "ec5897c392b05cb8712feadfc6d2bf-C001-41", "intents": ["@USE@"], "paper_id": "ABC_ec5897c392b05cb8712feadfc6d2bf_46", "text": "We have adopted the F-measure (Hatzivassiloglou and McKeown, 1993; Chincor, 1992) ."}
{"sent_id": "ec5897c392b05cb8712feadfc6d2bf-C001-70", "intents": ["@USE@"], "paper_id": "ABC_ec5897c392b05cb8712feadfc6d2bf_46", "text": "Once all classes in the two clusterings have been accounted for, calculate the precision, recall, and F-measure as explained in (Hatzivassiloglou and McKeown, 1993) ."}
{"sent_id": "2f7b64db6939786a5026fc033c85bd-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_2f7b64db6939786a5026fc033c85bd_46", "text": "Until recently, GRE algorithms have focussed on the generation of distinguishing descriptions that are either as short as possible (e.g. (Dale, 1992; Gardent, 2002) ) or almost as short as possible (e.g. (Dale and Reiter, 1995) )."}
{"sent_id": "2f7b64db6939786a5026fc033c85bd-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_2f7b64db6939786a5026fc033c85bd_46", "text": "allow the Full Brevity algorithm (Dale, 1992) to be viewed as minimising cost(S), and the incremental algorithm (Dale and Reiter, 1995) as hill-climbing (strictly, hill-descending), guided by the property-ordering which that algorithm requires."}
{"sent_id": "2f7b64db6939786a5026fc033c85bd-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_2f7b64db6939786a5026fc033c85bd_46", "text": "Standard GRE algorithms assume that the speaker knows what the hearer knows (Dale and Reiter, 1995) ."}
{"sent_id": "854679aec9cf4f53e97936f865f49c-C001-65", "intents": ["@BACK@"], "paper_id": "ABC_854679aec9cf4f53e97936f865f49c_46", "text": "It is, however, affected by domain variation - Foster et al. (2007) report that its f-score drops by approximately 8 percentage points when applied to the BNC domain."}
{"sent_id": "854679aec9cf4f53e97936f865f49c-C001-85", "intents": ["@BACK@"], "paper_id": "ABC_854679aec9cf4f53e97936f865f49c_46", "text": "The new system is trained on f-structureannotated parser output trees, and the performance of Charniak and Johnson's parser degrades when applied to BNC data (Foster et al., 2007) ."}
{"sent_id": "854679aec9cf4f53e97936f865f49c-C001-97", "intents": ["@EXT@"], "paper_id": "ABC_854679aec9cf4f53e97936f865f49c_46", "text": "We intend to continue this research by training our generator on parse trees produced by a BNC-self-trained version of the Charniak and Johnson reranking parser (Foster et al., 2007) ."}
{"sent_id": "31b06dfc081149e1e436f0bb5e0904-C001-9", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_31b06dfc081149e1e436f0bb5e0904_46", "text": "As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009 Martins et al., , 2013 ."}
{"sent_id": "31b06dfc081149e1e436f0bb5e0904-C001-26", "intents": ["@USE@"], "paper_id": "ABC_31b06dfc081149e1e436f0bb5e0904_46", "text": "The parser was built as an extension of a recent dependency parser, TurboParser (Martins et al., 2010 (Martins et al., , 2013 , with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD)."}
{"sent_id": "31b06dfc081149e1e436f0bb5e0904-C001-30", "intents": ["@USE@"], "paper_id": "ABC_31b06dfc081149e1e436f0bb5e0904_46", "text": "Most of these features were taken from TurboParser (Martins et al., 2013) , and others were inspired by the semantic parser of Johansson and Nugues (2008) ."}
{"sent_id": "fc4b56c865c8a9d0f6a7f5ae37ba96-C001-9", "intents": ["@USE@"], "paper_id": "ABC_fc4b56c865c8a9d0f6a7f5ae37ba96_46", "text": "Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 (Bojar et al., 2015) as well as the statistics of the ParFDA selected training and LM data."}
{"sent_id": "fc4b56c865c8a9d0f6a7f5ae37ba96-C001-20", "intents": ["@USE@"], "paper_id": "ABC_fc4b56c865c8a9d0f6a7f5ae37ba96_46", "text": "We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task (Bojar et al., 2015) , which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru)."}
{"sent_id": "ece5f95d3c616ceeb0b3061e606b41-C001-13", "intents": ["@MOT@"], "paper_id": "ABC_ece5f95d3c616ceeb0b3061e606b41_46", "text": "In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a) ."}
{"sent_id": "ece5f95d3c616ceeb0b3061e606b41-C001-45", "intents": ["@MOT@", "@SIM@"], "paper_id": "ABC_ece5f95d3c616ceeb0b3061e606b41_46", "text": "We chose these parameters for our system to obtain comparable results to the ones in (Mikolov et al., 2013a ) for a CBOW architecture but trained with 783 million words (50.4%)."}
{"sent_id": "ece5f95d3c616ceeb0b3061e606b41-C001-23", "intents": ["@USE@"], "paper_id": "ABC_ece5f95d3c616ceeb0b3061e606b41_46", "text": "The basic architecture that we use to build our models is CBOW (Mikolov et al., 2013a) ."}
{"sent_id": "511c17a6cb6bd74e0216c3d50eb9c0-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_511c17a6cb6bd74e0216c3d50eb9c0_46", "text": "As opposed to the situation for Irish Gaelic (Lynn et al., 2012a; Lynn et al., 2012b; Lynn et al., 2013; Lynn et al., 2014) there are no treebanks or tagging schemes for Scottish Gaelic, although there are machine-readable dictionaries and databases available from Sabhal Mòr Ostaig."}
{"sent_id": "511c17a6cb6bd74e0216c3d50eb9c0-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_511c17a6cb6bd74e0216c3d50eb9c0_46", "text": "A very important scheme is the Dublin scheme for Irish (Lynn et al., 2012a; Lynn et al., 2012b; Lynn et al., 2013) , which is of a similar size to the Stanford scheme, but the reason for its size relative to GR is that it includes a large number of dependencies intended to handle grammatical features found in Irish but not in English."}
{"sent_id": "511c17a6cb6bd74e0216c3d50eb9c0-C001-70", "intents": ["@DIF@"], "paper_id": "ABC_511c17a6cb6bd74e0216c3d50eb9c0_46", "text": "Note that this is different from the scheme in Lynn et al. (2012b) because of a difference between the two languages."}
{"sent_id": "ec3702a6b30057fcae65ca297656d2-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_ec3702a6b30057fcae65ca297656d2_46", "text": "While the number of overlapping words is dependent on the families of the source and target languages and their orthography, Krstovski and Smith (2011) showed that this approach yields good results across language pairs from different families and writing systems such as English-Greek, English-Bulgarian and EnglishArabic where, as one would expect, most shared words are numbers and named entities."}
{"sent_id": "ec3702a6b30057fcae65ca297656d2-C001-32", "intents": ["@USE@"], "paper_id": "ABC_ec3702a6b30057fcae65ca297656d2_46", "text": "Our bootstrapping approach (Figure 1 ) is a twostage system that used the Overlapping Cosine Distance (OCD) approach of Krstovski and Smith (2011) as its first step."}
{"sent_id": "d9d5fce2b33c15bf073a5840930be1-C001-77", "intents": ["@DIF@"], "paper_id": "ABC_d9d5fce2b33c15bf073a5840930be1_46", "text": "For example, we don't replace entity names (like genes) with shorter alternatives as is done with the noun phrases in the present version and with the gene names in our earlier version 11 ."}
{"sent_id": "d9d5fce2b33c15bf073a5840930be1-C001-94", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_d9d5fce2b33c15bf073a5840930be1_46", "text": "We also compare the present version of BioSimplify with the older version 11 which is limited in its functionality because it only implements the rules described by Siddharthan 4 ."}
{"sent_id": "c932ba05eb5cb30094dd98739daa95-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_c932ba05eb5cb30094dd98739daa95_46", "text": "Compared to other existing systems for predicate-argument extraction (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015) , the use of manual language-agnostic patterns on UD makes PredPatt a well-founded component across languages."}
{"sent_id": "c932ba05eb5cb30094dd98739daa95-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_c932ba05eb5cb30094dd98739daa95_46", "text": "We convert the PropBank annotations for all verbal predicates in these two corpora, and ignore roles of directional (DIR), manner (MNR), modals (MOD), negation (NEG) and adverbials (ADV), as they aren't extracted as distinct argument but instead are folded into the complex predicate by PredPatt and other systems for predicate-argument extraction (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015) ."}
{"sent_id": "c932ba05eb5cb30094dd98739daa95-C001-60", "intents": ["@USE@"], "paper_id": "ABC_c932ba05eb5cb30094dd98739daa95_46", "text": "In this section, we evaluate the original PredPatt (PredPatt v1) and the improved PredPatt (PredPatt v2) on the English Web Treebank (EWT) and the Wall Street Journal corpus (WSJ), and compare their performance with four prominent Open IE systems: OpenIE 4, 6 OLLIE (Mausam et al., 2012) , ClausIE (Del Corro and Gemulla, 2013) , and Stanford Open IE (Angeli et al., 2015) ."}
{"sent_id": "5cb56f6bf8123a9949a7f7c4ebc85c-C001-3", "intents": ["@USE@"], "paper_id": "ABC_5cb56f6bf8123a9949a7f7c4ebc85c_46", "text": "Our study focuses on evaluating transfer learning using BERT (Devlin et al., 2019) to classify tokens from hotel reviews in bahasa Indonesia."}
{"sent_id": "5cb56f6bf8123a9949a7f7c4ebc85c-C001-19", "intents": ["@USE@"], "paper_id": "ABC_5cb56f6bf8123a9949a7f7c4ebc85c_46", "text": "Our main contribution in this study is evaluating BERT (Devlin et al., 2019) as a pretrained transformer model on this token classification task on hotel reviews in bahasa Indonesia."}
{"sent_id": "5cb56f6bf8123a9949a7f7c4ebc85c-C001-34", "intents": ["@USE@"], "paper_id": "ABC_5cb56f6bf8123a9949a7f7c4ebc85c_46", "text": "We proposed to use transfer learning from pretrained BERT-Base, Multilingual Cased (Devlin et al., 2019) for this token classification problem."}
{"sent_id": "5cb56f6bf8123a9949a7f7c4ebc85c-C001-64", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5cb56f6bf8123a9949a7f7c4ebc85c_46", "text": "In their paper, Devlin et al. (2019) show that they can achieve state-of-the-art performance not only on sentence-level, but also on token-level tasks, such as for named entity recognition (NER)."}
{"sent_id": "4b17b24ec0203263e581cbeeaa9fc7-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_4b17b24ec0203263e581cbeeaa9fc7_46", "text": "There has been considerable recent interest in the use of statistical methods for grouping words in large on-line corpora into categories which capture some of our intuitions about the reference of the words we use and the relationships between them (e.g. Brown et al., 1992; Schiitze, 1993) ."}
{"sent_id": "4b17b24ec0203263e581cbeeaa9fc7-C001-22", "intents": ["@SIM@"], "paper_id": "ABC_4b17b24ec0203263e581cbeeaa9fc7_46", "text": "Using an approach similar to that of Brown et al. (1992) , each 'target word '1 wi in the corpus was represented as a vector in which each component j is the probability that any one word position in a 'context window' will be occupied by a 'context word' wj, given that the window is centred on word wi."}
{"sent_id": "4b17b24ec0203263e581cbeeaa9fc7-C001-31", "intents": ["@USE@"], "paper_id": "ABC_4b17b24ec0203263e581cbeeaa9fc7_46", "text": "The approach described here differs from that of Brown et al. (1992) in that context words both preceding and following the target word are considered (although information about the ordering of the context was not used), and in that Euclidean distance, rather than average mutual information, is used for clustering."}
{"sent_id": "13249ad2fd022b9b4f1d22d2ca77cd-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_13249ad2fd022b9b4f1d22d2ca77cd_46", "text": "The weights of this linear combination are usually trained to maximise some automatic translation metric (e.g. BLEU) [1] using Minimum Error Rate Training (MERT) [2, 3] or a variant of the Margin Infused Relaxed Algorithm (MIRA) [4, 5] ."}
{"sent_id": "13249ad2fd022b9b4f1d22d2ca77cd-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_13249ad2fd022b9b4f1d22d2ca77cd_46", "text": "The most common approach is an iterative algorithm MERT [3] which employs N-best lists (the best N translations decoded with a weight set from a previous iteration) as candidate translations C. In this way, the loss function is constructed as E(Ē,Ê) = S s=1 E(ē s ,ê s ), whereē is the reference sentence,ê is selected from N-best lists byê s = arg max e∈C K k=1 w k H k (e, f s ) and S represents the volume of sentences."}
{"sent_id": "15bacab4a8c520cfcdd7e7bd1e9ec5-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_15bacab4a8c520cfcdd7e7bd1e9ec5_46", "text": "We will introduce an in-depth case study of Generative Adversarial Networks for NLP, with a focus on dialogue generation (Li et al., 2017) ."}
{"sent_id": "15bacab4a8c520cfcdd7e7bd1e9ec5-C001-49", "intents": ["@USE@"], "paper_id": "ABC_15bacab4a8c520cfcdd7e7bd1e9ec5_46", "text": "Finally, we provide an in-depth case study of deploying two-agent GAN models for conversational AI (Li et al., 2017) ."}
{"sent_id": "7e52a90a9a0a703250d5c3c1890058-C001-12", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_7e52a90a9a0a703250d5c3c1890058_46", "text": "There have been efforts undertaken to apply the grammar engine technique instead (Byamugisha et al., 2016a; Byamugisha et al., 2016b; Byamugisha et al., 2016c) , which resulted in theoretical advances in verbalization rules for ontologies, pluralization of nouns, and verb conjugation that address the text generation needs for Runyankore."}
{"sent_id": "7e52a90a9a0a703250d5c3c1890058-C001-21", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_7e52a90a9a0a703250d5c3c1890058_46", "text": "The CFG specified in (Byamugisha et al., 2016b) was implemented using the CFG Java tool (Xu et al., 2011) ."}
{"sent_id": "4f0dec0ce2d7639c250be00d5efee4-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_4f0dec0ce2d7639c250be00d5efee4_46", "text": "Word collocation is one source of information that has been proposed as a useful tool to post-process word recognition results( [1, 4] )."}
{"sent_id": "45723171ec398550e687c57d42e7cc-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_45723171ec398550e687c57d42e7cc_46", "text": "The social media mining for health applications (SMM4H) shared task (Weissenbacher et al., 2018) hosts four tasks aiming to identify mentions of different aspects medication use on Twitter."}
{"sent_id": "5ed24e18f892d7092c183acab4b175-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_5ed24e18f892d7092c183acab4b175_46", "text": "Recently, high quality and easy to train Skip-gram shallow architectures were presented in [10] and considerably improved in [11] with the introduction of negative sampling and subsampling of frequent words."}
{"sent_id": "5ed24e18f892d7092c183acab4b175-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_5ed24e18f892d7092c183acab4b175_46", "text": "Google News is one of the biggest and richest text sets with 100 billion tokens and a vocabulary of 3 million words and phrases [10] ."}
{"sent_id": "2b7267b7b192aeca15c0d10a5f0a4b-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_2b7267b7b192aeca15c0d10a5f0a4b_46", "text": "An important work that has relevance here is [8] where authors present an even larger movie review dataset of 50,000 movie reviews from IMBD."}
{"sent_id": "2b7267b7b192aeca15c0d10a5f0a4b-C001-92", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_2b7267b7b192aeca15c0d10a5f0a4b_46", "text": "In [8] for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words."}
{"sent_id": "debdaa202ebd856991e09e5e00a12b-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_debdaa202ebd856991e09e5e00a12b_46", "text": "This architecture has been empirically shown to perform well at Named Entity Recognition (NER) tasks (Lample et al., 2016) ."}
{"sent_id": "debdaa202ebd856991e09e5e00a12b-C001-42", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_debdaa202ebd856991e09e5e00a12b_46", "text": "In the Named Entity Recognition task, we utilized a deep learning approach, given the demonstrated effectiveness of such an architecture in this domain (Lample et al., 2016) ."}
{"sent_id": "b6c33fbb73cbf0af580e8dd14dc59a-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_b6c33fbb73cbf0af580e8dd14dc59a_47", "text": "For the task of text generation, MaskGAN [13] uses a Reinforcement Learning signal from the discriminator, FMD-GAN [14] uses an optimal transport mechanism as an objective function."}
{"sent_id": "b6c33fbb73cbf0af580e8dd14dc59a-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_b6c33fbb73cbf0af580e8dd14dc59a_47", "text": "GANs have shown to be successful in image generation tasks [18] and recently, some progress has been observed in text generation [14, 13, 16] ."}
{"sent_id": "19b647ab74d28b59b7df2be729b2d7-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_19b647ab74d28b59b7df2be729b2d7_47", "text": "The style of an utterance can be altered based on requirements; introducing elements of sarcasm, or aspects of factual and emotional argumentation styles [15, 14] ."}
{"sent_id": "19b647ab74d28b59b7df2be729b2d7-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_19b647ab74d28b59b7df2be729b2d7_47", "text": "By using machine learning models designed to classify different classes of interest, such as sentiment, sarcasm, and topic, data can be bootstrapped to greatly increase the amount of data available for indexing and utterance selection [15] ."}
{"sent_id": "38ad38f25a2823c64cd16bc9f2af93-C001-5", "intents": ["@EXT@"], "paper_id": "ABC_38ad38f25a2823c64cd16bc9f2af93_47", "text": "Nous traduisons un corpus parallèle bilingue Mboshi-Français (Godard et al., 2017) dans quatre autres langues, et évaluons l'impact de la langue de traduction sur une tâche de segmentation en mots non supervisée."}
{"sent_id": "38ad38f25a2823c64cd16bc9f2af93-C001-11", "intents": ["@EXT@"], "paper_id": "ABC_38ad38f25a2823c64cd16bc9f2af93_47", "text": "We translate the bilingual Mboshi-French parallel corpus (Godard et al., 2017) into four other languages, and we perform bilingual-rooted unsupervised word discovery."}
{"sent_id": "38ad38f25a2823c64cd16bc9f2af93-C001-28", "intents": ["@EXT@"], "paper_id": "ABC_38ad38f25a2823c64cd16bc9f2af93_47", "text": "The Multilingual Mboshi Parallel Corpus: In this work we extend the bilingual Mboshi-French parallel corpus (Godard et al., 2017) , fruit of the documentation process of Mboshi (Bantu C25), an endangered language spoken in Congo-Brazzaville."}
{"sent_id": "38ad38f25a2823c64cd16bc9f2af93-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_38ad38f25a2823c64cd16bc9f2af93_47", "text": "This transcription bottleneck problem can be handled by translating into a widely spoken language to ensure subsequent interpretability of the collected recordings, and such parallel corpora have been recently created by aligning the collected audio with translations in a well-resourced language (Adda et al., 2016; Godard et al., 2017; Boito et al., 2018) ."}
{"sent_id": "d178b55f8d5928867b481ba89e165c-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_d178b55f8d5928867b481ba89e165c_47", "text": "Recently many datasets have been produced for reading comprehension such as SQuAD [5] ."}
{"sent_id": "0b334057bc358f5537497ed15344c1-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_0b334057bc358f5537497ed15344c1_47", "text": "This is probably the reason for growing interest in creation of annotated corpora [4] , development of methods for augmenting the existing annotation [5] , speeding up the annotation process [5] , and reducing its cost; evaluating the comparability of results obtained applying the same methods to different collections [6] , And increasing compatibility of different annotations [7] ."}
{"sent_id": "0b334057bc358f5537497ed15344c1-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_0b334057bc358f5537497ed15344c1_47", "text": "Increasingly sophisticated relation extraction methods [6, 8] are being applied to a broader set of iii relations [9] ."}
{"sent_id": "891d0e17bf2fb79a378c2d77dda768-C001-11", "intents": ["@MOT@"], "paper_id": "ABC_891d0e17bf2fb79a378c2d77dda768_47", "text": "While application of attention [3, 4] and advanced decoding mechanisms like beam search and variation sampling [5] have shown improvements, it does not solve the underlying problem."}
{"sent_id": "891d0e17bf2fb79a378c2d77dda768-C001-49", "intents": ["@MOT@"], "paper_id": "ABC_891d0e17bf2fb79a378c2d77dda768_47", "text": "Outputs are generated through sampling over a multinomial distribution for all methods, instead of argmax on the log-likelihood probabilities, as sampling has shown to produce better output quality [5] . Please refer to Supplementary Section Table 3 for training parameters of each dataset and Table 2 for hyperparameters of each encoder."}
{"sent_id": "2d2da2e9215691bffad74bfb97dbf3-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_2d2da2e9215691bffad74bfb97dbf3_47", "text": "This was the case in SemEval-2013, whose task 2 (Wilson et al., 2013) required sentiment analysis of Twitter and SMS text messages."}
{"sent_id": "2d2da2e9215691bffad74bfb97dbf3-C001-112", "intents": ["@SIM@"], "paper_id": "ABC_2d2da2e9215691bffad74bfb97dbf3_47", "text": "And perhaps this is the cause for lower score in the unconstrained mode, something that happened also with many systems in the past edition (Wilson et al., 2013) ."}
{"sent_id": "fc3775c0d23292160f5c5eb86861be-C001-26", "intents": ["@USE@"], "paper_id": "ABC_fc3775c0d23292160f5c5eb86861be_47", "text": "It was extracted from the text form of the morphological dictionary RoMorphoDict (Barbu, 2008) , which was also used by Nastase and Popescu (2009) for their Romanian classifier, where every entry has the following structure:"}
{"sent_id": "fc3775c0d23292160f5c5eb86861be-C001-67", "intents": ["@UNSURE@"], "paper_id": "ABC_fc3775c0d23292160f5c5eb86861be_47", "text": "Looking at the entries in the original dataset for two of the last five nouns completely misclassified (levantin/levantinȃ-levantinuri/levantine and bageac/bageacȃ-bageacuri/bageci), we notice that the latter receives an 'n' tag for the singular form bageacȃ, which in (Collective, 2002 ) is listed as a feminine, and the former receives the 'n/f' tag, meaning either a neuter, or a feminine (Barbu, 2008 (Barbu, , p. 1939 , for both the neuter levantin and the feminine levantinȃ singular form."}
{"sent_id": "b0701d41baf3b355d864f46821f34a-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_b0701d41baf3b355d864f46821f34a_47", "text": "With combination of Conventional Neural Network (CNN) (Kalchbrenner et al., 2014) , Recurrent Neural Network (RNN), Recursive Neural Network (Socher et al., 2013) and Attention, hundreds of models had been proposed to model text for further classification, matching (Fan et al., 2017) or other tasks."}
{"sent_id": "bbacc6539a7346e4f30a1ae42a636e-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_bbacc6539a7346e4f30a1ae42a636e_47", "text": "The remainder of the chapter introduces three predominant instantiations of syntax-based models: hierarchical phrase-based SMT (Hiero) (Chiang 2007) , which is a non-labeled syntax-based SMT approach arising from the phrase-based approach; syntax-augmented machine translation (SAMT), which introduces the notion of soft labels while keeping the nonlinguistic phrase notion; and GHKM (Galley et al. 2004 ), which only extracts translation rules consistent with constituency parse subtrees."}
{"sent_id": "4b8de05982074c30f4d03af60827cb-C001-9", "intents": ["@UNSURE@"], "paper_id": "ABC_4b8de05982074c30f4d03af60827cb_47", "text": "We will comprehensively review existing stateof-the-art approaches to selected tasks such as image captioning (Chen et al., 2015) , visual question answering (VQA) (Antol et al., 2015; Goyal et al., 2017) and visual dialog (Das et al., 2017a,b) , presenting the key architectural building blocks (such as co-attention (Lu et al., 2016) ) and novel algorithms (such as cooperative/adversarial games (Das et al., 2017b) ) used to train models for these tasks."}
{"sent_id": "053ce92029e643bfade157b3172c05-C001-16", "intents": ["@USE@"], "paper_id": "ABC_053ce92029e643bfade157b3172c05_47", "text": "In order to ease the process of engineering such a large grammar, we have made use of the lexical knowledge representation language DATR (Evans & Gazdar, 1996) to compactly encode the elementary trees (Evans et al., 1995; Smets & Evans, 1998) ."}
{"sent_id": "053ce92029e643bfade157b3172c05-C001-91", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_053ce92029e643bfade157b3172c05_47", "text": "Following (Evans et al., 1995) and (Smets & Evans, 1998 ) the LEXSYS grammar is encoded using DATR, a non-monotonic knowledge representation language."}
{"sent_id": "7f2383426952ddde6fd527cf0d63bd-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_7f2383426952ddde6fd527cf0d63bd_47", "text": "The most straightforward novel application is the possibility to embed the documents of all project languages into the same shared semantics vectorspace and compute document semantic similarity (Hill, Cho & Korhonen, 2016) irrespective of the document language."}
{"sent_id": "7f2383426952ddde6fd527cf0d63bd-C001-68", "intents": ["@FUT@"], "paper_id": "ABC_7f2383426952ddde6fd527cf0d63bd_47", "text": "It is still an open issue which vectorspace projections yield the semantically best clusters (Hill, Cho & Korhonen, 2016) and further experiments are needed."}
{"sent_id": "ce0441b3ae7b957520d329799f8b9f-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_ce0441b3ae7b957520d329799f8b9f_47", "text": "For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of \"deeper\" structure as suggested in three papers in this volume (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; Meyers et al., 2004) ."}
{"sent_id": "ce0441b3ae7b957520d329799f8b9f-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_ce0441b3ae7b957520d329799f8b9f_47", "text": "The first six papers describe linguistic annotation in four languages: Spanish (Alcántara and Moreno, 2004), English (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; Meyers et al., 2004) , Czech (Sgall et al., 2004) and German (Baumann et al., 2004)."}
{"sent_id": "ce0441b3ae7b957520d329799f8b9f-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_ce0441b3ae7b957520d329799f8b9f_47", "text": "The first six papers describe linguistic annotation in four languages: Spanish (Alcántara and Moreno, 2004) , English (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; Meyers et al., 2004) , Czech (Sgall et al., 2004) and German (Baumann et al., 2004) ."}
{"sent_id": "17eb0ea80e5a2f18096ef41521af4e-C001-27", "intents": ["@SIM@"], "paper_id": "ABC_17eb0ea80e5a2f18096ef41521af4e_47", "text": "Our work tries to learn the main concepts making up the template structure in domain summaries, similar to (Chambers and Jurafsky, 2011) ."}
{"sent_id": "17eb0ea80e5a2f18096ef41521af4e-C001-102", "intents": ["@DIF@"], "paper_id": "ABC_17eb0ea80e5a2f18096ef41521af4e_47", "text": "Our work demonstrates the possibility of learning conceptual information in several domains and languages, while previous work (Chambers and Jurafsky, 2011) has addressed sets of related domains (e.g., MUC-4 templates) in English."}
{"sent_id": "134baefab4d27e9dafd0c050c43775-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_134baefab4d27e9dafd0c050c43775_47", "text": "The only exceptions we are aware of are the Groningen Meaning Bank and the Parallel Meaning Bank (Abzianidze et al., 2017) , two annotation efforts which use a graphical user interface for annotating sentences with CCG derivations and other annotation layers, and which have produced CCG treebanks for English, German, Italian, and Dutch."}
{"sent_id": "134baefab4d27e9dafd0c050c43775-C001-55", "intents": ["@UNSURE@"], "paper_id": "ABC_134baefab4d27e9dafd0c050c43775_47", "text": "2 To make the resource more useful (e.g., for training parsers), we also include in the release the syntactic CCG derivations created so far in the Parallel Meaning Bank (Abzianidze et al., 2017) ."}
{"sent_id": "3a7f65a63e875db3e6d722a695aa5a-C001-27", "intents": ["@EXT@"], "paper_id": "ABC_3a7f65a63e875db3e6d722a695aa5a_47", "text": "The current system is backed by the EasyCCG parser (Lewis and Steedman, 2014) , slightly modified to allow for incorporating constraints, and other CCG parsers could be plugged in with similar modifications."}
{"sent_id": "3a7f65a63e875db3e6d722a695aa5a-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_3a7f65a63e875db3e6d722a695aa5a_47", "text": "Span Constraints Although constraining lexical categories is often enough to determine the entire CCG derivation (cf. Bangalore and Joshi, 1999; Lewis and Steedman, 2014) , this is not always the case."}
{"sent_id": "ff73758fbef3ddc779a772e634b74e-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_ff73758fbef3ddc779a772e634b74e_47", "text": "In previous work, explanations have been categorised as either explaining 1) machine learning as in [11] who showed that they can increase trust; 2) explaining plans [2, 13] ; 3) verbalising robot [12] or agent rationalisation [3] ."}
{"sent_id": "ff73758fbef3ddc779a772e634b74e-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_ff73758fbef3ddc779a772e634b74e_47", "text": "Similar human-provided rationalisation has been used to generate explanations of deep neural models for game play [3] ."}
{"sent_id": "0924035155d4bbac7768c65fbe8f9a-C001-32", "intents": ["@SIM@"], "paper_id": "ABC_0924035155d4bbac7768c65fbe8f9a_47", "text": "Since the shared task graphs used relations between nodes which were often not easily mappable to native OpenCCG relations, we trained a maxent classifier to tag the most likely relation, as well as an auxiliary maxent classifier to POS tag the graph nodes, much like hypertagging (Espinosa et al., 2008) ."}
{"sent_id": "1ebbddc6c6740aea71ade2ed915de4-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_1ebbddc6c6740aea71ade2ed915de4_48", "text": "To avoid complexities of asynchronous parallel training with shared parameter server (Dean et al., 2012) , the architecture in Fig.2 and Fig. 3 instead can be trained using the alternating training approach proposed in (Luong et al., 2016) , where each task is optimized for a fixed number of parameter updates (or mini-batches) before switching to the next task (which is a different language pair)."}
{"sent_id": "1ebbddc6c6740aea71ade2ed915de4-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_1ebbddc6c6740aea71ade2ed915de4_48", "text": "Neural translation attention mechanism (Bahdanau, Cho & Bengio, 2014) has been shown to be highly beneficial for bi-lingual neural translation of long sentences, but it is not compatible with the multi-task multilingual translation models (Dong et al., 2015; Luong et al, 2016) described in the previous Section and character-level translation models (Barzdins & Gosko, 2016) described in this Section."}
{"sent_id": "7ddd5b18d774575ae7acb97ae9eb33-C001-7", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_7ddd5b18d774575ae7acb97ae9eb33_48", "text": "It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (Ç akıcı, 2005) , German (Hockenmaier, 2006) , English (Hockenmaier and Steedman, 2007) , Italian (Bos et al., 2009) , Chinese (Tse and Curran, 2010) , Arabic (Boxwell and Brew, 2010) , Japanese (Uematsu et al., 2013) , and Hindi (Ambati et al., 2018) ."}
{"sent_id": "7ddd5b18d774575ae7acb97ae9eb33-C001-45", "intents": ["@USE@"], "paper_id": "ABC_7ddd5b18d774575ae7acb97ae9eb33_48", "text": "The main annotation guideline was to copy the annotation style of CCGrebank (Honnibal et al., 2010), a CCG treebank adapted from CCGbank (Hockenmaier and Steedman, 2007) , which is in turn based on the Penn Treebank (Marcus et al., 1993) ."}
{"sent_id": "cf7d01faf555f09973e44be400e768-C001-16", "intents": ["@USE@"], "paper_id": "ABC_cf7d01faf555f09973e44be400e768_48", "text": "We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (Wang et al., 2018b) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems."}
{"sent_id": "cf7d01faf555f09973e44be400e768-C001-59", "intents": ["@USE@"], "paper_id": "ABC_cf7d01faf555f09973e44be400e768_48", "text": "We will show case a recent study (Wang et al., 2018b ) that leverages hierarchical deep reinforcement learning for language and vision, and extend the discussion."}
{"sent_id": "13d1d79a4922d3b5d215d6f8f722ba-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_13d1d79a4922d3b5d215d6f8f722ba_48", "text": "De Cao et al. [2] proposed a method to detect the set of suitable WordNet senses able to evoke the same frame by exploiting the hypernym hierarchies that capture the largest number of LUs in the frame."}
{"sent_id": "13d1d79a4922d3b5d215d6f8f722ba-C001-52", "intents": ["@DIF@"], "paper_id": "ABC_13d1d79a4922d3b5d215d6f8f722ba_48", "text": "De Cao at al. [2] reported a better performance, particularly for recall, but evaluation of their mapping algorithm relied on a gold standard of 4 selected frames having at least 10 LUs and a given number of corpus instantiations."}
{"sent_id": "bb45a61408a0ade8ce0aba2b8f9ce7-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_bb45a61408a0ade8ce0aba2b8f9ce7_48", "text": "We will then discuss some of the current and upcoming challenges of combining language, vision and actions, and introduce some recently-released interactive 3D simulation environments designed for this purpose (Anderson et al., 2018b; Das et al., 2018) ."}
{"sent_id": "bb45a61408a0ade8ce0aba2b8f9ce7-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_bb45a61408a0ade8ce0aba2b8f9ce7_48", "text": "Although models that link vision, language and actions have a rich history (Tellex et al., 2011; Paul et al., 2016; Misra et al., 2017) , we will focus primarily on embodied 3D environments (Anderson et al., 2018b; , considering tasks such as visual navigation from natural language instructions (Anderson et al., 2018b) , and question answering (Das et al., 2018; Gordon et al., 2018) ."}
{"sent_id": "f58555aa8fc78903df83af8309a3d7-C001-37", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_f58555aa8fc78903df83af8309a3d7_48", "text": "Our representation model is built on the functionalities of Annotation Graph [7] and the underlying storage scheme is conceptually similar to Standoff XML format [9] , but we opted for a relational database structure built with an object-oriented design for efficiency, reusability and versatility."}
{"sent_id": "f58555aa8fc78903df83af8309a3d7-C001-50", "intents": ["@SIM@"], "paper_id": "ABC_f58555aa8fc78903df83af8309a3d7_48", "text": "The tagger is built on a generic, multifunctional relational database similar to the annotation graph model [7] that has been demonstrated to be capable of representing virtually all sorts of common linguistic annotations."}
{"sent_id": "7666d5a8e05e79f68ec60e47cddecd-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_7666d5a8e05e79f68ec60e47cddecd_48", "text": "Paraphrase templates containing concepts and typical strings were induced from comparable sentences in (Barzilay and Lee, 2003) using multisentence alignment to discover \"variable\" and fixed structures."}
{"sent_id": "7666d5a8e05e79f68ec60e47cddecd-C001-106", "intents": ["@DIF@"], "paper_id": "ABC_7666d5a8e05e79f68ec60e47cddecd_48", "text": "Our algorithm has a reasonable computational complexity, unlike alignment-based or clustering-based approaches (Barzilay and Lee, 2003) , which are computationally expensive."}
{"sent_id": "2fdfa1b36fcf0d77826c96101ac428-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_2fdfa1b36fcf0d77826c96101ac428_48", "text": "We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016) , semi-supervised text classification (Wu et al., 2018) , coreference (Clark and Manning, 2016; Yin et al., 2018) , knowledge graph reasoning (Xiong et al., 2017 ), text games (Narasimhan et al., 2015; He et al., 2016a) , social media (He et al., 2016b; Zhou and Wang, 2018) , information extraction (Narasimhan et al., 2016; Qin et al., 2018) , language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018) , etc."}
{"sent_id": "57af9690eb41ff3f9217da6138425f-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_57af9690eb41ff3f9217da6138425f_48", "text": "Ideally, in addition to papers such as Kameyama's and Walker's, this collection could perhaps also have featured extended versions of papers, such as those of Kehler (1997) and Hahn and Strube (1997) , that highlight certain weaknesses of the original centering model or suggest extensions or alternative solutions (Strube 1998) ."}
{"sent_id": "0526911ab71c85bfa4a20b630f34ae-C001-10", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_0526911ab71c85bfa4a20b630f34ae_48", "text": "Morphologically rich languages like Arabic (Beesley, K. 1996 ) present significant challenges to many natural language processing applications as the one described above because a word often conveys complex meanings decomposable into several morphemes (i.e. prefix, stem, suffix) ."}
{"sent_id": "41bd8c692ac513b8a9cabbd5aafbda-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_41bd8c692ac513b8a9cabbd5aafbda_48", "text": "The word2vec [10] is among the most widely used word embedding models today."}
{"sent_id": "eb591565efc03df1706710218a8f19-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_eb591565efc03df1706710218a8f19_48", "text": "Learning knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] ."}
{"sent_id": "eb591565efc03df1706710218a8f19-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_eb591565efc03df1706710218a8f19_48", "text": "knowledge from analyzing large-scaled unlabeled data is compulsory and proved useful in the previous works [4, 5, 6] ."}
{"sent_id": "e9b2f32ed29589b4a6d49d3b30fc3a-C001-11", "intents": ["@EXT@", "@MOT@"], "paper_id": "ABC_e9b2f32ed29589b4a6d49d3b30fc3a_48", "text": "By segmenting words into morphemes, we can improve the performance of natural language systems including machine translation (Brown et al. 1993 ) and information retrieval (Franz, M. and McCarley, S. 2002) ."}
{"sent_id": "0bfea881773f504206bef9c1394f20-C001-10", "intents": ["@USE@"], "paper_id": "ABC_0bfea881773f504206bef9c1394f20_48", "text": "We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4]), and of n-gram perplexity."}
{"sent_id": "0bfea881773f504206bef9c1394f20-C001-29", "intents": ["@USE@"], "paper_id": "ABC_0bfea881773f504206bef9c1394f20_48", "text": "We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4] ), and of n-gram perplexity."}
{"sent_id": "c07bc362ac7ee1f64e149fe8907fee-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_c07bc362ac7ee1f64e149fe8907fee_49", "text": "However, there have been recent successes in adapting parsers and POS taggers to social media data (Foster et al., 2011; Gimpel et al., 2011) ."}
{"sent_id": "6e5ee9176bcc54c3c9c32965765990-C001-19", "intents": ["@USE@"], "paper_id": "ABC_6e5ee9176bcc54c3c9c32965765990_49", "text": "The models were trained with multi-domain data and we improved performance following a domainmixing approach (Britz et al., 2017) ."}
{"sent_id": "c0d2bbf9dc7615040763019f5c668b-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_c0d2bbf9dc7615040763019f5c668b_49", "text": "Additionally, lexical normalisation and other preprocessing strategies have been shown to enhance the performance of NLP tools over social media data (Lui and Baldwin, 2012; Han et al., to appear) ."}
{"sent_id": "9b594c5b29175fc8ee598f61609c79-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_9b594c5b29175fc8ee598f61609c79_49", "text": "In previous work, we have also studied the problem of predicting topic boundaries at dierent levels of granularity and showed that a supervised classication approach performs better on predicting a coarser level of topic segmentation (Hsueh et al., 2006"}
{"sent_id": "6ca7c7782c33f51e9bdb2e8613c24d-C001-9", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_6ca7c7782c33f51e9bdb2e8613c24d_49", "text": "Natural language dialog has been used in many areas, such as for call-center/routing application (Carpenter & Chu-Carroll 1998) , email routing (Walker, Fromer & Narayanan 1998), information retrieval and database access (Androutsopoulos & Ritchie 1995) , and for telephony banking (Zadrozny et al. 1998) ."}
{"sent_id": "600317fc3ce88ea730993d3cc94f19-C001-6", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_600317fc3ce88ea730993d3cc94f19_49", "text": "Since the first approach [Wright and Wrigley 1991] of combining a probabilistic method into the GLR technique was published, Some probabilistic GLR parsers also have been implemented in which probabilities are assigned to actions of LR parsing tables by using lookaheads or LR states as simple context information of [Briscoe and Carroll 1993] , [Kentaro et al. 1998 ], and [Ruland, 2000] which does not use the stack information of the GLR parser effectively, because of highly complex internal GLR stack."}
{"sent_id": "a0614f13b4ed0c6370deb26032f62b-C001-35", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_a0614f13b4ed0c6370deb26032f62b_49", "text": "Beyond language modeling, methods for PCFG induction from treebanks have been a popular topic in the field over the past decade, and some understanding of the impact of flattening trees can be had in Johnson (1998) , where the beneficial impact of various tree transformations for probabilistic grammars is presented."}
{"sent_id": "66b6283cf1f20977286f99ef21b3c7-C001-43", "intents": ["@MOT@", "@FUT@"], "paper_id": "ABC_66b6283cf1f20977286f99ef21b3c7_49", "text": "My own slow progress (Cassell et al., 2000; Koller and Stone, 2007) shows that there's still lots of hard work needed to develop suitable techniques."}
{"sent_id": "e99193f62a8f3a9e46dee3cadd786f-C001-17", "intents": ["@USE@"], "paper_id": "ABC_e99193f62a8f3a9e46dee3cadd786f_49", "text": "Our dataset is a gold standard corpus of 1557 single-and multi-word disorder annotations (Ogren et al., 2008) ."}
{"sent_id": "d8c3d04514c0867d78a7603e49ea9b-C001-16", "intents": ["@USE@"], "paper_id": "ABC_d8c3d04514c0867d78a7603e49ea9b_49", "text": "The rest of the corpus was automatically validated synthetic material using general data from Leipzig (Goldhahn et al., 2012 Engine customization The data was cleaned using the Bicleaner tool (Sánchez-Cartagena et al., 2018) ."}
{"sent_id": "d8c3d04514c0867d78a7603e49ea9b-C001-37", "intents": ["@USE@"], "paper_id": "ABC_d8c3d04514c0867d78a7603e49ea9b_49", "text": "Engine customization The data was cleaned using the Bicleaner tool (Sánchez-Cartagena et al., 2018) ."}
{"sent_id": "a7223f2afa1afd5fbfa1257b98ec02-C001-17", "intents": ["@USE@"], "paper_id": "ABC_a7223f2afa1afd5fbfa1257b98ec02_49", "text": "In the second half we discuss is detail relevant applications including text classification (Crammer and Singer, 2003) , named entity recognition (McDonald et al., 2005) , parsing (McDonald, 2006) , and other tasks."}
{"sent_id": "bd12a9270c5f94056701b86eda9c8e-C001-18", "intents": ["@USE@"], "paper_id": "ABC_bd12a9270c5f94056701b86eda9c8e_49", "text": "The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE (Sennrich et al., 2016) approach."}
{"sent_id": "574ab9a51f3414e6587da7dfca2ff8-C001-2", "intents": ["@BACK@"], "paper_id": "ABC_574ab9a51f3414e6587da7dfca2ff8_49", "text": "The last few years have seen an increased interest in narrative within the field of Natural Language Generation (Reiter et al., 2008; Elson and McKeown, 2010; Siddharthan et al., 2012; Lester, 2012) ."}
{"sent_id": "b722b98f50669bf3b22208a25f6854-C001-11", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_b722b98f50669bf3b22208a25f6854_49", "text": "Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) [5] ."}
{"sent_id": "6cbc59d4cb2d3246b3efa1ee612270-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_6cbc59d4cb2d3246b3efa1ee612270_49", "text": "They also cleaned United Nations material and post-edited general-domain data that was previously filtered as indomain following the \"invitation model\" (Hoang and Sima'an, 2014) ."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-21", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "On the other hand, we can impose limited architectural constraints in the form of selective rationalization (Lei et al., 2016; Li et al., 2016b; Chen et al., 2018a,b) where the goal is to only expose the portion of the text relevant for prediction."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-53", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "Moreover, we empirically show that (1) the three-player framework on its own helps cooperative games such as (Lei et al., 2016) to improve both predictive accuracy and rationale quality; (2) by combining the two solutions -introspective generator and the three player game -we can achieve high predictive accuracy and non-degenerate rationales."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-124", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "Without the complement predictor, and thus the loss Lg, the framework reduces to the method in (Lei et al., 2016) ."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-157", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "Finally, as suggested by an anonymous reviewer, we evaluate on the text matching benchmark AskUbuntu, following Lei et al. (2016) ."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-159", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "Multi-aspect beer review This is the same data used in (Lei et al., 2016) ."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-229", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "For both cooperative methods, i.e. (Lei et al., 2016) with and without introspection, we train an independent extra predictor on the unselected words from the generator, which does not affect the training of the generator-predictor framework."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-237", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "We further conduct subjective evaluations by comparing the original model of (Lei et al., 2016) with our introspective threeplayer model."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-260", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "Since there is a rich form of su- pervised signal, i.e., the number of class labels is large, the chance of any visible degeneration of the Lei et al. (2016) 's model should be low."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-302", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "According to Lemma 1.1, Lg can be rewritten as Table 1 and Degeneration Cases of (Lei et al., 2016) This section provides the details to obtain the results in Table 1 in the introduction section, where the method of (Lei et al., 2016) generates degenerated rationales."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-303", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "The method of (Lei et al., 2016) works well in many applications."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-305", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "In this section, we design an experiment to confirm the existence of the problem in the original (Lei et al., 2016) model."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-310", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "During the training of (Lei et al., 2016) , we stipulate that the generated rationales are very concise: we punish it when the rationales have more than 3 pieces or more than 20% of the words are generated (both with hinge losses)."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-311", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "From the results, we can see that Lei et al. (2016) tends to predict color words, like dark-brown, yellow, as rationales."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-346", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "To this end, we mask the original texts with the rationales generated by (Lei et al., 2016) and our method."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-361", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "Setting Following the suggestion from the reviews, we evaluate the proposed method on the question retrieval task on AskUbuntu (Lei et al., 2016) ."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-364", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "We use the same data split provided by (Lei et al., 2016) ."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-367", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "In our experiments, we follow the same setting from (Lei et al., 2016) by only using the question bodies."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-387", "intents": ["@USE@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "8 We use the same word embeddings as released by (Lei et al., 2016) ."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-215", "intents": ["@EXT@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "Moreover, our reimplementation of (Lei et al., 2016) for the original regression task achieves 90.1% precision when highlighting 10% words, which suggest that rationalization for binary classification is more challenging compared to the regression where finer supervision is available."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-391", "intents": ["@EXT@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "Adding the proposed minimax game helps both the (Lei et al., 2016 ) and the introspection model to generate more informative texts as the rationales, which improves the MAP of the prediction while lowering the complement MAP."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-109", "intents": ["@DIF@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "Compared with (Lei et al., 2016) , as shown in Figure 1(a) , the three-player model introduces an additional complement predictor, which plays a minimax game in addition to the cooperative game in (Lei et al., 2016) ."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-207", "intents": ["@DIF@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "When only 10% of the words are used, Lei et al. (2016) has a significant performance downgrade compared to the accuracy when using the whole passage (82.05 v.s. 87.59)."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-209", "intents": ["@DIF@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "On the other hand, our introspection models are able to maintain higher predictive accuracy (86.16 v.s. 82.05) compared to (Lei et al., 2016) , while only sacrificing a little loss on highlighting precision (0.47% drop)."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-211", "intents": ["@DIF@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "Comparing the model of (Lei et al., 2016) with and without the proposed mini-max module, there is a huge gap of more than 5% on recall of generated rationales."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-216", "intents": ["@DIF@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "In summary, our three-player framework consistently improves the quality of extracted rationales on both of the original (Lei et al., 2016) and the introspective framework."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-230", "intents": ["@DIF@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "From the left part of Table 4 , we observe that the original model of (Lei et al., 2016 ) has a hard time maintaining the accuracy compared to the classifier trained with full texts."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-262", "intents": ["@DIF@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "In the first example, Lei et al. (2016) fails to highlight the second entity while ours does."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-263", "intents": ["@DIF@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "In the second example, the introspective three-player model selects more words than (Lei et al., 2016) ."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-390", "intents": ["@DIF@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "The original model from (Lei et al., 2016) fails to maintain the performance compared to the model trained with full texts."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "Ranging from count-based to predictive or task-based methods, in the past years, many approaches were developed to produce word embeddings, such as Neural Probabilistic Language Model [3] , Word2Vec [28] , GloVe [32] , and more recently ELMo [33] , to name a few."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "Neural Language Models can be tracked back to [3] , and more recently deep bi-directional language models (biLM) [33] have successfully been applied to word embeddings in order to incorporate contextual information."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "To solve these two issues, Embedding from Language Models (ELMo) [33] was recently introduced."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "ELMo [33] representations are a function of the internal layers of the bi-directional Language Model (biLM), which provides a very rich representation about the tokens."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "Like in fastText [4] , ELMo [33] breaks the tradition of word embeddings by incorporating sub-word units, but ELMo [33] has also some fundamental differences with previous shallow representations such as fastText or Word2Vec."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "In ELMo [33] , they use a deep representation by incorporating internal representations of the LSTM network, therefore capturing the meaning and syntactical aspects of words."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "Since ELMo [33] is based on a language model, each token representation is a function of the entire input sentence, which can overcome the limitations of previous word embeddings where each word is usually modeled as an average of their multiple contexts."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "Through the lens of the Ludwig Wittgenstein philosophy of language [40] , it is clear that the ELMo [33] embeddings are a better approximation to the idea of \"meaning is use\" [40] , where a word can contain a wide spectrum of different meanings depending on context, as opposed to traditional word embeddings that are not only context-independent but have a very limited definition of context."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-107", "intents": ["@BACK@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "As shown in [33] , the higher-level LSTM representations capture context-dependent aspects of meaning, while the lower level representations capture aspects of syntax."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-189", "intents": ["@BACK@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "Our hypothesis is that this is due to the fact that ELMo [33] is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging SOMO task."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-190", "intents": ["@BACK@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "ELMo [33] word embeddings can be seen as analogous to the hypercolumns [15] approach in Computer Vision, where multiple feature levels are aggregated to form a single pixelwise representation."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-209", "intents": ["@BACK@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "Especially for evaluating different levels of word representations, where it can be a very useful tool to provide insights on what kind of relationships and linguistic properties each representation level (in the case of deep representations such as ELMo [33] ) is capturing."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-96", "intents": ["@USE@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "ELMo (BoW, all layers, 5.5B) [33] : this model was obtained from the authors' website at https: //allennlp.org/elmo."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-99", "intents": ["@USE@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo [33] model."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-101", "intents": ["@USE@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "ELMo (BoW, all layers, original) [33] : this model was obtained from the authors website at https://allennlp.org/elmo."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-104", "intents": ["@USE@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "An averaging bag-of-words was employed to produce the sentence embeddings, using features from all three layers of the ELMo [33] model and averaging along the word dimension."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-165", "intents": ["@USE@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "As seen in Table 6 , although no method had a consistent performance among all tasks, ELMo [33] achieved best results in 5 out of 9 tasks."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-184", "intents": ["@USE@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "As we can see in Table 8 , ELMo [33] was one of the methods that were able to achieve high performance on a broad set of different tasks."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-185", "intents": ["@USE@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "Interestingly, in the BShift (bi-gram shift) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, ELMo [33] achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as \"This is my Eve Christmas\", a sample from the BShift dataset."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-188", "intents": ["@USE@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "However, the [33] bag-of-words not only achieved the best result in the SentLent task but also in many downstream tasks."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-193", "intents": ["@USE@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "However, in our evaluation, the p-mean [35] approach, which has achieved better results in the WC task did not exceed other techniques such as ELMo [33] bag-of-words or InferSent [10] and USE in the downstream classification tasks."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-100", "intents": ["@DIF@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "We did not employ the trainable task-specific weighting scheme described in [33] ."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-166", "intents": ["@DIF@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "Even though ELMo [33] was trained on a language model objective, it is important to note that in this experiment a bag-of-words approach was employed."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-106", "intents": ["@EXT@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "ELMo (BoW, top layer, original) [33] : the same model and procedure as in ELMo (BoW, all layers, original) was employed, except that in this experiment, we used only the top layer representation from the ELMo [33] model."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-167", "intents": ["@FUT@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "Therefore, these results are quite impressive, which lead us to believe that excellent results can be obtained by integrating ELMo [33] and the trainable task-specific weighting scheme described in [33] into InferSent [10] ."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-175", "intents": ["@FUT@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "Given that ELMo [33] demonstrated excellent results on a broad set of tasks, it is clear that a proper integration of deep representation from language models can potentially improve sentence embedding methods by a significant margin and it is a promising research line."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-191", "intents": ["@FUT@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "We leave the exploration of probing tasks for each ELMo [33] layer representation to future research, given that it could provide a framework to expose the linguistic properties capture by each representation level of the LSTM."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-212", "intents": ["@FUT@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "Finally, we believe that new embedding training techniques that include language models as a way to capture context and meaning, such as ELMo [33] , combined with clever techniques of encoding sentences such as in InferSent [10] , can improve the performance of these encoders by a significant margin."}
{"sent_id": "20330d309c218dc2e1521b9644ed9c-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_20330d309c218dc2e1521b9644ed9c_0", "text": "It should be noted that some applications has only the decoder self-attention such as sequence prediction (Dai et al., 2019) ."}
{"sent_id": "20330d309c218dc2e1521b9644ed9c-C001-101", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_20330d309c218dc2e1521b9644ed9c_0", "text": "For learned positional embedding (Dai et al., 2019; Ott et al., 2019) , each t i is a learned parameter and is fixed for the same position for different sequences."}
{"sent_id": "20330d309c218dc2e1521b9644ed9c-C001-78", "intents": ["@USE@"], "paper_id": "ABC_20330d309c218dc2e1521b9644ed9c_0", "text": "The new formulation defines a larger space for composing attention by manipulating its individual components, and at the same time it is able to categorize different variants of attention in prior work (Shaw et al., 2018; Huang et al., 2018b; Dai et al., 2019; Child et al., 2019; Wang et al., 2018; Tsai et al., 2019a) ."}
{"sent_id": "20330d309c218dc2e1521b9644ed9c-C001-114", "intents": ["@USE@", "@UNSURE@"], "paper_id": "ABC_20330d309c218dc2e1521b9644ed9c_0", "text": "(ii) Transformer-XL (Dai et al., 2019) , Music Transformer (Huang et al., 2018b) , Self-Attention with Relative Positional Embedding (Shaw et al., 2018) :"}
{"sent_id": "20330d309c218dc2e1521b9644ed9c-C001-142", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_20330d309c218dc2e1521b9644ed9c_0", "text": "In our experiment, we find it reaching competitive performance as comparing to the current state-of-the-art designs (Eq. (5) by Dai et al. (2019) )."}
{"sent_id": "20330d309c218dc2e1521b9644ed9c-C001-158", "intents": ["@USE@"], "paper_id": "ABC_20330d309c218dc2e1521b9644ed9c_0", "text": "For the choice of datasets, we pick IWSLT'14 German-English (De-En) dataset (Edunov et al., 2017) for NMT and WikiText-103 dataset (Merity et al., 2016) for SP as suggested by Edunov et al. (Edunov et al., 2017) and Dai et al. (Dai et al., 2019) ."}
{"sent_id": "20330d309c218dc2e1521b9644ed9c-C001-173", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_20330d309c218dc2e1521b9644ed9c_0", "text": "Second, we see that PE in the product kernel proposed by Dai et al. (Dai et al., 2019) may not constantly outperform the other integration types (it has lower BLEU score for NMT)."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-127", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "The idea of tree-based approaches [26] , [27] , [28] , [15] is to transform the derivation of the arithmetic expression to constructing an equivalent tree structure step by step in a bottom-up manner."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-137", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "Roy et al. [26] proposed the first algorithmic approach that leverages the concept of expression tree to solve arithmetic word problems."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-148", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "To further reduce the tree enumeration space, beam search is applied in [26] ."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-153", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "The solution in [27] , named ALGES, differs from [26] in two major ways."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-170", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "UnitDep [28] can be viewed as an extension work of [26] by the same authors."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-208", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "2) IL [26] ."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-214", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "3) CC [26] ."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-475", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "ExpressionTree [26] is an exceptional case without using Stanford Parser."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-514", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "A trivial trick used in [26] [28] [15] is to examine whether there exists comparative adverbs."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-519", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "A straightforward example is that if two quantities are associated with the same unit, they can be applied with addition and subtraction [26] [28] [15] [40] ."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-520", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "If one quantity is related to a rate and the other is associated with a unit that is part of the rate, their operator is likely to be multiplication or division [26] [27] [28] [15] ."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-536", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "Finally, a popular quantity-pair feature used in [26] [28] [15] [39] [40] [45] examines whether the value of one quantity is greater than the other, which is helpful to determine the correct operands for subtraction operator."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-551", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "[26] [27] [28] [15] directly use dependent verb as one of the features."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-559", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "[26] , [28] , [15] use the number of quantities in the problem text as part of feature space."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-573", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "Comparative adverbs [26] [28] [15] For \"If she drank 25 of them and then bought 30 more.\", \"more\" is a comparative term in the window of quantity \"30\"."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-576", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "Whether both quantities have the same unit [26] [28] [15] [40] For \"Student tickets cost 4 dollars and general admission tickets cost 6 dollars\", quantities \"4\" and \"6\" have the same unit."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-577", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "If one quantity is related to a rate and the other is associated with a unit that is part of the rate [26] [27] [28] [15] For \"each box has 9 pieces\" and \"Paul bought 6 boxes of chocolate candy\", \"9\" is related to a rate ( i.e., pieces/box) and \"6\" is associated to the unit \"box\"."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-584", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "Number of quantities which happen to have the maximum number of matching tokens with the question [26] [28] [15] For \"Rose have 9 apples and 12 erasers. ... 3 friends."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-587", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "Whether any component of the rate is present in the question [26] [28] [15] Given a question \"How many blocks does George have?\" and a quantity 6 associated with rate \"blocks/box\", the feature indicator is set to 1 since block appears in the question."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-588", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "Whether the question contains terms like \"each\" or \"per\" [26] [28] [15] Whether the question contains comparison-related terms like \"more\" or \"less\" [26] [28] [15] Whether the question contains terms like \"how many\" [39] [40] [45] [13] It implies that the solution is positive."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-591", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "Dependent verb of a quantity [26] [27] [28] [15] the verb closest to the quantity in the dependency tree Distance vector between the dependent verb and a small collection of predefined verbs that are useful for arithmetic operator classification [21] [24] [27] Whether two quantities have the same dependent verbs [26] [28] [15] For \"In the first round she scored 40 points and in the second round she scored 50 points\", the quantities \"40\" and \"50\" both have the same verb \"scored\". Note that \"scored\" appeared twice in the sentence."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-592", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "Whether both dependent verbs refer to the same verb mention [26] [28] [15] For \"She baked 4 cupcakes and 29 cookies.\", the quantities \"4\" and \"29\" both shared the verb \"baked\". Note that \"baked\" appeared only once in the sentence."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-595", "intents": ["@BACK@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "Number of quantities mentioned in text [26] [28] [15] Unigrams and bigrams of sentences in the problem text [20] [39] presented an efficient characteristic pattern detection method by scanning the distribution of black pixels and generating feature points graph."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-130", "intents": ["@USE@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "One is called expression tree that is used in [26] , [28] , [15] and the other is called equation tree in [27] ."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-257", "intents": ["@USE@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "It is interesting to observe that ALGES [27] , ExpressionTree [26] and UNITDEP [28] cannot perform equally well on the three datasets."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-504", "intents": ["@USE@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "As shown in Table 5 , a binary indicator to determine whether a quantity refers to a rate is adopted in many solvers [26] [28] [15] [40] [45] ."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-541", "intents": ["@USE@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "The remain question-related features presented in Table 5 were proposed by Roy et al. [26] , [28] and followed by MathDQN [15] ."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-553", "intents": ["@USE@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "Again, the remaining features come from the works [26] , [28] , [15] ."}
{"sent_id": "a0db9c3a74487d3fcd11e79d44e163-C001-377", "intents": ["@SIM@"], "paper_id": "ABC_a0db9c3a74487d3fcd11e79d44e163_0", "text": "This is similar to the relevance model trained in ExpressionTree [26] and UNITDEP [28] ."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Therefore, this task has attracted more attention in recent years (Serban et al., 2016; Elsahar et al., 2018) ."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-24", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "In order to improve the generalization for KBQG, Elsahar et al. (2018) utilized extra contexts as input via distant supervisions (Mintz et al., 2009) , then a decoder is equipped with attention and part-ofspeech (POS) copy mechanism to generate questions."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Previous work (Elsahar et al., 2018) usually obtained predicate textual contexts through distant supervision."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "In previous work, Elsahar et al. (2018) only regarded a most frequently mentioned entity type as the textual context for the subject or object in the triplet."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Therefore, generated questions from Elsahar et al. (2018) may be difficult to contain definitive answers."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-121", "intents": ["@BACK@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Elsahar et al. (2018) demonstrated the effectiveness of POS copy for the context."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-258", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "To improve the generalization, Elsahar et al. (2018) introduced extra contexts for the input fact, which achieved significant performances."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-264", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Elsahar et al. (2018) exploited POS copy action to better capture textual contexts."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-35", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "We make statistic in the resources released by Elsahar et al. (2018) , and find that only 44% predicates have predicate textual context 2 ."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-158", "intents": ["@USE@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "For the subject and object context, we combine the most frequently mentioned entity type (Elsahar et al., 2018) with the type that best describe the entity 3 ."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-163", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Following (Serban et al., 2016; Elsahar et al., 2018) , we adopt some word-overlap based metrics (WBMs) for natural language generation including BLEU-4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) ."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-178", "intents": ["@USE@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "(3) Elsahar et al. (2018) : We compare our methods with the model utilizing copy actions, the best performing model in Elsahar et al. (2018) ."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-213", "intents": ["@USE@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Table 4 : Ablation study by removing the main components, where \"w/o\" means without, and \"w/o diversified contexts\" represents that diversified contexts are replaced by contexts used in Elsahar et al. (2018) ."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-220", "intents": ["@USE@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Specifically, the last line in Table 4 , replacing diversified contexts with contexts used in Elsahar et al. (2018) , has more obvious performance degradation."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-226", "intents": ["@USE@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Following Elsahar et al. (2018) , we sample 100 questions from each system, and then two annotators measure the naturalness by a score of 0-5."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-228", "intents": ["@USE@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "As shown in Table 5 , Elsahar et al. (2018) Pre-trained KB embeddings may provide rich structured relational information among entities."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-47", "intents": ["@EXT@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Specifically, besides using predicate contexts from the distant supervision utilized by Elsahar et al. (2018) , we further leverage the domain, range and even topic for the given predicate as contexts, which are off-the-shelf in KBs (e.g. the range and the topic for the predicate fb:location/containedby are \"location\" and \"containedby\", respectively 1 )."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-49", "intents": ["@EXT@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Furthermore, in addition to the most frequently mentioned entity type as contexts used by Elsahar et al. (2018) , we leverage the type that best describes the entity as contexts (e.g. a refined entity type 3 \"US state\" combines a broad type \"administrative region\" for the entity \"New York\"), which is helpful to refine the entity information."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-96", "intents": ["@DIF@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Similarly, the predicate embedding e p and the object embedding e o are mapped from the KB embedding matrix E f , where E f is pre-trained using TransE (Bordes et al., 2013) to capture much more fact information in previous work (Elsahar et al., 2018) ."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-189", "intents": ["@DIF@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "It is evident that our model is remarkably better than baselines on all metrics, where the BLEU4 score increases 4.53 compared with the strongest baseline (Elsahar et al., 2018) ."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-196", "intents": ["@DIF@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Identification Serban et al. (2016) 53.5 Elsahar et al. (2018) 71.5 Our Model ans loss 75.5 from each model, and then two annotators are employed to judge whether the generated question expresses the given predicate."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-225", "intents": ["@DIF@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Naturalness Serban et al. (2016) 2.96 Elsahar et al. (2018) 2.23 Our Model ans loss 3.56 Human evaluation is important for generated questions."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-231", "intents": ["@DIF@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "In comparison, Elsahar et al. (2018) obtain obvious degradation on all metrics while there is only a slight decline in our model."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-235", "intents": ["@DIF@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "Data Type Accuracy human-labeled data 68.97 + gen data (Serban et al., 2016) 68.53 + gen data (Elsahar et al., 2018) 69.13 + gen data (Our Model ans loss ) 69.57 Previous experiments demonstrate that our model can deliver more precise questions."}
{"sent_id": "d2028986dc30ccc0ca840ca3b2f454-C001-182", "intents": ["@SIM@"], "paper_id": "ABC_d2028986dc30ccc0ca840ca3b2f454_0", "text": "To make our model comparable to the comparison methods, we keep most parameter values the same as Elsahar et al. (2018) ."}
{"sent_id": "97852048a123350455f398728d6d34-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013) ."}
{"sent_id": "97852048a123350455f398728d6d34-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "To the best of our knowledge, there have been only three studies on DA for RE (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; ."}
{"sent_id": "97852048a123350455f398728d6d34-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "In contrast, Plank and Moschitti (2013) and Nguyen and Grishman (2014) work on the unsupervised DA."}
{"sent_id": "97852048a123350455f398728d6d34-C001-26", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "One way is to use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e.g., by resembling the method of Plank and Moschitti (2013) that exploited LSA (in the semantic syntactic tree kernel (SSTK), cf."}
{"sent_id": "97852048a123350455f398728d6d34-C001-29", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "(ii) Between the feature-based method in Nguyen and Grishman (2014) and the SSTK method in Plank and Moschitti (2013) , which method is better for DA of RE, given the recent discovery of word embeddings for both methods?"}
{"sent_id": "97852048a123350455f398728d6d34-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "In particular, while Plank and Moschitti (2013) only use the path-enclosed trees induced from the constituent parse trees as the representation for relation mentions, Nguyen and Grishman (2014) include a rich set of features extracted from multiple resources such as constituent trees, dependency trees, gazetteers, semantic resources in the representation."}
{"sent_id": "97852048a123350455f398728d6d34-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "Besides, Plank and Moschitti (2013) consider the direction of relations in their evaluation (i.e, distinguishing between relation classes and their inverses) but Nguyen and Grishman (2014) disregard this relation direction."}
{"sent_id": "97852048a123350455f398728d6d34-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "In the tree kernel-based method (Moschitti, 2006; Moschitti, 2008; Plank and Moschitti, 2013) , a relation mention (the two entity mentions and the sentence containing them) is represented by the path-enclosed tree (PET), the smallest constituency-based subtree including the two target entity mentions (Zhang et al., 2006) ."}
{"sent_id": "97852048a123350455f398728d6d34-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "For instance, in the following example taken from Plank and Moschitti (2013) , the two fragments \"governor from Texas\" and \"head of Mary-land\" would not match in STK although they have very similar syntactic structures and basically convey the same relationship."}
{"sent_id": "97852048a123350455f398728d6d34-C001-79", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "In particular, one limitation of the syntactic semantic tree kernel presented in Plank and Moschitti (2013) ( §2.1) is that semantics is highly tied to syntax (the PET trees) in the kernel computation, limiting the generalization capacity of semantics to the extent of syntactic matches."}
{"sent_id": "97852048a123350455f398728d6d34-C001-198", "intents": ["@BACK@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "However, as shown by Plank and Moschitti (2013) , instance weighting is not very useful for DA of RE."}
{"sent_id": "97852048a123350455f398728d6d34-C001-13", "intents": ["@USE@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "We here focus on the unsupervised domain adaptation (i.e., no labeled target data) and singlesystem DA (Petrov and McDonald, 2012; Plank and Moschitti, 2013) , i.e., building a single system that is able to cope with different, yet related target domains."}
{"sent_id": "97852048a123350455f398728d6d34-C001-70", "intents": ["@USE@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "In order to ensure the two methods (Plank and Moschitti, 2013; Nguyen and Grishman, 2014 ) are compared compatibly on the same resources, we make sure the two systems have access to the same amount of information."}
{"sent_id": "97852048a123350455f398728d6d34-C001-71", "intents": ["@USE@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "Thus, we follow Plank and Moschitti (2013) and use the PET trees (beside word clusters and word embeddings) as the only resource the two methods can exploit."}
{"sent_id": "97852048a123350455f398728d6d34-C001-115", "intents": ["@USE@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "We use the word clusters trained by Plank and Moschitti (2013) on the ukWaC corpus (Baroni et al., 2009 ) with 2 billion words, and the C&W word embeddings from Turian el al. (2010) 2 with 50 dimensions following Nguyen and Grishman (2014) ."}
{"sent_id": "97852048a123350455f398728d6d34-C001-122", "intents": ["@USE@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "The dataset partition is exactly the same as in Plank and Moschitti (2013) ."}
{"sent_id": "97852048a123350455f398728d6d34-C001-152", "intents": ["@USE@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "This section aims to compare the tree kernel-based method in Plank and Moschitti (2013) and the feature-based method in Nguyen and Grishman (2014) for DA of RE on the same settings (i.e, same dataset partition, the same pre-processing Table 3 : Tree kernel-based in Plank and Moschitti (2013) vs feature-based in Nguyen and Grishman (2014) ."}
{"sent_id": "97852048a123350455f398728d6d34-C001-203", "intents": ["@USE@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "Moreover, we perform a compatible comparison between the tree kernel-based method and the feature-based method on the same settings and resources, which suggests that the tree kernel-based method (Plank and Moschitti, 2013) is better than the feature-based method (Nguyen and Grishman, 2014) for DA of RE."}
{"sent_id": "97852048a123350455f398728d6d34-C001-35", "intents": ["@MOT@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "In fact, the problem of incompatible comparison is unfortunately very common in the RE literature (Wang, 2008; Plank and Moschitti, 2013) and we believe there is a need to tackle this increasing confusion in this line of research."}
{"sent_id": "97852048a123350455f398728d6d34-C001-109", "intents": ["@SIM@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "SIM: Finally, for completeness, we experiment with a more obvious way to introduce word embeddings into tree kernels, resembling more closely the approach of Plank and Moschitti (2013) ."}
{"sent_id": "97852048a123350455f398728d6d34-C001-116", "intents": ["@EXT@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "In order to make the comparisons compatible, we introduce word embeddings into the tree kernel by extending the package provided by Plank and Moschitti (2013) , which uses the Charniak parser to obtain the constituent trees, the SVM-light-TK for the SSTK kernel in SVM, the directional relation classes, etc."}
{"sent_id": "97852048a123350455f398728d6d34-C001-142", "intents": ["@EXT@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "In particular, we take the systems using the PET trees, word clusters and LSA in Plank and Moschitti (2013) as the baselines and augment them with the embeddings WED = HEAD+PHRASE."}
{"sent_id": "97852048a123350455f398728d6d34-C001-173", "intents": ["@EXT@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "Word Embeddings for the Tree-kernel based Method We focus on the comparison of the best model in Plank and Moschitti (2013) (row 11 in Table 2 ) (called P) with the same model but augmented with the embedding WED (row 12 in Tabel 2) (called P+WED)."}
{"sent_id": "97852048a123350455f398728d6d34-C001-147", "intents": ["@DIF@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "Third and most importantly, for all the systems in Plank and Moschitti (2013) (the baselines) and for all the target domains, whether word clusters and LSA are utilized or not, we consistently witness the performance improvement of the baselines when combined with word embedding (comparing systems X and X+WED where X is some baseline system)."}
{"sent_id": "97852048a123350455f398728d6d34-C001-149", "intents": ["@DIF@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "To be more concrete, the best system with word embeddings (row 12 in Table 2 ) significantly outperforms the best system in Plank and Moschitti (2013) with p < 0.05, an improvement of 3.7%, 1.1% and 2.7% on the target domains bc, cts and wl respectively, demonstrating the benefit of word embeddings for DA of RE in the tree kernel-based method."}
{"sent_id": "97852048a123350455f398728d6d34-C001-202", "intents": ["@DIF@"], "paper_id": "ABC_97852048a123350455f398728d6d34_0", "text": "The method demonstrates strong promise for the DA of RE, i.e, it significantly improves the best system of Plank and Moschitti (2013) (up to 7% relative improvement)."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "Word2vec [23] is a recently proposed family of algorithms for training such vector representations from unstructured text data via shal- * Work done while with Yahoo, Inc."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "The geometry of the resulting vectors was shown in [23] to capture word semantic similarity through the cosine similarity of the corresponding vectors as well as more complex semantic relationships through vector differences, such as vec(\"Madrid\") -vec(\"Spain\") + vec(\"France\") ≈ vec(\"Paris\")."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "In word2vec, each vocabulary word has two associated d-dimensional vectors which must be trained, respectively referred to as input and output vectors, each of which is represented as an array of d single precision floating point numbers [23] ."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "This has been found to yield the best results among the proposed variants on a variety of semantic tests of the resulting vectors [19, 23] ."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-76", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "The algorithm for maximizing (1) advocated in [23] , and implemented in its open-source counterpart, is a minibatch stochastic gradient descent (SGD)."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-85", "intents": ["@BACK@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "These include the original open source implementation of word2vec [23] , as well as those of Medallia [22] , and Rehurek [28] ."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-65", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "In this paper we focus on the skipgram approach with random negative examples proposed in [23] ."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-71", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "• window sizes bi,j are randomly selected so that each inner sum includes between 1 and a maximum B terms, as in [23] and its open-source implementation; 2"}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-72", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "• negative examples N i,j,k associated with positive output word w i,k are selected randomly according to a probability distribution suggested in [23] ;"}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-74", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "We follow [23] for setting V and select words occurring in the corpus a sufficient number of times (e.g., at least 5 times), or, if this results in too many words, as the most frequently occurring N words, where N is the largest number words that can be handled by available computational resources."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-75", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "We further also assume a randomized version of (1) according to the subsampling technique of [23] , which removes some occurrences of frequent words."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-111", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "For w = 10, n = 10, d = 500, values within the ranges recommended in [23] , this works out to r(10, 10, 500) ≈ 200, 000 bytes transferred per word with each get and put."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-140", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "The vectors are initialized in the parameter server shards as in [23] ."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-152", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "• dotprod: Select negative examplesw in (4) according to a probability distribution derived from the vocabulary histogram proposed in [23] , but with the client thread supplied seed initializing the random number generation, and then return all partial dot products required to evaluate the gradient (4) for all positive output, negative output, and input word vectors associated with the minibatch, wherein the partial dot products involve those vector components stored on the designated shard: usv T s ."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-243", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "To compare the proposed distributed system we trained vectors on a publicly available data set collected and processed by the script 'demo-train-big-model-v1-compute-only.sh' from the open-source package of [23] ."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-244", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "This script collects a variety of publicly available text corpuses and processes them using the algorithm described in [23] to coalesce sufficiently co-occurring words into phrases."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-249", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "The first column shows results for the single machine implementation of [23] , the second for a 'low parallelism' configuration of our system using 50 Spark executors, minibatch size of 1, and 1 thread per executor, and the third column for a 'high parallelism' configuration again with 50 executors, but with minibatch size increased to 50 and 8 threads per executor."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-250", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "The various systems were run using the skipgram variant with 500 dimensional vectors, maximum window size of 20 (10 in each direction), 5 negative examples, subsample ratio of 1e-6 (see [23] ), initial learning rate of 0.01875, and 3 iterations over the data set."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-265", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "We also compared the cosine similarities for pairs of vectors trained using the proposed distributed system and for corresponding vector pairs trained using the open-source implementation of [23] , again on a large search session data set."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-272", "intents": ["@USE@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "One model was trained using implementation from [23] and the other was trained using the proposed distributed system."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-77", "intents": ["@DIF@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "Our training system is also based on minibatch SGD optimization of (1), however, as described in Section 5, it is carried out in a distributed fashion in a manner quite different from the implementation of [23] ."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-254", "intents": ["@DIF@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "We are unsure why our system yields better results than the implementation of [23] on the wordsim test, yet worse scores on the analogies test."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-255", "intents": ["@DIF@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "We also note that the analogies test scores reported here involve computing the closest vector for each analogy \"question\" over the entire vocabulary and not just over the 1M most frequent words, as in the script 'demo-train-big-model-v1-computeonly.sh' of [23] ."}
{"sent_id": "98eef9a1dbea3ddd0a8fd1b9c9376c-C001-161", "intents": ["@SIM@"], "paper_id": "ABC_98eef9a1dbea3ddd0a8fd1b9c9376c_0", "text": "The data set is iterated over multiple times and after each iteration, the learning rate α is reduced in a manner similar to the open source implementation of [23] ."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "To improve the precision of retrieval output, especially within the very few (e.g, 5 or 10) highest-ranked documents that are returned, a number of researchers [36, 13, 16, 7, 22, 34, 25, 1, 18, 9] have considered a structural re-ranking strategy."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "In particular, in our prior work [18] we adapted PageRank [3] -which, due to the success of Google, is surely the most well-established algorithm for defining and computing centrality within a directed graph -to the task of re-ranking non-hyperlinked document sets."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "Our previous study [18] applied HITS to non-Web documents."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-21", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "We found that its performance was comparable to or better than that of algorithms that do not involve structural re-ranking; however, HITS was not as effective as PageRank [18] ."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-78", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "This is a natural comparison because PageRank is the most well-known centrality-induction algorithm utilized for ranking documents, and because in earlier work [18] , PageRank performed quite well as a tool for structural re-ranking of non-Web documents, at least when applied to document-to-document graphs."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-97", "intents": ["@BACK@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "Earlier work [18] also considered scoring a node v by its influx, P u∈V w t(u → v)."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-117", "intents": ["@BACK@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "Examples include document (re-)ranking [7, 24, 9, 18, 39 ], text summarization [11, 26] , sentence retrieval [28] , and document representation [10] ."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-127", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "Section 4.1 of [18] provides a more detailed justification of the experimental design."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-136", "intents": ["@BACK@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "The asymmetry of this measure corresponds nicely to the intuition that relevance flow is not symmetric [18] ."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-137", "intents": ["@BACK@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "Moreover, this function 6 Some of the PageRank results appearing in our previous paper [18] accidentally reflect experiments utilizing a suboptimal choice of Dinit."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-140", "intents": ["@BACK@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "is somewhat insensitive to large length differences between the items in question [18] , which is advantageous when both documents and clusters (which we treat as very long documents) are considered."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-141", "intents": ["@BACK@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "Previous work [18, 33] makes heavy use of the idea of nearest neighbors in language-model space."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-229", "intents": ["@BACK@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "In previous work, we showed that PageRank centrality scores induced over documentbased graphs can be used as a multiplicative weight on document query-likelihood terms, the intent being to cope with cases in which centrality in Dinit and relevance are not strongly correlated [18] ."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-232", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "While PageRank scores correspond to a stationary distribution that could be loosely interpreted as a prior [18] , in which case multiplicative combination with query likelihood is sensible, it is not usual to assign a probabilistic interpretation to hub or authority scores."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-235", "intents": ["@BACK@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "Thus, it may be the case that centrality scores induced over a document-based graph are more effective as a multiplicative bias on query-likelihood than as direct representations of relevance in Dinit (see also [18] ); but, modulo the caveat above, it seems that when centrality is induced over cluster-based one-way bipartite graphs, the correlation with relevance is much stronger, and hence this kind of centrality serves as a better \"bias\" on query-likelihood."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-167", "intents": ["@MOT@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "There are two motivations underlying our approach to choosing values for our algorithms' parameters [18] ."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-184", "intents": ["@MOT@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "We first consider our main question: can we substantially boost the effectiveness of HITS by applying it to cluster-to-document graphs, which we have argued are more suitable for it than the document-to-document graphs we constructed in our previous work [18] ?"}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-32", "intents": ["@EXT@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "Specifically, our experimental results show that the centralityinduction methods that we previously studied solely in the context of document-only graphs [18] result in much better re-ranking performance if implemented over bipartite graphs of documents (on one side) and clusters (on the other side)."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-126", "intents": ["@USE@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "Most aspects of the evaluation framework described below are adopted from our previous experiments with noncluster-based structural re-ranking [18] so as to facilitate direct comparison."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-132", "intents": ["@USE@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "To estimate the degree to which one item, if considered relevant, can vouch for the relevance of another, we follow our previous work on document-based graphs [18] and utilize p [µ] d (·), the unigram Dirichlet-smoothed language model induced from a given document d (µ is the smoothing parameter) [38] ."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-187", "intents": ["@USE@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "We now turn to Figure 2 , which gives the results for the re-ranking algorithms docInflux, doc-PageRank and doc-Auth as applied to either the document-based graph d↔d (as in [18] ) or the clusterdocument graph c→d."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-128", "intents": ["@DIF@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "The main conceptual changes 6 here are: a slightly larger parameter search-space for the \"out-degree\" parameter δ (called the \"ancestry\" parameter α in [18] ); and, of course, the incorporation of clusters."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-215", "intents": ["@DIF@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "As the notation suggests, this corresponds to running HITS and PageRank on the same graph, d↔d. But an alternative interpretation [18] is that non-smoothed (or no-random-jump) PageRank, as expressed by Equation (3), is applied to a different version of d↔d wherein the original edge weights w t(u → v) have been smoothed as follows:"}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-15", "intents": ["@USE@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "Our work builds on sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) , which have been extensively applied to the task of abstractive summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) ."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-44", "intents": ["@USE@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "The mechanism allows the decoder to keep track of its progress and dissuades the decoder from generating repeated information (Vaswani et al., 2017; Paulus et al., 2017) ."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-140", "intents": ["@USE@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "We evaluate on two versions of this dataset, the entity anonymized version (Hermann et al., 2015; Nallapati et al., 2016; Paulus et al., 2017) and the full text version (See et al., 2017) 1 ."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-152", "intents": ["@USE@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "To avoid repetition, we prevent the decoder from generating the same trigram more than once during test, following (Paulus et al., 2017) ."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-155", "intents": ["@USE@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "We evaluate using the standard ROUGE metric (Lin, 2004) and report the F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L. We compare to existing abstractive baselines (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) and report results on the Lead-3 extraction baseline which simply selects the first three sentences of the input article as its summary."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-171", "intents": ["@USE@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "This simulates a user which expresses preferences through specifying values of Model ROUGE-1 ROUGE-2 ROUGE-L Lead-3 (Nallapati et al., 2017) 39.2 15.7 35.5 ML words-lvt2k-temp-att (Nallapati et al., 2016) 35.46 13.30 32.65 ML, no intra-attention (Paulus et al., 2017) 37.86 14.69 34.99 ML, with intra-attention (Paulus et al., 2017) 38 the control variables."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-210", "intents": ["@USE@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "Table 4 shows results on the entity-anonymized version of the dataset used by (Nallapati et al., 2016; Paulus et al., 2017) and Table 5 reports results on the original version of the dataset used by (See et al., 2017) ."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "Recent summarization models build upon pointer networks (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) and have a few main architectural differences."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "For instance, (See et al., 2017) pairs attention with a coverage mechanism to avoid repetition and (Paulus et al., 2017) relies on intra-decoder attention to enable generating coherent multi-sentence summaries."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-97", "intents": ["@BACK@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "Recently, sequence-to-sequence neural networks (Sutskever et al., 2014) have been applied to abstractive summarization (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) following their success in both machine translation (Bahdanau et al., 2015; Luong et al., 2015b) , parsing (Luong et al., 2015a) and image captioning (Vinyals et al., 2015b) ."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-98", "intents": ["@BACK@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "Research in abstractive summarization with sequence-to-sequence models focuses on neural architectures (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) and learning objectives (Paulus et al., 2017) ."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-101", "intents": ["@BACK@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "Pointer mechanisms have been useful for abstractive summarization where copying entities and other rare words from the input is highly advantageous (See et al., 2017; Paulus et al., 2017) ."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-104", "intents": ["@BACK@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "To address this impediment, (See et al., 2017) introduce coverage modeling, (Paulus et al., 2017) propose intra-decoder attention, and (Suzuki and Nagata, 2017) equip the decoder with an estimator of unigram frequency."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-105", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "Regarding learning objectives, (Paulus et al., 2017) investigates the potential improvement from replacing maximum likelihood training with reinforcement learning to optimize ROUGE, the most common automatic metric to assess summarization."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-108", "intents": ["@DIF@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "Following (Gehring et al., 2017) , we rely on convolutional networks, in contrast to previous work using recurrent networks (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) ."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-115", "intents": ["@DIF@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "Contrary to (Paulus et al., 2017; See et al., 2017; Nallapati et al., 2016) , we rely on Byte-Pair Encoding (BPE) (Sennrich et al., 2016) to improve the model copy mechanism instead of using an additional pointer mechanism."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-116", "intents": ["@DIF@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "Contrary to (Paulus et al., 2017) , we did not explore training objectives and our training procedure aims at maximizing the likelihood of the training summaries given the source document."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-213", "intents": ["@DIF@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "On the entity-anonymized text, we report 38.68 F1-ROUGE1 as opposed to 38.30 for the best maximum likelihood training setting of (Paulus et al., 2017) ."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-214", "intents": ["@DIF@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "Our model does not outperform the reinforcement learning model of (Paulus et al., 2017) which optimizes ROUGE."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-110", "intents": ["@SIM@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "Like (Paulus et al., 2017) , we rely on intra-attention for generating multi-sentence text."}
{"sent_id": "8c57f595f1acecd03c32181cad444b-C001-113", "intents": ["@SIM@"], "paper_id": "ABC_8c57f595f1acecd03c32181cad444b_0", "text": "Like (Paulus et al., 2017) , we share the word embeddings in the encoder and decoder lookup tables with the embeddings from the output layer of the decoder."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016b) ."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "In addition to character embeddings, distributed representations of character bigrams Pei et al., 2014) and words (Morita et al., 2015; Zhang et al., 2016b) have also been shown to improve segmentation accuracies."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a) , as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b) ."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b) ."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-23", "intents": ["@USE@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "Following Cai and Zhao (2016) and Zhang et al. (2016b) , we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016) , which allows word information to be modelled explicitly."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-74", "intents": ["@USE@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "For the latter, we follow recent work (Chen et al., 2015b; Zhang et al., 2016b) , using a bidirectional LSTM to encode input character sequence."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-90", "intents": ["@USE@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "For the latter, we follow Zhang et al. (2016b) and Cai and Zhao (2016) , using an uni-directional LSTM on words that have been recognized."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-142", "intents": ["@USE@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "We fine-tune character and character bigram embeddings, but not word embeddings, acccording to Zhang et al. (2016b) ."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-205", "intents": ["@USE@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "We study the F-measure distribution with respect to sentence length on our baseline model, multitask pretraining model and Zhang et al. (2016b) ."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-44", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "Similar to Zhang et al. (2016b) and Cai and Zhao (2016) , we use word context on top of character context."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-57", "intents": ["@SIM@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "Similar to Zhang et al. (2016b) and Cai and Zhao (2016) , our model is a global structural model, using the overall score to disambiguate states, which correspond to sequences of inter-dependent transition actions."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-202", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "Zhang et al. (2016b) Both our model and Zhang et al. (2016b) use global learning and beam search, but our network is different."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-179", "intents": ["@DIF@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "This shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word-based segmentors (Zhang et al., 2016b; Cai and Zhao, 2016) ."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-180", "intents": ["@DIF@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "This is likely due to the difference in our neural network structures, and that we fine-tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with Zhang et al. (2016b) ."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-204", "intents": ["@DIF@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "Besides, the character and character bigram embeddings are fine-tuned in our model while Zhang et al. (2016b) set the embeddings fixed during training."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-211", "intents": ["@DIF@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "As shown in Figure 5 , the models give different error distributions, with our models being more robust to the sentence length compared with Zhang et al. (2016b) ."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-217", "intents": ["@DIF@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "Without multitask pretraining, our model gives an F-score of 95.44%, which is higher than the neural segmentor of Zhang et al. (2016b) , which gives the best accuracies among pure neural segments on this dataset."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-222", "intents": ["@DIF@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "In addition, it also outperforms the best neural models, in particular Zhang et al. (2016b) Table 7 : Main results on CTB6."}
{"sent_id": "2b5d8cab263c9edbe005674910a7b1-C001-240", "intents": ["@DIF@"], "paper_id": "ABC_2b5d8cab263c9edbe005674910a7b1_0", "text": "Similar to Table 7 , our method gives the best accuracies on all corpora except for MSR, where it underperforms the hybrid model of Zhang et al. (2016b) by 0.2%."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "More generally, the goal for VQA is to have a single system that can accurately answer any natural language question about an image or video [2] , [3] , [4] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "Specifically, researchers in fields as diverse as computer vision [3] , computational linguistics [2] , and machine learning [4] rely on large datasets to improve their VQA algorithms."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "Current methods to create these datasets assume a fixed number of human answers per visual question [3] , [5] , thereby either compromising on quality by not collecting all plausible answers or cost by collecting additional answers when they are redundant."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2] , [3] , [1] , [4] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual question [2] , [3] , [4] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-54", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "Other systems ensure a fixed number of answers are collected per visual question [3] , [5] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-93", "intents": ["@BACK@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "The remaining 90,000 VQAs are about abstract scenes that were created with clipart and show 100 types of everyday objects often observed in real images [3] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-231", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "Today's status quo is to either uniformly collect N answers for every visual question [3] or collect multiple answers where the number is determined by external crowdsourcing conditions [1] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-269", "intents": ["@BACK@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "This predictor illustrates the best a user can achieve today with crowd-powered systems [1] , [6] or with current dataset collection methods [3] , [5] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-50", "intents": ["@DIF@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "We demonstrate the predictive advantage of our system over relying on the uncertainty of a VQA algorithm in its predicted answer [3] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-192", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "Therefore, we employ as a baseline a related VQA algorithm [25] , [3] which produces for a given visual question an answer with a confidence score."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-202", "intents": ["@DIF@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "Both our proposed classification systems outperform the VQA Algorithm [3] baseline; e.g., Ours -RF yields a 12 percentage point improvement with respect to AP."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-284", "intents": ["@DIF@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "Figure 6b also illustrates the advantage of our system over a related VQA algorithm [3] for our novel application of costsensitive answer collection from a crowd."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-89", "intents": ["@USE@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "VQA Datasets: We conduct our analysis on a total of 459,861 visual questions and 4,598,610 answers coming from today's largest freely-available VQA benchmark [3] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-95", "intents": ["@USE@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \"stump a smart robot\" [3] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-98", "intents": ["@USE@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "Each answer was collected by showing a worker an image with associated question and asking him/her to respond with \"a brief phrase and not a complete sentence\" [3] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-105", "intents": ["@USE@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "We pre-process each answer by converting all letters to lower case, converting numbers to digits, and removing punctuation and articles (i.e., \"a\", \"an\", \"the\"), as was done in prior work [3] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-188", "intents": ["@USE@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "We capitalize on today's largest visual question answering dataset [3] to evaluate our prediction system, which includes 369,861 visual questions about real images."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-265", "intents": ["@USE@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "VQA Algorithm [3] :"}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-221", "intents": ["@SIM@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "Our overall finding that most of the predictive power stems from language-based features parallels feature analysis findings in the automated VQA literature [3] , [22] ."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "Lately, there have been a number of attempts to develop a purely neural machine translation system (NMT) [12, 5, 2, 22] ."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "NMT systems also use minimal domain knowledge, which makes them applicable to any other problem that can be formulated as mapping a sequence to another sequence [22] ."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-25", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "Empirically, both Sutskever et al. [22] and Bahdanau et al. [2] have observed that sentences with many rare words tend to be translated much more poorly than sentences containing mainly frequent words."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "For example, Kalchbrenner et al. [12] used a combination of a convolutional neural network and a recurrent neural network, Sutskever et al. [22] used a large and deep Long Short-Term Memory (LSTM) model, Cho et al. [5] used an architecture similar to the LSTM, and Bahdanau et al. [2] used a more elaborate neural network architecture that uses an attentional mechanism over the input sequence, similarly to Graves [9] and Graves et al. [10] ."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-44", "intents": ["@USE@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "In this work, we use the exact model of Sutskever et al. [22] , which has a large deep LSTM to encodue the input sequence and a separate deep LSTM to produce a translation from the input sequence."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-55", "intents": ["@USE@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "We treat the NMT system [12, 22, 5] as a black box and train it on a dataset annotated with alignment information specified by one of the models below."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-95", "intents": ["@USE@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "To be comparable with the results reported by previous work on neural machine translation systems [22, 5, 2] , we train our models on the same training data of 12M parallel sentences (348M French and 304M English words)."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-108", "intents": ["@USE@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "Like Sutskever et al. [22] , we reverse the words in the source sentences which has been shown to improve LSTM memory utilization and results in better translations of long sentences."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-110", "intents": ["@USE@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "We also follow the GPU parallelization scheme proposed in [22] , allowing us to reach a training speed of 9.0K words per second ([22] achieved 6.3K words per second with a larger vocabulary of 80K; our target vocabulary has 40K words)."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-144", "intents": ["@USE@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "To analyze the effect of rare words on translation quality, we follow Sutskever et al. [22] and sort the sentences in newstest2014 by the average frequency rank of their words."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-146", "intents": ["@USE@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "We evaluate our systems before and after translating the OOV words and compare with the standard MT systems -we use the state-of-the-art (SOTA) system from WMT'14 [7] , and neural MT systems -we use the ensemble system described in [22] (See Section 4)."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-148", "intents": ["@USE@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "The translation quality of our model before applying the unknown word translations is shown by the green star line, and the current best NMT system [22] is the purple diamond line."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-151", "intents": ["@USE@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "Overall, our rare word translation model interpolates between the SOTA system and the system of Sutskever et al. [22] , which allows us to outperform SOTA on sentences that consist predominantly of frequent words and approach its performance on sentences with many OOV words."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-97", "intents": ["@DIF@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "2 Due to the computationally intensive nature of the naive softmax in the target language, we limit the French vocabulary to the 40K most frequent French words (note that [22] used a vocabulary of 80k French words)."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-125", "intents": ["@DIF@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "Our best result (36.9 BLEU) outperforms all other NMT systems by a large margin, and in particular, it outperforms the current best NMT system [22] by 2.1 BLEU points (we even outperform Sutskever et al. [22] when they rerank the n-best list of a phrase-based baseline [22] )."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-149", "intents": ["@DIF@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "While [22] produces excellent translations of sentences with frequent words (the left part of the graph), they are worse than SOTA system (red triangle line) on sentences with many rare words (the right side of the graph)."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-188", "intents": ["@DIF@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "A key advantage of our technique is the fact that it is applicable to any NMT system and not only to the deep LSTM model of Sutskever et al. [22] ."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-106", "intents": ["@SIM@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "Our training procedure and hyperparameter choices are similar to those used by Sutskever et al. [22] ."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-116", "intents": ["@SIM@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "However, all other systems that we compared against have been evaluated on the tokenized translations using the multi-bleu.pl script, which is consistent with previous work [5, 2, 19, 22] ."}
{"sent_id": "4b34c36a93a049b8fd637fee768438-C001-117", "intents": ["@SIM@"], "paper_id": "ABC_4b34c36a93a049b8fd637fee768438_0", "text": "Thus, to make it possible to compare our system against the system of Durrani et al. [7] , we evaluated its tokenized predictions (which can be downloaded from statmt.org [7] ) on the test set (newstest2014) and arrived at the BLEU score of 37.0 points [22] ."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-12", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "Recently, sequence-to-sequence models [Sutskever et al., 2014; have gain superior performance in machine translation Vaswani et al., 2017 ]."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "For example, previous interactive NMT [ Sanchis-Trilles et al., 2014; Alvaro Peris et al., 2016; Knowles and Koehn, 2016] proposes to ask human to revise the translation output from the beginning of a sentence to the end (i.e. from the left to right), and regenerates the partial translation on the right side of the * Equal contribution, part of this work was done while Rongxiang Weng was a research intern at ByteDance AI Lab."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-43", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "be directly applied to current uni-directional NMT models, because after revising critical mistakes, uni-directional interactive models cannot correct minor mistakes to the left of the revision, even with advanced decoding algorithm [Hokamp and Liu, 2017; Post and Vilar, 2018; Hasler et al., 2018] ."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-67", "intents": ["@BACK@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "Neural machine translation (NMT) is based on a standard Seq2Seq model, which adopts an encoder-decoder architecture for sentence modeling and generation [Sutskever et al., 2014; Vaswani et al., 2017] ."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-75", "intents": ["@BACK@", "@UNSURE@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "It is computed by the attention mechanism [Luong et al., 2015b; Vaswani et al., 2017] ."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-104", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "Uni-directional interactive model only updates the right part [Álvaro Peris et al., 2016; Knowles and Koehn, 2016] , ignoring potential mistakes in the left part, which means the revised word should always be the left most error, otherwise the errors in the left part will never be corrected."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-230", "intents": ["@BACK@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "Interactive machine translation has been widely exploited to improve the translation by using interaction feedback from human users [Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016] in statistic machine translation (SMT) [Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2007] ."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-54", "intents": ["@DIF@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "Different from previous work about bi-directional decoder Mou et al., 2016; Liu et al., 2018] , our method is designed to take human revisions for interactive NMT."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-93", "intents": ["@USE@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "Following previous work [Álvaro Peris et al., 2016; Knowles and Koehn, 2016; Cheng et al., 2016] , we focus on the replacement and others can be implemented with several replacement operations."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-150", "intents": ["@USE@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "When generating a word in decoding, the model will read the revision memory, trying to automatically fix mistakes with a copy mechanism [Gu et al., 2016] ."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-165", "intents": ["@USE@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "By learning from the sentence level interaction history, our Seq2Seq model better fits We measure the translation quality with the IBM-BLEU score [Papineni et al., 2002] ."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-174", "intents": ["@USE@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "Following previous work [Cheng et al., 2016; Álvaro Peris et al., 2016; Hokamp and Liu, 2017] , we experiment on both the ideal and real environments."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-121", "intents": ["@SIM@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "The training stage is similar with multi-task models [Dong et al., 2015; Luong et al., 2015a] , both decoders could be trained using cross-entropy as the objective:"}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-127", "intents": ["@EXT@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "To solve the problem, we propose to combine the grid beam search [Hokamp and Liu, 2017] with our bi-directional decoder."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-20", "intents": ["@USE@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "Top row: baseline approach by Zellers et al. [18] ."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-100", "intents": ["@USE@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "We use the same backbone architecture to predict answers and rationales as in [18] ."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-152", "intents": ["@USE@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "The dataset used in all experiments in this work is VCR 1.0 [18] ."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-163", "intents": ["@USE@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "We train the model using the whole VCR dataset for 20 epochs as was done in [18] to align with the baselines."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-178", "intents": ["@USE@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "Finally, we train the two networks separately and combine the results of answer prediction module and rationale prediction module using \"AND\" operation as was done by Zellers et al. [18] ."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-179", "intents": ["@USE@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "For completeness, we also mention the results of four other baselines from [18] ."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-180", "intents": ["@USE@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "These baseline methods use the ResNet-50 (same as [18] ) visual architecture and Glove as text representations."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-211", "intents": ["@USE@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "We also summarize results of other baselines reported in [18] ."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "Precisely, the task is introduced and formulated in [18] as follows: given an image and a question related to the image, the model has to predict the correct answer from four possible choices and at the same time, it has to pick the right rationale, again from four options."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "As the task is new, Zellers et al. [18] provided a new baseline for it which seeks to tackle the task of predicting answers and predicting rationales separately."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "The Visual Commonsense Reasoning task, Zellers et al. [18] answers are predicted given the question and image and then, rationales are predicted given the image and question with the correct answer (see figure 1 and 2)."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "The task in Zellers et al. [18] is essentially posed as a question-answering task."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-78", "intents": ["@BACK@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "Anderson et al. [1] propose an orthogonal work to [18] , in which Faster-RCNN [15] is used to predict the image regions the model should attend to."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-87", "intents": ["@BACK@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "Zellers et al. [18] propose to jointly learn language and image representation using Bi-LSTM by feeding in image features from CNN for all annotated words."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "[18] call this step Grounding."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-184", "intents": ["@BACK@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "• Bottom-up and Top-down attention (BottomUpTop-Down) [1] : [18] adopted this model as another baseline by passing object regions referenced by the query and response."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-51", "intents": ["@DIF@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "Concurrently, it makes our approaches incomparable to the baselines provided by Zellers et al. [18] ."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-79", "intents": ["@DIF@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "We note the difference from our current proposed work -annotations are provided in the VCR 1.0 dataset [18] in form of bounding box and segmentation maps."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-172", "intents": ["@DIF@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "As mentioned earlier, our approach is not directly comparable to the baseline provided by Zellers et al. [18] since they feed the correct answer to the rationale module while we feed in the predicted answer."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-194", "intents": ["@DIF@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "Rather, a weighted average of answers (according to probabilities predicted by Q->A module) is provided to the rationale prediction module, unlike the baseline [18] , which gives the correct answer as input."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-212", "intents": ["@DIF@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "As can be seen from table 3, our Gumbel-softmax method performs better than the baseline [18] in Q->A task."}
{"sent_id": "90604f1ab33aafff682df0556aaa4e-C001-92", "intents": ["@EXT@"], "paper_id": "ABC_90604f1ab33aafff682df0556aaa4e_0", "text": "We seek to build on [18] by proposing a method to jointly train prediction and reasoning networks."}
{"sent_id": "e826db8ca46b47ba56945c50512a03-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_e826db8ca46b47ba56945c50512a03_0", "text": "Large-scale knowledge bases (KBs), such as Freebase [Bollacker et al., 2008] , WordNet [Miller, 1995] , Yago [Suchanek et al., 2007] , and NELL [Carlson et al., 2010] , are critical to natural language processing applications, e.g., question answering [Dong et al., 2015] , relation extraction [Riedel et al., 2013] , and language modeling [Ahn et al., 2016] ."}
{"sent_id": "e826db8ca46b47ba56945c50512a03-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_e826db8ca46b47ba56945c50512a03_0", "text": "TransH [Wang et al., 2014] and TransR [Lin et al., 2015b] are representative variants of TransE. These variants consider entities from multiple aspects and various relations on different aspects."}
{"sent_id": "e826db8ca46b47ba56945c50512a03-C001-24", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_e826db8ca46b47ba56945c50512a03_0", "text": "However, the consistent semantics expressed by some relation paths p is unreliable for reasoning new facts of that entity pair [Lin et al., 2015a] ."}
{"sent_id": "e826db8ca46b47ba56945c50512a03-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_e826db8ca46b47ba56945c50512a03_0", "text": "As the path ranking algorithm (PRA) [Lao et al., 2011] suggests, relation paths that end in many possible tail entities are more likely to be unreliable for the entity pair."}
{"sent_id": "e826db8ca46b47ba56945c50512a03-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_e826db8ca46b47ba56945c50512a03_0", "text": "A third current related work is PTransE [Lin et al., 2015a ] and the path ranking algorithm (PRA) [Lao et al., 2011] ."}
{"sent_id": "e826db8ca46b47ba56945c50512a03-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_e826db8ca46b47ba56945c50512a03_0", "text": "PRA, as one of the most promising research innovations for knowledge base completion, has also attracted considerable attention [Lao et al., 2015; Gardner and Mitchell, 2015; Wang et al., 2016; Nickel et al., 2016] ."}
{"sent_id": "e826db8ca46b47ba56945c50512a03-C001-13", "intents": ["@MOT@"], "paper_id": "ABC_e826db8ca46b47ba56945c50512a03_0", "text": "Traditional KB completion approaches, such as Markov logic networks [Richardson and Domingos, 2006] , suffer from feature sparsity and low efficiency."}
{"sent_id": "e826db8ca46b47ba56945c50512a03-C001-21", "intents": ["@MOT@"], "paper_id": "ABC_e826db8ca46b47ba56945c50512a03_0", "text": "Consequently, a promising new research direction is to use relation paths to learn knowledge embeddings [Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016] ."}
{"sent_id": "e826db8ca46b47ba56945c50512a03-C001-143", "intents": ["@USE@"], "paper_id": "ABC_e826db8ca46b47ba56945c50512a03_0", "text": "We follow the same evaluation procedures as used in [Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b] ."}
{"sent_id": "e826db8ca46b47ba56945c50512a03-C001-152", "intents": ["@USE@"], "paper_id": "ABC_e826db8ca46b47ba56945c50512a03_0", "text": "Because we use the same dataset, the baseline results reported in [Lin et al., 2015b; Lin et al., 2015a; Ji et al., 2016] are directly used for comparison."}
{"sent_id": "e826db8ca46b47ba56945c50512a03-C001-190", "intents": ["@USE@"], "paper_id": "ABC_e826db8ca46b47ba56945c50512a03_0", "text": "We directly compare our model with prior work using the results about knowledge embedding models reported in [Lin et al., 2015b] n=50, m=50, γ 1 =5, γ 2 =6, α=0.0001, B=1440, λ=0.8, and η=0.05, taking the L 1 norm on WN11; n=100, m=100, γ 1 =3, γ 2 =6, α=0.0001, B=960, λ=0.8, and η=0.05, taking the L 1 norm on FB13; and n=100, m=100, γ 1 =4, γ 2 =5, α=0.0001, B=4800, λ=1, and η =0.05, taking the L 1 norm on FB15K."}
{"sent_id": "e826db8ca46b47ba56945c50512a03-C001-172", "intents": ["@DIF@"], "paper_id": "ABC_e826db8ca46b47ba56945c50512a03_0", "text": "In particular, for the 1-to-N, N-to-1, and N-to-N types of relations [Bordes et al., 2013] 75.9 70.9 77.8 TransE (bern) [Bordes et al., 2013] 75.9 81.5 85.3 TransH (unif) [Wang et al., 2014] 77.7 76.5 78.4 TransH (bern) [Wang et al., 2014] 78.8 83.3 85.8 TransR (unif) [Lin et al., 2015b] 85.5 74.7 79.2 TransR (bern) [Lin et al., 2015b] 85.9 82.5 87.0 PTransE (ADD, 2-hop) [Lin et al., 2015a] 80.9 73.5 83.4 PTransE (MUL, 2-hop) [Lin et al., 2015a] 79.4 73.6 79.3 PTransE (ADD, 3-hop) [Lin et al., 2015a] 80 that plague knowledge embedding models, RPE (ACOM) improves 4.1%, 4.6%, and 4.9% on head entity's prediction and 6.9%, 7.0%, and 5.1% on tail entity's prediction compared with previous state-of-the-art performances achieved by PTransE (ADD, 2-hop)."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "The key challenge in open RE is to reason jointly over the universal schema consisting of KB relations and surface relations (Riedel et al., 2013) ."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "A number of matrix or tensor factorization models have recently been proposed in the context of relation extraction Riedel et al., 2013; Huang et al., 2014; ."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "For this reason, Riedel et al. (2013) argued and experimentally validated that open RE models can outperform targeted IE methods."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "This assumption generally does not hold for the surface relations extracted by open IE systems (Riedel et al., 2013) ; examples of other types of relationships between relations include implication or mutual exclusion."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-235", "intents": ["@BACK@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "NFE is the full model proposed in the \"universal schema\" work of Riedel et al. (2013) ."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-242", "intents": ["@BACK@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "The NFE model outperformed tensor models as well as clustering methods and distantly supervised methods in the experimental study of Riedel et al. (2013) for open RE tasks."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-40", "intents": ["@EXT@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "CORE is inspired by the combined factorization and entity model (FE) of Riedel et al. (2013) ."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-199", "intents": ["@EXT@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "We made use of the dataset of Riedel et al. (2013) , but extended it with contextual information."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-85", "intents": ["@SIM@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "Closest to our work is the \"universal schema\" matrix factorization approach of Riedel et al. (2013) , which combines a latent features model, a neighborhood model and an entity model but does not incorporate context."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-162", "intents": ["@USE@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "Following Riedel et al. (2013) , we adopt the open-world assumption instead, i.e., we treat each unobserved facts as unknown."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-178", "intents": ["@USE@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "Since we ultimately use our model to rank tuples for each relation individually, we consider as negative evidence for x only unobserved facts from the same relation (Riedel et al., 2013) ."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-195", "intents": ["@USE@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "2 Our experimental study closely follows the one of Riedel et al. (2013) ."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-216", "intents": ["@USE@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "From the raw dataset described above, we filtered out all surface relations with less than 10 instances, and all tuples with less than two instances, as in Riedel et al. (2013) ."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-224", "intents": ["@USE@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "To keep the experimental study feasible and comparable to previous studies, we use the full training data but evaluate each model's predictions on only the subsample of 10k tuples (≈ 6% of all tuples) of Riedel et al. (2013) ."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-243", "intents": ["@USE@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "We use the original source code of Riedel et al. (2013) for training."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-254", "intents": ["@USE@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "To evaluate the prediction performance of each method, we followed Riedel et al. (2013) ."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-266", "intents": ["@USE@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "These choices correspond to the ones of Riedel et al. (2013) ; no further tuning was performed."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-220", "intents": ["@DIF@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "In contrast to previous work (Riedel et al., 2013; , we retain partially-linked and non-linked facts in our dataset."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-229", "intents": ["@DIF@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "Our study focused on these two factorization models because they outperformed other models (including nonfactorization models) in previous studies (Riedel et al., 2013; ."}
{"sent_id": "029def00e36495beb31bde4cc87298-C001-262", "intents": ["@DIF@"], "paper_id": "ABC_029def00e36495beb31bde4cc87298_0", "text": "If all # facts are found and ranked top, then MAP 100 # = 1. Note that our definition of MAP 100 # differs slightly from Riedel et al. (2013) ; our metric is more robust because it is based on completely labeled evaluation data."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "Such paths are not only useful for link prediction (Lao et al., 2011; Gardner et al., 2014) , but also for finding explanations for direct links and help with targeted information extraction to fill in incomplete knowledge repositories (Yin et al., 2018; Zhou and Nastase, 2018) ."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "The idea of using paths in the graph has then been applied to the task of link prediction (Lao et al., 2011) , and extended to incorporate textual information (Gardner et al., 2014) ."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "The paths themselves can be incorporated in different ways in a model -as features (Lao et al., 2011; Gardner et al., 2014) , as Horn clauses to provide rules for inference in KGs whether directly or through scores that represent the strength of the path as a direct relation (Neelakantan et al., 2015; Guu et al., 2015) , also taking into account information about intermediary nodes (Das et al., 2017; Yin et al., 2018) ."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-89", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "Algorithms that harness path information often mine paths either by performing costly random walks (Guu et al., 2015) , traversals (Gardner et al., 2014; Neelakantan et al., 2015; Das et al., 2016) or by constructing paths through generative models (Das et al., 2017; Ding et al., 2018) ."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-102", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "They can be used in different ways, e.g. (i) as features in a link prediction system (e.g. (Gardner et al., 2014) ), (ii) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-118", "intents": ["@BACK@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "The graphs built by Gardner et al. (2014) cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-128", "intents": ["@BACK@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "Negative sampling The number of negative instances used in (Gardner et al., 2014) is not clearly stated."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-157", "intents": ["@BACK@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "For all Freebase KG configurations, Gardner et al. (2014) have 1000 paths for most relations (approx. 6 of the relations have between 230 and 973)."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-22", "intents": ["@USE@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "We test the extracted paths through the link prediction task on Freebase (Bollacker et al., 2008) and NELL (Carlson et al., 2010a) , using Gardner et al. (2014) 's experimental set-up: pairs of nodes are represented using their connected paths as fea-tures, and a model for predicting the direct relations is learned and tested on training and test sets for 24 relations in Freebase and 10 relations in NELL."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-75", "intents": ["@USE@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "Figure 1: Knowledge graphs statistics on a logarithmic scale: relation and nodes frequencies for Freebase and NELL (the version used by (Gardner et al., 2014) and in this paper)."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-86", "intents": ["@USE@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "The Path Ranking Algorithm formalism originally proposed by (Lao and Cohen, 2010) performs two main steps to represent of a pair of nodes in a graph: (i) feature selection -adding paths that connect the node pair; (ii) feature computation - Table 1 : Graph statistics on the datasets used by (Gardner et al., 2014) , and their abstract versions associating a value for each added path."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-110", "intents": ["@USE@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "Because we want to compare the abstract paths found using the abstract graph with paths found using PRA, we use the experimental set-up of (Gardner et al., 2014) , where we replace the feature selection and feature computation steps with the approach presented here."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-116", "intents": ["@USE@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "We build abstract graphs and paths from the Freebase and NELL data described in (Gardner et al., 2014) ."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-123", "intents": ["@USE@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "The results presented by Gardner et al. (2014) show that this configuration very rarely (and never overall) leads to better results than the other graph variations."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-145", "intents": ["@USE@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "The overall results of the experiments are presented in Table 3, and the relation-level results are  in Tables 4 for NELL, and 5 Table 3 : Results on the three graph variations of Freebase and NELL as reported by (Gardner et al., 2014) (G) and using abstract graphs (KG A )."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-146", "intents": ["@USE@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "Overall, the results indicate that enhancing Freebase and NELL with additional facts from textual sources leads to better results, particularly when these additional facts (< subject, verb, object > triples) are processed and clustered using low dimensional dense representations Gardner et al. (2014; use embeddings obtained by running PCA on the matrix of SVO triples)."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-150", "intents": ["@USE@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "The second column is the best result for each relation reported by (Gardner et al., 2014) ."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-167", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "We include the best result on PRA (on any variation of the graph), as reported by (Gardner et al., 2014) , although since we used different negative instances the results are not directly comparable."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-50", "intents": ["@SIM@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "To test the quality of these paths we ground them using the original KG and use these grounded paths in a learning framework similar to (Gardner et al., 2014) ."}
{"sent_id": "b7a7b7d9a6594dadb5853c49cddddf-C001-113", "intents": ["@SIM@"], "paper_id": "ABC_b7a7b7d9a6594dadb5853c49cddddf_0", "text": "The data thus obtained is used for training a linear regression model (similarly to (Gardner et al., 2014) ), and tested on the provided test sets and evaluated using mean average precision (MAP)."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-16", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "To address these limitations, Vaswani et al. have proposed the Transformer, a machine translation model that introduces a new deep learning architecture solely based on \"attention\" mechanisms [2] ."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "Vaswani et al. were the first to apply attention directly over the word-embeddings, and thus derived a new neural network architecture which, without any recurrence, achieved state-ofthe-art results in machine translation [2] ."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "In recent years, attention mechanisms have been used with success in a variety of NLP tasks, such as machine translation [2] , [27] and natural language inference [28] , [29] ."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-51", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "The Transformer is a machine translation model introduced in [2] that achieved state-of-the-art results by combining feedforward neural networks with a multiplicative attention mechanism applied over position-encoded embedding vectors."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "Additionally, Vaswani et al. [2] suggest a multi-head attention, in which U, K and V are divided into n heads heads and the attention in the i th head is computed as"}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "This type of attention is often called \"self-attention\" or \"self-alignment\" [2] , [21] ."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-24", "intents": ["@EXT@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "• Column-wise cross-attention: we modify the crossattention operation by [2] and propose a new technique that is better suited to question-answering."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-65", "intents": ["@EXT@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "We also identified another QA model [32] that is inspired by the architecture introduced by Vaswani et al. [2] ."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-72", "intents": ["@EXT@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "In particular, we introduce the convolutional attention, the column-wise cross-attention, and the reduction layer, which build on the Transformer model [2] to enable its application to question-answering."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-98", "intents": ["@EXT@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "In FABIR the attention mechanism is inspired by the Transformer model introduced in [2] ."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-152", "intents": ["@MOT@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "Nonetheless, we observed that the new architecture introduced by Vaswani et al. [2] is more susceptible to overfitting than RNNs when presented with large embedding sizes."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-23", "intents": ["@USE@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "• Reduction layer: a new layer design that fits the pipeline proposed by Vaswani et al. [2] and compresses the input embedding size for subsequent layers (this is especially beneficial when employing pre-trained embeddings)."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-86", "intents": ["@USE@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "We add positional information to each word embedding using a trigonometric encoder as proposed in [2] ."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-88", "intents": ["@USE@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "where f k are scalars, which were chosen according to [2] ."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-121", "intents": ["@USE@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "Following the architecture suggested by Vaswani et al. [2] , the feedforward sublayer is implemented in (15) with a two-layer neural network:"}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-182", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "As suggested in [2] , we have chosen the Adam optimizer [41] with the same hyperparameters, except for the learning rate, which was divided by two in our implementation."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-183", "intents": ["@USE@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "For regularization, we applied residual and attention dropout [2] of 0.9 in processing layers and of 0.8 in the reduction layer."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-193", "intents": ["@USE@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "Most importantly, when the convolutional attention was replaced by the standard attention mechanism proposed in [2] , the performance dropped by 2.4% in F1 and 2.5% in EM."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-247", "intents": ["@USE@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "Moreover, being thoroughly compatible with the Transformer [2] , these new mechanisms are valuable assets to further developments in attention models."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-114", "intents": ["@DIF@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "In contrast to Vaswani et al. [2] , where the softmax in (12d) is applied in a row-wise manner, we suggest column-wise cross-attention."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-9", "intents": ["@USE@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "We aim to categorize a clause based on its aspectual property and more specifically, based on the type of Situation Entity (SE) 1 (e.g., events, states, generalizing statements and generic statements) the clause introduces to the discourse, following the recent work by (Friedrich et al., 2016) ."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-82", "intents": ["@USE@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "Following (Friedrich et al., 2016) , in order to capture SE label patterns in our hierarchical recurrent neural network model, we add a CRF layer at the top of the softmax prediction layer (shown in figure 2 ) to fine-tune predicted situation entity types."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-101", "intents": ["@USE@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "The MASC+Wiki Corpus: We evaluated our neural network model on the MASC+Wiki corpus 7 (Friedrich et al., 2016) (Friedrich et al., 2016; Becker et al., 2017) , we used the same 80:20 traintest split with balanced genre distributions."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-102", "intents": ["@USE@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "Preprocessing: As described in (Friedrich et al., 2016) , texts were split into clauses using SPADE (Soricut and Marcu, 2003) ."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-122", "intents": ["@USE@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "Following the previous work (Friedrich et al., 2016) on the same task and dataset, we report accuracy and macro-average F1-score across SE types on the test set of MASC+Wiki."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-134", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "We noticed that the previous work (Friedrich et al., 2016) did not publish the class-wise performance of their model on the test set, instead, they reported the detailed performance on the training set using 10-fold cross-validation."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-12", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "Recently, Friedrich et al. (2016) used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "In addition, Friedrich et al. (2016) implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001 ) for finetuning a sequence of predicted SE types."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "The situation entity types annotated in the MASC+Wiki corpus (Friedrich et al., 2016) were initially introduced by Smith (2003) , which were then extended by (Palmer et al., 2007; Friedrich and Palmer, 2014b) ."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "To bridge the gap, Friedrich et al. (2016) created a much larger dataset MASC+Wiki (more than 40,000 clauses) and achieved better SE type classification performance (around 75% accuracy) by using rich features extracted from the target clause."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-48", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "Friedrich et al. (2016) further improved the performance by implementing a sequence labeling (CRF) model to fine-tune a sequence of SE type predictions and noted that much of the performance gain came from modeling the label pattern that GENERIC clauses often occur together."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-81", "intents": ["@BACK@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "For example, Friedrich et al. (2016) reported the fact that GENERIC sentences usually occur together in a paragraph."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-24", "intents": ["@EXT@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "In order to further improve SE type classification performance, we also add an extra CRF layer at the top of our paragraph-level model to fine-tune a sequence of SE type predictions over clauses (Friedrich et al., 2016) , which however is not our contribution."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "Experimental results show that our paragraphlevel neural network model greatly improves the performance of SE type classification on the same MASC+Wiki (Friedrich et al., 2016) corpus and achieves robust performance close to human level."}
{"sent_id": "9e7f3943d8f8aff2eb91c291cd020e-C001-149", "intents": ["@DIF@"], "paper_id": "ABC_9e7f3943d8f8aff2eb91c291cd020e_0", "text": "As we expected, both the macroaverage F1-score and class-wise F1 scores are lower compared with the results in Table 2 where in-genre data were used for model training as well. But the performance drop on the paragraph-level models is little, which clearly outperform the previous system (Friedrich et al., 2016) and the baseline model by a large margin."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "Recently, there has been much interest in applying neural network models to solve the problem, where little or no linguistic analysis is performed except for tokenization (Filippova et al., 2015; Rush et al., 2015; Chopra et al., 2016) ."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "For example, Filippova et al. (2015) used close to two Figure 1 : Examples of in-domain and out-ofdomain results by a standard abstractive sequenceto-sequence model trained on the Gigaword corpus."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-70", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "(3) Although Filippova et al. (2015) did not use any syntactic information in their basic model, they introduced some features based on dependency parse trees in their advanced models."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-159", "intents": ["@BACK@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "LSTM+: This is advanced version of the model proposed by Filippova et al. (2015) , where the authors incorporated some dependency parse tree information into the LSTM model and used the prediction on the previous word to help the prediction on the current word."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-96", "intents": ["@MOT@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "For example, the method above as well as the original method by Filippova et al. (2015) cannot impose any length constraint on the compressed sentences."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-27", "intents": ["@EXT@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "To this end, we extend the deletionbased LSTM model for sentence compression by Filippova et al. (2015) ."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-34", "intents": ["@EXT@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "Specifically, we propose two major changes to the model by Filippova et al. (2015) : (1) We explicitly introduce POS embeddings and dependency relation embeddings into the neural network model."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-37", "intents": ["@USE@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "We evaluate our method using around 10,000 sentence pairs released by Filippova et al. (2015) and two other data sets representing out-ofdomain data."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-49", "intents": ["@USE@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "Our problem setup is the same as that by Filippova et al. (2015) ."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-58", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "This base model is largely based on the model by Filippova et al. (2015) with some differences, which will be explained below."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-63", "intents": ["@USE@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "Following Filippova et al. (2015) , our bi-LSTM has three layers, as shown in Figure 2 ."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-137", "intents": ["@USE@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "Google News: The first dataset contains 10,000 sentence pairs collected and released by Filippova et al. (2015) 2 ."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-157", "intents": ["@USE@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "We compare our methods with a few baselines: LSTM: This is the basic LSTM-based deletion method proposed by (Filippova et al., 2015) ."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-167", "intents": ["@USE@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "We took the first 1,000 sentence pairs from Google News as the test set, following the same practice as Filippova et al. (2015) ."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-198", "intents": ["@USE@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "In order to evaluate whether sentences generated by our method are readable, we adopt the manual evaluation procedure by Filippova et al. (2015) to compare our method with LSTM+ and Traditional ILP in terms of readability and informativeness."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-244", "intents": ["@USE@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "Our work is based on the deletion-based LSTM model for sentence compression by Filippova et al. (2015) ."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-67", "intents": ["@DIF@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "There are some differences between our base model and the LSTM model by Filippova et al. (2015) ."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-69", "intents": ["@DIF@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "(2) Filippova et al. (2015) used only a single-directional LSTM while we use bi-LSTM to capture contextual information from both directions."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-176", "intents": ["@DIF@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "(2) In the in-domain setting, with the same amount of training data (8,000), our BiLSTM method with syntactic features (BiLSTM+SynFeat and BiLSTM+SynFeat+ILP) performs similarly to or better than the LSTM+ method proposed by Filippova et al. (2015) , in terms of both F1 and accuracy."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-17", "intents": ["@SIM@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "However, like more recent approaches (Artetxe et al., 2017) , our model operates directly over pretrained word embeddings, induces a joint cross-lingual embedding space, and scales to large vocabulary sizes."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-180", "intents": ["@SIM@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "Implementation details Similar to Artetxe et al. (2017) , we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 × 10 −6 between succeeding iterations."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-201", "intents": ["@SIM@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "12 Training takes a similar amount of time as (Artetxe et al., 2017) due to faster convergence."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-236", "intents": ["@SIM@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "Surprisingly, the method by Artetxe et al. (2017) a similar self-learning method that uses word embeddings, with an implicit one-to-many alignment based on nearest neighbor queries."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-23", "intents": ["@EXT@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "To this end, we provide a reinterpretation of Artetxe et al. (2017) as a latent-variable model with an IBM Model 1-style (Brown et al., 1993 ) dictionary prior, which allows a clean side-by-side analytical comparison."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-166", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "Why a Reinterpretation? The reinterpretation of Artetxe et al. (2017) as a probabilistic model yields a clear analytical comparison between our method and theirs."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-244", "intents": ["@EXT@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "We have presented a novel latent-variable model for bilingual lexicon induction, building on the work of Artetxe et al. (2017) ."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-24", "intents": ["@DIF@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "Viewed in this light, the difference between our approach and Artetxe et al. (2017) , the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over Artetxe et al. (2017) ."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-208", "intents": ["@DIF@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "On cross-lingual word similarity, our approach yields the best performance on WordSim-353 and RG-65 for English-German and is only outperformed by Artetxe et al. (2017) on English-Italian Wordsim-353."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-227", "intents": ["@DIF@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "17 Hubs are fewer and occur less often with our method, demonstrating that the prior-to some en-tr en-bn en-hi et-fi Artetxe et al. (2017) extent-aids with resolving hubness."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-86", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "As for assumption (ii), previous work (Xing et al., 2015; Artetxe et al., 2017) has achieved some success using an orthogonal transformation; recently, however, demonstrated that monolingual embedding spaces are not approximately isomorphic and that there is a complex relationship between word form and meaning, which is only inadequately modeled by current approaches, which for example cannot model polysemy."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-135", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "We additionally enforce the constraint that Ω is a real orthogonal matrix, i.e., Ω Ω = I. Previous work (Xing et al., 2015; Artetxe et al., 2017) found that the orthogonality constraint leads to noticeable improvements."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-190", "intents": ["@BACK@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "Likewise, as discussed in §5, Artetxe et al. (2017) make use of a Viterbi EM style algorithm with a different prior over edge sets."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-147", "intents": ["@USE@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "The self-training method of Artetxe et al. (2017) , our strongest baseline in §6, may also be interpreted as a latent-variable model in the spirit of our exposition in §3."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-152", "intents": ["@USE@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "We show how this corresponds to an alignment distribution that is equivalent to IBM Model 1 (Brown et al., 1993) , and that Artetxe et al. (2017) 's selftraining method is actually a form of Viterbi EM."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-153", "intents": ["@USE@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "To formalize Artetxe et al. (2017) 's contribution as a latent-variable model, we lay down some more notation."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-173", "intents": ["@USE@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "Datasets For bilingual dictionary induction, we use the English-Italian dataset by and the English-German and English-Finnish datasets by Artetxe et al. (2017) ."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-177", "intents": ["@USE@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "We follow Artetxe et al. (2017) and train monolingual embeddings with word2vec, CBOW, and negative sampling (Mikolov et al., 2013a ) on a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC), a 1.6 billion word corpus for Italian (itWaC), a 0.9 billion word corpus for German (SdeWaC), and a 2.8 billion word corpus for Finnish (Common Crawl)."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-178", "intents": ["@USE@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "Seed dictionaries Following Artetxe et al. (2017), we use dictionaries of 5,000 words, 25 words, and a numeral dictionary consisting of words matching the [0-9]+ regular expression in both vocabularies."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-187", "intents": ["@USE@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "Finally, we compare with Artetxe et al. (2016) who add dimension-wise mean centering to Xing et al. (2015) , and Artetxe et al. (2017) ."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-188", "intents": ["@USE@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "Both Mikolov et al. (2013c) and Artetxe et al. (2017) are special cases of our famework and comparisons to these approaches thus act as an ablation study."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-226", "intents": ["@USE@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "We show the target language words with the highest hubness using our method and Artetxe et al. (2017) for English-German with a 5,000 seed lexicon and the full vocabulary in Table 3 ."}
{"sent_id": "10319b67b6fcc1870791cf67b39299-C001-232", "intents": ["@USE@"], "paper_id": "ABC_10319b67b6fcc1870791cf67b39299_1", "text": "We perform experiments with our method with and without a rank constraint and Artetxe et al. (2017) for three truly lowresource language pairs, English-{Turkish, Bengali, Hindi}. We additionally conduct an experiment for Estonian-Finnish, similarly to ."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-7", "intents": ["@USE@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-22", "intents": ["@USE@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "In order to compare ROOT9 with the state-of-the-art, we have also evaluated it in the Weeds et al. (2014) datasets."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-113", "intents": ["@USE@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "We have performed three tasks: i) an ablation test to evaluate the contribution of the features on our dataset (henceforth, ROOT9 Dataset; see Section 4.2); ii) an evaluation against the state of the art, and -in particularagainst the best performant models in Weeds et al. (2014) ; iii) an evaluation on switched pairs to verify whether the actual semantic relations or the prototypical hypernyms (Levy et al., 2015) were learnt."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-119", "intents": ["@USE@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "The second task, which is described in Section 6, consisted in binary classification tasks on the four datasets proposed by Weeds et al. (2014) ."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-121", "intents": ["@USE@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "The task allowed us to compare ROOT9 against the state of the art models reported in Weeds et al. (2014) ."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-137", "intents": ["@USE@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "In order to compare ROOT9 to the state-of-the-art, we have evaluated it with the datasets created by Weeds et al. (2014) ."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-154", "intents": ["@USE@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "In the second task (see Section 6), we have used as baselines the most competitive models reported in Weeds et al. (2014) , namely the SVM classifiers trained on the PPMI vector of the second word (svmSINGLE), or on the concatenated (svmCAT), summed (svmADD), multiplied (svmMULT) and subtracted (svmDIFF) PPMI vectors of the words in the pair."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-174", "intents": ["@USE@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "However, it is worth noticing here that such difference disappears with the WN datasets proposed by Weeds et al. (2014) ."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-182", "intents": ["@USE@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "In Table 4 , we show ROOT9's performance compared to the best systems reported by Weeds et al. (2014) ."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-183", "intents": ["@USE@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "The scores are all calculated on subsets of Weeds et al. (2014) 's datasets, as reported in Section 4.3."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-202", "intents": ["@USE@", "@FUT@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "The impressive results in our dataset, developed by randomly extracting 9,600 pairs from EVALution , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011) , were further tested against the state-of-the-art models presented in Weeds et al. (2014 In future experiment, we plan to increase the number of features, investigating new distributional properties that may help in the classification without incurring in memorization effects such as those described by Levy et al. (2015) ."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar (Weeds et al., 2014) ."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015) ."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014 , Weeds et al., 2014 Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003) ."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-18", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "The former have been shown to outperform the latter in Weeds et al. (2014) , even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b) , a supervised method based on a Random Forest algorithm and thirteen corpus-based features."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "Roller et al. (2014) used the vectors' difference, while Weeds et al. (2014) implemented numerous combinations (difference, multiplication, sum, concatenation, etc.), comparing them against the most common unsupervised methods."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-139", "intents": ["@BACK@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "The WN dataset (Weeds et al., 2014 ) -meaning both WN Hyper and WN Co-Hyp -in particular, was built after noticing that supervised systems tended to perform well also on random vectors."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-153", "intents": ["@BACK@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "The discrepancy with what found by Weeds et al. (2014) namely that random vectors perform particularly well when words are re-used in the dataset -may depend on the small number of features, which does not allow the system to identify discriminative random dimensions."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-184", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "Considering all the datasets, ROOT9 is the second best performing system, after svmCAT (Weeds et al., 2014) , which uses the SVM classifier on the concatenation of PPMI vectors, containing as features all major grammatical dependency relations involving open class Parts Of Speech."}
{"sent_id": "bbb91e450b3503166bcfae60e9ba72-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_bbb91e450b3503166bcfae60e9ba72_1", "text": "Also in 1 The 9,600 pairs are available at https://github.com/esantus/ROOT9 relation to the state of the art, ROOT9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmCAT model (Weeds et al., 2014) , which is a Support Vector Machine (SVM) classifier run on the concatenation of the distributional vectors of the words in the pairs."}
{"sent_id": "52b5ed7a0753402ad4bceb83a2b495-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_52b5ed7a0753402ad4bceb83a2b495_1", "text": "Recently other neural-based sentence and paragraph level representations appeared to provide a fixed length representation like Skip-Thought Vectors (Kiros et al., 2015) and FastSent (Hill et al., 2016) ."}
{"sent_id": "52b5ed7a0753402ad4bceb83a2b495-C001-134", "intents": ["@DIF@"], "paper_id": "ABC_52b5ed7a0753402ad4bceb83a2b495_1", "text": "We contrast our results against the methods reported in (Hill et al., 2016) ."}
{"sent_id": "52b5ed7a0753402ad4bceb83a2b495-C001-131", "intents": ["@USE@"], "paper_id": "ABC_52b5ed7a0753402ad4bceb83a2b495_1", "text": "We follow the setup used in (Hill et al., 2016) ."}
{"sent_id": "52b5ed7a0753402ad4bceb83a2b495-C001-139", "intents": ["@USE@"], "paper_id": "ABC_52b5ed7a0753402ad4bceb83a2b495_1", "text": "We use same evaluation measures (Hill et al., 2016) ."}
{"sent_id": "52b5ed7a0753402ad4bceb83a2b495-C001-158", "intents": ["@USE@"], "paper_id": "ABC_52b5ed7a0753402ad4bceb83a2b495_1", "text": "We use linear (Hill et al., 2016) ."}
{"sent_id": "52b5ed7a0753402ad4bceb83a2b495-C001-161", "intents": ["@USE@"], "paper_id": "ABC_52b5ed7a0753402ad4bceb83a2b495_1", "text": "We use classification accuracy as the evaluation measure for this experiment as (Hill et al., 2016) ."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-4", "intents": ["@USE@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-15", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "We present a replication study of BERT pretraining (Devlin et al., 2019) , which includes a careful evaluation of the effects of hyperparmeter tuning and training set size."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-27", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "In this section, we give a brief overview of the BERT (Devlin et al., 2019) pretraining approach and some of the training choices that we will examine experimentally in the following section."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-104", "intents": ["@USE@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "Our finetuning procedure follows the original BERT paper (Devlin et al., 2019) ."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-113", "intents": ["@USE@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "For SQuAD V1.1 we adopt the same span prediction method as BERT (Devlin et al., 2019) ."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-134", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "Results Table 1 compares the published BERT BASE results from Devlin et al. (2019) to our reimplementation with either static or dynamic masking."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-145", "intents": ["@USE@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "• SEGMENT-PAIR+NSP: This follows the original input format used in BERT (Devlin et al., 2019) , with the NSP loss."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-158", "intents": ["@USE@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "We first compare the original SEGMENT-PAIR input format from Devlin et al. (2019) to the SENTENCE-PAIR format; both formats retain the NSP loss, but the latter uses single sentences."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-254", "intents": ["@USE@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "For SQuAD v1.1 we follow the same finetuning procedure as Devlin et al. (2019) ."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "Self-training methods such as ELMo (Peters et al., 2018) , GPT (Radford et al., 2018) , BERT (Devlin et al., 2019) , XLM (Lample and Conneau, 2019) , and XLNet have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-189", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "The original BERT implementation (Devlin et al., 2019) uses a character-level BPE vocabulary of size 30K, which is learned after preprocessing the input with heuristic tokenization rules."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-274", "intents": ["@BACK@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "Pretraining methods have been designed with different training objectives, including language modeling (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018) , machine translation (McCann et al., 2017) , and masked language modeling (Devlin et al., 2019; Lample and Conneau, 2019) ."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-277", "intents": ["@BACK@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "Performance is also typically improved by training bigger models on more data (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019) ."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-73", "intents": ["@DIF@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "Unlike Devlin et al. (2019) , we do not randomly inject short sequences, and we do not train with a reduced sequence length for the first 90% of updates."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-161", "intents": ["@DIF@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "We find that this setting outperforms the originally published BERT BASE results and that removing the NSP loss matches or slightly improves downstream task performance, in contrast to Devlin et al. (2019) ."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-202", "intents": ["@DIF@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "For example, the recently proposed XLNet architecture ) is pretrained using nearly 10 times more data than the original BERT (Devlin et al., 2019) ."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-234", "intents": ["@DIF@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "This formulation significantly simplifies the task, but is not directly comparable to BERT (Devlin et al., 2019) ."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-251", "intents": ["@DIF@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "In particular, while both BERT (Devlin et al., 2019) and XLNet augment their training data with additional QA datasets, we only finetune RoBERTa using the provided SQuAD training data."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-205", "intents": ["@SIM@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "We pretrain for 100K steps over a comparable BOOK-CORPUS plus WIKIPEDIA dataset as was used in Devlin et al. (2019) ."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "Lexical Simplification (LS) aims at replacing complex words with simpler alternatives, which can help various groups of people, including children [De Belder and Moens, 2010] , non-native speakers [Paetzold and Specia, 2016] , people with cognitive disabilities [Feng, 2009; Saggion, 2017] , to understand text better."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "For avoiding the need for resources such as databases or parallel corpora, recent work utilizes word embedding models to extract simplification candidates for complex words [Glavaš andŠtajner, 2015; Paetzold and Specia, 2016; Paetzold and Specia, 2017a] ."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "original word embeddings trained on text, and Paetzold et al. (2017) used a retrofitted context-aware word embedding model trained on text with the POS tag."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-28", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "For complex words 'composed' and 'verses' in the sentence \"John composed these verses.\", the top three substitution candidates of the two complex words generated by the LS systems based on word embeddings [Glavaš andŠtajner, 2015; Paetzold and Specia, 2017a] are only related with the complex words itself without without paying attention to the original sentence."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "Lexical simplification (LS) contains identifying complex words and finding the best candidate substitution for these complex words [Shardlow, 2014; Paetzold and Specia, 2017b] ."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "The popular lexical simplification (LS) approaches are rule-based, which each rule contain a complex word and its simple synonyms [Lesk, 1986; Pavlick and Callison-Burch, 2016; Maddela and Xu, 2018] ."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-57", "intents": ["@BACK@", "@DIF@", "@MOT@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "Afterward, they further extracted candidates for complex word by combining word embeddings with WordNet and parallel corpora [Paetzold and Specia, 2017a] ."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-99", "intents": ["@BACK@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "The substitution ranking of the lexical simplification pipeline is to decide which of the candidate substitutions that fit the context of a complex word is the simplest [Paetzold and Specia, 2017b]."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-16", "intents": ["@USE@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "Given one sentence \"John composed these verses.\" and complex words 'composed' and 'verses', the top three simplification candidates for each complex word are generated by our method BERT-LS and the state-of-the-art two baselines based word embeddings (Glavaš[Glavaš andŠtajner, 2015] and Paetzold-NE [Paetzold and Specia, 2017a] )."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-144", "intents": ["@USE@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "(2) BenchLS 6 [Paetzold and Specia, 2016] ."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-150", "intents": ["@USE@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "We choose the following eight baselines to evaluation: Devlin [Devlin and Tait, 1998 ], Biran [Biran et al., 2011 ], Yamamoto [Kajiwara et al., 2013 , Horn [Horn et al., 2014] , Glavaš [Glavaš andŠtajner, 2015] , SimplePPDB [Pavlick and Callison-Burch, 2016] , Paetzold-CA [Paetzold and Specia, 2016] , and Paetzold-NE [Paetzold and Specia, 2017a] ."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-154", "intents": ["@USE@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "The following three widely used metrics are used for evaluation [Paetzold and Specia, 2015; Paetzold and Specia, 2016; Paetzold and Specia, 2017b] ."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-159", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "The results of the baselines on LexMTurk are from [Paetzold and Specia, 2017b] and the results on BenchLS and NNSeval are from [Paetzold and Specia, 2017a] ."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-167", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "We chooses the state-of-the-art two baselines based word embeddings (Glavaš[Glavaš andŠtajner, 2015] and Paetzold-NE [Paetzold and Specia, 2017a]) as comparison."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-175", "intents": ["@USE@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "We adopt the following two well-known metrics used by these work [Horn et al., 2014; Paetzold and Specia, 2017b] ."}
{"sent_id": "fb87be2081ce1515dd8dbda46b4f3f-C001-126", "intents": ["@DIF@"], "paper_id": "ABC_fb87be2081ce1515dd8dbda46b4f3f_1", "text": "In this paper, we are not focused on identifying complex words [Paetzold and Specia, 2017b] , which is a separate task."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-4", "intents": ["@USE@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "We show how this approach can be combined with additional features, in particular, the discourse features presented by Jansen et al. (2014) ."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-26", "intents": ["@USE@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "First, we present a novel neural approach to answer reranking that achieves competitive results on a public dataset of Yahoo! Answers (YA) that was previously introduced by Jansen et al. (2014) and later used in several other studies (Fried et al., 2015; Sharp et al., 2015; Bogdanova and Foster, 2016) ."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-92", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "Based on the intuition that modelling questionanswer structure both within and across sentences could be useful, Jansen et al. (2014) propose an answer reranking model based on discourse features combined with lexical semantics."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-98", "intents": ["@USE@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "We illustrate the feature extraction process of Jansen et al. (2014) in Figure 2 ."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-114", "intents": ["@USE@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "For comparability, we use the dataset created by Jansen et al. (2014) which contains 10K how questions from Yahoo! Answers. 50% of it is used for training, 25% for development and 25% for testing."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-117", "intents": ["@USE@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "Further details about this dataset can be found in (Jansen et al., 2014) ."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-144", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "The CR baseline uses the same scoring as in Jansen et al. (2014) : the questions and the candidate answers are represented using tf-idf over lemmas; the candidate answers are ranked according to their cosine similarity to the respective question."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-146", "intents": ["@USE@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "On the YA dataset, we also compare our results to the ones reported by Jansen et al. (2014) and by Bogdanova and Foster (2016) ."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-152", "intents": ["@USE@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "We use the software provided by Jansen et al. (2014) 7 to extract the discourse features described in Section 4 and referred to as x ext in Section 3."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-154", "intents": ["@USE@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "Following Jansen et al. (2014) , we train them using the skip-gram model (Mikolov et al., 2013) We use the L6 Yahoo dataset 8 to train the skip-gram model for the YA dataset and the Ask Ubuntu September 2015 data dump for the AU dataset."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-226", "intents": ["@USE@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "As external features, we evaluate the discourse features that were found useful for this task by Jansen et al. (2014) ."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "Most previous attempts to perform non-factoid answer reranking on CQA data are supervised, feature-based, learning-to-rank approaches (Jansen et al., 2014; Fried et al., 2015; Sharp et al., 2015) ."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "These methods represent the candidate answers as meaningful handcrafted features based on syntactic, semantic and discourse parses (Surdeanu et al., 2011; Jansen et al., 2014) , web correlation (Surdeanu et al., 2011) , and translation probabilities (Fried et al., 2015; Surdeanu et al., 2011) ."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "Fried et al. (2015) improve on the lexical semantic models of Jansen et al. (2014) by exploiting indirect associations between words using higher-order models."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-108", "intents": ["@BACK@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "Further details can be found in (Jansen et al., 2014) ."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-37", "intents": ["@EXT@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "The main contributions of this paper are as follows: 1) we propose a novel neural approach for non-factoid answer reranking that achieves state-4 http://askubuntu.com of-the-art performance on a public dataset of Yahoo! Answers; 2) we combine this approach with an approach based on discourse features that was introduced by Jansen et al. (2014) , with the hybrid approach outperforming the neural approach and the previous state-of-the-art; 3) we introduce a new dataset of Ask Ubuntu questions and answers."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-167", "intents": ["@DIF@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "They also perform better than the approach of Jansen et al. (2014) who used SVMrank with a linear kernel."}
{"sent_id": "ef742defff1c2bdf145f72796cf3af-C001-180", "intents": ["@DIF@"], "paper_id": "ABC_ef742defff1c2bdf145f72796cf3af_1", "text": "On the YA dataset, the results are better than Jansen et al. (2014) and very similar to Bogdanova and Foster (2016) ."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-20", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "In contrast, using multi-step relation paths (e.g., husband(barack, michelle) ∧ mother(michelle, sasha) to train KB embeddings has been proposed very recently (Guu et al., 2015; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015) ."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-75", "intents": ["@BACK@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "The natural composition function of a BILINEAR model is matrix multiplication (Guu et al., 2015) ."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-202", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "The most relevant prior approach is Guu et al. (2015) ."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-72", "intents": ["@USE@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "The two approaches we consider here are: using relation paths to generate new auxiliary triples for training (Guu et al., 2015) and using relation paths as features for scoring (Lin et al., 2015) ."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-153", "intents": ["@USE@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "This can be done in time O T where Triples is the same quantity used in the analysis of Guu et al. (2015) ."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-154", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "The memory requirements of this method are the same as these of (Guu et al., 2015) , up to a constant to store random-walk probabilities for paths."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-162", "intents": ["@USE@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "We should note that whether this method or the one of Guu et al. (2015) will be faster in training depends on whether the average number of paths per node pair multiplied by E kb is bigger or smaller than the total number of triples T ."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-174", "intents": ["@USE@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "Given the values of the quantities from our knowledge graph and d = 50, η = 50, and maximum path length of 5, the estimated memory for (Guu et al., 2015) and PRUNED-PATHS is 4.0 × 10 18 and for ALL-PATHS the memory is 1.9×10 9 ."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-175", "intents": ["@USE@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "The time estimates are 2.4×10 21 , 2.6 × 10 25 , and 7.3 × 10 15 for (Guu et al., 2015) , PRUNED-PATHS, and ALL-PATHS, respectively."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-180", "intents": ["@USE@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "Our experiments are designed to study three research questions: (i) What is the impact of using path representations as a source of compositional regularization as in (Guu et al., 2015) versus using them as features for scoring as in PRUNED-PATHS and ALL-PATHS? (ii) What is the impact of using textual mentions for KB completion in different models?"}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-189", "intents": ["@USE@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "As a second dataset, we used a WordNet KB with the same train, dev, and test splits as Guu et al. (2015) ."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-231", "intents": ["@USE@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "Model MAP HITS@10 BILINEAR-DIAG (Guu et al., 2015) N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. (2015) N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 Table 2 : KB completion results on the WordNet test set: comparison of our compositional learning approach (ALL-PATHS) with baseline systems."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-234", "intents": ["@USE@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "The MAP results were not reported in Guu et al. (2015) ; hence the NA value for MAP in row one."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-235", "intents": ["@USE@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "12 On this dataset, our implementation of the baseline model does not have substantially different results than Guu et al. (2015) and we use their reported results for the baseline and compositionally trained model."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-242", "intents": ["@USE@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "This performance degradation could be avoided with 12 We ran the trained model distributed by Guu et al. (2015) and obtained a much lower Hits@10 value of 6.4 and MAP of of 3.5."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-163", "intents": ["@DIF@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "Unlike the method of Guu et al. (2015) , the evaluation-time memory requirements of this approach are the same as its training memory requirements, or they could be reduced slightly to match the evaluation-time memory requirements of ALL-PATHS, if these are lower as determined by the specific problem instance."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-227", "intents": ["@DIF@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "The implementation of Guu et al. (2015) with default parameters performed significantly worse than our re-implementation."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-208", "intents": ["@EXT@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "For our implementation of (Guu et al., 2015) , we run 5 random walks of each length starting from each node and we found that adding a weight β to the multi-step path triples improves the results."}
{"sent_id": "10b9ec42ac06344cd575a66161ad91-C001-236", "intents": ["@SIM@"], "paper_id": "ABC_10b9ec42ac06344cd575a66161ad91_1", "text": "Compositional training improved performance in Hits@10 from 12.9 to 14.4 in Guu et al. (2015) , and we find that using PRUNED-PATHS as features gives similar, but a bit higher performance gains."}
{"sent_id": "b645eee5044c17502bcdbc95934a01-C001-13", "intents": ["@MOT@"], "paper_id": "ABC_b645eee5044c17502bcdbc95934a01_1", "text": "of effectively making use of multiple treebanks under heterogeneous annotations for improving output accuracies (Jiang et al., 2015; Johansson, 2013; Li et al., 2015) ."}
{"sent_id": "b645eee5044c17502bcdbc95934a01-C001-18", "intents": ["@USE@"], "paper_id": "ABC_b645eee5044c17502bcdbc95934a01_1", "text": "The second approach is based on multi-view learning (Johansson, 2013; Li et al., 2015) ."}
{"sent_id": "b645eee5044c17502bcdbc95934a01-C001-26", "intents": ["@USE@"], "paper_id": "ABC_b645eee5044c17502bcdbc95934a01_1", "text": "We follow Li et al. (2015) , taking POS-tagging for case study, using the methods of Jiang et al. (2009) and Li et al. (2015) as the discrete stacking and multi-view training baselines, respectively, and building neural network counterparts to their models for empirical comparison."}
{"sent_id": "b645eee5044c17502bcdbc95934a01-C001-90", "intents": ["@USE@"], "paper_id": "ABC_b645eee5044c17502bcdbc95934a01_1", "text": "As shown in Figure 1(b) , multi-view learning (Li et al., 2015) utilizes corpus A and corpus B simultaneously for training."}
{"sent_id": "b645eee5044c17502bcdbc95934a01-C001-121", "intents": ["@USE@"], "paper_id": "ABC_b645eee5044c17502bcdbc95934a01_1", "text": "For neural multi-view model, we follow Li et al. (2015) and take a the corpus-weighting strategy to sample a number of training instances from both corpora for each training iteration, as shown in Algorithm 1."}
{"sent_id": "b645eee5044c17502bcdbc95934a01-C001-127", "intents": ["@USE@"], "paper_id": "ABC_b645eee5044c17502bcdbc95934a01_1", "text": "We adopt the Penn Chinese Treebank version 5.0 (CTB5) (Xue et al., 2005) as our main corpus, with the standard data split following previous work (Zhang and Clark, 2008; Li et al., 2015) ."}
{"sent_id": "b645eee5044c17502bcdbc95934a01-C001-152", "intents": ["@USE@"], "paper_id": "ABC_b645eee5044c17502bcdbc95934a01_1", "text": "As a result, we choose a dropout rate of 20% for the remaining experiments, which strikes the balance between over-System Accuracy CRF Baseline (Li et al., 2015) 94.10 CRF Stacking (Li et al., 2015) 94.81 CRF Multi-view (Li et al., 2015) 95 Table 2 : Accuracies on CTB-test."}
{"sent_id": "b645eee5044c17502bcdbc95934a01-C001-163", "intents": ["@USE@"], "paper_id": "ABC_b645eee5044c17502bcdbc95934a01_1", "text": "We lists the results of stacking method of Jiang et al. (2009) re-implemented by Li et al. (2015) , and CRF multi-view method reported by Li et al. (2015) ."}
{"sent_id": "b645eee5044c17502bcdbc95934a01-C001-192", "intents": ["@USE@"], "paper_id": "ABC_b645eee5044c17502bcdbc95934a01_1", "text": "We compare the efficiencies of neural and discrete multi-view training by running our models and the model of Li et al. (2015) 4 with default configurations on the CTB5 training data."}
{"sent_id": "b645eee5044c17502bcdbc95934a01-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_b645eee5044c17502bcdbc95934a01_1", "text": "First, it is free from the necessity of manual cross-labelset interactive feature engineering, which is far from trivial for representing annotation correspondence (Li et al., 2015) ."}
{"sent_id": "b645eee5044c17502bcdbc95934a01-C001-182", "intents": ["@DIF@"], "paper_id": "ABC_b645eee5044c17502bcdbc95934a01_1", "text": "With respect of the multiview training method, the NN model improves over the NN baseline from 94.24 to 95.40, by a margin of +1.16, which is higher than that of 0.90 brought by discrete method of Li et al. (2015) over its baseline, from 94.10 to 95.00."}
{"sent_id": "b645eee5044c17502bcdbc95934a01-C001-184", "intents": ["@SIM@"], "paper_id": "ABC_b645eee5044c17502bcdbc95934a01_1", "text": "This is consistent with the observation of Li et al. (2015) , who showed that discrete label coupling training gives slightly better improvement compared with discrete stacking."}
{"sent_id": "b645eee5044c17502bcdbc95934a01-C001-193", "intents": ["@EXT@"], "paper_id": "ABC_b645eee5044c17502bcdbc95934a01_1", "text": "The CRF baseline is adapted from Li et al. (2015) ."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "One method for addressing this difficulty is the reordering-as-preprocessing approach, exemplified by Collins et al. (2005) and Xia and McCord (2004) , where PSMT is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "Although this leads to improved performance overall, Collins et al. (2005) show that the reordering-as-preprocessing system does not consistently provide better translations than the PSMT baseline on a sentence-by-sentence basis."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "Working with German-to-English translation, Collins et al. (2005) parse input sentences with a constituent-structure parser and apply six hand-crafted rules to reorder the German text toward English word order."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "Despite the success of the reordering-aspreprocessing approach overall, Collins et al. (2005) found that in a human evaluation on 100 sentences, there were still several cases in which the baseline system translation was preferred over that produced with the reordering."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-148", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "It may also be that the inconsistency of improvement noted by Collins et al. (2005) is the cause; sometimes the reordering produces better results and sometimes the baseline, with the effect just by chance favouring the baseline here."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-22", "intents": ["@USE@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "We reimplement the Collins et al. (2005) reordering preprocessing step and conduct some preliminary experiments in German-toEnglish translation ( §4)."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-74", "intents": ["@USE@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "For reordering, we use the Berkeley parser (Petrov et al., 2006 ) and the rules given by Collins et al. (2005) , but any reordering preprocessing step could equally be used."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-120", "intents": ["@USE@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "5 For the reordering preprocessing step we reimplement the Collins et al. (2005) rules and use this to recreate the Collins et al. (2005) reordering-aspreprocessing system as our second baseline."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-124", "intents": ["@USE@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "We compare four systems on German-toEnglish translation: the Moses baseline (MOSES), the Collins et al. (2005) baseline (REORDER), the lattice system with just the reordering indicator feature (LATTICE), and the lattice system with all 3 It is possible that in practice the imbalance in number of non-zero features between the two paths could cause the system some difficulty in assigning the weights for each feature."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-146", "intents": ["@USE@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "8 Interestingly, our reimplementation of the Collins et al. (2005) baseline does not outperform the plain PSMT baseline."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-183", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "While our reordering step is a reimplementation of the Collins et al. (2005) system, contrary to their findings we do not see an improvement using the reordering step alone."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-23", "intents": ["@DIF@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "Our results ( §5) do not replicate the finding of Collins et al. (2005) that the preprocessing step produces better translation results overall."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-122", "intents": ["@DIF@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "Since the German parsing model provided on the parser website does not include the function labels needed by the Collins et al. (2005) rules, we trained a new parsing model on the Tiger corpus (version 1)."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-144", "intents": ["@DIF@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "We note that these numbers are lower than those reported by Collins et al. (2005) ."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-154", "intents": ["@DIF@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "The BLEU score for the oracle is higher than that of both baselines; from this and the distribution of the oracle's choices, we conclude that the difference between our findings and those of Collins et al. (2005) is at least partly due to the inconsistency that they identified."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-37", "intents": ["@EXT@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "Our work builds on the reordering-aspreprocessing approach of Collins et al. (2005) ."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-173", "intents": ["@FUT@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "For future systems, we would like to replace the Collins et al. (2005) reordering rules with a set of automatically-extracted reordering rules (as in Xia and McCord (2004) ) so that we may more easily explore the usefulness of our system and confidence features in new language pairs with a variety of reordering requirements."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-176", "intents": ["@FUT@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "In addition, we wish to explore more fully our negative result with the reimplementation of the Collins et al. (2005) system, to investigate the effect of balancing features in the lattice, and to examine the variability of the BLEU scores for each system."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-20", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "Previous approaches (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) bypassed this problem by heavily under-sampling the \"negative\" class."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011) ."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-100", "intents": ["@BACK@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "More details can be found in Surdeanu et al. (2012) ."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-128", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "Mintz++ is a strong baseline (Surdeanu et al., 2012) and an improved version of Mintz et al. (2009) ."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-22", "intents": ["@EXT@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "We proposed an extension to Surdeanu et al. (2012) that can train on this dataset."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-58", "intents": ["@EXT@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "The input to the model is a list of n bags with a vector of binary labels, either Positive (P), or Unlabled (U) for each relation r. Our model can be viewed as a semi-supervised 6 framework that extends a state-of-the-art Multi-Instance Multi-Label (MIML) model (Surdeanu et al., 2012) ."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-46", "intents": ["@USE@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "We randomly picked 200 unlabeled bags 5 from each of the two datasets (Riedel et al., 2010; Surdeanu et al., 2012 ) generated by DS, and we manually annotate all relation mentions in these bags."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-53", "intents": ["@USE@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "Our goal is to model the bag-level label noise, caused by the incomplete KB problem, in addition Table 2 : False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset (Surdeanu et al., 2012) ."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-56", "intents": ["@USE@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "to modeling the instance-level noise using a 3-layer MIL or MIML model (e.g., Surdeanu et al. (2012) )."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-77", "intents": ["@USE@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "We use the set of features in Surdeanu et al. (2012) ."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-99", "intents": ["@USE@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "which can be solved with an approximate solution in Surdeanu et al. (2012) (step 9-11): update z i independently with:"}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-109", "intents": ["@USE@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "We implement our model on top of the MIML (Surdeanu et al., 2012) code base."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-110", "intents": ["@USE@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "8 We use the same mention-level and aggregate-level feature sets as Surdeanu et al. (2012) ."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-116", "intents": ["@USE@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "Data set: We use the KBP (Ji et al., 2011) dataset 9 prepared and publicly released by Surdeanu et al. (2012) for our experiment since it is 1) large and realistic, 2) publicly available, 3) most importantly, it is the only dataset that has associated human-labeled ground truth."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-122", "intents": ["@USE@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "For a fair comparison, we follow Surdeanu et al. (2012) and begin by downsampling the \"negative\" class to 5%."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-124", "intents": ["@USE@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "Evaluation: We compare our algorithm (MIMLsemi) to three algorithms: 1) MIML (Surdeanu et al., 2012) , the Multiple-Instance Multiple Label algorithm which labels the bags directly with the KB (y = ℓ)."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-126", "intents": ["@USE@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "It also imposes y = ℓ. 3) Mintz++ (Surdeanu et al., 2012) , a variant of the single-instance learning algorithm (section 3)."}
{"sent_id": "a672da8cba61074fe4b8ba1a452a47-C001-74", "intents": ["@SIM@"], "paper_id": "ABC_a672da8cba61074fe4b8ba1a452a47_1", "text": "Similar to Surdeanu et al. (2012) , we also define the following parameters and conditional probabilities (details are in Surdeanu et al. (2012) ):"}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-2", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "Previous work on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b) use syntactic preposition patterns to calculate word relatedness."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "Most previous empirical research on bridging (Poesio and Vieira, 1998; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013b) focus on bridging anaphora resolution, a subtask of bridging resolution that aims to choose the antecedents for bridging anaphors."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "For this substask, most previous work (Poesio et al., 2004; Lassalle and Denis, 2011; Hou et al., 2013b) calculate semantic relatedness between an anaphor and its antecedent based on word co-occurrence counts using certain syntactic patterns."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "Pre-vious work on bridging anaphora resolution (Poesio et al., 2004; Lassalle and Denis, 2011; Hou et al., 2013b ) explored word co-occurrence counts in certain syntactic preposition patterns to calculate word relatedness."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "Based on this corpus, Hou et al. (2013b) proposed a joint inference framework for bridging anaphora resolution using Markov logic networks (Domingos and Lowd, 2009 framework resolves all bridging anaphors in one document together by modeling that semantically related anaphors are likely to share the same antecedent."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-139", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "In Hou et al. (2013b) , features are extracted by using entity information."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-142", "intents": ["@BACK@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "In Hou et al. (2013b) , salient entities on each text are measured through the lengths of the coreference chains based on the gold coreference annotation."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-187", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "For bridging anaphora resolution, Hou (2018) integrates a much simpler deterministic approach by combining an NP head with its noun modifiers (appearing before the head) based on embeddings PP into the MLN II system (Hou et al., 2013b) ."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-8", "intents": ["@DIF@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "We show that this simple approach achieves the competitive results compared to the best system in Hou et al. (2013b) which explores Markov Logic Networks to model the problem."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-32", "intents": ["@DIF@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "We show that this simple, efficient method achieves the competitive results on ISNotes for the task of bridging anaphora resolution compared to the best system in Hou et al. (2013b) which explores Markov Logic Networks to model the problem."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-58", "intents": ["@DIF@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "Our approach is deterministic and simple, but achieves the competitive results compared to the advanced machine learning-based approach (Hou et al., 2013b) ."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-141", "intents": ["@DIF@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "Note that our simple antecedent candidate selection strategy (described in Section 4) allows us to include 76% of NP antecedents compared to 77% in pairwise model III from Hou et al. (2013b) where they add top 10% salient entities as additional antecedent candidates."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-225", "intents": ["@DIF@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "We show that this simple and efficient method achieves the competitive result on bridging anaphora resolution compared to the advanced machine learning-based approach in Hou et al. (2013b) which is heavily dependent on a lot of carefully designed complex features."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-9", "intents": ["@EXT@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "Additionally, we further improve the results for bridging anaphora resolution reported in Hou (2018) by combining our simple deterministic approach with Hou et al. (2013b)'s best system MLN II."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-59", "intents": ["@EXT@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "We also improve the result reported in Hou (2018) on the same corpus by combining our deterministic approach with the best system from Hou et al. (2013b) ."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-136", "intents": ["@USE@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "Following Hou et al. (2013b) 's experimental setup, we resolve bridging anaphors to entity antecedents."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-143", "intents": ["@USE@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "Following Hou et al. (2013b) , we measure accuracy on the number of bridging anaphors, instead of on all links between bridging anaphors and their antecedent instantiations."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-161", "intents": ["@USE@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "Table 6 lists the best results of the two models for bridging anaphora resolution from Hou et al. (2013b) ."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-168", "intents": ["@USE@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "acc models from Hou et al. (2013b) Table 6 : Results of using NP head plus modifications in different word representations for bridging anaphora resolution compared to the best results of two models from Hou et al. (2013b) ."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-173", "intents": ["@SIM@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "Finally, our method based on embeddings bridging achieves an accuracy of 39.52%, which is competitive to the best result (41.32%) reported in Hou et al. (2013b) ."}
{"sent_id": "087a2a35ad2428d6b17c6906447349-C001-227", "intents": ["@FUT@"], "paper_id": "ABC_087a2a35ad2428d6b17c6906447349_1", "text": "For the task of bridging anaphora resolution, Hou et al. (2013b) pointed out that considering only head noun knowledge is not enough and future work needs to explore wider context to resolve context-specific bridging relations."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-6", "intents": ["@USE@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "It performs two phases: first, machine understandable logical forms (programs) are generated from natural language questions following the work of [Pasupat and Liang, 2015] ."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-23", "intents": ["@USE@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "Each natural language question is translated into a set of computer understandable candidate representations, called logical forms, based on the work of [Pasupat and Liang, 2015] ."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-28", "intents": ["@USE@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "We empirically confirm our approach on a series of experiments on WikiTableQuestions [Pasupat and Liang, 2015] , a real-world dataset containing 22,033 pairs of questions and their corresponding manually retrieved answers with about 2,108 randomly selected Wikipedia tables."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-38", "intents": ["@USE@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "To tackle this problem, our method follows recent work of [Reddy et al., 2014; Kwiatkowski et al., 2013] that relies solely on weak-supervision through question-answertable triples."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-41", "intents": ["@USE@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "In this work, we generate logical form candidates in the same way as [Pasupat and Liang, 2015] ."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-62", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "Systems vary from operating on structured knowledge bases [Bordes et al., 2014b] ; [Bordes et al., 2014a ] to semi-structured tables [Pasupat and Liang, 2015] , [Neelakantan et al., 2016] , [Jauhar et al., 2016] and completely unstructured text, which is related to information extraction [Clark et al., 2016] ."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-68", "intents": ["@USE@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "For every question q : i) a set of candidate logical forms {z i } i∈Iq is generated using the method of [Pasupat and Liang, 2015] ; ii) each such candidate program z i is paraphrased in a textual representation t i that offers accuracy gain, interpretability and comprehensibility ; iii) all textual forms t i are scored against the input question q using a neural network model; iv) the logical form z * i corresponding to the highest ranked t * i is selected as the machine-understandable translation of question q; v) z * i is executed on the input table and its answer is returned to the user."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-69", "intents": ["@USE@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "Our contributions are the novel models that perform the steps ii) and iii), while for i), iv) and v) we rely on the work of [Pasupat and Liang, 2015] (henceforth: PL2015)."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-79", "intents": ["@USE@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "Each candidate logical form is represented in Lambda DCS form [Liang, 2013] and can be transformed into a SPARQL query, whose execution against the KG yields an answer."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-165", "intents": ["@USE@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "Table 1 shows the performance of our models compared to Neural Programmer [Neelakantan et al., 2016] and PL2015 [Pasupat and Liang, 2015] baselines."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-9", "intents": ["@DIF@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "Our best single model achieves 34.8% accuracy on the WikiTableQuestions dataset, while the best ensemble of our models pushes the state-of-the-art score on this task to 38.7%, thus slightly surpassing both the engineered feature scoring baseline, as well as the Neural Programmer model of [Neelakantan et al., 2016] ."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "One example are systems able to answer complex questions about a specific topic (e.g. [Wang et al., 2015] )."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "In the context of QA for semi-structured tables and dealing with multi-compositional queries, [Pasupat and Liang, 2015] generate and rank candidate logical forms with a log-linear model trained on question-answer pairs."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "Paraphrases have been successfully used to facilitate semantic parsers [Wang et al., 2015; ."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "[ Neelakantan et al., 2016] also focus on compositional questions, but instead of generating and ranking multiple logical forms, they propose a model that directly constructs a logical form from an embedding of the question."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "Recently, [Yin et al., 2015] propose Neural Enquirer, a fully neural, end-to-end differentiable network that executes queries across multiple tables."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "Representation learning using deep learning architectures has been widely explored in other domains, e.g. in the context of sentiment classification, [Kim, 2014; Socher et al., 2013] , or for image-hashtag prediction [Denton et al., 2015] ."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "Details about Lambda DCS language can be found in [Liang, 2013] return t t is the textual paraphrase of the Lambda DCS logical form 18: end procedure information from the KG facilitates the process of parsing a question into a set of candidate logical forms."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-145", "intents": ["@BACK@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "PL2015 report an oracle score of 76.7%, but a manual annotation by [Pasupat and Liang, 2015] reveals that PL2015 can answer only 53.5% of the questions correctly."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-2", "intents": ["@USE@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "The paper presents an application of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for domain adaptation of a stochastic attribute-value grammar (SAVG)."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-38", "intents": ["@USE@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "We examine the effectiveness of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for this task, a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-84", "intents": ["@USE@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "Applying the projection W T x (where x is a training instance) would give us m new features, however, for \"both computational and statistical reasons\" (Blitzer et al., 2006; Ando and Zhang, 2005 ) a low-dimensional approximation of the original feature space is computed by applying Singular Value Decomposition (SVD) on W (step 4)."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-94", "intents": ["@USE@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "So far, pivot features on the word level were used (Blitzer et al., 2006; Blitzer, 2008) , e.g. \"Does the bigram not buy occur in this document?\" (Blitzer, 2008) ."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-110", "intents": ["@USE@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "Predictive features As pointed out by Blitzer et al. (2006) , each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself)."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-113", "intents": ["@USE@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "Following Blitzer et al. (2006) (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-162", "intents": ["@USE@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "In our empirical setup, we followed Blitzer et al. (2006) and tried to balance the size of source and target data."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-3", "intents": ["@BACK@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis (Blitzer et al., 2006; )."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daumé III and Marcu, 2006; Daumé III, 2007; Blitzer et al., 2006; McClosky et al., 2006; ."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-23", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "studies on semi-supervised domain adaptation (McClosky et al., 2006; Blitzer et al., 2006; ."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "Similarly, Structural Correspondence Learning (Blitzer et al., 2006; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-65", "intents": ["@BACK@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "(Structural Correspondence Learning) (Blitzer et al., 2006; Blitzer, 2008 ) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available) (Blitzer et al., 2006) ."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-120", "intents": ["@BACK@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006; Blitzer et al., 2006; Blitzer, 2008) besides the ones discussed above."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-126", "intents": ["@BACK@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "When training the supervised model on the augmented feature space x, θx , Blitzer et al. (2006) only regularize the weight vector of the original features, but not the one for the new low-dimensional features."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-130", "intents": ["@BACK@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "Due to the positive results in Ando (2006), Blitzer et al. (2006) include this in their standard setting of SCL and report results using block SVDs only."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-238", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "While SCL has been successfully applied to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; , its effectiveness for parsing was rather unexplored."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-192", "intents": ["@SIM@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "We can confirm that changing the dimensionality parameter h has rather little effect (Table 4) , which is in line with previous findings (Ando and Zhang, 2005; Blitzer et al., 2006) ."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-204", "intents": ["@DIF@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "While Blitzer et al. (2006) found it necessary to normalize (and scale) the projection features, we did not observe any improvement by normalizing them (actually, it slightly degraded performance in our case)."}
{"sent_id": "9bd974b0fa4732f361f1c397e6b4c7-C001-239", "intents": ["@DIF@"], "paper_id": "ABC_9bd974b0fa4732f361f1c397e6b4c7_1", "text": "The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in Blitzer et al. (2006) ."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-16", "intents": ["@USE@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "The starting point of our study is the goal oriented dialogue task of Kottur et al. (2017) , summarized in Fig. 2 ."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-42", "intents": ["@USE@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "The starting point for our investigation is the recent work of Kottur et al. (2017) , which investigates compositionality using a cooperative reference game between two agents."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-80", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "In Kottur et al. (2017) it was used to generate a somewhat compositional language given Algorithm 1: Training with Replacement and Multiple Agents"}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-24", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "Existing work has investigated conditions under which compositional languages emerge between neural agents in simple environments (Mordatch & Abbeel, 2018; Kottur et al., 2017) , but it only investigates how language changes within a generation."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "This desire for structure motivates the previously mentioned work on compositional language emergence in neural agents (Kottur et al., 2017; Mordatch & Abbeel, 2018; Choi et al., 2018) ."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-95", "intents": ["@BACK@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "In Kottur et al. (2017) there is only one pair of agents (N Q = N A = 1) so we cannot replace both agents at the same round because all existing language would be lost."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-139", "intents": ["@BACK@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "Previous work also measures generalization to held out compositions of attributes to measure compositionality (Kottur et al., 2017; Kirby et al., 2015) ."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-252", "intents": ["@BACK@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "Even though cultural transmission may aid the emergence of compositionality, recent results in evolutionary linguistics (Raviv et al., 2018) and deep learning (Kottur et al., 2017; Mordatch & Abbeel, 2018) show cultural transmission may not be necessary for compositionality to emerge."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-258", "intents": ["@BACK@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "Most relevant is similar work which focuses on conditions under which compositional language emerges as deep agents learn to cooperate (Mordatch & Abbeel, 2018; Kottur et al., 2017) ."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-259", "intents": ["@BACK@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "Both Mordatch & Abbeel (2018) and Kottur et al. (2017) find that limiting the vocabulary size so that there aren't too many more words than there are objects to refer to encourages compositionality, which follows earlier results in evolutionary linguistics (Nowak et al., 2000) ."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-74", "intents": ["@SIM@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "As in Kottur et al. (2017) , we implement Q, A, and U as neural networks."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-132", "intents": ["@SIM@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "As in Kottur et al. (2017) , our world contains objects with 3 attributes (shape, size, color) such that each attribute has 4 possible values."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-145", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "Our A-bots and Q-bots have the same architecture and hyperparameter variations as in Kottur et al. (2017) , but with our cultural transmission training procedure and some other differences identified below."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-146", "intents": ["@SIM@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "Like Kottur et al. (2017) , our hyperparameter variations consider the number of vocab words Q-bot (V Q ) and A-bot (V A ) may utter and whether or not A-bot has memory between dialog rounds."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-189", "intents": ["@SIM@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "Test set accuracies (with standard deviations) are reported against our new harder dataset using models similar to those in (Kottur et al., 2017) ."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-195", "intents": ["@SIM@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "This agrees with factors noted elsewhere (Kottur et al., 2017; Mordatch & Abbeel, 2018; Nowak et al., 2000) ."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-140", "intents": ["@DIF@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "Unlike Kottur et al. (2017) , we use a slightly harder version of their dataset which aligns better with the goal of compositional language."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-155", "intents": ["@DIF@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "7 This differs from Kottur et al. (2017) , which stopped once train accuracy reached 100%."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-166", "intents": ["@DIF@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "Thus we compare to the Replace All baseline, which has the greatest chance of seeing a lucky initialization and thereby ensures that gains over the No Replacement baseline 6 This is slightly different from Small Vocab in (Kottur et al., 2017) ."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-53", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "BLE can be used to address the accuracy problem of SMT, which estimates comparable features for the translation pairs in the translation model (Klementiev et al., 2012) ."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-150", "intents": ["@BACK@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "The intuition of temporal similarity is that news stories across languages tend to discuss the same world events on the same day, and the occurrences of a translated phrase pair over time tend to spike on the same dates (Klementiev and Roth, 2006; Klementiev et al., 2012) ."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-142", "intents": ["@USE@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "In this paper, we estimate topical feature in a scalable way following (Klementiev et al., 2012) ."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-151", "intents": ["@USE@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "We estimate temporal feature following (Klementiev and Roth, 2006; Klementiev et al., 2012) ."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-193", "intents": ["@USE@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "In our experiments, we compared our proposed method with (Klementiev et al., 2012) ."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-194", "intents": ["@USE@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "We estimated comparable features from comparable corpora using the method of (Klementiev et al., 2012) and our proposed method respectively."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-205", "intents": ["@USE@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "For decoding, we used the state-of-theart PBSMT toolkit Moses (Koehn et al., 2007) with default options, except for the phrase length limit (7→3) following (Klementiev et al., 2012) ."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-214", "intents": ["@USE@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "We treated the two sides of the parallel corpus as independent monolingual corpora, following (Haghighi et al., 2008; Klementiev et al., 2012 We used an open-source Python script 13 to extract and clean the text from the dumps."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-231", "intents": ["@USE@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "\"Klementiev+\" denotes the system that appends the comparable features estimated following (Klementiev et al., 2012) to the phrase table. \"Proposed\" denotes the system that uses the comparable features estimated by our proposed method."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-239", "intents": ["@USE@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "We also investigated the comparable features estimated by the method of (Klementiev et al., 2012) and our proposed method."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-242", "intents": ["@USE@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "Table 8 shows the comparable feature scores estimated by the method of (Klementiev et al., 2012) (above the bold line) and our proposed method (below the bold line)."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-243", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "We can see that the method of (Klementiev et al., 2012) suffers from the data sparseness problem."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-235", "intents": ["@DIF@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "The reason for this is that the comparable features estimated by (Klementiev et al., 2012) are inaccurate. \"Proposed\" performs significantly better than both \"Baseline\" and \"Klementiev+\"."}
{"sent_id": "79e36756354f61b087655bc6afede8-C001-240", "intents": ["@DIF@"], "paper_id": "ABC_79e36756354f61b087655bc6afede8_1", "text": "Based on our investigation, most comparable features estimated by our proposed method are more accurate than the ones estimated by the method of (Klementiev et al., 2012 Table 8 : Examples of comparable feature scores estimated by the method of (Klementiev et al., 2012) (above the bold line) and our proposed method (below the bold line) for the phrase pairs shown in Table 1 (\"con\", \"top\" and \"tem\" denote phrasal contextual, topical and temporal features respectively, \"con lex\", \"top lex\" and \"tem lex\" denote lexical contextual, topical and temporal features respectively)."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-2", "intents": ["@USE@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "We present the first application of the head-driven statistical parsing model of Collins (1999) as a simultaneous language model and parser for largevocabulary speech recognition."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-37", "intents": ["@USE@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "Our work is different from Roark (2001) in that we use a bottom-up parsing algorithm with dynamic programming based on the parsing model II of Collins (1999) ."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-46", "intents": ["@USE@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "In this section we outline the adaptation of the Collins (1999) parsing model to word lattices."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-50", "intents": ["@USE@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "The parameterization of model II of Collins (1999) is used in our word lattice parser."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-57", "intents": ["@USE@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "Edges created by the bottom-up parsing are assigned a score which is the product of the inside and outside probabilities of the Collins (1999) model."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-73", "intents": ["@USE@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "The parameter estimation techniques (smoothing and back-off) of Collins (1999) are reimplemented."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-150", "intents": ["@USE@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "sections (Collins, 1999) ."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "Collins (1999) presents three lexicalized models which consider long-distance dependencies within a sentence."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "These models use much less conditioning information than the parsing models of Collins (1999) , and do not provide Penn Treebank format parse trees as output."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "Collins (1999) falls back to the POS tagging of Ratnaparkhi (1996) for words seen fewer than 5 times in the training corpus."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-79", "intents": ["@BACK@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "In practice, these heuristics have a negative effect on parse accuracy, but the amount of pruning can be tuned to balance relative time and space savings against precision and recall degradation (Collins, 1999) ."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-80", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "Collins (1999) uses a fixed size beam (10 000)."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-21", "intents": ["@EXT@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "We find that the parsing model of Collins (1999) can be successfully adapted as a language model for speech recognition."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-75", "intents": ["@EXT@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "The main technique we employ is a variation of the beam search of Collins (1999) to restrict the chart size by excluding low probability edges."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-129", "intents": ["@EXT@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "The WER scores for this, the first application of the Collins (1999) model to parsing word lattices, are comparable to other recent work in syntactic language modelling, and better than a simple trigram model trained on the same data."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-228", "intents": ["@EXT@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "In this work we present an adaptation of the parsing model of Collins (1999) for application to ASR."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-52", "intents": ["@DIF@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "One notable difference between the word lattice parser and the original implementation of Collins (1999) is the handling of part-of-speech (POS) tagging of unknown words (words seen fewer than 5 times in training)."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-128", "intents": ["@DIF@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "Results show scores for parsing strings which are lower than the original implementation of Collins (1999) ."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-136", "intents": ["@DIF@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "The scores for our experiments are lower than the scores of the original implementation of model II (Collins, 1999) ."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-138", "intents": ["@DIF@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "Tag accuracy for our model was 93.2%, whereas for the original implementation of Collins (1999) , model II achieved tag accuracy of 96.75%."}
{"sent_id": "d636df7cc0eb06323ef159528caf49-C001-200", "intents": ["@DIF@"], "paper_id": "ABC_d636df7cc0eb06323ef159528caf49_1", "text": "By contrast, (Collins, 1999) calculates parameter values by looking up event counts at run-time."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "On the other hand, recent studies for abstractive summarization (Chen and Bansal, 2018; Hsu et al., 2018; Gehrmann et al., 2018) have attempted to exploit extractive models."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "Among these, a notable one is Chen and Bansal (2018) , in which a sophisticated model called Reinforce-Selected Sentence Rewriting is proposed."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-254", "intents": ["@BACK@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "Some notable examples include the use of inconsistency loss (Hsu et al., 2018) , key phrase extraction (Li et al., 2018; Gehrmann et al., 2018) , and sentence extraction with rewriting (Chen and Bansal, 2018) ."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-21", "intents": ["@EXT@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "In this paper, we improve the model of Chen and Bansal (2018) , addressing two primary issues."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-38", "intents": ["@EXT@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "In this paper, we focus on single-document multisentence summarization and propose a neural abstractive model based on the Sentence Rewriting framework (Chen and Bansal, 2018; Xu and Dur-rett, 2019) which consists of two parts: a neural network for the extractor and another network for the abstractor."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-96", "intents": ["@USE@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "As the decoder structure is almost the same with the previous work, we convey the equations of Chen and Bansal (2018) to avoid confusion, with minor modifications to agree with our notations."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-121", "intents": ["@USE@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "Thus, we pre-train the network using cross entropy (CE) loss like previous work (Bahdanau et al., 2017; Chen and Bansal, 2018) ."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-146", "intents": ["@USE@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "Following Chen and Bansal (2018) , we use the Advantage Actor Critic (Mnih et al., 2016) method to train."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-182", "intents": ["@USE@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "On the CNN/Daily Mail and DUC-2002 dataset, we use standard ROUGE-1, ROUGE-2, and ROUGE- L (Lin, 2004) on full length F 1 with stemming as previous work did (Nallapati et al., 2017; See et al., 2017; Chen and Bansal, 2018) ."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-192", "intents": ["@USE@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "However, as Table 2 shows, ROUGE scores of lead baselines and extractors from previous work in Sentence Rewrite framework (Chen and Bansal, 2018; Xu and Durrett, 2019) are almost tie."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-203", "intents": ["@USE@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "We tried Trigram Blocking (Liu, 2019) for extractor and Reranking (Chen and Bansal, 2018) for abstractor, and we empirically found that the reranking only improves the performance."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-225", "intents": ["@USE@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "We train the same model with different training signals; Sentencelevel reward from Chen and Bansal (2018) and combinatorial reward from ours."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-233", "intents": ["@USE@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "We randomly select 20 samples from the CNN/Daily Mail test set and ask the human testers (3 for each sample) to rank summaries (for relevance and readability) produced by 3 different models: our final model, that of Chen and Bansal (2018) and that of Liu (2019) ."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-105", "intents": ["@SIM@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "Our abstractor is practically identical to the one proposed in Chen and Bansal (2018) ."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-211", "intents": ["@SIM@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "This search method matches with the best reward from Chen and Bansal (2018) ."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-198", "intents": ["@DIF@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "The combined model (BERT-ext + abs) without additional RL training outperforms the Sentence Rewrite model (Chen and Bansal, 2018) without reranking, showing the effectiveness of our extractor network."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-199", "intents": ["@DIF@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "With the proposed RL training procedure (BERT-ext + abs + RL), our model exceeds the best model of Chen and Bansal (2018) ."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-200", "intents": ["@DIF@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "In addition, the result is better than those of all the other abstractive methods exploiting extractive approaches in them (Hsu et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018) ."}
{"sent_id": "2a01240f628d7deb74e6e9fe750378-C001-217", "intents": ["@DIF@"], "paper_id": "ABC_2a01240f628d7deb74e6e9fe750378_1", "text": "Relevance Readability Total Sentence Rewrite (Chen and Bansal, 2018) 56 59 115 BERTSUM (Liu, 2019) 58 60 118 BERT-ext + abs + RL + rerank (ours) 66 61 127 which has the highest summary-level ROUGE-L score, from all the possible combinations of sentences."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "This field is referred to as \"data-to-text\" [8] and has its place in several application domains (such as journalism [22] or medical diagnosis [25] ) or wide-audience applications (such as financial [26] and weather reports [30] , or sport broadcasting [4, 39] )."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "Recent datato-text models [18, 28, 29, 39] mostly rely on an encoder-decoder architecture [2] in which the data-structure is first encoded sequentially into a fixed-size vectorial representation by an encoder."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "This task stems from the neural machine translation (NMT) domain, and early work [1, 15, 39] represent the data records as a single sequence of facts to be entirely translated into natural language."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "Wiseman et al. [39] show the limits of traditional NMT systems on larger structured-data, where NMT systems fail to accurately extract salient elements."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-94", "intents": ["@BACK@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "As outlined in Section 2, most previous work [16, 28, 29, 39, 40 ] make use of flat encoders that do not exploit the data structure."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-144", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "The second category designed by [39] is more qualitative."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-170", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "• Wiseman [39] is a standard encoder-decoder system with copy mechanism."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-233", "intents": ["@BACK@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "Please note that, as in previous work [16, 28, 29, 39] , generated texts still contain a number of incorrect facts, as well hallucinations (in blue): sentences that have no basis in the input data (e.g. \"[...] he's now averaging 22 points [...].\")."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-36", "intents": ["@SIM@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "Our contribution focuses on the encoding of the data-structure, thus the decoder is chosen to be a classical module as used in [28, 39] ."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-97", "intents": ["@SIM@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "In the low-level encoder, the traditional embedding layer is replaced by a record embedding layer as in [18, 28, 39] ."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-39", "intents": ["@USE@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "We report experiments on the RotoWire benchmark [39] which contains around 5K statistical tables of NBA basketball games paired with humanwritten descriptions."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-87", "intents": ["@USE@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "This equation is intractable in practice, we approximate a solution using beam search, as in [18, 17, 28, 29, 39] ."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-89", "intents": ["@USE@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "Because our contribution focuses on the encoding process, we chose the decoding module used in [28, 39] : a two-layers LSTM network with a copy mechanism."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-102", "intents": ["@USE@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "As in previous work [18, 28, 39] , each record embedding r i,j is computed by a linear projection on the concatenation [k i,j ; v i,j ] followed by a non linearity:"}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-134", "intents": ["@USE@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "To evaluate the effectiveness of our model, and demonstrate its flexibility at handling heavy data-structure made of several types of entities, we used the Ro-toWire dataset [39] ."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-152", "intents": ["@USE@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "To do so, we follow the protocol presented in [39] ."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-185", "intents": ["@USE@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "The decoder is the one used in [28, 29, 39] with the same hyper-parameters."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-187", "intents": ["@USE@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "To fit with the small number of record keys in our dataset (39) , their embedding size is fixed to 20."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-200", "intents": ["@USE@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "Results are compared to baselines [28, 29, 39] and variants of our models."}
{"sent_id": "f34768f61dd3d95648ad9a70e83d2c-C001-63", "intents": ["@DIF@"], "paper_id": "ABC_f34768f61dd3d95648ad9a70e83d2c_1", "text": "First, instead of flatly concatenating elements from the data-structure and encoding them as a sequence [18, 28, 39] , we constrain the encoding to the underlying structure of the input data, so that the delimitation between entities remains clear throughout the process."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-4", "intents": ["@USE@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "We also explore the effects of using joke structure, in the form of humour anchors (Yang et al., 2015) , for improving the performance of semantic features and show that, while an intriguing idea, humour anchors contain several pitfalls that can hurt performance."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-16", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "For example, the representation of joke semantics has been fairly basic, typically computing word embedding similarities between all word pairs in a document (Yang et al., 2015) , and bear little resemblance to the way humans actually interpret humour."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-24", "intents": ["@USE@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "Specifically, we experiment with integrating the extraction of humour anchors, the \"meaningful, complete, minimal set of word spans\" (Yang et al., 2015) that allow humour to occur, into the humour classification process itself, the first work to do so."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-115", "intents": ["@USE@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "We evaluate our classifiers across two separate datasets: Pun of the Day (PotD), collected in Yang et al. (2015) , and 16000 One-Liner (OL), collected in Mihalcea and Strapparava (2005) ."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-153", "intents": ["@USE@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "HAs are extracted using the method described in Yang et al. (2015) using the same baseline humour model described in Section 3.2 for anchor candidate evaluation."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-196", "intents": ["@USE@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "We chose our baseline Yang et al. (2015) humour classifier as our anchor candidate scorer for simplicity but their HA extraction algorithm is able to work with any humour recognition model so long as it is robust to word order and capable of generating a humour score (in our case, we used humour probability)."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "As overlap and incongruity are difficult to measure directly, one common approach is instead to use word embeddings, such as Word2Vec (Mikolov et al., 2013) , to calculate the cosine similarities between pairs of vectors representing words in a document (Yang et al., 2015; Shahaf et al., 2015; Kukovačec et al., 2017) ."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-95", "intents": ["@BACK@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "Despite the issues mentioned in Section 2.1, Yang et al. (2015) 's \"incongruity\" feature set, maximum and minimum word embedding similarities between pairs of words in a document, perform fairly well."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-102", "intents": ["@BACK@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "However, Yang et al. (2015) does not use their extracted HAs to improve their humour classification performance."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-110", "intents": ["@BACK@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "However, these models are much more complex than Yang et al. (2015) 's approach, require more training data, and suffer from a lack of interpretability."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-116", "intents": ["@BACK@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "PotD consists of positive examples collected from the Pun of the Day website 2 and negative examples collected from a combination of news sources, question/answer forums, and lists of proverbs (Yang et al., 2015) ."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-128", "intents": ["@BACK@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "Not only is this a common humour recognition feature (Yang et al., 2015; Shahaf et al., 2015; Kukovačec et al., 2017) , but it also acts as a point of comparison for word association strength."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-189", "intents": ["@BACK@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "The wonderfully simple extraction method described in Yang et al. (2015) only makes HAs more intriguing."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-192", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "As described in Section 2.2, Yang et al. (2015) 's HA extraction algorithm requires a fully trained humour model, the accuracy of which undoubtedly affects the quality of the extracted HAs."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-120", "intents": ["@EXT@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "For our baseline we implemented our own version of Yang et al. (2015) 's highest performing classifier."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-127", "intents": ["@SIM@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "Similar to Yang et al. (2015) , we compute the minimum, maximum, and average Word2Vec similarity between ordered word pairs."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-160", "intents": ["@DIF@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "In general, our model performs slightly worse than the Yang et al. (2015) baseline."}
{"sent_id": "1f5326cacca33bfc80a9ddcb4ae313-C001-161", "intents": ["@DIF@"], "paper_id": "ABC_1f5326cacca33bfc80a9ddcb4ae313_1", "text": "One interesting aspect to note is that our model uses only 28 feature dimensions compared to Yang et al. (2015) 's 318."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-28", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "May et al. [21] establish a preliminary study of social bias in BERT, but their analysis relies only on sentence level encodings."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "Zhao et al. [35] and Basta et al. [1] demonstrate gender bias in ELMo [25] word embeddings, whereas May et al. [21] evaluate various models of contextual word representations on a sentential generalization of WEAT."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-92", "intents": ["@BACK@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "May et al. [21] adopt the WEAT tests [5] into Sentence Encoder Association Tests (SEATs) to test biases using sentence encodings."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-99", "intents": ["@BACK@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "May et al. [21] suggest that although they find less bias in sentence encoders than context free word embeddings, the sentence templates may not be as semantically bleached as expected, and that a lack of evidence of bias should not be taken as a lack of bias."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-115", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "We also prefer tests using names (e.g. Alice) as concept words over group terms (e.g. European American), since names were demonstrated to have a significant association more often than group terms [21] For intersectional identities, we are focused primarily on the identity which is the subject of discussion in the work of Crenshaw [9] : being both African American and female."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-116", "intents": ["@BACK@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "Specifically, [21] , which targets the stereotype of black women as loud, angry, and imposing [8] ."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-233", "intents": ["@BACK@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "The Caliskan Tests are detailed in Caliskan et al. [5] , and the double bind tests are detailed in May et al. [21] ."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-31", "intents": ["@EXT@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "We adapt the Sentence Encoder Association Test (SEAT) [21] to evaluate how these techniques displays bias in contextual word representations."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-80", "intents": ["@USE@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "We adopt the methodology of Caliskan et al. [5] and May et al. [21] to test social and intersectional bias using embedding association tests with contextual word representations."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-82", "intents": ["@USE@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "We follow May et al. [21] in describing WEATs and SEATs."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-111", "intents": ["@USE@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "For race and gender, we are interested in attributes of pleasantness (P/U: Pleasant/Unpleasant), work (Career/Family), discipline (Science/Arts) [5] and the Heilman double bind [15, 21] ."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-114", "intents": ["@USE@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "We preserve and report the original WEATs, SEATs and tests introduced by May et al. [21] where possible."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-121", "intents": ["@USE@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "For comparison with previous work [1, 21, 35] , we also report on other word representation models: CBoW-GLoVe [24] , ELMo [25] , BERT bert-base-cased (bbc) and bert-large-cased (blc) versions [10] , and GPT [26] ."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-123", "intents": ["@USE@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "We use PyTorch, as well as the framework and code from May et al. [21] , to conduct the experiments 6 ."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-208", "intents": ["@USE@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "Following May et al. [21] , the sentence encoding of ELMo is a sequence of vectors, one for each token."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-237", "intents": ["@USE@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "Following May et al. [21] , examples of such templates (non-exhaustive) are as below."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-202", "intents": ["@SIM@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "Similar to May et al. [21] , in the continuous bag of words (CBoW) model we encode sentences as the average of word embeddings using 300-dimensional GloVe vectors 7 trained on the Common Crawl corpus [24] ."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-222", "intents": ["@SIM@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "Similar to May et al. [21] and the original word [26] , we use the representation corresponding to the last word in the sequence as the sentence encoding."}
{"sent_id": "41a23b1ae47d83315a047497691117-C001-226", "intents": ["@DIF@"], "paper_id": "ABC_41a23b1ae47d83315a047497691117_1", "text": "Different from May et al. [21] , we use the implementation of GPT from Hugging Face, and not the jiant project 10 ."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "Transformer [34] is based solely on the attention mechanism, and dispensing with recurrent and convolutions entirely."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-26", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "Specifically, Ashish et.al [34] compute the attention function on a set of queries simultaneously, packed together into a matrix Q, while the keys and values are also packed together into matrices K and V , respectively."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "The second challenge is that the attention model after compressing can not be directly integrated into the encoder and decoder framework of Transformer [34, 7] ."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-84", "intents": ["@BACK@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "In practice, Transformer [34] processes query, keys and values as matrices Q, K, and V respectively."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "where d is the number of columns of Q and K. In these work [34, 12, 7] , they all use the multi-head attention, as introduced in [34] ,"}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-88", "intents": ["@BACK@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "In this work [34] , multiple groups of parameters (W Q i , W K i and W V i ) are used, which results in a large number of redundant parameters."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-145", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "Previous work [34] gets the multi-head attention by multiple groups of linear mappings."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-149", "intents": ["@BACK@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "h is the number of heads in [34] , and d is the dimension of factor matrices."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-163", "intents": ["@BACK@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "Different from the architectures of convolutional neural network (CNNs) and recurrent neural networks (RNNs) language modeling, the Transformer [34] and its variants [7, 12, 10] achieve excellent results in language modeling processing."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-165", "intents": ["@BACK@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "Vaswani et al. [34] uses a segment-level recurrence mechanism and a novel positional encoding scheme to resolve this question."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-30", "intents": ["@USE@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "In order to address this challenge, we first prove that the output of the attention function of the self-attention model [34] can be linearly represented by a group of orthonormal base vectors."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-40", "intents": ["@USE@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "After that, it can be integrated into the encoder and decoder framework of Transformer [34, 7] and trained end-to-end."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-59", "intents": ["@USE@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "Then, we describe in Section 2.2 multi-head attention [34] ."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-228", "intents": ["@USE@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "In this task, we have trained the Transformer model [34] on WMT 2016 English-German dataset [30] ."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-232", "intents": ["@USE@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "In this section, we only compared the results with Transformer [34] ."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-237", "intents": ["@USE@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "For the other baseline, we use the basic Transformer architecture [34] ."}
{"sent_id": "41de1ad534ca00b7a99260de7bb0b2-C001-159", "intents": ["@DIF@"], "paper_id": "ABC_41de1ad534ca00b7a99260de7bb0b2_2", "text": "The minimum number of sequential operations in Multi-linear attention for different layers is lower than that of the self-attention in Transformer [34] ."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-12", "intents": ["@SIM@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "Restricting our attention to voicemail transcripts means that our focus and goals are similar to those of Huang et al. (2001) , but the features and techniques we use are very different."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-18", "intents": ["@SIM@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "We shall see that hand-crafted rules achieve very good recall, just as Huang et al. (2001) had observed, and the pruning phase successfully eliminates most undesirable candidates without affecting recall too much."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-66", "intents": ["@SIM@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "Like Huang et al. (2001) , we count a proposed caller phrase as correct if and only if it matches the annotation of the evaluation data perfectly."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-70", "intents": ["@SIM@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "This is corroborated by the fact that we were able to obtain performance much closer to that of the best, finely tuned log-linear model from (Huang et al., 2001 ) by using a generic named entity tagger that was not adapted in any way to the particular task at hand."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-142", "intents": ["@SIM@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "Also, for most phone numbers the labeling is uncontroversial, so we expect the corpora used by Huang et al. (2001) and ourselves to be extremely similar in terms of mark-up of phone numbers."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "Huang et al. (2001) discuss three approaches: hand-crafted rules; grammatical inference of subsequential transducers; and log-linear classifiers with bigram and trigram features used as taggers (Ratnaparkhi, 1996) ."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "While this is less of a problem when evaluating on manual transcriptions, the experience reported in (Huang et al., 2001) suggests that the relatively high error rate of speech recognizers may negatively affect performance of caller name extraction on automatically generated transcripts."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-133", "intents": ["@BACK@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "Rows HZP rules and HZP log-linear refer to the rule-based baseline and the best log-linear model of (Huang et al., 2001 ) and the figures are simply taken from that paper; row Col log-linear refers to the same named entity tagger we used in the previous section and is included for comparison with the HZP models; row JA digits refers to the simple baseline where we extract strings of spoken digits of plausible lengths."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-155", "intents": ["@BACK@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "Generic methods like the named entity tagger used by Huang et al. (2001) may not be the best tools for particular tasks; in fact, we do not expect the bigram and trigram features used by such taggers to be sufficient for accurately extracting phone numbers."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-64", "intents": ["@USE@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "Since a direct comparison to the log-linear named entity tagger described in (Huang et al., 2001 ) (we refer to this approach as HZP log-linear below) is not possible due to the use of different corpora and annotation standards, we applied a similar named entity tagger based on a log-linear model with trigram features to our data (we refer to this approach as Col log-linear as the tagger was provided by Michael Collins)."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-140", "intents": ["@USE@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "The results are summarized in Table 5 , which also repeats the best results from (Huang et al., 2001) , using the same terminology as earlier: rows HZP strict and HZP containment refer to the best model from (Huang et al., 2001 ) -corresponding to row HZP log-linear in Table 4 -when evaluated using the strict criterion and containment, respectively; and row JA containment refers to our own best model -corresponding to row JA extract + classify in Ta It is not very plausible that the differences between the approaches in Table 5 would be due to a difference in the performance of the ASR components that generated the message transcripts."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-152", "intents": ["@USE@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "This places less of a burden on the grammar developers than having to write an accurate set of rules like the baseline of (Huang et al., 2001 )."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-65", "intents": ["@UNSURE@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "Table 1 summarizes precision (P), recall (R), and F-measure (F) for three approaches evaluated on manual transcriptions: row HZP loglinear repeats the results of the best model from (Huang et al., 2001 ); row Col log-linear contains the results we obtained using a similar named entity tagger on our own data; and row JA classifiers shows the performance of the classifier method proposed in this section."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-69", "intents": ["@DIF@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "While the results for the approach proposed here appear clearly worse than those reported by Huang et al. (2001) , we hasten to point out that this is most likely not due to any difference in the corpora that were used."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-79", "intents": ["@DIF@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "The difference between the approach in (Huang et al., 2001 ) and ours may be partly due to the performance of the ASR components: Huang et al. (2001) report a word error rate of 'about 35%', whereas we used a recognizer (Bacchiani, 2001 ) with a word error rate of only 23%."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-153", "intents": ["@DIF@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "• The combined performance of our simple extraction grammar and the second-phase classifier exceeded the performance of all other methods, including the current state of the art (Huang et al., 2001 )."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "It has been more recently that graph-based methods for knowledge-based WSD have gained much attention in the NLP community ( (Sinha and Mihalcea, 2007) , (Navigli and Lapata, 2007) , (Agirre and Soroa, 2008) , (Agirre and Soroa, 2009) )."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "In (Agirre and Soroa, 2009 ), a comparative analysis of different graph-based models over two well known WSD benchmarks is reported."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "In particular, a variant called Personalized PageRank (P P R) is proposed (Agirre and Soroa, 2009 ) that tries to trade-off between the amount of the employed lexical information and the overall efficiency."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-38", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "In (Agirre and Soroa, 2009 ), a possible, and more accurate alternative, is also presented called PPR word2word (P P Rw2w) where a different personalization vector is used for each word in a sentence."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-74", "intents": ["@BACK@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "Many algorithms (as well as the one proposed by (Agirre and Soroa, 2009) ) initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices)."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-85", "intents": ["@BACK@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "In order to address the above problems, in line with the notion of topic-sensitive PageRank, a personalized PageRank approach has been recently devised (Agirre and Soroa, 2009 ) as discussed in the next section."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-88", "intents": ["@BACK@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "In (Agirre and Soroa, 2009 ), a novel use of PageRank for word sense disambiguation is presented."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-95", "intents": ["@BACK@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "The alternative proposed in (Agirre and Soroa, 2009 ) allows a more static use of the full LKB."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-103", "intents": ["@BACK@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "A word oriented version of the algorithm is also proposed in (Agirre and Soroa, 2009 )."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "This approach to the personalized PageRank is termed word-by-word or P P Rw2w version in (Agirre and Soroa, 2009) ."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-111", "intents": ["@BACK@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "The key idea in (Agirre and Soroa, 2009 ) is to adapt the matrix initialization step in order to exploit the available contextual evidence."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-42", "intents": ["@DIF@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "The intuition is that distributional evidence is able to cover the gap between word oriented usages of the P P R as for the P P Rw2w defined in (Agirre and Soroa, 2009) , and its sentence oriented counterpart."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-189", "intents": ["@DIF@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "An even more interesting outcome is that the improvement implied by the proposed LSA method on the sentence oriented model (i.e. the standard PPR method of (Agirre and Soroa, 2009) ) is higher, so that the difference between the performances of the P P Rw2w model are no longer strikingly better than the P P R one."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-155", "intents": ["@USE@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "In order to compare the quality of the proposed approach, the results of the personalized PageRank proposed in (Agirre and Soroa, 2009 ) over the same dataset are reported in Table 1 (The * systems, denoted by UKB)."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-157", "intents": ["@USE@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "The * systems was presented in (Agirre and Soroa, 2009 )."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-172", "intents": ["@USE@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "In (Agirre and Soroa, 2009 ) the suggested parameters are α = 0.85 as the damping factor and 30 as the upper limit to the PageRank iterations."}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-162", "intents": ["@SIM@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "In line with (Agirre and Soroa, 2009) , different types of WordNet graphs are employed in our experiments:"}
{"sent_id": "70c786a0affcc9f206cc4252112cd2-C001-184", "intents": ["@SIM@"], "paper_id": "ABC_70c786a0affcc9f206cc4252112cd2_2", "text": "As a confirmation of the outcome in (Agirre and Soroa, 2009 ), different lexical resources achieve different results."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-4", "intents": ["@USE@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-188", "intents": ["@USE@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "We create lists of biased and appropriate words using the Bolukbasi et al. (2016) lists of gender-biased and gender-appropriate analogies."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-196", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "Where S is a set of gender-defining word pairs 1 from Bolukbasi et al. (2016) and λ , α are the model-specific constants defined in section 5.1,"}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-240", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "Recall that we use the same debiasing method as Bolukbasi et al. (2016) ; the difference in performance can only be ascribed to how we choose the gender-appropriate words."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "In addition to gender-appropriate analogies such as king:queen::man:woman, stereotypical analogies such as doctor:nurse::man:woman also hold in SGNS embedding spaces (Bolukbasi et al., 2016) ."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "While this subspace projection method precluded gender-biased analogies from holding in the embedding space, Bolukbasi et al. (2016) did not provide any theoretical guarantee that the vectors were unbiased (i.e., equivalent to vectors that would be obtained from training on a gender-agnostic corpus with no reconstruction error)."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-131", "intents": ["@BACK@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "The only other metric of note quantifies association as |cos( w, b)| c , where b is the bias subspace and c ∈ R the \"strictness\" of the measurement (Bolukbasi et al., 2016) ."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-141", "intents": ["@BACK@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "To use the terminology in Bolukbasi et al. (2016) , RIPA is the scalar projection of a word vector onto a onedimensional bias subspace defined by the unit vector b. In their experiments, Bolukbasi et al. (2016) defined b as the first principal component for a set of gender difference vectors (e.g., man − woman)."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-219", "intents": ["@BACK@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "To use the subspace projection method (Bolukbasi et al., 2016) , one must have prior knowledge of which words are gender-appropriate, so that they are not debiased."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-221", "intents": ["@BACK@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "To create an exhaustive list of gender-appropriate words, Bolukbasi et al. (2016) started with a small, human-labelled set of words and then trained an SVM to predict more genderappropriate terms in the vocabulary."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-223", "intents": ["@BACK@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "The way in which Bolukbasi et al. (2016) evaluated their method is unorthodox: they tested the ability of their debiased embedding space to generate new analogies."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-21", "intents": ["@DIF@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "We find that contrary to what Bolukbasi et al. (2016) suggested, word embeddings should not be normalized before debiasing, as vector length can contain important information (Ethayarajh et al., 2018) ."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-37", "intents": ["@DIF@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "In contrast to the supervised method proposed by Bolukbasi et al. (2016) for identifying these gender-specific words, we introduce an unsupervised method."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-90", "intents": ["@DIF@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "This implies that the co-occurrence matrix M d that is reconstructed using the debiased word matrix W d is also unbiased with respect to S. The subspace projection method is therefore far more powerful than initially stated in Bolukbasi et al. (2016) : not only can it be applied to any embedding model that implicitly does matrix factorization (e.g., GloVe, SGNS), but debiasing word vectors in this way is equivalent to training on a perfectly unbiased corpus when there is no reconstruction error."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-199", "intents": ["@DIF@", "@UNSURE@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "The Bolukbasi et al. (2016) method of identifying these words is ineffective: it ends up precluding most gender-appropriate analogies (dotted line, left) while preserving most gender-biased analogies (dotted line, right)."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-239", "intents": ["@DIF@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "In contrast, the Bolukbasi et al. (2016) approach 2 Available at https://github.com/tolga-b/debiaswe preserves only 16.5% of appropriate analogies with a strength of at least 0.5 while preserving 80.0% of biased ones."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "To address this drawback, new VQA datasets [44, 8, 37] have been recently proposed with questions that explicitly require understanding and reasoning about text in the image, which is referred to as the TextVQA task."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "Several approaches [44, 8, 37, 7] have been proposed for the TextVQA task, based on OCR results of the image."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-22", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "In particular, LoRRA [44] extends previous VQA models [43] with an OCR attention branch and adds OCR tokens as a dynamic vocabulary to the answer classifier, allowing copying a single OCR token from the image as the answer."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "Recently, a few datasets and methods [44, 8, 37, 7] have been proposed for visual question answering based on text in images (referred to as the TextVQA task)."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "LoRRA [44] , a prominent prior work on this task, extends the Pythia [43] framework for VQA and allows it to copy a single OCR token from the image as the answer, by applying a single attention hop (conditioned on the question) over the OCR tokens and including the OCR token indices in the answer classifier's output space."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-58", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "For the TextVQA task, recent works [44, 37] have proposed to copy OCR tokens by adding their indices to classifier outputs."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-134", "intents": ["@BACK@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "The TextVQA dataset [44] contains 28,408 images from the Open Images dataset [27] , with human-written questions asking to reason about text in the image."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-18", "intents": ["@DIF@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "Compared to previous work (e.g. [44] ) on the TextVQA task, our model, accompanied by rich features for image text, handles all modalities with a multimodal transformer over a joint embedding space instead of pairwise fusion mechanisms between modalities."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-36", "intents": ["@DIF@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "4) Our model significantly outperforms previous work on three challenging datasets for the TextVQA task: TextVQA [44] (+25% relative), ST-VQA [8] (+65% relative), and OCR-VQA [37] (+32% relative)."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-90", "intents": ["@DIF@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "We follow this intuition in our model and use a rich OCR representation consisting of four types of features, which is shown in our experiments to be significantly better than word embedding (such as FastText) alone in prior work [44] ."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-142", "intents": ["@DIF@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "Unlike the prior work LoRRA [44] that uses a multilingual Rosetta version, in our model we use an English-only version of Rosetta that we find has higher recall."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-189", "intents": ["@DIF@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "Compared to the previous work LoRRA [44] which selects one answer from training set or copies only a single OCR token, our model can copy multiple OCR tokens and combine them with its fixed vocabulary through iterative decoding."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-248", "intents": ["@DIF@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "However, we note that even without fixed answer vocabulary, our restricted model (M4C w/o fixed vocabulary in Table 5 ) still outperforms the previous work LoRRA [44] , suggesting that it is particularly important to learn to copy multiple OCR tokens to form an answer (a key feature in our model but not in LoRRA)."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-81", "intents": ["@USE@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "Following prior work [3, 43, 44] , we extract appearance feature x fr m using the detector's output from the m-th object (where m = 1, · · · , M )."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-130", "intents": ["@USE@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "We evaluate our model on three challenging datasets for the TextVQA task, including the TextVQA dataset [44] , the ST-VQA dataset [8] , and the OCR-VQA dataset [37] ."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-137", "intents": ["@USE@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "For visual objects, following Pythia [43] and LoRRA [44] , we detect objects with a Faster R-CNN detector [41] pretrained on the Visual Genome dataset [26] , and keeps 100 top-scoring objects per image."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-154", "intents": ["@USE@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "As a notable prior work on this dataset, we show a stepby-step comparison with the LoRRA model [44] ."}
{"sent_id": "5e0b1b085a7a10b1e1c17286f7048e-C001-179", "intents": ["@USE@"], "paper_id": "ABC_5e0b1b085a7a10b1e1c17286f7048e_2", "text": "Figure 4 shows qualitative examples (more examples in appendix) of our M4C model on the TextVQA dataset in comparison to LoRRA [44] , where our model is capable of selecting multiple OCR tokens and combining them with its fixed vocabulary in predicted answers."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "However, Collobert et al. (2011b) proposed an effective neural network model that requires little feature engineering and instead learns important features from word embeddings trained on large quantities of unlabelled text -an approach made possible by recent advancements in unsupervised learning of word embeddings on massive amounts of data (Collobert and Weston, 2008; Mikolov et al., 2013) and neural network training algorithms permitting deep architectures (Rumelhart et al., 1986) ."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-12", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "Unfortunately there are many limitations to the model proposed by Collobert et al. (2011b) ."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-105", "intents": ["@BACK@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "This objective function and its gradients can be efficiently computed by dynamic programming (Collobert et al., 2011b) ."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-254", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "Much later, with the advent of neural word embeddings, Collobert et al. (2011b) presented SENNA, which employs a deep FFNN and word embeddings to achieve near state of the art results on POS tagging, chunking, NER, and SRL."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-32", "intents": ["@MOT@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "Our neural network is inspired by the work of Collobert et al. (2011b) , where lookup tables transform discrete features such as words and characters into continuous vector representations, which are then concatenated and fed into a neural network."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-28", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "Furthermore, as lexicons are crucial to NER performance, we propose a new lexicon encoding scheme and matching algorithm that can make use of partial matches, and we compare it to the simpler approach of Collobert et al. (2011b) ."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-52", "intents": ["@USE@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "Our best model uses the publicly available 50dimensional word embeddings released by Collobert et al. (2011b) 2 , which were trained on Wikipedia and the Reuters RCV-1 corpus."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-55", "intents": ["@USE@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "6 Following Collobert et al. (2011b) , all words are lower-cased before passing through the lookup table  Text Hayao Tada , commander of the Japanese North China Area Army to convert to their corresponding embeddings."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-67", "intents": ["@USE@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "As capitalization information is erased during lookup of the word embedding, we evaluate Collobert's method of using a separate lookup table to add a capitalization feature with the following options: allCaps, upperInitial, lowercase, mixedCaps, noinfo (Collobert et al., 2011b) ."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-85", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "In addition, since Collobert et al. (2011b) released their lexicon with their SENNA system, we also applied their lexicon to our model for comparison and investigated using both lexicons simultaneously as distinct features."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-101", "intents": ["@USE@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "We train our network to maximize the sentencelevel log-likelihood from Collobert et al. (2011b) ."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-178", "intents": ["@USE@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "We re-implemented the FFNN model of Collobert et al. (2011b) as a baseline for comparison."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-186", "intents": ["@USE@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "On the other hand, we suspect that Google's embeddings perform poorly because of vocabulary mismatch -in particular, Google's embeddings were trained in a case-sensitive manner, and embeddings for many common punctuations and 27 Wilcoxon rank sum test, p < 0.001 28 To make a direct comparison to Collobert et al. (2011b) , we do not exclude the CoNLL-2003 NER task test data from the word vector training data."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-213", "intents": ["@USE@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "Compared to the SENNA lexicon, our DBpedia lexicon is noisier but has broader coverage, which explains why when applying it using the same method as Collobert et al. (2011b) , it performs worse on CoNLL-2003 but better on OntoNotesa dataset containing many more obscure named entities."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-84", "intents": ["@DIF@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "As we will see in Section 4.5, we found that this more sophisticated method outperforms the method presented by Collobert et al. (2011b) , which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches, and marks tokens with YES/ NO."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "Answering detailed questions about an image is a type of task which requires more sophisticated patterns of reasoning, and there has been a rapid recent proliferation of computer vision approaches for tackling the visual question answering (Visual QA) task [6, 7] ."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "to handle many objects and their complex relations while also integrating rich background knowledge, and attention has emerged as a promising strategy for achieving good performance [7, 8, 9, 10, 11, 12, 13, 14] ."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "Existing attention models [7, 8, 9, 10] are predominantly based on soft attention, in which all information is adaptively re-weighted before being aggregated."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "Creating a good Visual QA dataset has proved non-trivial: biases in the early datasets [6, 22, 23, 33] rewarded algorithms for exploiting spurious correlations, rather than tackling the reasoning problem head-on [7, 34, 35] ."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-47", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "However, only soft attention is used in the majority of Visual QA works [7, 8, 9, 10, 11, 12, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52] ."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "As is common in question answering [7, 9, 22, 23, 24] , the question is a sequence of words q = [q 1 , ..., q n ], while the output is reduced to a classification problem between a set of common answers (this is limited compared to approaches that generate answers [41] , but works better in practice)."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-152", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "This dataset [7] consists of about 121K (98K) images, 438K (220K) questions, and 4.4M (2.2M) answers in the train (test) set; and it is created so that the distribution of the answers between train and test splits differ, and hence the models cannot excessively rely on the language prior [7] ."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-153", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "As expected, [7] show that performance of all Visual QA approaches they tested drops significantly between train to test sets."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-31", "intents": ["@USE@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "We rely on a canonical Visual QA pipeline [7, 9, 22, 23, 24, 25] augmented with a hard attention mechanism that uses the L2-norms of the feature vectors to select subsets of the information for further processing."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-42", "intents": ["@USE@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "Thus, we focus on the recently-introduced VQA-CP [7] and CLEVR [34] datasets, which aim to reduce the dataset biases, providing a more difficult challenge for rich visual reasoning."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-71", "intents": ["@USE@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "We compute a combined representation by copying the question representation to every spatial location in the CNN, and concatenating it with (or simply adding it to) the visual features, like previous Otherwise, we follow the canonical Visual QA pipeline [7, 9, 22, 23, 24, 25] ."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-76", "intents": ["@USE@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "After a few layers of combined processing, we apply attention over spatial locations, following previous works which often apply soft attention mechanisms [7, 8, 9, 10] at this point in the architecture."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-173", "intents": ["@USE@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "In particular, we include previous results using a basic soft attention network [7, 9] , as well as our own re-implementation of the soft attention pooling algorithm presented in [7, 9] with the same features used in other experiments."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-208", "intents": ["@USE@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "Table 1 shows SAN's [9] results reported by [7] together with our in-house implementation (denoted as \"ours\")."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-176", "intents": ["@DIF@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "Our results overall are somewhat worse than the state-of-the-art [7] , but this is likely due to several architectural decisions not included here, such as a split pathway for different kinds of questions, special question embeddings, and the use of the question extractor."}
{"sent_id": "7bdb51a3ca6c322ef6e04d18ba8483-C001-241", "intents": ["@SIM@"], "paper_id": "ABC_7bdb51a3ca6c322ef6e04d18ba8483_2", "text": "Surprisingly, simple-HAN+sum achieves about 24% performance on the same split, on-par with the performance of normal SAN that uses more complex and deeper visual architecture [67] ; the results are reported by [7] ."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "Prior argumentative relation mining studies have often used features extracted from argument components to model different aspects of the relations between the components, e.g., relative distance, word pairs, semantic similarity, textual entailment (Cabrio and Villata, 2012; Stab and Gurevych, 2014b; Boltužić andŠnajder, 2014; Peldszus and Stede, 2015b) ."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "Unlike argument component identification where textual inputs are typically sentences or clauses (Moens et al., 2007; Stab and Gurevych, 2014b; Levy et al., 2014; Lippi and Torroni, 2015) , textual inputs of argumentative relation mining vary from clauses (Stab and Gurevych, 2014b; Peldszus, 2014 ) to multiple-sentences (Biran and Rambow, 2011; Cabrio and Villata, 2012; Boltužić anď Snajder, 2014) ."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-61", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "Finally, prior research has explored predicting different argumentative relationship labels between pairs of argument components, e.g., attachment (Peldszus and Stede, 2015a) , support vs. non-support (Biran and Rambow, 2011; Cabrio and Villata, 2012; Stab and Gurevych, 2014b) , {implicit, explicit}×{support, attack} (Boltužić andŠnajder, 2014) , verifiability of support (Park and Cardie, 2014) ."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-95", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "Stab and Gurevych (2014b) used a 55-discourse marker set to extract indicator features."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-98", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "Stab and Gurevych (2014b) used predicted label of argument components as features for both training and testing their argumentation structure identification model."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-56", "intents": ["@USE@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "In our work, we follow Stab and Gurevych (2014b) and use the predicted labels of argument components as features during argumentative relation mining."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-77", "intents": ["@USE@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "4 Argumentative Relation Tasks 4.1 Task 1: Support vs. Non-support Our first task follows (Stab and Gurevych, 2014b) : given a pair of source and target argument components, identify whether the source argumentatively supports the target or not."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-92", "intents": ["@USE@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "Given a pair of argument components, we follow (Stab and Gurevych, 2014b) by first extracting 3 feature sets: structural (e.g., word counts, sentence position), lexical (e.g., word pairs, first words), and grammatical production rules (e.g., S→NP,VP)."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-154", "intents": ["@USE@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "Because temporal relations were shown not helpful for argument mining tasks (Biran and Rambow, 2011; Stab and Gurevych, 2014b) , we exclude them here."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-169", "intents": ["@USE@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "We use the training set as determined in (Stab and Gurevych, 2014b) to train/test 9 the models using LibLINEAR algorithm (Fan et al., 2008) without parameter or feature optimization."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-178", "intents": ["@USE@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "We also compare our baseline to the reported performance (REPORT) for Support vs. Non-support classification in (Stab and Gurevych, 2014b) ."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-57", "intents": ["@DIF@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "We, however, take advantage of an enhanced argument component model (Nguyen and Litman, 2016 ) to obtain more reliable argument component labels than in (Stab and Gurevych, 2014b) ."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-66", "intents": ["@EXT@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "3 Because the corpus has been utilized for different argument mining tasks (Stab and Gurevych, 2014b; Nguyen and Litman, 2015; Nguyen and Litman, 2016) , we use this corpus to demonstrate our context-aware argumentative relation mining approach, and adapt the model developed by Stab and Gurevych (2014b) to serve as the baseline for evaluating our proposed approach."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-86", "intents": ["@EXT@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "Because this task was not studied in (Stab and Gurevych, 2014b) , we adapt Stab and Gurevych's model to use as the baseline."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-49", "intents": ["@SIM@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "Our design most closely resembles the pipeline proposed by the top system last year (Wang and Lan, 2015) , in that argument extraction for explicit relations is performed separately for Arg1 and Arg2, the non-explicit sense classifier is run twice."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-69", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "The features are based on previous work (Pitler et al., 2009; Lin et al., 2014; Wang and Lan, 2015) ."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-77", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "We employ the features proposed in Lin et al. (2014) and additional features described in last year's top system (Wang and Lan, 2015) ."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-79", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "In line with prior work (Wang and Lan, 2015) , we consider PS to be the sentence that immediately precedes the connective."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-85", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "We follow the constituent-based approach proposed in Kong et al. (2014) , without the joint inference and enhance it using features in Wang and Lan (2015) ."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-87", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "PS Arg1 Extractor: We implement features described in Wang and Lan (2015) and add novel features."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-89", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "We used the constituent split implemented in Wang and Lan (2015) ."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-90", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "Based on earlier work (Wang and Lan, 2015; Lin et al., 2014) , we implement the following features: surface form of the verbs in the sentence (three features), last word of the current constituent (curr), last word of the previous constituent (prev), the first word of curr, and the lowercased form of the connective."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-93", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "PS Arg2 Extractor: Similar to PS Arg1 extractor, for this component we implement features described in Wang and Lan (2015) and add novel features."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-109", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "• C parent-category linked context, previous connective and its POS of \"as\"(the connective and its POS of previous relation, if the connective of current relation is \"as\"), previous connective and its POS of \"when\", adopted from Wang and Lan (2015) ."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-119", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "Following Wang and Lan (2015), we extract sentence pairs that satisfy the following three criteria:"}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-140", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "We use features in Lin et al. (2009) and Wang and Lan (2015) and augment these with novel features."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-147", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "Implicit Arg2 Extractor: We use most of the features in Lin et al. (2014) and Wang and Lan (2015) to train the Arg2 extractor (for more details and explanation about the features, we refer the reader to the respective papers):"}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-173", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "Base features refer to features used in Wang and Lan (2015) ."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-181", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "We compare our baseline model that implements the features proposed in Wang and Lan (2015) with the model that employs additional features introduced in 4.4."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-185", "intents": ["@USE@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "We implement the features in Wang and Lan (2015) and add our novel features shown in Table 1 ."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-182", "intents": ["@DIF@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "Our baseline model performs slightly better than the one reported in Wang and Lan (2015) : we obtain 90.55 vs. 90.14, as reported in Wang and Lan (2015) ."}
{"sent_id": "d2ce392240108203377d8e51e89d09-C001-192", "intents": ["@DIF@"], "paper_id": "ABC_d2ce392240108203377d8e51e89d09_2", "text": "We note that in Wang and Lan (2015) the numbers that correspond to the entire sentence baselines are not the same as those that we obtain, so we do not report a direct comparison with their models."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "With the availability of annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008) , statistical discourse parsers were developed (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012) ."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "In the first approach the parser decision is not conditioned on whether the relation is intra-or inter-sentential (e.g. (Ghosh et al., 2011) )."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-26", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "In (Ghosh et al., 2011 ) the decision is made on tokenlevel, and the problem is cast as sequence labeling using conditional random fields (CRFs) (Lafferty et al., 2001) ."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-67", "intents": ["@BACK@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "CRF-based discourse parser of Ghosh et al. (2011) , which processes SS and PS cases with the same model, uses ±2 sentence window as a hypothesis space (5 sentences: 1 sentence containing the connective, 2 preceding and 2 following sentences)."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-96", "intents": ["@BACK@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "The task has a very high baseline and even higher performance on supervised machine learning, Table 3 : Feature sets for Arg2 and Arg1 argument span extraction in (Ghosh et al., 2011) which is an additional motivation to process intra-and inter-sentential relations separately."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-117", "intents": ["@BACK@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "Previous Sentence Feature (PREV) signals if a sentence immediately precedes the sentence starting with a connective, and its value is the first token of the connective (Ghosh et al., 2011) ."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-123", "intents": ["@BACK@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "Via templates these features are enriched with ngrams: tokens with 2-grams in the window of ±1 to- Figure 1: Single model discourse parser architecture of (Ghosh et al., 2011) ."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-130", "intents": ["@BACK@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "The discourse parser of (Ghosh et al., 2011 ) is a cascade of CRF models to sequentially label Arg2 and Arg1 spans (since Arg2 label is a feature for Arg1 model) (see Figure 1 )."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-169", "intents": ["@BACK@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "Ghosh et al. (2011) report using CONLL-based evaluation script."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-172", "intents": ["@BACK@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "Thus, CONLL-based evaluation yields incorrect number of test instances: Ghosh et al. (2011) report 1,028 SS and 617 PS test instances for PDTB sections 23-24 (see caption of Table 7 in the original paper), which is 1,645 in total; whereas there is only 1,595 explicit relations in these sections."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-27", "intents": ["@EXT@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "In this paper we focus on argument span extraction, and extend the token-level sequence labeling approach of (Ghosh et al., 2011) with the separate models for arguments of intra-sentential and intersentential explicit discourse relations."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-99", "intents": ["@EXT@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "We replicate and evaluate the discourse parser of (Ghosh et al., 2011) , then modify it to process intraand inter-sentential explicit relations separately."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-219", "intents": ["@EXT@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "We extend the approach of (Ghosh et al., 2011) to argument span extraction cast as token-level sequence labeling using CRFs and integrate argument position classification and immediately previous sentence heuristic."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-60", "intents": ["@USE@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "In this paper we follows the approach of (Ghosh et al., 2011 (Prasad et al., 2008) ); and distribution of Arg2 with respect to extent in inter-sentential explicit discourse relations."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-166", "intents": ["@USE@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "Following (Ghosh et al., 2011) PDTB is split as Sections 02-22 for training, 00-01 for development, and 23-24 for testing."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-174", "intents": ["@USE@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "Following (Ghosh et al., 2011) and (Lin et al., 2012) , argument initial and final punctuation marks are removed; and precision (p), recall (r) and F 1 score are computed using the equations 1 -3."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-133", "intents": ["@DIF@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "The performance is higher than (Ghosh et al., 2011 ) -Arg2: F 1 of 79.1 and Arg1: F 1 of 57.3 -due to improvements in feature and instance extraction, such as the treatment of multi-word connectives."}
{"sent_id": "06917a1dd02d55c827e7e07eeae2da-C001-161", "intents": ["@DIF@"], "paper_id": "ABC_06917a1dd02d55c827e7e07eeae2da_2", "text": "First, in this paper it is different from (Ghosh et al., 2011) ; thus, we first describe it and evaluate the difference."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "Domain shift is a fundamental problem in machine learning, that has attracted a lot of attention in the natural language processing and vision communities [2, 6, 11, 13, 29, 30, 32, 37, 39, 40, 42] ."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "Interestingly, some recent works [13, 18] indicate that string kernels can yield robust results in the cross-domain setting without any domain adaptation."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "In fact, methods based on string kernels have demonstrated impressive results in various text classification tasks ranging from native language identification [22] [23] [24] 36] and authorship identification [34] to dialect identification [4, 18, 21] , sentiment analysis [13, 35] and automatic essay scoring [7] ."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-18", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "As long as a labeled training set is available, string kernels can reach state-of-the-art results in various languages including English [7, 13, 23] , Arabic [4, 17, 18, 24] , Chinese [35] and Norwegian [24] ."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "In recent years, methods based on string kernels have demonstrated remarkable performance in various text classification tasks [7, 10, 13, 18, 23, 27, 34] ."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "Giménez-Pérez et al. [13] have used string kernels for single-source and multi-source polarity classification."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-73", "intents": ["@DIF@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "Different from all these recent approaches [13, 18, 23] , we use unlabeled data from the target domain to significantly increase the performance of string kernels in cross-domain text classification, particularly in English polarity classification."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-141", "intents": ["@DIF@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "Using string kernels, Giménez-Pérez et al. [13] reported better performance than SST [3] and KE-Meta [12] in the multi-source domain setting."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-152", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "Although Giménez-Pérez et al. [13] used a different classifier, namely Kernel Discriminant Analysis, we observed that Kernel Ridge Regression produces similar results (±0.1%) when we employ the same string kernels."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-171", "intents": ["@DIF@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "Without exception, the accuracy rates reached by the transductive string kernels are significantly better than the best baseline string kernel [13] , according to the McNemar's test performed at a confidence level of 0.01."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-183", "intents": ["@DIF@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "Indeed, the polarity classification experiments demonstrate that our framework achieves better accuracy rates than other state-ofthe-art methods [3, 12, 13, 15, 32, 40] ."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-140", "intents": ["@USE@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "We compare our approach with several methods [3, 12, 13, 15, 32, 40] in two cross-domain settings."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-144", "intents": ["@USE@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "Multi-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels [13] , as well as SST [3] and KE-Meta [12] ."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-148", "intents": ["@USE@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "We follow the same evaluation methodology of Giménez-Pérez et al. [13] , to ensure a fair comparison."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-153", "intents": ["@USE@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "As Giménez-Pérez et al. [13] , we evaluate our approach in two cross-domain settings."}
{"sent_id": "c897c2ea0d641f1f35072be4a5a7d3-C001-164", "intents": ["@USE@"], "paper_id": "ABC_c897c2ea0d641f1f35072be4a5a7d3_2", "text": "Single-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels [13] , as well as SFA [32] , CORAL [40] and TR-TrAdaBoost [15] ."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-3", "intents": ["@USE@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016) ."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-21", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "On the other hand, in the field of word embedding construction, some previous researchers found that character n-grams are more useful than single characters (Wieting et al. 2016; Bojanowski et al. 2017) ."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-22", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "In particular, (Wieting et al. 2016) demonstrated that constructing word embeddings from character n-gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM)."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-48", "intents": ["@USE@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "We incorporate charn-MS-vec, which is an embedding constructed from character n-gram embeddings, into RNN language models since, as discussed earlier, previous studies revealed that we can construct better word embeddings by using character n-gram embeddings (Wieting et al. 2016; Bojanowski et al. 2017 )."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-145", "intents": ["@USE@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "Since AWD-LSTM-MoS (Yang et al. 2018 ) and AWD-LSTM-DOC (Takase, Suzuki, and Nagata 2018) achieved the stateof-the-art scores on PTB and WT2, we combined char3-MSvec with them."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-191", "intents": ["@USE@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "Finally, (Merity, Keskar, and Socher 2018b) introduced DropConnect (Wan et al. 2013 ) and averaged SGD (Polyak and Juditsky 1992) into the LSTM language model and achieved state-of-the-art perplexities on PTB and WT2."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-227", "intents": ["@USE@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "Based on the research in the field of word embedding construction (Wieting et al. 2016) , we focused on character n-gram embeddings to construct word embeddings."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "For example, neural encoderdecoder models, which are becoming the de facto standard for various natural language generation tasks including machine translation (Sutskever, Vinyals, and Le 2014) , summarization (Rush, Chopra, and Weston 2015) , dialogue (Wen et al. 2015) , and caption generation (Vinyals et al. 2015) can be interpreted as conditional neural language models."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "In general, neural language models require word embeddings as an input (Zaremba, Sutskever, and Vinyals 2014)."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "However, as described by (Verwimp et al. 2017) , this approach cannot make use of the internal structure of words although the internal structure is often an effective clue for considering the meaning of a word."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "To incorporate the internal structure, (Verwimp et al. 2017) concatenated character embeddings with an input word embedding."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "Previous studies demonstrated that additive composition, which computes the (weighted) sum of embeddings, is a suitable method for embedding construction Wieting et al. 2016 the number of character n-grams extracted from the word, and let S be the matrix whose i-th column corresponds to s i , that is, S = [s 1 , ..., s I ]."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-205", "intents": ["@BACK@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "Their proposed method achieved perplexity competitive with the basic LSTM language model (Zaremba, Sutskever, and Vinyals 2014) even though its parameter size is small."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-218", "intents": ["@BACK@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "On the other hand, (Bojanowski et al. 2017 ) and (Wieting et al. 2016) focused on character n-gram."}
{"sent_id": "6b1432f4aac35e6acd8ca8770fe484-C001-220", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_6b1432f4aac35e6acd8ca8770fe484_2", "text": "In addition, (Wieting et al. 2016) found that the sum of character n-gram embeddings also outperformed word embeddings constructed from character embeddings with CNN and LSTM."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-3", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "In particular, the technique proposed by Yuan et al. (2016) returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "Among the best-performing ones is the approach by Yuan et al. (2016) , in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "The method proposed by Yuan et al. (2016) performs WSD by annotating each lemma in a text with one WordNet synset that is associated with its meaning."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "In Yuan et al. (2016) , the first operation consists of constructing an LSTM language model to capture the meaning of words in context."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-102", "intents": ["@BACK@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "Yuan et al. (2016) argue that the averaging procedure is suboptimal because of two reasons."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-30", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "First, a positive result is that we were able to reproduce the method from Yuan et al. (2016) and obtain similar results to the ones originally published."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-57", "intents": ["@USE@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "The work by Yuan et al. (2016) , which we consider in this paper, belongs to this last category."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-122", "intents": ["@USE@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "For the training of the sense embeddings, we use the same two corpora used by Yuan et al. (2016): 1. SemCor (Miller et al., 1993 ) is a corpus containing approximately 240,000 sense annotated words."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-157", "intents": ["@USE@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "Three scorers are used: \"framework\" refers to the WSD evaluation framework from Raganato et al. (2017a) ; \"mapping to WN3.0\" refers to the evaluation used by Yuan et al. (2016) while \"competition\" refers to the scorer provided by the competition itself (e.g., semeval2013)."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-162", "intents": ["@USE@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "In this section, we report our reproduction of the results of Yuan et al. (2016) and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-165", "intents": ["@USE@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "We trained the LSTM model with the best reported settings in Yuan et al. (2016) (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-171", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "The top part of the table presents our reproduction results, the middle part reports the results from Yuan et al. (2016) , while the bottom part reports a representative sample of the other state-of-the-art approaches."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-191", "intents": ["@USE@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "Table 2 shows that the method by Yuan et al. (2016) does not overfit towards the MFS to the same extent as other supervised systems since the recall on LFS instances is still quite high 0.41 (a lower recall on LFS instances than on MFS ones is expected due to the reduced training data for them)."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-234", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "The data points at 100 billion (10 11 ) tokens correspond to Yuan et al. (2016) 's reported results."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-249", "intents": ["@USE@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "This paper reports the results of a reproduction study of the model proposed by Yuan et al. (2016) and an additional analysis to gain a deeper understanding of the impact of various factors on its performance."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-5", "intents": ["@DIF@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "Our study showed that similar results can be obtained with much less data than hinted at by Yuan et al. (2016) ."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-183", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "Different from Yuan et al. (2016), we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation)."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-251", "intents": ["@DIF@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than Yuan et al. (2016) 's proprietary corpus, and got similar performance on senseval2 and semeval2013."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-18", "intents": ["@MOT@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "Even though the results obtained by Yuan et al. (2016) outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-22", "intents": ["@MOT@", "@EXT@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "To address these issues, we reimplemented Yuan et al. (2016) 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-173", "intents": ["@UNSURE@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "However, not all answers in senseval2 can be mapped to WN3.0 and we do not know how Yuan et al. (2016) handled these cases."}
{"sent_id": "97f8d0af85eda3e453fc4fb00819f0-C001-230", "intents": ["@UNSURE@"], "paper_id": "ABC_97f8d0af85eda3e453fc4fb00819f0_2", "text": "The fact that Yuan et al. (2016) used a 100-billion-token corpus only reinforces this intuition."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "Taking a small step towards this goal, recent work has begun developing artificial agents that follow natural language navigation instructions in perceptually-rich, simulated environments [4, 6] ."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "Google StreetView images in Touchdown [6] or Matterport3D panoramas captured in homes in Vision-and-Language Navigation (VLN) [4] ."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-81", "intents": ["@BACK@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "There have been a number of recent tasks proposed in this space [4, 6, 13, 20] ."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "Most related to our work is the Vision-and-Language Navigation (VLN) task of Anderson et al. [4] ."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-133", "intents": ["@BACK@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "To enable agent interaction with these panoramas, Anderson et al. [4] developed the Matterport3D Simulator."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-139", "intents": ["@BACK@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "Based on this simulator, Anderson et al. [4] collect the Roomto-Room (R2R) dataset containing 7189 trajectories each with three humangenerated instructions on average."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-217", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "Aside from modeling details, much of the remaining progress in VLN has come from adjusting the training regime -adding auxiliary losses / rewards [17, 29] , mitigating exposure bias during training [4, 29] , or reducing data sparsity by incorporating synthetically generated data augmentation [9, 26] ."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-229", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "Prior work has shown significant gains by addressing this issue for VLN through scheduled sampling [4] or reinforcement learning fine-tuning [26, 29] ."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-47", "intents": ["@USE@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "In this work, we focus in on the Vision-and-Language Navigation (VLN) [4] task and lift these implicit assumptions by instantiating it in continuous 3D environments rendered in a high-throughput simulator [19] ."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-113", "intents": ["@USE@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "In overview, we develop this setting by transferring nav-graph-based Room-to-Room (R2R) [4] trajectories to reconstructed continuous Matterport3D environments in the Habitat simulator [19] ."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-246", "intents": ["@USE@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "We report standard metrics for visual navigation tasks defined in [2, 4, 18] -trajectory length in meters (TL), navigation error in meters from goal at termination (NE), oracle success rate (OS), success rate (SR), success weighted by inverse path length (SPL), and normalized dynamic-time warping (nDTW)."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-248", "intents": ["@USE@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "For full details on these metrics, see [2, 4, 18] ."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-119", "intents": ["@DIF@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "In contrast to the simulator used in VLN [4] , Habitat allows agents to navigate freely in the continuous environments."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-126", "intents": ["@DIF@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "In contrast, actions to move between panoramas in [4] traverse 2.25m on average and can include avoiding obstacles."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-123", "intents": ["@SIM@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "Note that this is similar to the egocentric RGB perception in the original VLN task [4] but differs from the panoramic observation space adopted by nearly all follow-up work [9, 17, 26, 29] ."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-179", "intents": ["@SIM@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "While there are many differences in the details, these models are conceptually similar to early [4] and more recent [29] work in the nav-graph based VLN task."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015; Ponti et al., 2018) or lexical entailment (Nguyen et al., 2017; over other types of semantic association in the word vector space."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "Conflating distinct (both paradigmatic and syntagmatic) lexico-semantic relations is a well-known property of distributional word vectors; semantic specialization of such spaces for a particular lexicosemantic relation (e.g., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking Ponti et al., 2018) , spoken language understanding (Kim et al., 2016b,a) , text simplification (Glavaš and Vulić, 2018b; Ponti et al., 2018) , and cross-lingual transfer of resources (Vulić et al., 2017a) ."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "Post-specialization Ponti et al., 2018; Kamath et al., 2019) is a generalization of retrofitting that specializes the entire distributional space: 1) it learns a global specialization function using before-and after-retrofitting vectors of words from lexical constraints as training examples and 2) it applies the global specialization functions to vectors of words unseen in lexical constraints."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-174", "intents": ["@BACK@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "A standard language understanding evaluation task used in prior work on semantic specialization Ponti et al., 2018, inter alia) is dialog state tracking (DST) (Henderson et al., 2014; ."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-188", "intents": ["@BACK@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "First, as already confirmed in prior work Ponti et al., 2018) , vectors specialized for semantic similarity are indeed important for DST: we observe improvements with all specialized vectors."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-203", "intents": ["@BACK@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "As shown in previous work Ponti et al., 2018) , retrofitting (CLSRI-AR) and the cross-lingual post-specialization transfer (X-PS) are substantially better in the LS task than the original distributional space."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-31", "intents": ["@DIF@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "Moreover, we show that the proposed specialization transfer method consistently outperforms the direct specialization transfer based on the composition of the crosslingual projection and the post-specialization function (Ponti et al., 2018) , with substantial gains across all experimental setups."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-51", "intents": ["@DIF@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "Our experiments show that the proposed specialization transfer via lexical relation induction (CLSRI) outperforms the previous state-of-the-art specialization transfer method of Ponti et al. (2018)."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-64", "intents": ["@DIF@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "Our proposed CLSRI specialization conceptually differs from an existing cross-lingual specialization transfer methodology (Ponti et al., 2018; Glavaš and Vulić, 2018b) , in which the global specialization function is learned in the source language L s and then transferred directly to the target language L s via a shared cross-lingual embedding space."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-123", "intents": ["@DIF@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "In contrast, existing work Glavaš and Vulić, 2018b; Ponti et al., 2018) transfers the post-specialization function learned for the source language L s to the target language L t via a cross-lingual vector space."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-152", "intents": ["@DIF@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "X-PS refers to the baseline model of Ponti et al. (2018) based on direct cross-lingual post-specialization."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-168", "intents": ["@DIF@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "The full CLSRI-PS model outperforms both the distributional vectors and the baseline method for cross-lingual specialization (Ponti et al., 2018) ."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-62", "intents": ["@USE@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "As the final step, we generalize AR's specialization to the entire target vocabulary with a post-specialization model (Ponti et al., 2018) that learns the global specialization function from pairs of distributional and ARspecialized vectors of words from L t constraints."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-118", "intents": ["@SIM@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "In line with and Ponti et al. (2018), we implement this function as a deep feed-forward neural network with l hidden layers of size h and a final linear layer with weight W ∈ R h×d ."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-128", "intents": ["@SIM@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "The assortment of English constraints for specialization is the same as in prior work (Zhang et al., 2014; Ono et al., 2015; Ponti et al., 2018) ."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-142", "intents": ["@SIM@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "6 https://github.com/facebookresearch/ fastText/tree/master/alignment Owing to the difference in the amount of supervision, the post-specialization model has partially non-overlapping configurations for the baseline model of Ponti et al. (2018) and our CLSRI model."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-9", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "Generative models defining joint distributions over parse trees and sentences are good theoretical models for interpreting natural language data, and appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016) ."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-140", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "As shown in Rubinstein and Kroese (2008) , the optimal choice of the proposal distribution is in fact the true posterior p(a|x), in which case the importance weight p(a,x) p(a|x) = p(x) is constant with respect to a. In Dyer et al. (2016) , the proposal distribution depends on x, i.e., q(a) q(a|x), and is computed with a separately-trained, discriminative model."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-21", "intents": ["@USE@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "We showcase the framework using Recurrent Neural Network Grammars (RNNGs; Dyer et al. 2016 ), a recently proposed probabilistic model of phrase-structure trees based on neural transition systems."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-27", "intents": ["@USE@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "In this section we briefly describe Recurrent Neural Network Grammars (RNNGs; Dyer et al. 2016 ), a top-down transition-based algorithm for parsing and generation."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-59", "intents": ["@USE@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "Specifically, we use the following features: 1) the stack embedding d t which encodes the stack of the decoder and is obtained with a stack-LSTM (Dyer et al., 2015 (Dyer et al., , 2016 ; 2) the output buffer embedding o t ; we use a standard LSTM to compose the output buffer and o t is represented as the most recent state of the LSTM; and 3) the parent non-terminal embedding n t which is accessible in the generative model because the RNNG employs a depth-first generation order."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-83", "intents": ["@USE@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "6 See § 4 and Appendix A for comparison between this objective and the importance sampler of Dyer et al. (2016"}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-93", "intents": ["@USE@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "7 Another way of computing p(x) (without lower bounding) would be to use the variational approximation q(a|x) as the proposal distribution as in the importance sampler of Dyer et al. (2016) ."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-105", "intents": ["@USE@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "Further connections can be drawn with the importance-sampling based inference of Dyer et al. (2016) ."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-115", "intents": ["@USE@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "To find the MAP parse tree argmax a p(a, x) (where p(a, x) is used rank the output of q(a|x)) and to compute the language modeling perplexity (where a ∼ q(a|x)), we collect 100 samples from q(a|x), same as Dyer et al. (2016) ."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-120", "intents": ["@USE@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "methods for parsing, ranking approximated MAP trees from q(a|x) with respect to p(a, x) yields a small improvement, as in Dyer et al. (2016) ."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-133", "intents": ["@USE@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "In the future, we would like to perform grammar induction based on Equation (8), with gradient descent and posterior regularization techniques (Ganchev et al., 2010 A Comparison to Importance Sampling (Dyer et al., 2016) In this appendix we highlight the connections between importance sampling and variational inference, thereby comparing our method with Dyer et al. (2016) ."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-121", "intents": ["@DIF@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "It is worth noting that our parsing performance lags behind Dyer et al. (2016) ."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-123", "intents": ["@DIF@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "While Dyer et al. (2016) use an LSTM as the syntactic composition function of each subtree, we adopt a rather simple composition function based on embedding averaging, which gains computational efficiency but loses accuracy."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-124", "intents": ["@DIF@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "On language modeling, our framework achieves lower perplexity compared to Dyer et al. (2016) and baseline models."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-126", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "However, we acknowledge a subtle difference between Dyer et al. (2016) and our approach compared to baseline language models: while the latter incrementally estimate the next word probability, our approach (and Dyer et al. 2016 ) directly assigns probability to the entire sentence."}
{"sent_id": "25e03048cd34685cec34754bdade4e-C001-127", "intents": ["@DIF@"], "paper_id": "ABC_25e03048cd34685cec34754bdade4e_2", "text": "Overall, the advantage of our framework compared to Dyer et al. (2016) is that it opens an avenue to unsupervised training."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-4", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-20", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "Moreover, Finegan-Dollak et al. (2018) demonstrated that the sequence-tosequence model also lack the ability to generate SQL queries of unseen templates."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "However, Finegan-Dollak et al. (2018) showed that the sequence-to-tree approach was inefficient when generating complex SQL queries from a natural language question."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "Iyer et al. (2017); Finegan-Dollak et al. (2018) focused on the dataset that contains more complex queries such as ATIS (Dahl et al., 1994) and GeoQuery (Zelle and Mooney, 1996) ."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-65", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "plied a sequence-to-sequence approach with attention mechanism, and Finegan-Dollak et al. (2018) proposed a template-based model and another sequence-to-sequence model with a copy mechanism."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-191", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "The model from Finegan-Dollak et al. (2018) showed accuracies of 0%, 32%, 20%, and 5% for each benchmark and accuracies of Iyer et al. (2017) showed 1%, 17%, 40%, and 3%, meaning that they also lack the capability to generate unseen templates of SQL."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-66", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "However, Finegan-Dollak et al. (2018) showed that both approaches lack the ability to generate SQL of the unseen template in the training stage."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-31", "intents": ["@DIF@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "2. It minimizes unnecessary search space, unlike sequence-to-sequence approaches (Iyer et al., 2017; Finegan-Dollak et al., 2018) ; thus, the model is guaranteed to be free of SQL syntax errors."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-81", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "This architecture is based on an idea similar to the template-based model of Finegan-Dollak et al. (2018) ."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-186", "intents": ["@DIF@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "Our model shows 3-27% query generation accuracy gain, compared to the sequence-to-sequence model, 5-9% gain, compared to template-based model (Finegan-Dollak et al., 2018) , and 15-56% gain, compared to Iyer et al. (2017) ."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-193", "intents": ["@DIF@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "Our model outperforms the sequence-to-sequence model (Finegan-Dollak et al., 2018) by 1-60%, the template-based model (Finegan-Dollak et al., 2018) by 17-52%, Iyer et al. (2017) by 14-62%."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-38", "intents": ["@USE@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "We conducted experiments with four different text-to-SQL datasets on both of the questionbased split and query-based split (Finegan-Dollak et al., 2018) ."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-146", "intents": ["@USE@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "We used a SQL version of the dataset processed by Finegan-Dollak et al. (2018) ."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-150", "intents": ["@USE@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "We used a template and variables for each SQL from the preprocessed versions provided by Finegan-Dollak et al. (2018) ."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-152", "intents": ["@USE@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "For the query-based split, we used the same split as in Finegan-Dollak et al. (2018) ."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-159", "intents": ["@USE@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "We evaluated the query generation accuracy for both the question-based split and query-based split (Finegan-Dollak et al., 2018) ."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-172", "intents": ["@USE@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "We compare our results with three different previous approaches: a sequence-to-sequence model from Iyer et al. (2017) , template-based model, and another sequence-to-sequence model from Finegan-Dollak et al. (2018) ."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-15", "intents": ["@BACK@", "@MOT@", "@EXT@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015) ."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "Many approaches have been recently developed that build hierarchies purely by identifying either lexical patterns or statistical features in text corpora (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Zhu et al., 2013; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2014; Tuan et al., 2015; Kiela et al., 2015) ."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-26", "intents": ["@DIF@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "To effectively capture these patterns, in contrast to previous works that rely on various hand-crafted features Bansal et al., 2014) , we extract features by leveraging the distributed representations that embed images (Simonyan and Zisserman, 2014) and words as compact vectors, based on which the semantic closeness is directly measured in vector space."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-217", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "We compare the following methods: (1) Fu2014, (2) Ours (L), and (3) Ours (LV), as described above; (4) Bansal2014: The model by Bansal et al. (2014) retrained using our dataset; (5) Ours (LB): By excluding visual features, but including other language features from Bansal et al. (2014) ; (6) Ours (LVB): Our full model further enhanced with all semantic features from Bansal et al. (2014) ; (7) Ours (LVB -E): By excluding word embeddingbased language features from Ours (LVB)."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-225", "intents": ["@DIF@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "However, when introducing visual features, our performance is comparable (pvalue = 0.058).Furthermore, if we discard visual features but add semantic features from Bansal et al. (2014) , we achieve a slight improvement of 0.02 over Bansal2014 (p-value = 0.016), which is largely attributed to the incorporation of word embedding-based features that encode high-level linguistic regularity."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-226", "intents": ["@DIF@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "Finally, if we enhance our full model with all semantic features from Bansal et al. (2014) , our model outperforms theirs by a gap of 0.04 (p-value < 0.01), which justifies our intuition that perceptual semantics underneath visual contents are quite helpful."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-246", "intents": ["@DIF@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "Compared to Bansal et al. (2014) , a major difference of our model is that different layers of the taxonomy correspond to different weights w l , while in (Bansal et al., 2014) all layers share the same weights."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-42", "intents": ["@SIM@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "The works of Fu et al. (2014) and Bansal et al. (2014) use similar language-based features as ours."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-110", "intents": ["@USE@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "To apply the model to discover the underlying taxonomy from a given set of categories, we first obtain the marginals of z by averaging over the samples generated through eq 3, then output the optimal taxonomy z * by finding the maximum spanning tree (MST) using the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Bansal et al., 2014) ."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-159", "intents": ["@USE@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "Specifically, we employ the Capitalization, Ends with, Contains, Suffix match, LCS and Length different features, which are commonly used in previous works in taxonomy induction (Yang and Callan, 2009; Bansal et al., 2014) ."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-163", "intents": ["@USE@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "We then compare our model with previous state-of-the-art methods (Fu et al., 2014; Bansal et al., 2014) with two taxonomy induction tasks."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-200", "intents": ["@USE@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "We use Ancestor F 1 as our evaluation metric (Kozareva and Hovy, 2010; Navigli et al., 2011; Bansal et al., 2014) ."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-202", "intents": ["@USE@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "We compare our method to two previously state-of-the-art models by Fu et al. (2014) and Bansal et al. (2014) , which are closest to ours."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-203", "intents": ["@USE@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "Table 2 : Comparisons among different variants of our model, Fu et al. (2014) and Bansal et al. (2014) on two tasks."}
{"sent_id": "a8ba807b94f6f7ff4f7e77a9fcde35-C001-17", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_a8ba807b94f6f7ff4f7e77a9fcde35_2", "text": "Machine Translation systems have been adapted to translate complex sentences into simple ones (Zhu et al., 2010; Wubben et al., 2012; Coster and Kauchak, 2011) ."}
{"sent_id": "a8ba807b94f6f7ff4f7e77a9fcde35-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_a8ba807b94f6f7ff4f7e77a9fcde35_2", "text": "Using both the PWKP corpus developed by Zhu et al. (2010) and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar (Smith and Eisner, 2006) describing a loose alignment between parse trees of complex and of simple sentences."}
{"sent_id": "a8ba807b94f6f7ff4f7e77a9fcde35-C001-36", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_a8ba807b94f6f7ff4f7e77a9fcde35_2", "text": "They evaluate their model on the same dataset used by Zhu et al. (2010) namely, an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU score."}
{"sent_id": "a8ba807b94f6f7ff4f7e77a9fcde35-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_a8ba807b94f6f7ff4f7e77a9fcde35_2", "text": "A human evaluation on 20 sentences randomly selected from the test data indicates that, in terms of fluency and adequacy, their system is judged to outperform both Zhu et al. (2010) and Woodsend and Lapata (2011) systems."}
{"sent_id": "a8ba807b94f6f7ff4f7e77a9fcde35-C001-146", "intents": ["@BACK@"], "paper_id": "ABC_a8ba807b94f6f7ff4f7e77a9fcde35_2", "text": "To construct this bi-text, Zhu et al. (2010) extracted complex and simple sentences from EWKP and SWKP respectively and automatically aligned them using TF*IDF as a similarity measure."}
{"sent_id": "a8ba807b94f6f7ff4f7e77a9fcde35-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_a8ba807b94f6f7ff4f7e77a9fcde35_2", "text": "When compared against current state of the art methods (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012) , our model yields significantly simpler output that is both grammatical and meaning preserving."}
{"sent_id": "a8ba807b94f6f7ff4f7e77a9fcde35-C001-55", "intents": ["@DIF@"], "paper_id": "ABC_a8ba807b94f6f7ff4f7e77a9fcde35_2", "text": "While our semantic based approach naturally accounts for this by copying the phrase corresponding to the shared entity in both phrases, syntax based approach such as Zhu et al. (2010) and Woodsend and Lapata (2011) will often fail to appropriately reconstruct the shared phrase and introduce agreement mismatches because the alignment or rules they learn are based on syntax alone."}
{"sent_id": "a8ba807b94f6f7ff4f7e77a9fcde35-C001-56", "intents": ["@DIF@"], "paper_id": "ABC_a8ba807b94f6f7ff4f7e77a9fcde35_2", "text": "For instance, in example (2), Zhu et al. (2010) fails to copy the shared argument \"The judge\" to the second clause whereas Woodsend and Lapata (2011) learns a synchronous rule matching (VP and VP) to (VP."}
{"sent_id": "a8ba807b94f6f7ff4f7e77a9fcde35-C001-68", "intents": ["@DIF@"], "paper_id": "ABC_a8ba807b94f6f7ff4f7e77a9fcde35_2", "text": "By contrast, syntax based approaches (Zhu et al., 2010; Woodsend and Lapata, 2011) do not distinguish between optional and obligatory arguments."}
{"sent_id": "a8ba807b94f6f7ff4f7e77a9fcde35-C001-122", "intents": ["@USE@"], "paper_id": "ABC_a8ba807b94f6f7ff4f7e77a9fcde35_2", "text": "For an efficient implementation of EM algorithm, we follow the work of Yamada and Knight (2001) and Zhu et al. (2010) ; and build training graphs (Figure 2 ) from the pair of complex and simple sentence pairs in the training data."}
{"sent_id": "a8ba807b94f6f7ff4f7e77a9fcde35-C001-142", "intents": ["@USE@"], "paper_id": "ABC_a8ba807b94f6f7ff4f7e77a9fcde35_2", "text": "To evaluate performance, we compare our approach with three other state of the art systems using the test set provided by Zhu et al. (2010) and relying both on automatic metrics and on human judgments."}
{"sent_id": "a8ba807b94f6f7ff4f7e77a9fcde35-C001-145", "intents": ["@USE@"], "paper_id": "ABC_a8ba807b94f6f7ff4f7e77a9fcde35_2", "text": "The DRS-Based simplification model is trained on PWKP, a bi-text of complex and simple sentences provided by Zhu et al. (2010) ."}
{"sent_id": "a8ba807b94f6f7ff4f7e77a9fcde35-C001-151", "intents": ["@USE@"], "paper_id": "ABC_a8ba807b94f6f7ff4f7e77a9fcde35_2", "text": "We evaluate our model on the test set used by Zhu et al. (2010) namely, an aligned corpus of 100/131 EWKP/SWKP sentences."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "For example, Levy and Manning (2003) , Kübler (2005) , and Kübler et al. (2006) highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004; Kübler et al., 2006) ."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "Kübler et al. (2006) compares the Negra and TüBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from Kübler et al. (2006) ."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-31", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "2 While the focus of Kübler et al. (2006) is on comparing parsing results across corpora, Dubey (2004) focuses on improving parsing for Negra, including corpus-specific enhancements leading to better results."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-133", "intents": ["@BACK@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "In effect, single-word arguments in Negra -mainly pronouns and bare nouns -are not considered in the evaluation from Kübler et al. (2006) ."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-36", "intents": ["@MOT@", "@EXT@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "3 Addressing these issues, we resolve the apparent discrepancy between Kübler et al. (2006) and Dubey (2004) and establish a firm grammatical function comparison of Negra and TüBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-37", "intents": ["@DIF@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "Such an evaluation, which abstracts away from the specifics of the annotation schemes, shows that, in contrast to the claims made in Kübler et al. (2006) , the parsing results for PCFG parsers trained on these heterogeneous corpora are very similar."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-96", "intents": ["@DIF@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "Unlike Kübler et al. (2006) , which ignored edge labels on words, we incorporate all edge labels present in both corpora."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-140", "intents": ["@DIF@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "Contrary to the finding in Kübler et al. (2006) , the PAR-SEVAL evaluation does not echo the grammatical function label evaluation."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-169", "intents": ["@DIF@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "Shifting the focus to the grammatical function evaluation, we showed that a grammatical function evaluation based on phrasal arguments as provided by Kübler et al. (2006) is inadequate for comparing parsers trained on the Negra and TüBa-D/Z corpora."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-171", "intents": ["@DIF@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "The models trained on both corpora perform very similarly in the grammatical function evaluation, in contrast to the claims in Kübler et al. (2006) ."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-194", "intents": ["@DIF@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "In contrast to Kübler et al. (2006) a grammatical function evaluation on subjects, accusative objects, and dative objects establishes that Negra and TüBa-D/Z perform similarly when all types of words and phrases appearing as arguments are taken into consideration."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-87", "intents": ["@USE@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "Following Kübler et al. (2006) , only sentences with fewer than 35 words were used, which results in 20,002 sentences for Negra and 21,365 sentences for TüBa-D/Z. Because punctuation is not attached within the sentence in the corpus annotation, punctuation was removed."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-90", "intents": ["@USE@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "To remove discontinuities, we used the conversion program included with the Negra corpus annotation tools (Brants and Plaehn, 2000) , the same tool used in Kübler et al. (2006) , which raises non-head elements to a higher tree until there are no more discontinuities."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-129", "intents": ["@USE@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "Kübler et al. (2006) present the results shown in Table 3 for the parsing performance of the unlexicalized model of the Stanford Parser (Klein and Manning, 2002) ."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-138", "intents": ["@USE@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "The results are shown in Table 4 In contrast to the results for NP grammatical functions of Kübler et al. (2006) we saw in Table 3 , Negra and TüBa-D/Z perform quite similarly overall, with Negra slightly outperforming TüBa-D/Z for all types of arguments."}
{"sent_id": "366231b855f226f63d637e6b2e1667-C001-122", "intents": ["@SIM@"], "paper_id": "ABC_366231b855f226f63d637e6b2e1667_2", "text": "Thus, it is useful to per-7 Our experimental setup is designed to support a comparison between Negra and TüBa-D/Z for the three evaluation metrics and is intended to be comparable to the setup of Kübler et al. (2006) ."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-5", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "We also compare our model with an end-toend tree-based LSTM model (SPTree) by Miwa and Bansal (2016) and show that our model performs within 1% on entity mentions and 2% on relations."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-31", "intents": ["@SIM@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "In comparison to the dependency treebased LSTM model of Miwa and Bansal (2016) , our model performs within 1% on entities and 2% on relations on ACE05 dataset."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-78", "intents": ["@SIM@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "We formulate entity detection as a sequence labeling task using BILOU scheme similar to Li and Ji (2014) and Miwa and Bansal (2016) ."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-138", "intents": ["@SIM@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "We select the positive and more confident label similar to Miwa and Bansal (2016) ."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-162", "intents": ["@SIM@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "For the scope of this paper, we only use the entity head phrase similar to Li and Ji (2014) and Miwa and Bansal (2016) ."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-170", "intents": ["@SIM@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "In order to compare our system with the previous systems, we report micro F1-scores, Precision and Recall on both entities and relations similar to Li and Ji (2014) and Miwa and Bansal (2016) ."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-258", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "We also compare our model to an endto-end LSTM model by Miwa and Bansal (2016) which comprises of a sequence layer for entity extraction and a tree-based dependency layer for relation classification."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-165", "intents": ["@USE@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "We use the same data splits as Li and Ji (2014) and Miwa and Bansal (2016) such that there are 351 documents for training, 80 for development and the remaining 80 documents for the test set."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-192", "intents": ["@USE@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "1 We ran the system made publicly available by Miwa and Bansal (2016) , on ACE05 dataset for filling in the missing values and comparing our system with theirs at fine-grained level."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-218", "intents": ["@USE@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "In this section, we perform a fine-grained comparison of our model with respect to the SPTree (Miwa and Bansal, 2016) model."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "Miwa and Bansal (2016) , for example, propose an RNN comprised of a sequencebased long short term memory (LSTM) for entity identification and a separate tree-based dependency LSTM layer for relation classification using shared parameters between the two components."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-44", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "Recently, Miwa and Bansal (2016) proposed an end-to-end LSTM based sequence and treestructured model."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-225", "intents": ["@BACK@", "@FUT@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "Miwa and Bansal (2016) , in one of the ablation tests on ACE05 development set, show that their model can gain upto 2% improvement in recall by entity pretraining."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-231", "intents": ["@BACK@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "Miwa and Bansal (2016) also use additional features such as POS tags in addition to pretrained word embeddings at the input layer."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-139", "intents": ["@DIF@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "Multiple Relations Our approach to relation extraction is different from Miwa and Bansal (2016) ."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-140", "intents": ["@DIF@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "Miwa and Bansal (2016) present each pair of entities to their model for relation classification."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-261", "intents": ["@FUT@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "In future, we plan to explore pretraining methods for our model which were shown to improve recall on entity and relation performance by Miwa and Bansal (2016) ."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "Recent work has revisited these classic critiques through studies of modern neural architectures [10, 15, 3, 20, 22, 2, 6] , with a focus on the sequence-to-sequence (seq2seq) models used successfully in machine translation and other NLP tasks [32, 4, 36] ."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "After training, the aim is to execute, zero-shot, novel instructions such as \"walk around right after look twice.\" Previous studies show that seq2seq recurrent neural networks (RNN) generalize well when the training and test sets are similar, but fail catastrophically when generalization requires systematic compositionality [15, 3, 20] ."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-169", "intents": ["@BACK@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "On the \"add jump\" test set [15] , standard seq2seq modeling completely fails to generalize compositionally, reaching an average performance of only 0.03% correct (SD = 0.02)."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-263", "intents": ["@BACK@", "@DIF@", "@MOT@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "After learning how to \"dax,\" people understand how to \"dax twice,\" \"dax slowly,\" or even \"dax like there is no tomorrow.\" These abilities are central to language and thought yet they are conspicuously lacking in modern neural networks [15, 3, 20, 22, 2] ."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-277", "intents": ["@BACK@", "@FUT@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "Hybrid models could also address the challenge of generalizing to longer output sequences, a problem that continues to vex neural networks [15, 3, 28] including meta seq2seq learning."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-92", "intents": ["@EXT@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "The meta seq2seq architecture builds upon the seq2seq architecture from [15] that performed best across a range of SCAN evaluations."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-96", "intents": ["@USE@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "A greedy decoder is used since it is effective on SCAN's deterministic outputs [15] ."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-132", "intents": ["@USE@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "This experiment applies meta seq2seq learning to the SCAN task of adding a new primitive [15] ."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-134", "intents": ["@USE@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "The goal is to learn a new primitive instruction and use it compositionally, operationalized in SCAN as the \"add jump\" split [15] ."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-136", "intents": ["@USE@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "First, the original seq2seq problem from [15] is described."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-142", "intents": ["@USE@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "Following [15] , the critical \"jump\" demonstration is overrepresented in training to ensure it is learned."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-241", "intents": ["@USE@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "This experiment uses the SCAN \"length\" split [15] ."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-171", "intents": ["@SIM@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "This replicates the results from [15] which trained many seq2seq models, finding the best network performed at only 1.2% accuracy."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-210", "intents": ["@DIF@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "The standard seq2seq learner takes advantage of the augmented training to generalize better than in standard SCAN training (Experiment 4.3 and [15] ), achieving 12.26% accuracy (SD = 8.33) on the test instructions (with >99% accuracy during training)."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-18", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "However, recently the reverse process, i.e. the generation of texts from AMRs, has started to receive scholarly attention (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017) ."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "Motivated by this similarity, Pourdamghani et al. (2016) proposed an AMR-to-text method that organises some of these concepts and edges in a flat representation, commonly known as Linearisation."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "Once the linearisation is complete, Pourdamghani et al. (2016) map the flat AMR into an English sentence using a Phrase-Based Machine Translation (PBMT) system."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "In addition, Pourdamghani et al. (2016) use PBMT, which is devised for translation but also utilised in other NLP tasks, e.g. text simplification (Wubben et al., 2012; Štajner et al., 2015) ."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "To address this, Pourdamghani et al. (2016) look for special realisation component for names, dates and numbers in development and test sets and add them on the training set."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-66", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "Following the aligner of Pourdamghani et al. (2014) , Pourdamghani et al. (2016) clean an AMR by removing some nodes and edges independent of the context."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-181", "intents": ["@DIF@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "We do not use lexicalised reordering models as Pourdamghani et al. (2016) ."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-191", "intents": ["@DIF@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "The models of Flanigan et al. (2016) and Pourdamghani et al. (2016) were officially trained with 10,313 AMR-sentence pairs from the LDC2014T12 corpus, and with 36,521 AMR-sentence pairs from the LDC2016E25 in our study (as our models)."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-76", "intents": ["@SIM@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "After Compression, we flatten the AMR to serve as input to the translation step, similarly as proposed in Pourdamghani et al. (2016) ."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-196", "intents": ["@SIM@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "All the models with the preordering method in Linearisation Song et al. (2017) and introduce competitive results with Pourdamghani et al. (2016) ."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-207", "intents": ["@SIM@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "Our best model (PBMT-Delex+Compress+Preorder) presents competitive results to Pourdamghani et al. (2016) with the advantage that no technique is necessary to overcome data sparsity."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-221", "intents": ["@SIM@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "PBMT models trained on small data sets clearly outperform NMT ones, e.g. Konstas et al. (2017) reported 22.0 BLEU, whereas Pourdamghani et al. (2016) 's best model achieved 26.9 BLEU, and our best model performs comparably (26.8 BLEU)."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-225", "intents": ["@SIM@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "In such situations, our PBMT models, like Pourdamghani et al. (2016) , look appear to be a good alternative option."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-78", "intents": ["@USE@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "In a second step, also following Pourdamghani et al. (2016), we implemented a version of the 2-Step Classifier from Lerner and Petrov (2013) to preorder the elements from an AMR according to the target side."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-176", "intents": ["@USE@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "We compare BLEU scores for some of the AMRto-text systems described in the literature (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017) ."}
{"sent_id": "e4452ce844b74c35f257c916aae120-C001-210", "intents": ["@USE@"], "paper_id": "ABC_e4452ce844b74c35f257c916aae120_3", "text": "We note that the preordering success was expected, based on previous results (Pourdamghani et al., 2016) ."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-20", "intents": ["@DIF@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "The model improves over previous work on reference resolution applied to the same data (Iida et al., 2010; Iida et al., 2011) ."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-112", "intents": ["@DIF@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "These properties differ somewhat from the features for the Ling model presented in Iida et al. (2011) ."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-122", "intents": ["@DIF@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "Our Gaze properties are made up of these 4 properties, as opposed to the 14 features in Iida et al. (2011) ."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-160", "intents": ["@DIF@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "The SIUM model performs better than the combined approach of Iida et al. (2011) , and performs better than their separated model-when not including gaze (there is a significant difference between SIUM and the separated models for Ling+TaskSp, though (2011) SIUM only got one more correct than the separated model)."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-187", "intents": ["@DIF@", "@BACK@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "In contrast, previous work in RR (Iida et al., 2011; Chai et al., 2014 ) used a hand-coded concept-labeled semantic representation and checked if aspects of the RE match that of a particular object."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-199", "intents": ["@DIF@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "However, in the current work we observed that REs with pronouns were more difficult for the model to resolve than the model presented in Iida et al. (2011) ."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-200", "intents": ["@DIF@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "We surmise that SIUM had a difficult time grounding certain properties, as the Japanese pronoun sore can be used anaphorically or demonstratively in this kind of context (i.e., sometimes sore refers to previously-manipulated objects, or objects that are newly identified with a mouse pointer over them); the model presented in Iida et al. (2011) made more use of contextual information when pronouns were used, particularly in the combined model which incorporated gaze information, as shown above."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008) , in human-human interactive puzzle tasks (Iida et al., 2010; Iida et al., 2011) , in web browsing (Hakkani-tür et al., 2014) , and in a moving car where speakers look at objects in their vicinity (Misu et al., 2014) ."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "The corpora presented in Iida et al. (2011) and Spanger et al. (2012) are a collection of human/human interaction data where the participants collaboratively solved Tangram puzzles."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-82", "intents": ["@BACK@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "Iida et al. (2011) applied a support vector machine-based ranking algorithm (Joachims, 2002) to the task of resolving REs in this corpus."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-165", "intents": ["@BACK@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "Second, and more importantly, separated models means less feature confusion: in Iida et al. (2011) (Section 5.2) , the authors give a comparison of the most informative features for each model; task and gaze features were prominent for the pronoun model, whereas gaze and language features were prominent for the non-pronoun model."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-80", "intents": ["@USE@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "Further details of the corpus can be found in (Iida et al., 2011) ."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-92", "intents": ["@USE@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "In order to compare our results directly with those of Iida et al. (2011) , we provide our model with the same training and evaluation data, in a 10-fold cross-validation of the 1192 REs from 27 dialogues (the T2009-11 corpus in )."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-153", "intents": ["@USE@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "We compare non-incremental results to three evaluations performed in Iida et al. (2011) , namely when Ling is used alone, Ling+TaskSP used together, and Ling+TaskSp+Gaze."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-98", "intents": ["@SIM@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "3 We derive these properties from a representation of the scene; similar to how Iida et al. (2011) computed features to present to their classifier: namely Ling (linguistic features), TaskSp (task specific features), and Gaze (from SV only)."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-117", "intents": ["@SIM@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "Gaze Similar to Iida et al. (2011) , we consider gaze during a window of 1500ms before the onset of the RE."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-152", "intents": ["@EXT@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "Going beyond Iida et al. (2011) , our model computes a resolution hypothesis incrementally; for the performance of this aspect of the system we followed previously used metrics for evaluation : first correct: how deep into the RE does the model predict the referent for the first time? first final: how deep into the RE does the model predict the correct referent and keep that decision until the end? edit overhead: how often did the model unnecessarily change its prediction (the only necessary prediction happens when it first makes a correct prediction)?"}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "Lee et al. (2017; Lee et al. (2018) first introduced a neural mention detector as a part of their end-to-end coreference system; however, the system does not output intermediate mentions, hence the mention detector cannot be used by other coreference systems directly."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "The system has been later extended by Zhang et al. (2018) and Lee et al. (2018) ."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-212", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "The Lee et al. (2018) system is an extended version of the Lee et al. (2017) system, hence they share most of the network architecture."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-26", "intents": ["@USE@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-69", "intents": ["@USE@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "In HIGH RECALL mode we output mentions based on a fixed mention/word ratio λ; this is the same method used by Lee et al. (2018) ."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-72", "intents": ["@USE@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "Our first system is based on the mention detection part of the Lee et al. (2018) system."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-141", "intents": ["@USE@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "For the mention detection evaluation we use the Lee et al. (2018) system as baseline."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-143", "intents": ["@USE@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "For the coreference evaluation we use the state-of-the-art Lee et al. (2018) system as our baseline for the end-to-end system, and the Clark and Manning (2016a) system as our baseline for the pipeline system."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-148", "intents": ["@USE@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "For our first model (LEE MD) we use the default settings of Lee et al. (2018) ."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-167", "intents": ["@USE@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "For mention detection on the CONLL data set, we first take the best model from Lee et al. (2018) and use its default mention/token ratio (λ = 0.4) to output predicted mentions before coreference resolution."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-177", "intents": ["@USE@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "Evaluation on the CRAC data set 3 For the CRAC data set, we train the Lee et al. (2018) system end-to-end on the reduced corpus with singleton mentions removed and extract mentions from the system by set λ = 0.4."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-197", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "We first evaluate our BIAFFINE MD in combination with the end-to-end Lee et al. (2018) system."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-216", "intents": ["@USE@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "We further evaluated the Lee et al. (2018) system on the CRAC data set."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-217", "intents": ["@USE@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "We first train the original Lee et al. (2018) on the reduced version (with singletons removed) of the CRAC data set to create a baseline."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-32", "intents": ["@EXT@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "Thirdly, by using better mentions from our mention detector, we can improve the end-to-end Lee et al. (2018) system and the Clark and Manning (2016a) pipeline system by up to 0.7% and 1.7% respectively."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-144", "intents": ["@EXT@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "During the evaluation, we slightly modified the Lee et al. (2018) system to allow the system to take the mentions predicted by our model instead of its internal mention detector."}
{"sent_id": "6c4264bedb6683e909c1e530f22262-C001-178", "intents": ["@DIF@"], "paper_id": "ABC_6c4264bedb6683e909c1e530f22262_3", "text": "We then train our models with the same λ but on the full corpus, since our mention detectors naturally support both mention 3 As the Lee et al. (2018) system does not predict singleton mentions, the results on CRAC data set in Table 2 are evaluated without singleton mentions."}
{"sent_id": "af39041414dec545df878404328aab-C001-3", "intents": ["@EXT@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "In this paper, we propose to scale up discriminative training of (He and Deng, 2012) to train features with 150 million parameters, which is one order of magnitude higher than previously published effort, and to apply discriminative training to redistribute probability mass that is lost due to model pruning."}
{"sent_id": "af39041414dec545df878404328aab-C001-11", "intents": ["@EXT@", "@BACK@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "The maximum expected BLEU training of (He and Deng, 2012 ) is a recent effort towards this direction, and in this paper, we extend their work to a scaled-up task of discriminative training of the features of a strong hierarchical phrase-based model and confirm its effectiveness empirically."}
{"sent_id": "af39041414dec545df878404328aab-C001-20", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "Our contributions in this paper are two-folded: First of all, we scale up the maximum expected BLEU training proposed in (He and Deng, 2012) in a number of ways including using 1) a hierarchical phrase-based model, 2) a richer feature set, and 3) a larger training set with a much larger parameter set, resulting in more than 150 million parameters in the model being updated, which is one order magnitude higher than the phrase-based model reported in (He and Deng, 2012) ."}
{"sent_id": "af39041414dec545df878404328aab-C001-95", "intents": ["@EXT@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "We see this result as confirming the effectiveness of discriminative training but on a larger-scale task, adding to what was reported by (He and Deng, 2012) ."}
{"sent_id": "af39041414dec545df878404328aab-C001-126", "intents": ["@EXT@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "In this paper, we first extend the maximum expected BLEU training of (He and Deng, 2012) to train two features of a state-of-the-art hierarchical phrasebased system, namely: p(f |e) and p(e|f )."}
{"sent_id": "af39041414dec545df878404328aab-C001-79", "intents": ["@DIF@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "As shown, we are working with a stronger system than (He and Deng, 2012) , especially in terms of the number of parameters under consideration |θ|."}
{"sent_id": "af39041414dec545df878404328aab-C001-135", "intents": ["@DIF@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "We describe the process to simplify Eq. 1 to Eq. 2, which is omitted in (He and Deng, 2012) ."}
{"sent_id": "af39041414dec545df878404328aab-C001-28", "intents": ["@USE@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "where B(Ê 1 ...Ê N ) is the BLEU score of the concatenated hypothesis of the entire training data, following (He and Deng, 2012) ."}
{"sent_id": "af39041414dec545df878404328aab-C001-40", "intents": ["@USE@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "To alleviate overfitting, we introduce KL-distance based reguralization as in (He and Deng, 2012) ."}
{"sent_id": "af39041414dec545df878404328aab-C001-43", "intents": ["@USE@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "The optimization algorithm is based on the Extended Baum Welch (EBW) (Gopalakrishnan et al., 1991) as derived by (He and Deng, 2012) ."}
{"sent_id": "af39041414dec545df878404328aab-C001-73", "intents": ["@USE@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "Following (He and Deng, 2012) , we focus on discriminative training of p(f |e) and p(e|f ), which in practice affects around 150 million of parameters; hence the title."}
{"sent_id": "af39041414dec545df878404328aab-C001-78", "intents": ["@USE@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "Table 1 compares the key components of our baseline system with that of (He and Deng, 2012) ."}
{"sent_id": "af39041414dec545df878404328aab-C001-88", "intents": ["@USE@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "For each τ , we run several iterations of discriminative training where each iteration involves one simultaneous update of p(f |e) and p(e|f ) according to Eq. 4, followed by one update of λ via PRO (as in (He and Deng, 2012) )."}
{"sent_id": "af39041414dec545df878404328aab-C001-127", "intents": ["@USE@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "Compared to (He and Deng, 2012) , we apply the algorithm to a strong baseline that is trained on a bigger parallel corpora and comes with a richer feature set."}
{"sent_id": "af39041414dec545df878404328aab-C001-93", "intents": ["@SIM@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "This improvement is in the same ballpark as in (He and Deng, 2012 ) though on a scaledup task."}
{"sent_id": "af39041414dec545df878404328aab-C001-129", "intents": ["@SIM@"], "paper_id": "ABC_af39041414dec545df878404328aab_3", "text": "Our experiments show that discriminative training these two features (out of 50) gives around 0.40 BLEU point improvement, which is consistent with the conclusion of (He and Deng, 2012) but in a much larger-scale system."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-29", "intents": ["@EXT@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "• We present a novel adaptation of graph-based semi-supervised learning (Zhu et al., 2003) to the sentiment analysis domain, extending past supervised learning work by Pang and Lee (2005) ;"}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-47", "intents": ["@USE@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "We experiment with positive-sentence percentage (PSP) based similarity which is proposed in (Pang and Lee, 2005) , and mutual-information modulated word-vector cosine similarity."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-49", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "2. Optionally, we are given numerical rating predictionsŷ l+1 , . . . ,ŷ n on the unlabeled documents from a separate learner, for instance -insensitive support vector regression (Joachims, 1999; Smola and Schölkopf, 2004) used by (Pang and Lee, 2005) ."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-117", "intents": ["@USE@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "We performed experiments using the movie review documents and accompanying 4-class (C = {0, 1, 2, 3}) labels found in the \"scale dataset v1.0\" available at http://www.cs.cornell.edu/people/pabo/ movie-review-data/ and first used in (Pang and Lee, 2005) ."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-128", "intents": ["@USE@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "We compare our graph-based semi-supervised method with two previously studied methods: regression and metric labeling as in (Pang and Lee, 2005) ."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-142", "intents": ["@USE@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "PSP i is defined in (Pang and Lee, 2005) as the percentage of positive sentences in review x i ."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-108", "intents": ["@SIM@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "Before moving on to experiments, we note an interesting connection to the supervised learning method in (Pang and Lee, 2005) , which formulates rating inference as a metric labeling problem (Kleinberg and Tardos, 2002) ."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-140", "intents": ["@SIM@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "For consistency with (Pang and Lee, 2005) , supervised metric labeling results with this measure are reported under 'reg+PSP."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-144", "intents": ["@SIM@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "We identified positive sentences using SVM instead of Naïve Bayes, but the trend is qualitatively the same as in (Pang and Lee, 2005) ."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-159", "intents": ["@SIM@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "In an attempt to reproduce the findings in (Pang and Lee, 2005) , we tuned c, α with cross validation."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-112", "intents": ["@DIF@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "It corresponds exactly to the supervised, non-transductive version of metric labeling, except we use squared difference while (Pang and Lee, 2005) used absolute difference."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-161", "intents": ["@DIF@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "Note that we learned a single set of shared parameters for all authors, whereas (Pang and Lee, 2005) tuned k and α on a per-author basis."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-166", "intents": ["@BACK@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "(Pang and Lee, 2005) found that their metric labeling method, when applied to the 4-class data we are using, was not statistically better than regression, though they observed some improvement for authors (c) and (d)."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-218", "intents": ["@BACK@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "For example, several positive sentences followed by a few concluding negative sentences could indicate an overall negative review, as observed in prior work (Pang and Lee, 2005) ."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-26", "intents": ["@USE@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "We report experimental results for two answer selection datasets: (1) InsuranceQA (Feng et al., 2015) 1 , a recently released large-scale non-factoid QA dataset from the insurance domain."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-38", "intents": ["@USE@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "The approaches for non-factoid question answering generally pursue the solution on the following directions: Firstly, the question and answer representations are learned and matched by certain similarity metrics (Feng et al., 2015; Yu et al., 2014; dos Santos et al., 2015) ."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-71", "intents": ["@USE@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "Following the same ranking loss in (Feng et al., 2015; Weston et al., 2014; Hu et al., 2014) , we define the training objective as a hinge loss."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-78", "intents": ["@USE@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "As discussed in (Feng et al., 2015) , this is reasonable, because for a shared layer network, the corresponding elements in question and answer vectors represent the same biLSTM outputs."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-122", "intents": ["@USE@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "Having described a number of models in the previous section, we evaluate the proposed approaches on the insurance domain dataset, InsuranceQA, provided by Feng et al. (2015) ."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-125", "intents": ["@USE@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "One can see the details of InsuranceQA data in (Feng et al., 2015) ."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-208", "intents": ["@USE@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "We implemented the Architecture II in (Feng et al., 2015) from scratch."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-42", "intents": ["@DIF@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "There are two major differences between our approaches and the work in (Feng et al., 2015) : (1) The architectures developed in (Feng et al., 2015) are only based on CNN, whereas our models are based on bidirectional LSTMs, which are more capable of exploiting long-range sequential context information."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-44", "intents": ["@DIF@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "(2) Feng et al. (2015) tackle the question and answer independently, while the proposed structures develop an efficient attentive models to generate answer embeddings according to the question."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-173", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "Row F shared a highly analogous CNN structure with Architecture II in (Feng et al., 2015) , except that the later used a shallow hidden layer to transform the word embeddings into the input of CNN structure, while Row F take the output of biLSTM as CNN input."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-179", "intents": ["@DIF@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "Compared to Architecture II in (Feng et al., 2015) , which involved a large number of CNN filters, (H) model also has fewer parameters."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-219", "intents": ["@DIF@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "Model (D), which combines the ideas of Model (B) and (C), achieves the performance, competitive to the best baselines on MAP, and 2∼4% improvement on MRR compared to (Wang & Nyberg, 2015) and (Feng et al., 2015) ."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-82", "intents": ["@SIM@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "The structure of CNN in this work is similar to the one in (Feng et al., 2015) , as shown in Figure 2 ."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-157", "intents": ["@BACK@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "Architecture-II in (Feng et al., 2015) : Instead of using LSTM, a CNN model is employed to learn a distributed vector representation of a given question and its answer candidates, and the answers are scored by cosine similarity with the question."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-162", "intents": ["@BACK@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "This is the model which achieved the best performance in (Feng et al., 2015) ."}
{"sent_id": "f24dde456e02fdb8e65799685275d2-C001-209", "intents": ["@BACK@"], "paper_id": "ABC_f24dde456e02fdb8e65799685275d2_3", "text": "Wang & Nyberg (2015) and Feng et al. (2015) are the best baselines on MAP and MRR respectively."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "There are several recently proposed VQA datasets on real images e.g. [2, 24, 25, 14, 29] , as well as on abstract scenes [2] ."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "For example, in the VQA dataset (with images from MS COCO) [2] , the most common sport answer \"tennis\" is the correct answer for 41% of the questions starting with \"What sport is\"."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "Almost half of all questions in the VQA datatset [2] can be answered correctly by a neural network that ignores the image completely and uses the question alone, relying on systematic regularities in the kinds of questions that are asked and what answers they tend to have."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "Recent work has proposed several datasets and methods to promote research on the task of visual question answering [15, 4, 33, 24, 2, 25, 14, 29] , ranging from constrained settings [15, 24, 29] to freeform natural language questions and answers [4, 33, 2, 25, 14] ."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "A number of recent papers [2, 14, 25, 29] proposed neural network models for VQA composing LSTMs (for questions) and CNNs (for images)."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "[2] introduced a large-scale dataset for free-form and open-ended VQA, along with several natural VQA models."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-49", "intents": ["@USE@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "Although our approach of visual verification is applicable to real images (more discussion in Sec. 6), we choose to use abstract images [2, 3, 39, 38, 40] as a test bed because abstract scene images allow us to focus on high-level semantic reasoning."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-51", "intents": ["@USE@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "Our main contributions are as follows: (1) We balance the existing abstract binary VQA dataset [2] by creating complementary scenes so that all questions 1 have an answer of \"yes\" for one scene and an answer of \"no\" for another closely related scene."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-91", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "We first describe the VQA dataset for abstract scenes collected by [2] ."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-182", "intents": ["@USE@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "Our model is an ensemble of two similar models-Q-model and Tuple-model, whose common architecture is inspired from a recently proposed VQA approach [2] ."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-244", "intents": ["@USE@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "Note that the accuracy is higher than 50% because this is not binary classification accuracy but the VQA accuracy [2] , which provides partial credit when there is inter-human disagreement in the ground-truth answers."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-212", "intents": ["@SIM@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "SOTA Q+Tuple+H-IMG: This VQA model has a similar architecture as our approach, except that it uses holistic image features (H-IMG) that describe the entire scene layout, instead of focusing on specific regions in the scene as determined by P and S. This model is analogous to the state-ofthe-art models presented in [2, 25, 29, 14] , except applied to abstract scenes."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-247", "intents": ["@DIF@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "Specifically, our model gives improvement in performance relative to the state-of-the-art VQA model from [2] (Q+Tuple+H-IMG), showing that attending to relevant regions and describing them in detail helps, as also seen in Sec. 5.2."}
{"sent_id": "809ad258132199e3eae8add5d1bfdf-C001-259", "intents": ["@DIF@"], "paper_id": "ABC_809ad258132199e3eae8add5d1bfdf_3", "text": "We observe that our model trained on the balanced dataset performs the best. And again, our model that focuses on relevant regions in the image to answer the question outperforms the state-of-the-art approach of [2] (Q+Tuple+H-IMG) that does not model attention."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "In several cases, neural network approaches exceeded the previous state of the art on essay scoring (Taghipour and Ng, 2016) ."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "For example, Taghipour and Ng (2016) explore simple LSTM and CNN-based architectures with regression and evaluate on the ASAP-AES data."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-31", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "On the other hand, recurrent neural networks may derive some of their predictive power in AES from more redundant signals in longer input sequences (as sketched by Taghipour and Ng (2016) )."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-154", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "We hypothesized that the mean-over-time layer is helpful when the input consists of longer responses (as was the case for the essay data in Taghipour and Ng (2016) )."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-183", "intents": ["@BACK@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "\"Baseline\" is the baseline non-neural system. \"T&N best\" is the best-performing parameter set in Taghipour and Ng (2016) : tuned embeddings (here, GLOVE 100 dimensions), 300-dimensional LSTM, unidirectional, mean-over-time layer."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-200", "intents": ["@BACK@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "\"Baseline\" is the baseline non-neural system. \"T&N best\" is the best-performing parameter set in Taghipour & Ng (2016) : tuned embeddings (here, GLOVE 100 dimensions), 300-dimensional LSTM, unidirectional, mean-over-time layer."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-33", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "To explore the effectiveness of neural network architectures on SAS, we use the basic architecture and parameters of Taghipour and Ng (2016) on three publicly available short answer datasets: ASAP-SAS (Shermis, 2015), Powergrading (Basu et al., 2013) , and SRA (Dzikovska et al., 2016 (Dzikovska et al., , 2013 ."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-35", "intents": ["@USE@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "We explore how well the optimal parameters for AES from Taghipour and Ng (2016) fare on these datasets, and whether different architectures and parameters perform better on the SAS task."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-82", "intents": ["@USE@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "We took the best parameter set from Taghipour and Ng (2016) as our reference since it performed best on the AES data."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-104", "intents": ["@USE@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "We work with the basic neural network architecture explored by Taghipour and Ng (2016) (Figure  4 )."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-110", "intents": ["@USE@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "We use the same attention mechanism employed in Taghipour and Ng (2016) , which involves taking the dot product of each LSTM hidden state and a vector that is trained with the network."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-115", "intents": ["@USE@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "The text is lightly preprocessed as input to the neural networks following Taghipour and Ng (2016) ."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-142", "intents": ["@USE@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "Following Taghipour and Ng (2016) , for our parameter exploration experiments on the development set, we report the best performance across epochs."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-148", "intents": ["@USE@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "Our focus in this section is comparing different architecture and parameter choices for the neural networks with the best parameters from Taghipour and Ng (2016) ."}
{"sent_id": "a5f33403d23cdc0532547266f1841a-C001-19", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_a5f33403d23cdc0532547266f1841a_3", "text": "On the other hand, Noraset et al. (2017) attempted to generate a definition of a word from its word embedding induced from massive text, followed by Gadetsky et al. (2018) that refers to a local context to define a polysemous word with a local context by choosing relevant dimensions of their embeddings."}
{"sent_id": "a5f33403d23cdc0532547266f1841a-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_a5f33403d23cdc0532547266f1841a_3", "text": "\" Previous work on the definition generation task (Noraset et al., 2017) has shown that global contexts can be useful clues when generating definitions of unknown words."}
{"sent_id": "a5f33403d23cdc0532547266f1841a-C001-190", "intents": ["@BACK@"], "paper_id": "ABC_a5f33403d23cdc0532547266f1841a_3", "text": "Recently, Noraset et al. (2017) introduced a task of generating a definition sentence of a word from its pre-trained embedding."}
{"sent_id": "a5f33403d23cdc0532547266f1841a-C001-197", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_a5f33403d23cdc0532547266f1841a_3", "text": "Their model does not take advantage of global contexts (word embeddings induced from massive text) as was used in Noraset et al. (2017) ."}
{"sent_id": "a5f33403d23cdc0532547266f1841a-C001-25", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_a5f33403d23cdc0532547266f1841a_3", "text": "Considering various contexts where we need definitions of phrases, we evaluated our method with four datasets including WordNet (Noraset et al., 2017) for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slangs, and a newlycreated Wikipedia dataset for entities."}
{"sent_id": "a5f33403d23cdc0532547266f1841a-C001-54", "intents": ["@USE@"], "paper_id": "ABC_a5f33403d23cdc0532547266f1841a_3", "text": "To incorporate the different types of contexts, we propose to use a GATE function (Noraset et al., 2017) to dynamically control how the global and local contexts influence the generation of the description."}
{"sent_id": "a5f33403d23cdc0532547266f1841a-C001-64", "intents": ["@USE@"], "paper_id": "ABC_a5f33403d23cdc0532547266f1841a_3", "text": "In order to capture prefixes and suffixes in X trg , we construct character-level CNNs (Eq. (5)) following (Noraset et al., 2017) ."}
{"sent_id": "a5f33403d23cdc0532547266f1841a-C001-65", "intents": ["@USE@"], "paper_id": "ABC_a5f33403d23cdc0532547266f1841a_3", "text": "Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following Noraset et al. (2017) , we set the kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg ."}
{"sent_id": "a5f33403d23cdc0532547266f1841a-C001-67", "intents": ["@USE@"], "paper_id": "ABC_a5f33403d23cdc0532547266f1841a_3", "text": "We achieve this by two different strategies proposed by Noraset et al. (2017) ."}
{"sent_id": "a5f33403d23cdc0532547266f1841a-C001-97", "intents": ["@USE@"], "paper_id": "ABC_a5f33403d23cdc0532547266f1841a_3", "text": "Datasets To evaluate our model on the word description task on WordNet, we followed Noraset et al. (2017) and extracted data from WordNet 7 using the dict-definition 8 toolkit."}
{"sent_id": "a5f33403d23cdc0532547266f1841a-C001-113", "intents": ["@USE@"], "paper_id": "ABC_a5f33403d23cdc0532547266f1841a_3", "text": "We implemented four methods including three baselines: (1) Global, (2) Local, (3) I-Attention, and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the strongest model (S + G + CH) in (Noraset et al., 2017) ."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-37", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "Sagae and Lavie (2005) and Wang et al. (2006) only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-45", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "Wang et al. (2006) give a detailed example showing how a segmented and POS-tagged sentence can be incrementally processed using the shift-reduce actions to produce a binary tree."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "In the deterministic parser of Wang et al. (2006) , the highest scoring action predicted by the classifier may prevent a valid binary tree from being built."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-107", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "However, Wang et al. (2006) used a polynomial kernel function with an SVM and did not manually create feature combinations."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-120", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "Wang et al. (2006) used a range of other features, including rhythmic features of S 0 and S 1 (Sun and Jurafsky, 2003) , features from the most recently found node that is to the left or right of S 0 and S 1 , the number of words and the number of punctuations in S 0 and S 1 , the distance between S 0 and S 1 and so on."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-17", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "We apply the same shift-reduce procedure as Wang et al. (2006) , but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-25", "intents": ["@USE@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "The shift-reduce process used by our beam-search decoder is based on the greedy shift-reduce parsers of Sagae and Lavie (2005) and Wang et al. (2006) ."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-106", "intents": ["@USE@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "The context S 0 , S 1 , S 2 , S 3 and N 0 , N 1 , N 2 , N 3 for the feature templates is taken from Wang et al. (2006) ."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-141", "intents": ["@USE@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003) , Xiong et al. (2005) , Bikel (2004), Chiang and Bikel (2002) , the SVM model from Wang et al. (2006) and the ensemble system from Wang et al. (2006) , and the parser of this paper, respectively."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-171", "intents": ["@USE@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "Our parser is based on the shift-reduce parsing process from Sagae and Lavie (2005) and Wang et al. (2006) , and therefore it can be classified as a transition-based parser (Nivre et al., 2006 )."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-142", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "Our parser gave comparable accuracies to the SVM and ensemble models from Wang et al. (2006) ."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-172", "intents": ["@DIF@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "An important difference between our parser and the Wang et al. (2006) parser is that our parser is based on a discriminative learning model with global features, whilst the parser from Wang et al. (2006) is based on a local classifier that optimizes each individual choice."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-109", "intents": ["@SIM@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "The \"Bracket\" row shows bracket-related features, which were inspired by Wang et al. (2006) ."}
{"sent_id": "e9a7e0d6d09fb2a2dd1972d6d16682-C001-138", "intents": ["@SIM@"], "paper_id": "ABC_e9a7e0d6d09fb2a2dd1972d6d16682_3", "text": "The rows represent the model from Bikel and Chiang (2000) , Bikel (2004) , the SVM and ensemble models from Wang et al. (2006) , and our parser, respectively."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-12", "intents": ["@USE@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "In this paper we aim to improve the state-of-the-art for the task of learning a TAG supertagger from an annotated treebank (Kasai et al., 2018) ."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-47", "intents": ["@USE@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "For our baseline supertagging model we use the state-of-the-art model that currently has the highest accuracy on the Penn treebank dataset (Kasai et al., 2018) ."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-48", "intents": ["@USE@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "For the supertagging model the main contribution of Kasai et al. (2018) was two-fold: the first was to add a character CNN for modeling word embeddings using subword features, and the second was to add highway connections to add more layers to a standard bidirectional LSTM."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-66", "intents": ["@USE@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "(Kasai et al., 2018) we use two components in the word embedding:"}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-67", "intents": ["@USE@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "• a 30-dimensional character level embedding vector computed using a char-CNN which captures the morphological information (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016; Kasai et al., 2018) ."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-78", "intents": ["@USE@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "For the hyperparameters, we use the settings in Kasai et al. (2018) in order to ensure a fair comparison."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-141", "intents": ["@USE@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "In our case, because we re-use the same training set for multi-task learning, we have made sure our experimental settings exactly match the previous best state-of-the-art method for supertagging (Kasai et al., 2018) and we use the same pre-trained word embeddings to ensure a fair comparison."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-170", "intents": ["@USE@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "We use the dataset that has been widely used by previous work in supertagging and TAG parsing (Bangalore et al., 2009; Chung et al., 2016; Friedman et al., 2017; Kasai et al., , 2018 ."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-24", "intents": ["@DIF@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "Our experimental results show that our novel multi-task learning framework leads to a new state-of-the-art accuracy score of 91.39% for TAG supertagging on the Penn Treebank dataset (Marcus et al., 1993; Chen et al., 2006) which is a significant improvement over the previous multi-task result for supertagging that combines supertagging with graph-based parsing (Kasai et al., 2018) ."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-73", "intents": ["@DIF@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "Unlike (Kasai et al., 2018) we do not use predicted part of speech (POS) tags as part of the input sequence."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-79", "intents": ["@DIF@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "Unlike (Kasai et al., 2018) we do not use highway connections in our model."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-199", "intents": ["@DIF@", "@UNSURE@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "All of those words are Kasai et al. (2018) refers to highway connections, and POS refers to the use of predicted part-of-speech tags as inputs."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "Neural linear-time transition based parsers are still not accurate enough to compete with the state-of-the-art supertagging models or parsers that use supertagging as the initial step (Chung et al., 2016; Kasai et al., 2018) ."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-216", "intents": ["@BACK@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "Neural network based supertagging models in TAG (Kasai et al., 2018) and CCG (Xu Lewis et al., 2016; Xu, 2016; Vaswani et al., 2016) have shown substantial improvement in performance, but the supertagging models are all quite similar as they all use a bi-directional RNN feeding into a prediction layer."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-219", "intents": ["@BACK@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "(Kasai et al., 2018) combines supertagging with parsing which does provide state-of-the-art accuracy but at the expense of computational complexity."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-220", "intents": ["@BACK@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "extends the BiLSTM model with predicted part-of-speech tags and suffix embeddings as inputs, then Kasai et al. (2018) further extends the BiLSTM model with highway connection as well as character CNN as input, and jointly train the supertagging model with parsing model and this work had the state-of-the-art accuracy before our paper on the Penn treebank dataset."}
{"sent_id": "0c3f9588b6f587d04c286384ca24e0-C001-50", "intents": ["@EXT@"], "paper_id": "ABC_0c3f9588b6f587d04c286384ca24e0_3", "text": "Another extension to the standard sequence prediction model in Kasai et al. (2018) was to combine supertagging with graph-based parsing."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "The issue is an open research area in computer vision and machine learning [1, 2, 3, 4, 5, 6] ."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "In recent years, recurrent neural networks (RNNs) implemented by long short-term memory (LSTM) especially show good performances in sequence data processing and they are widely used as decoders to generate a natural language description from an image in many methods [3, 4, 5, 6, 7] ."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "High-performance approaches on convolutional neural networks (CNNs) have been proposed [8, 9] , which are employed to represent the input image with a feature vector for the caption generation [3, 4, 5] ."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "High-level semantic concepts of the image are effective to describe a unique situation and a relation between objects in an image [4, 10] ."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "Extracting specific arXiv:1807.09434v1 [cs.CV] 25 Jul 2018 semantic concepts encoded in an image, and applying them into RNN network has improved the performance significantly [4] ."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "Combinations of CNNs and RNNs have been widely used for the image captioning networks [1, 2, 3, 4, 12, 13] ."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-48", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "Gan et al. [4] proposed Semantic Concept Network (SCN) integrating semantic concept to a LSTM network."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-122", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "In the perspective of vocabulary size, Gan [4] and Fang [12] selected 1000 words and Wu [13] selected 256 words, respectively."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-75", "intents": ["@USE@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "We used SCN-LSTM [4] as a decoder which is a tag integrated network."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-88", "intents": ["@USE@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "Most of the previous methods constituted semantic information, that was a ground truth attribute, as a binary form [4, 12, 13, 19] ."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-180", "intents": ["@USE@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "We use the public implementation [30] of this method opened by Gan who is the author of the published paper [4] ."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-188", "intents": ["@USE@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "We average inferred probability for 5 identical SCN-LSTM model as [4] did."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-264", "intents": ["@USE@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "In the experiment, we compared our method with SCN [4, 30] that uses extracted tags according to their semantic concept detection method."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-81", "intents": ["@SIM@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "Similar to [4, 13, 18] , the objective function is composed of the conditional log-likelihood on the image feature and the attribute as"}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-179", "intents": ["@SIM@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "SCN-LSTM training procedure generally follows [4] except for the dimension of the input attribute vector."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-153", "intents": ["@DIF@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "We note that our network does not contain softmax as a final layer, different from other attribute predictors described in previous papers [4, 13] ."}
{"sent_id": "33096f1e855d23046cb4cbfe95eef0-C001-192", "intents": ["@DIF@"], "paper_id": "ABC_33096f1e855d23046cb4cbfe95eef0_3", "text": "The output attribute of previous methods [4, 12, 13, 19] represent probabilities, on the other hand, that of the proposed method are the distinctiveness score itself."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-19", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "Already many sentence alignment techniques have been implemented for some languages pairs such as English-French (Gale and Church, 1993; Brown et al., 1991; Chen, 1993; Braune and Fraser 2010; Lamraoui and Langlais, 2013) , English-German (Gale and Church, 1993) English-Chinese (Wu, 1994; Chuang and Yeh, 2005) and Hungarian-English (Varga et al., 2005; Tóth et al., 2008) ."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "While the parameters such as mean and variance for Gale and Church's (1993) method are considered language independent for European languages, tuning these for non-'European language pairs has improved results (Zotti et al, 2014) ."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "The method used by Wu (1994) is a modification of Gale and Church's (1993) length-based statistical method for the task of aligning English with Chinese."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "The method by Chen (1993) is a word-correspondence-based model that gives a better accuracy than length based methods, however, it was reported to be much slower than the algorithms of Brown et al., (1991) and Gale and Church (1993) ."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-88", "intents": ["@BACK@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "According to Gale and Church (1993) a considerably large parallel corpus having a small error percentage can be built without lexical constraints."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-151", "intents": ["@BACK@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "Most of the above methods (Gale and Church, 1993; Brown et al., 1991; Chen and S.F, 1993; Braune and Fraser, 2010) have been first used for English and French sentence alignment."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-166", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "Even after changing the parameters for Sinhala and Tamil in the Gale and Church (1993) method, we obtained a comparatively low precision because this method does not only look at one to one alignments but also one to zero, many to one, one to many or many to many alignments."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-26", "intents": ["@USE@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "The highest F-measure value of 0.791 was obtained for Varga et al.'s (2005) Hunalign method, the hybrid method that combined the use of a bilingual dictionary with the statistical method by Gale and Church (1993) ."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-97", "intents": ["@USE@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "This research used the method proposed by Gale and Church (1993) citing the close linguistic similarities between languages of these pairs, causing parallel sentences to be of similar lengths."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-129", "intents": ["@USE@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "Constrained by the available resources, we compared methods by Gale and Church (1993) , Moore (2002) , Varga et al. (2005) , Braune and Fraser (2010) , Lamraoui and Langlais (2013) , and Sennrich and Volk (2011) ."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-133", "intents": ["@USE@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "The mean and variance for the number of Tamil characters per Sinhala was found and these values were used for the Gale and Church's (1993) method."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-161", "intents": ["@USE@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "We used Gale and Church (1993) method even though we could not align the paragraphs before aligning the sentences, due the dissimilarities among the text converted from pdfs."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-28", "intents": ["@USE@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "Given our interest in the analysis of multi-party dialogues, we used the STAC corpus of multiparty chats, an initial version of which is described in (Afantenos et al., 2015; Perret et al., 2016) ."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-66", "intents": ["@USE@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "To compare our approach to earlier efforts, we also used the corpus from (Perret et al., 2016) ."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-68", "intents": ["@USE@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "The corpus from (Perret et al., 2016) is an early version of a \"linguistic only\" version of the STAC corpus."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-70", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "3 It also contains quite a few errors; for example, about 60 stories in the (Perret et al., 2016) dataset have no discourse structure in them at all and consist of only one DU."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-84", "intents": ["@USE@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "3. Following (Muller et al., 2012; Perret et al., 2016) we \"flatten\" CDUs by connecting all relations incoming or outgoing from a CDU to the \"head\" of the CDU, or its first DU."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-86", "intents": ["@USE@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "We also performed these operations on our version of the linguistic only corpus used by (Perret et al., 2016) ."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-151", "intents": ["@USE@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "As in previous work (Muller et al., 2012; Afantenos et al., 2015; Perret et al., 2016) , we use the Maximum Spanning Tree (MST) algorithm, and a variation thereof, to ensure that the dialogue structures predicted conform to some more general structural principle."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-175", "intents": ["@USE@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "We also implemented BERT+LogReg*, a learning algorithm that uses BERT's encodings together with a Logistic Regression classifier trained on STAC's gold data with handcrafted features from (Afantenos et al., 2015) and used in (Perret et al., 2016) ."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-188", "intents": ["@USE@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "Finally, we wanted to see how GEN and our other models fared on our version of the (Perret (Perret et al., 2016) data set, GEN has higher scores than LogReg*'s local model; but with a decoding mechanism similar to that reported in (Perret et al., 2016) , Lo-gReg*'s global model significantly improves over the GEN's."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-75", "intents": ["@DIF@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "report significant error rates in annotation on the earlier versions of the STAC corpus and that the current linguistic only corpus of STAC offers an improvement over the (Perret et al., 2016) corpus."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-193", "intents": ["@DIF@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "Note, however, that even on the (Perret et al., 2016) data set, the MST decoding mechanism provided LogReg* only a boost of 12 F1 points, as seen in Table 4 , which is significantly lower than what is reported in (Perret et al., 2016) ."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-74", "intents": ["@SIM@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "The dataset from (Perret et al., 2016) is similar to our linguistic only STAC corpus but is still substantially different and degraded in quality."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-105", "intents": ["@BACK@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "LFs also exploit information about the DUs' linguistic or non-linguistic status, the dialogue acts they express, their lexical content, grammatical category and speaker, and the distance between them-features also used in supervised learning methods (Perret et al., 2016; Afantenos et al., 2015; Muller et al., 2012) ."}
{"sent_id": "46faad9d86cda118df5eb9c1e7df65-C001-156", "intents": ["@EXT@"], "paper_id": "ABC_46faad9d86cda118df5eb9c1e7df65_3", "text": "Since SDRT structures can contain nodes with multiple incoming relations, i.e. are not always tree-like, we altered the MST algorithm in the manner of (Muller et al., 2012; Afantenos et al., 2015; Perret et al., 2016) , forcing the MST to include all high-probability incoming relations which do not create cycles."}
{"sent_id": "d6d8f08147e45acc0a61692abb37a9-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_d6d8f08147e45acc0a61692abb37a9_3", "text": "[9] followed the method of [8] , but tried to resolve the ambiguous relative problem by using just unambiguous relatives."}
{"sent_id": "d6d8f08147e45acc0a61692abb37a9-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_d6d8f08147e45acc0a61692abb37a9_3", "text": "Another difference from [8] is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words instead of International Roget's Thesaurus."}
{"sent_id": "d6d8f08147e45acc0a61692abb37a9-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_d6d8f08147e45acc0a61692abb37a9_3", "text": "However, the evaluation was conducted on a small part of senses of the target words like [8] ."}
{"sent_id": "d6d8f08147e45acc0a61692abb37a9-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_d6d8f08147e45acc0a61692abb37a9_3", "text": "Like [8] , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words."}
{"sent_id": "d6d8f08147e45acc0a61692abb37a9-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_d6d8f08147e45acc0a61692abb37a9_3", "text": "They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as [8] and [9] ."}
{"sent_id": "d6d8f08147e45acc0a61692abb37a9-C001-25", "intents": ["@SIM@"], "paper_id": "ABC_d6d8f08147e45acc0a61692abb37a9_3", "text": "[8] introduced the relative based method using International Roget's Thesaurus as a hierarchical lexical database."}
{"sent_id": "d6d8f08147e45acc0a61692abb37a9-C001-61", "intents": ["@DIF@"], "paper_id": "ABC_d6d8f08147e45acc0a61692abb37a9_3", "text": "Our method makes use of ambiguous relatives as well as unambiguous relatives unlike [9] and hence overcomes the shortage problem of relatives and also reduces the problem of ambiguous relatives in [8] by handling relatives separately instead of putting example sentences of the relatives together into a pool."}
{"sent_id": "d6d8f08147e45acc0a61692abb37a9-C001-152", "intents": ["@DIF@"], "paper_id": "ABC_d6d8f08147e45acc0a61692abb37a9_3", "text": "Hence, we may have an idea that our method handles relatives and in particular ambiguous relatives more effectively than [8] and [9] ."}
{"sent_id": "d6d8f08147e45acc0a61692abb37a9-C001-157", "intents": ["@DIF@"], "paper_id": "ABC_d6d8f08147e45acc0a61692abb37a9_3", "text": "When evaluated on the same nouns of the lexical sample task, our proposed method achieved 47.26%, and the method of [8] 45.61%, and the method of [9] 38.03%."}
{"sent_id": "d6d8f08147e45acc0a61692abb37a9-C001-174", "intents": ["@DIF@"], "paper_id": "ABC_d6d8f08147e45acc0a61692abb37a9_3", "text": "Also our method more correctly disambiguates senses than [8] and [9] ."}
{"sent_id": "d6d8f08147e45acc0a61692abb37a9-C001-176", "intents": ["@DIF@"], "paper_id": "ABC_d6d8f08147e45acc0a61692abb37a9_3", "text": "In consequence, our method has two advantages over the previous methods ( [8] and [9] ): our method 1) handles the ambiguous relatives and unambiguous relatives more effectively, and 2) utilizes only one co-occurrence matrix for disambiguating all contents words instead of collecting training data of the content words."}
{"sent_id": "d6d8f08147e45acc0a61692abb37a9-C001-144", "intents": ["@USE@"], "paper_id": "ABC_d6d8f08147e45acc0a61692abb37a9_3", "text": "Considering the difference, we implemented the method of [8] to include the ambiguous relatives into relatives, but the method of [9] to exclude the ambiguous relatives."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-3", "intents": ["@EXT@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "The metric is built on the LFG F-structurebased approach presented in (Owczarzak et al., 2007) ."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-22", "intents": ["@EXT@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "In this paper we extend the work of (Owczarzak et al., 2007) in a different manner: we use an adapted version of the Malt parser (Nivre et al., 2006) to produce 1-best LFG dependencies and allow triple matches where the dependency labels are different."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-75", "intents": ["@EXT@", "@BACK@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "In (Owczarzak et al., 2007) , lexical variations at the word-level are captured by WordNet."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-117", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "Building upon the LFGbased metric described in (Owczarzak et al., 2007) , we use a publicly available parser instead of an in-house parser to produce dependency labels, so that the metric can run on a third party machine."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "With the addition of partial matching and n-best parses, Owczarzak et al. (2007) 's method considerably outperforms Liu and Gildea's (2005) w.r.t."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "The EDPM metric (Kahn et al., 2010) improves this line of research by using arc labels derived from a Probabilistic Context-Free Grammar (PCFG) parse to replace the LFG labels, showing that a PCFG parser is sufficient for preprocessing, compared to a dependency parser in (Liu and Gildea, 2005) and (Owczarzak et al., 2007) ."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "The metric in (Owczarzak et al., 2007) performs triple matching over the Hyp-and Ref-Triples and calculates the metric score using the F-score of matching precision and recall."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "The score of the hypothesis in (Owczarzak et al., 2007) is the Fscore based on the precision and recall of matching as in (1): Owczarzak et al., 2007) uses several techniques to facilitate triple matching."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "The metric described in (Owczarzak et al., 2007) uses the DCU LFG parser (Cahill et al., 2004) to produce LFG dependency triples."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-63", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "In (Owczarzak et al., 2007) , triple matching on f-structures produced by this paradigm correlates well with human judgement, but this paradigm is not adequate for the WMTMetricsMatr evaluation in two respects: 1) the inhouse LFG annotation algorithm is not publicly available and 2) the speed of this paradigm is not satisfactory."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-80", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "The metric described in (Owczarzak et al., 2007) does not explicitly consider word order and fluency."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-31", "intents": ["@USE@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "In this section, we briefly review the metric presented in (Owczarzak et al., 2007) ."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-38", "intents": ["@USE@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "The basic method of (Owczarzak et al., 2007) can be illustrated by the example in Table 1 ."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-107", "intents": ["@USE@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "The first two settings compare the effect of allowing/not allowing soft matches, but only uses WordNet as in (Owczarzak et al., 2007) ."}
{"sent_id": "6b11cfba6ee73c1f67941cf73506be-C001-70", "intents": ["@DIF@"], "paper_id": "ABC_6b11cfba6ee73c1f67941cf73506be_3", "text": "Compared to the 50-best parses in (Owczarzak et al., 2007) , the 1-best parse limits the number of triple matches that can be found."}
{"sent_id": "48add0c1226863808c3c3a8c29a12e-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_48add0c1226863808c3c3a8c29a12e_3", "text": "It has attracted considerable attention in recent years [1, 2, 3, 4, 5, 6, 7, 8] ."}
{"sent_id": "48add0c1226863808c3c3a8c29a12e-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_48add0c1226863808c3c3a8c29a12e_3", "text": "To deal with this problem, Huang and Lee [3] proposed a contrastive approach based on documentlevel top-bag-of-word similarity to reflect distances among the three varieties of Mandarin in China, Taiwan and Singapore, which is a kind of word-level uni-gram feature."}
{"sent_id": "48add0c1226863808c3c3a8c29a12e-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_48add0c1226863808c3c3a8c29a12e_3", "text": "Huang and Lee [3] presented the top-bag-of-word similarity based contrastive approach to reflect distances among the three varieties of Mandarin in Mainland China, Taiwan and Singapore."}
{"sent_id": "48add0c1226863808c3c3a8c29a12e-C001-30", "intents": ["@EXT@"], "paper_id": "ABC_48add0c1226863808c3c3a8c29a12e_3", "text": "In order to investigate the detailed characteristics of different dialects of Mandarin Chinese, we extend 3 dialects in Huang and Lee [3] to 6 dialects."}
{"sent_id": "48add0c1226863808c3c3a8c29a12e-C001-57", "intents": ["@EXT@"], "paper_id": "ABC_48add0c1226863808c3c3a8c29a12e_3", "text": "In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee [3] to 6 dialects."}
{"sent_id": "48add0c1226863808c3c3a8c29a12e-C001-54", "intents": ["@SIM@"], "paper_id": "ABC_48add0c1226863808c3c3a8c29a12e_3", "text": "Among the above related works, study [3] is the most related work to ours."}
{"sent_id": "48add0c1226863808c3c3a8c29a12e-C001-194", "intents": ["@SIM@"], "paper_id": "ABC_48add0c1226863808c3c3a8c29a12e_3", "text": "Similar to Huang and Lee [3] 's work, in order to eliminate the trivial issue of character encoding (simplified and traditional character), we convert Taiwan and Hong Kong texts to the same simplified character set using Zhconvertor 6 utility to focus on actual linguistic and textual features."}
{"sent_id": "48add0c1226863808c3c3a8c29a12e-C001-55", "intents": ["@DIF@"], "paper_id": "ABC_48add0c1226863808c3c3a8c29a12e_3", "text": "The differences between study [3] and our work are two-fold:"}
{"sent_id": "48add0c1226863808c3c3a8c29a12e-C001-74", "intents": ["@DIF@"], "paper_id": "ABC_48add0c1226863808c3c3a8c29a12e_3", "text": "However, Huang and Lee [3] did not use character-level n-grams."}
{"sent_id": "48add0c1226863808c3c3a8c29a12e-C001-161", "intents": ["@DIF@"], "paper_id": "ABC_48add0c1226863808c3c3a8c29a12e_3", "text": "Also the bi-gram and word segmentation based features are better than the Huang and Lee [3] 's method (baseline system 1) for 6-way, 3-way and 2-way dialect identification in the GCR."}
{"sent_id": "48add0c1226863808c3c3a8c29a12e-C001-145", "intents": ["@USE@"], "paper_id": "ABC_48add0c1226863808c3c3a8c29a12e_3", "text": "Baseline system 1: As mentioned in Section 2, we take the Huang and Lee [3] 's top-bag-of-word similarity-based approach as one of our baseline system."}
{"sent_id": "d53d1b53168041baea5b5002b46627-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_d53d1b53168041baea5b5002b46627_3", "text": "To bridge the quality gap between a streaming recurrent neural network transducer (RNN-T) [6] and a large conventional model [8] , a two-pass framework has been proposed in [10] , which uses a non-streaming LAS decoder to rescore the RNN-T hypotheses."}
{"sent_id": "d53d1b53168041baea5b5002b46627-C001-34", "intents": ["@DIF@"], "paper_id": "ABC_d53d1b53168041baea5b5002b46627_3", "text": "The results show that our MWER trained 8-hypothesis deliberation model performs 11% relatively better than LAS rescoring [10] in VS WER, and up to 15% for proper noun recognition."}
{"sent_id": "d53d1b53168041baea5b5002b46627-C001-59", "intents": ["@DIF@"], "paper_id": "ABC_d53d1b53168041baea5b5002b46627_3", "text": "There are two major differences between our model and the LAS rescoring [10] ."}
{"sent_id": "d53d1b53168041baea5b5002b46627-C001-60", "intents": ["@DIF@"], "paper_id": "ABC_d53d1b53168041baea5b5002b46627_3", "text": "First, the deliberation model attends to both e and yr, while [10] only attends to the acoustic embedding, e. Second, our deliberation model encodes yr bidirectionally, while [10] only relies on unidirectional encoding e for decoding."}
{"sent_id": "d53d1b53168041baea5b5002b46627-C001-90", "intents": ["@DIF@"], "paper_id": "ABC_d53d1b53168041baea5b5002b46627_3", "text": "Note the difference from [10] when rescoring a hypothesis is that the deliberation network sees all candidate hypotheses."}
{"sent_id": "d53d1b53168041baea5b5002b46627-C001-102", "intents": ["@DIF@"], "paper_id": "ABC_d53d1b53168041baea5b5002b46627_3", "text": "The SxS set contains utterances where the LAS rescoring model [10] performs inferior to a state-of-the-art conventional model [8] , and one reason is due to proper nouns."}
{"sent_id": "d53d1b53168041baea5b5002b46627-C001-153", "intents": ["@DIF@"], "paper_id": "ABC_d53d1b53168041baea5b5002b46627_3", "text": "We propose to use the deliberation decoder to rescore first-pass RNN-T results, and expect bidirectional encoding to help compared to LAS rescoring [10] ."}
{"sent_id": "d53d1b53168041baea5b5002b46627-C001-42", "intents": ["@SIM@"], "paper_id": "ABC_d53d1b53168041baea5b5002b46627_3", "text": "As shown in Fig. 1 , our deliberation network consists of three major components: A shared encoder, an RNN-T decoder [1] , and a deliberation decoder, similar to [10, 16] ."}
{"sent_id": "d53d1b53168041baea5b5002b46627-C001-84", "intents": ["@SIM@"], "paper_id": "ABC_d53d1b53168041baea5b5002b46627_3", "text": "The joint training is similar to \"deep finetuning\" in [10] but without a pre-trained decoder."}
{"sent_id": "d53d1b53168041baea5b5002b46627-C001-70", "intents": ["@USE@"], "paper_id": "ABC_d53d1b53168041baea5b5002b46627_3", "text": "However, we find training a two-pass model from scratch tends to be unstable in practice [10] , and thus use a two-step training process: Train the RNN-T as in [6] , and then fix the RNN-T parameters and only train the deliberation decoder and additional encoder layers as in [7, 10] ."}
{"sent_id": "d53d1b53168041baea5b5002b46627-C001-89", "intents": ["@USE@"], "paper_id": "ABC_d53d1b53168041baea5b5002b46627_3", "text": "In rescoring, we run the deliberation decoder on yr in a teacher-forcing mode [10] ."}
{"sent_id": "d53d1b53168041baea5b5002b46627-C001-159", "intents": ["@USE@"], "paper_id": "ABC_d53d1b53168041baea5b5002b46627_3", "text": "In Table 4 , we compare deliberation models with an RNN-T [6] and LAS rescoring model [10] To understand where the improvement comes from, in Fig. 2 we show an example of deliberation attention distribution on the RNN-T hypotheses (x-axis) at every step of the second-pass decoding (yaxis)."}
{"sent_id": "d53d1b53168041baea5b5002b46627-C001-166", "intents": ["@UNSURE@"], "paper_id": "ABC_d53d1b53168041baea5b5002b46627_3", "text": "However, we note that the computation can be parallelized across hypotheses [10] and should have less impact on latency."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006) , often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al., 2003) , and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al., 2011) ."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "The AA system described in Yannakoudakis et al. (2011) exploited features based on POS tag sequences, but did not consider the distribution of POS types across grades."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-21", "intents": ["@UNSURE@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "Our contribution is threefold: 1) we present the first systematic analysis of several methods for assessing discourse coherence in the framework of AA of learner free-text responses, 2) we identify new discourse features that serve as proxies for the level of (in)coherence in texts and outperform previously developed techniques, and 3) we improve the best results reported by Yannakoudakis et al. (2011) on the publically available 'English as a Second or Other Language' (ESOL) corpus of learner texts (to date, this is the only public-domain corpus that contains grades)."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-22", "intents": ["@USE@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in Yannakoudakis et al. (2011) ."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-35", "intents": ["@USE@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "We use the First Certificate in English (FCE) ESOL examination scripts 2 (upper-intermediate level assessment) described in detail in Yannakoudakis et al. (2011) , extracted from the Cambridge Learner Corpus 3 (CLC)."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-39", "intents": ["@USE@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "Moreover, we identify the best model on year 2000 and we also test it on 97 texts from the examination year 2001, previously used in Yannakoudakis et al. (2011) to report the best published results."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-46", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "As in Yannakoudakis et al. (2011) , we analyze all texts using the RASP toolkit (Briscoe et al., 2006) 4 ."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-162", "intents": ["@USE@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "In particular, we use the system described in Yannakoudakis et al. (2011) as our baseline AA system."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-182", "intents": ["@USE@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in Yannakoudakis et al. (2011) to report results of the final best system."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-188", "intents": ["@USE@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "Finally, we explore the utility of our best model for assessing the publically available 'outlier' texts used in Yannakoudakis et al. (2011) ."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-26", "intents": ["@EXT@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "Specifically, we describe a number of different experiments improving on the AA system presented in Yannakoudakis et al. (2011) ; AA is treated as a rank preference supervised learning problem and ranking Support Vector Machines (SVMs) (Joachims, 2002) are used to explicitly model the grade relationships between scripts."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-71", "intents": ["@DIF@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "Among the features used in Yannakoudakis et al. (2011) , none explicitly captures coherence and none models intersentential relationships."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-248", "intents": ["@DIF@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "A significant improvement over the AA system presented in Yannakoudakis et al. (2011) and the best published result on the FCE dataset was obtained by augmenting the system with an ISA-based local coherence feature."}
{"sent_id": "845c66e6dfafc21ab90e5aa5cbf947-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_845c66e6dfafc21ab90e5aa5cbf947_4", "text": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17] ."}
{"sent_id": "845c66e6dfafc21ab90e5aa5cbf947-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_845c66e6dfafc21ab90e5aa5cbf947_4", "text": "Kamper et al. [14] compared several types of acoustic word embeddings for a word discrimination task related to query-by-example search, finding that embeddings based on convolutional neural networks (CNNs) trained with a contrastive loss outperformed the reference vector approach of Levin et al. [12] as well as several other CNN and DNN embeddings and DTW using several feature types."}
{"sent_id": "845c66e6dfafc21ab90e5aa5cbf947-C001-81", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_845c66e6dfafc21ab90e5aa5cbf947_4", "text": "As in [14, 11] , our first approach is to use the word labels of the training segments and train the networks to classify the word."}
{"sent_id": "845c66e6dfafc21ab90e5aa5cbf947-C001-108", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_845c66e6dfafc21ab90e5aa5cbf947_4", "text": "As in [14] , when training the classificationbased embeddings, we use a subset of the training set containing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments."}
{"sent_id": "845c66e6dfafc21ab90e5aa5cbf947-C001-88", "intents": ["@USE@"], "paper_id": "ABC_845c66e6dfafc21ab90e5aa5cbf947_4", "text": "The second training approach, based on earlier work of Kamper et al. [14] , is to train \"Siamese\" networks [31] ."}
{"sent_id": "845c66e6dfafc21ab90e5aa5cbf947-C001-106", "intents": ["@USE@"], "paper_id": "ABC_845c66e6dfafc21ab90e5aa5cbf947_4", "text": "The acoustic features in each frame (the input to the word embedding models x t ) are 39-dimensional MFCCs+∆+∆∆. We use the same train, development, and test partitions as in prior work [14, 12] , and the same acoustic features as in [14] , for as direct a comparison as possible."}
{"sent_id": "845c66e6dfafc21ab90e5aa5cbf947-C001-133", "intents": ["@DIF@"], "paper_id": "ABC_845c66e6dfafc21ab90e5aa5cbf947_4", "text": "This is a slight departure from earlier work [14] , which we found to improve stability in training and performance on the development set."}
{"sent_id": "845c66e6dfafc21ab90e5aa5cbf947-C001-170", "intents": ["@DIF@"], "paper_id": "ABC_845c66e6dfafc21ab90e5aa5cbf947_4", "text": "There is clear utility in stacking additional layers; however, even with 4 stacked layers the RNNs still underperform the CNN-based embeddings of [14] until we begin adding fully connected layers."}
{"sent_id": "845c66e6dfafc21ab90e5aa5cbf947-C001-180", "intents": ["@DIF@"], "paper_id": "ABC_845c66e6dfafc21ab90e5aa5cbf947_4", "text": "This analysis shows that the embeddings learned by the Siamese RNN network are quite robust to reduced dimensionality, outperforming the classifier model for all dimensionalities 32 or higher and outperforming previously reported dev set performance with CNN-based embeddings [14] for all dimensionalities ≥ 16."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "According to (Becker, 1985; Huang, 1985; Gu et al., 1991; Chung, 1993; Kuo, 1995; Fu et al., 1996; Lee et al., 1997; Hsu et al., 1999; Chen et al., 2000; Tsai and Hsu, 2002; Gao et al., 2002; Lee, 2003; Tsai, 2005) , the approaches of Chinese input methods (i.e. Chinese input systems) can be classified into two types: (1) keyboard based approach: including phonetic and pinyin based (Chang et al., 1991; Hsu et al., 1993; Hsu, 1994; Hsu et al., 1999; Kuo, 1995; Lua and Gan, 1992) , arbitrary codes based (Fan et al., 1988) and structure scheme based (Huang, 1985) ; and (2) non-keyboard based approach: including optical character recognition (OCR) (Chung, 1993) , online handwriting and speech recognition (Fu et al., 1996; Chen et al., 2000) ."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (Hsu, 1994; Hsu et al., 1999; Tsai and Hsu, 2002; Gao et al., 2002; Microsoft Research Center in Beijing; Tsai, 2005) are addressed on STW conversion."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "As per (Chung, 1993; Fong and Chung, 1994; Tsai and Hsu, 2002; Gao et al., 2002; Lee, 2003; Tsai, 2005) , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "From the studies (Hsu 1994; Tsai and Hsu, 2002; Gao et al., 2002; Kee, 2003; Tsai, 2005) , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "In our previous work (Tsai, 2005) , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "In (Tsai, 2005) , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-96", "intents": ["@BACK@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "(Note that, as per (Tsai, 2005) , the differences between the tonal and toneless STW accuracies of the BiGram and the TriGram are less than 0.3%)."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-21", "intents": ["@MOT@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "Since the identified character ratio of the WP identifier (Tsai, 2005 ) is about 55%, there are still about 15% improving room left."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-23", "intents": ["@USE@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method Editor 2003, MSIME) , and an optimized bigram model, BiGram (Tsai, 2005) , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-37", "intents": ["@USE@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "Following (Tsai, 2005) , the three steps of autogenerating word-pairs (AUTO-WP) for a given Chinese sentence are as below: (the details of AUTO-WP can be found in (Tsai, 2005)) Step 1. Get forward and backward word segmentations: Generate two types of word segmentations for a given Chinese sentence by forward maximum matching (FMM) and backward maximum matching (BMM) techniques (Chen et al., 1986; Tsai et al., 2004) with the system dictionary."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-81", "intents": ["@USE@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "The comparative system is the WP identifier (Tsai, 2005) ."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-88", "intents": ["@USE@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "In addition, following (Tsai, 2005) , an optimized bigram model called BiGram was developed."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-111", "intents": ["@SIM@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "This observation is similarly with that of our previous work (Tsai, 2005) ."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-113", "intents": ["@SIM@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "This observation is similarly with that of (Tsai, 2005) ."}
{"sent_id": "cce566b9111abdc7ab7576662922dd-C001-120", "intents": ["@EXT@"], "paper_id": "ABC_cce566b9111abdc7ab7576662922dd_4", "text": "In this paper, we present a word support model (WSM) to improve the WP identifier (Tsai, 2005) and support the Chinese Language Processing on the STW conversion problem."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "Visual conversational agents (Das et al. 2017a; Das et al. 2017b; ) are AI agents Figure 1 : A human and an AI (a visual conversation agent called ALICE) play the proposed GuessWhich game."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "Specifically, (Das et al. 2017b ) train two visual conversational agents -a questioning bot QBOT, and an answering bot ABOT -for an image-guessing task."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "(Das et al. 2017b ) compare supervised baseline models with QBOT-ABOT teams trained through reinforcement learning based self-talk on this image-guessing task."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (Das et al. 2017a; Das et al. 2017b; . (Das et al. 2017a ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer)."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-65", "intents": ["@BACK@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "(Das et al. 2017b ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-72", "intents": ["@BACK@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "Human evaluation of conversations is typically in the format where humans rate the quality of machine utterances given context, without actually taking part in the conversation, as in (Das et al. 2017b ) and (Li et al. 2016) ."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-82", "intents": ["@BACK@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "(Das et al. 2017b ) formulate a self-supervised imageguessing task between a questioner bot (QBOT) and an answerer bot (ABOT) which plays out over multiple rounds of dialog."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-182", "intents": ["@BACK@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "(Das et al. 2017b ) evaluate these agents against strong baselines and report AI-AI team results that are significantly better than chance on a pool of ∼10k images (rank ∼1000 for SL, rank ∼500 for RL)."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-38", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "Mirroring the setting of (Das et al. 2017b) , GuessWhich is an image-guessing game that consists of 2 participants -questioner and answerer."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-44", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "ALICE RL which is pre-trained with supervised learning and fine-tuned via reinforcement learning for an imageguessing task as in (Das et al. 2017b) ."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-57", "intents": ["@DIF@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "Our main experimental finding is that despite significant differences between SL and RL agents reported in previous work (Das et al. 2017b) , we find no significant difference in performance between ALICE SL or ALICE RL when paired with human partners (Sec. 6.1)."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-190", "intents": ["@DIF@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "Unlike (Das et al., 2017b) , we find no significant difference between ALICE SL and ALICE RL ."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-193", "intents": ["@DIF@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "This interesting finding stands in stark contrast to the results reported by (Das et al. 2017b) , where ALICE RL was found to be significantly more accurate than ALICE SL when evaluated in an AI-AI team."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "The supervision was either given in the form of meaning representations aligned with sentences (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007) or in a somewhat more relaxed form, such as lists of candidate meanings for each sentence (Kate and Mooney, 2007; Chen and Mooney, 2008) or formal representations of the described world state for each text (Liang et al., 2009) ."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "Section 3 redescribes the semantics-text correspondence model (Liang et al., 2009) in the context of our learning scenario."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "The semantics m can be represented either as a logical formula (see, e.g., (Poon and Domingos, 2009 )) or as a set of field values if database records are used as a meaning representation (Liang et al., 2009 )."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-138", "intents": ["@BACK@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "When the world state is observable, learning does not require any approximations, as dynamic programming (a form of the forward-backward algorithm) can be used to infer the posterior distribution on the E-step (Liang et al., 2009) ."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-44", "intents": ["@USE@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "We study our set-up on the weather forecast data (Liang et al., 2009) where the original textual weather forecasts were complemented by additional forecasts describing the same weather states (see figure 1 for an example)."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-95", "intents": ["@USE@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "As soon as semantics m k are inferred for every k, we find ourselves in the set-up of learning with unaligned semantic states considered in (Liang et al., 2009) ."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-151", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "As soon as the meaning representations m are inferred, we find ourselves in the set-up studied in (Liang et al., 2009 ): the state s is no longer latent and we can run efficient inference on the E-step."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-161", "intents": ["@USE@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "To perform the experiments we used a subset of the weather dataset introduced in (Liang et al., 2009 )."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-174", "intents": ["@USE@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "Following Liang et al. (2009) we evaluate the models on how well they predict these alignments."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-175", "intents": ["@USE@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "When estimating the model parameters, we followed the training regime prescribed in (Liang et al., 2009) ."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-218", "intents": ["@USE@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "We showed how it can be instantiated for the semantics-text correspondence model (Liang et al., 2009 ) and evaluated it on a dataset of weather forecasts."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-101", "intents": ["@EXT@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "In this section we redescribe the semantics-text correspondence model (Liang et al., 2009 ) with an extension needed to model examples with latent states, and also explain how the inference algorithm defined in section 2 can be applied to this model."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-200", "intents": ["@SIM@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "However, correlation between rain and overcast, as also noted in (Liang et al., 2009) , results in the wrong assignment of the rain-related words to the field value corresponding to very cloudy weather."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-178", "intents": ["@DIF@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "Instead of prohibiting records from crossing punctuation, as suggested by Liang et al. (2009) , in our implementation we disregard the words not attached to specific fields (attached to the nullfield, see section 3.1) when computing spans of records."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-4", "intents": ["@BACK@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "Recently, Canny et al. (2013) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "Recently, Canny et al. (2013) proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "A further drawback of the dense approach in Canny et al. (2013) is that it only computes Viterbi parses."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-89", "intents": ["@BACK@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "Canny et al. (2013) proposed an implementation of a PCFG parser that sacrifices standard sparse methods like coarse-to-fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-97", "intents": ["@BACK@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "The Canny et al. (2013) system is benchmarked on a batch size of 1200 sentences, the others on 20,000."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "One important feature of Canny et al. (2013) 's system is grammar compilation."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-112", "intents": ["@BACK@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "Canny et al. (2013) found they had to partition the grammar into multiple different kernels."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-158", "intents": ["@BACK@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "Canny et al. (2013) clustered symbols of the grammar using a sophisticated spectral clustering algorithm to obtain a permutation of the symbols."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-276", "intents": ["@BACK@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "Apart from the model of Canny et al. (2013) , there have been a few attempts at using GPUs in NLP contexts before."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-41", "intents": ["@DIF@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "1 We should note that our experimental condition differs from that of Canny et al. (2013) : they evaluate on sentences of length ≤ 30."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-42", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "Furthermore, they 1 The implementation of Canny et al. (2013) cannot handle batches so large, and so we tested it on batches of 1200 sentences."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-70", "intents": ["@DIF@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "The original system of Canny et al. (2013) only used the fine pass, with no pruning."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-120", "intents": ["@DIF@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "All in all, Canny et al. (2013) 's system is able to compute Viterbi charts at 164 sentences per second, for sentences up to length 40."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-173", "intents": ["@DIF@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to Canny et al. (2013) 's system, and nearly 50% over our reimplemented baseline."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-175", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "The unpruned Viterbi computations in a fine grammar using the clustering method of Canny et al. (2013) yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-176", "intents": ["@DIF@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "(See Table 1 .) This is not as efficient as Canny et al. (2013) 's highly tuned method, but it is still fairly fast, and much simpler to implement."}
{"sent_id": "ca1391f1f908fc081589b1a7dd8229-C001-139", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_ca1391f1f908fc081589b1a7dd8229_4", "text": "Once on the GPU, parse items are processed using the same style of compiled kernel as in Canny et al. (2013) ."}
{"sent_id": "5203c1037fe57bd1b813c0bf1ff5c4-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_5203c1037fe57bd1b813c0bf1ff5c4_4", "text": "On the other hand, Noraset et al. (2017) attempted to generate a definition of a word from an embedding induced from massive text (which can be seen as global context)."}
{"sent_id": "5203c1037fe57bd1b813c0bf1ff5c4-C001-34", "intents": ["@USE@"], "paper_id": "ABC_5203c1037fe57bd1b813c0bf1ff5c4_4", "text": "Considering various applications where we need definitions of expressions, we evaluated our method with four datasets including WordNet (Noraset et al., 2017) for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slang, and a newlycreated Wikipedia dataset for entities."}
{"sent_id": "5203c1037fe57bd1b813c0bf1ff5c4-C001-77", "intents": ["@USE@"], "paper_id": "ABC_5203c1037fe57bd1b813c0bf1ff5c4_4", "text": "In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following (Noraset et al., 2017) ."}
{"sent_id": "5203c1037fe57bd1b813c0bf1ff5c4-C001-78", "intents": ["@USE@"], "paper_id": "ABC_5203c1037fe57bd1b813c0bf1ff5c4_4", "text": "Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following Noraset et al. (2017), we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg ."}
{"sent_id": "5203c1037fe57bd1b813c0bf1ff5c4-C001-105", "intents": ["@USE@"], "paper_id": "ABC_5203c1037fe57bd1b813c0bf1ff5c4_4", "text": "Datasets To evaluate our model on the word description task on WordNet, we followed Noraset et al. (2017) and extracted data from WordNet using the dict-definition 9 toolkit."}
{"sent_id": "5203c1037fe57bd1b813c0bf1ff5c4-C001-112", "intents": ["@USE@"], "paper_id": "ABC_5203c1037fe57bd1b813c0bf1ff5c4_4", "text": "Since not all entries in WordNet have usage examples, our dataset is a small subset of Noraset et al. (2017) ."}
{"sent_id": "5203c1037fe57bd1b813c0bf1ff5c4-C001-123", "intents": ["@USE@"], "paper_id": "ABC_5203c1037fe57bd1b813c0bf1ff5c4_4", "text": "Global (Noraset et al., 2017) , (2) Local (Ni and Wang, 2017) with CNN, (3) I-Attention (Gadetsky et al., 2018) , and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the best model (S + G + CH) in Noraset et al. (2017) ."}
{"sent_id": "5203c1037fe57bd1b813c0bf1ff5c4-C001-57", "intents": ["@SIM@"], "paper_id": "ABC_5203c1037fe57bd1b813c0bf1ff5c4_4", "text": "To incorporate the different types of contexts, we propose to use a gate function similar to Noraset et al. (2017) to dynamically control how the global and local contexts influence the description."}
{"sent_id": "5203c1037fe57bd1b813c0bf1ff5c4-C001-81", "intents": ["@SIM@"], "paper_id": "ABC_5203c1037fe57bd1b813c0bf1ff5c4_4", "text": "In order to capture the interaction between the local and global contexts, we adopt a GATE(·) function (Eq. (7)) which is similar to Noraset et al. (2017) ."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "Several papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction (BDI) (Barone, 2016; Artetxe et al., 2017; Zhang et al., 2017; Conneau et al., 2018; , the task of identifying translational equivalents across two languages."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "In addition to an unsupervised approach to aligning monolingual word embedding spaces with adversarial training, Conneau et al. (2018) present a supervised alignment algorithm that assumes a gold-standard seed dictionary and performs Procrustes Analysis (Schönemann, 1966) ."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "The implementation of PA in Conneau et al. (2018) yields notable improvements over earlier work on BDI, even though it learns a simple linear transform of the source language space into the target language space."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-151", "intents": ["@BACK@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "In most recent work, this mapping is constrained to be orthogonal and solved using Procrustes Analysis (Xing et al., 2015; Artetxe et al., 2017 Artetxe et al., , 2018 Conneau et al., 2018; Lu et al., 2015) ."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-9", "intents": ["@UNSURE@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "Pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties (for an adversarial approach, see Conneau et al. (2018) )."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-114", "intents": ["@UNSURE@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "P@1 over the BosnianEnglish test set of Conneau et al. (2018) is 31.33, 34.80, and 34.47 for PA, GPA and MGPA+, respectively."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-19", "intents": ["@DIF@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "In this work, we show that projecting both source and target vector spaces into a third space (Faruqui and Dyer, 2014) , using a variant of PA known as Generalized Procrustes Analysis (Gower, 1975) , makes it easier to learn the alignment between two word vector spaces, as compared to the single linear transform used in Conneau et al. (2018) ."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-56", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "In our experiments, we generally use the same hyper-parameters as used in Conneau et al. (2018) , unless otherwise stated."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-57", "intents": ["@USE@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "When extracting dictionaries for the bootstrapping procedure, we use cross-domain local scaling (CSLS, see Conneau et al. (2018) for details) as a metric for ranking candidate translation pairs, and we only use the ones that rank higher than 15,000."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-59", "intents": ["@USE@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "Instead of doing a single training epoch, however, we run PA and GPA with early stopping, until five epochs of no improvement in the validation criterion as used in Conneau et al. (2018) , i.e. the average cosine similarity between the top 10,000 most frequent words in the source language and their candidate translations as induced with CSLS."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-60", "intents": ["@USE@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "Our metric is Precision at k×100 (P@k), i.e. percentage of correct translations retrieved among the k nearest neighbor of the source words in the test set (Conneau et al., 2018) ."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-61", "intents": ["@USE@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "Unless stated otherwise, experiments were carried out using the publicly available pre-trained fastText embeddings, trained on Wikipedia data, 5 and bilingual dictionaries-consisting of 5000 and 1500 unique word pairs for training and testing, respectively-provided by Conneau et al. (2018) 6 ."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-69", "intents": ["@USE@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "Common benchmarks For a more extensive comparison with previous work, we include results on English-{Finnish, German, Italian} dictionaries used in Conneau et al. (2018) and Artetxe et al. (2018) -the second best approach to BDI known to us, which also uses Procrustes Analysis."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-71", "intents": ["@USE@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "We use train and test bilingual dictionaries from Dinu et al. (2015) for English-Italian and from Artetxe et al. (2017) for English-{Finnish, German}. Following Conneau et al. (2018) , we report results with a set of CBOW embeddings trained on the WaCky corpus (Barone, 2016) , and with Wikipedia embeddings."}
{"sent_id": "edfce6b99a4804c0908b39ea38d707-C001-129", "intents": ["@USE@"], "paper_id": "ABC_edfce6b99a4804c0908b39ea38d707_4", "text": "To explore the latter issue and to further compare the capabilities of PA and GPA, we perform a Procrustes fit test, where we learn alignments in a fully supervised fashion, using the test dictionaries of Conneau et al. (2018) 9 for both training and evaluation 10 ."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-28", "intents": ["@USE@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "We make use of the RefCOCO and RefCOCO+ datasets in our work along with another recently collected referring expression dataset, released by Google, denoted in our paper as RefCOCOg [26] ."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-69", "intents": ["@USE@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "The first set of models are recent state of the art deep learning approaches from Mao et al [26] ."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-75", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "For comparison, we implement both the baseline and strong model of Mao et al [26] ."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-136", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "One dataset, RefCOCOg [26] is collected in a non-interactive setting, while the other two datasets, RefCOCO and RefCOCO+, are collected interactively in a two-player game [19] ."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-169", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "We use this split for RefCOCOg since same division was used in the previous state-of-the-art approach [26] ."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-179", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "In experiments for the referring expression comprehension task, we use the same evaluation as Mao et al [26] , namely we first predict the region referred by the given expression, then we compute the intersection over union (IOU) ratio between the true and predicted bounding box."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-194", "intents": ["@USE@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "To verify this intuition, we implemented both the baseline and strong MMI models from Mao et al [26] , and compare the results for referring expression comprehension task with and without global context on RefCOCO and Refcoco+ in Table 1 ."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-214", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "For RefCOCOg, we evaluate on the per-object split as previous work [26] ."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-217", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "We observe that our implementation of Mao et al [26] achieves comparable performance to the numbers reported in their paper."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-222", "intents": ["@USE@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "For RefCOCOg, we use the detection results provided by [26] , which were trained uisng Multibox [4] ."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "The most relevant work to ours is Mao et al [26] which introduced the first deep learning approach to REG."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "Since then, another three REG datasets based on the object labels in MSCOCO have been collected [19, 26] ."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "In Mao et al's baseline [26] , the model uses maximum likelihood training and outputs the most likely referring expression given the target object, context, and location/size features."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-193", "intents": ["@BACK@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "Context Representation As previously discussed, we suggest that the approaches proposed in recent referring expression works [26, 14] make use of relatively weak contextual information, by only considering a single global image context for all objects."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-53", "intents": ["@DIF@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "Three recent approaches for referring expression generation [26] and comprehension [14, 33] also take a deep learning approach."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-119", "intents": ["@DIF@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "For the referring expression generation task, rather than generating sentences for each object in an image separately [15] [26], we consider tying the generation process together into a single task to jointly generate expressions for all objects of the same object category depicted in an image."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification (Blitzer et al., 2007) , the reasons to this success are not entirely understood."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "In the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sentiment classification, has been structural correspondence learning (SCL) (Blitzer et al., 2006 (Blitzer et al., , 2007 ."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "Pivot and Non-Pivot Features The definitions of this approach are given in Blitzer et al. (2006 Blitzer et al. ( , 2007 , where SCL is presented in the context of POS tagging and sentiment classification, respectively."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "An important observation of Blitzer et al. (2007) , is that some pivot features are similar to each other to the level that they indicate the same information with respect to the classification task."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-162", "intents": ["@BACK@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "The test set for each target domain of Blitzer et al. (2007) consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-28", "intents": ["@EXT@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "We experiment with the task of cross-domain product sentiment classification of (Blitzer et al., 2007) , consisting of 4 domains (12 domain pairs) and further add an additional target domain, consisting of sentences extracted from social media blogs (total of 16 domain pairs)."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-91", "intents": ["@USE@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "Following previous work (e.g. (Blitzer et al., 2006 (Blitzer et al., , 2007 Chen et al., 2012) our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-127", "intents": ["@USE@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "Cross-domain Sentiment Classification To demonstrate the power of our models for domain adaptation we experiment with the task of crossdomain sentiment classification (Blitzer et al., 2007) ."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-137", "intents": ["@USE@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "The first baseline is SCL with pivot features selected using the mutual information criterion (SCL-MI, (Blitzer et al., 2007) )."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-161", "intents": ["@USE@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "We experiment with a 5-fold cross-validation on the source domain (Blitzer et al., 2007) : 1600 reviews for training and 400 reviews for development."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-173", "intents": ["@USE@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "Baselines: For SCL-MI, following (Blitzer et al., 2007) we tuned the number of pivot features (Gillick and Cox, 1989; Blitzer et al., 2006) between 500 and 1000 and the SVD dimensions among 50,100 and 150."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-245", "intents": ["@USE@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "Variants of the Product Review Data There are two releases of the datasets of the Blitzer et al. (2007) cross-domain product review task."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-249", "intents": ["@DIF@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "Note that Blitzer et al. (2007) used the other release where the unlabeled data consists of the same number of positive and negative reviews."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-250", "intents": ["@DIF@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "Test Set Size While Blitzer et al. (2007) used only 400 target domain reviews for test, we use the entire set of 2000 reviews."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "Spectral-decomposition methods such as singular value decomposition (SVD) and principal component analysis (PCA) are usually used in this line of research (Caron 2001; Bullinaria and Levy 2012; Turney 2012; Levy and Goldberg 2014; Levy, Goldberg, and Dagan 2015; Mu and Viswanath 2018) ."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "Among different unsupervised word vector postprocessing schemes, the all-but-the-top approach (Mu and Viswanath 2018 ) is a prominent example."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "Empirically studying the latent features encoded by principal components (PCs) of distributional word vectors, Mu and Viswanath (2018) found that the variances explained by the leading PCs \"encode the frequency of the word to a significant degree\"."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "Instead of discarding a fixed number of PCs, we softly filter word vectors using matrix conceptors (Jaeger 2014; 2017) , which characterize the linear space of those word vector features having high variances -the features most contaminated by word frequencies according to Mu and Viswanath (2018) ."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "This section is an overview of the all-but-the-top (ABTT) word vector post-processing approach introduced by Mu and Viswanath (2018) ."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "First, using a PCA, Mu and Viswanath (2018) revealed that word vectors are strongly influenced by a few leading principal components (PCs)."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "In practice, Mu and Viswanath (2018) found that the improvements yielded by ABTT are particularly impressive for word similarity tasks."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-65", "intents": ["@BACK@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "Removing the leading PCs of word vectors using the ABTT algorithm described above is effective in practice, as seen in the elaborate experiments conducted by Mu and Viswanath (2018) ."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-93", "intents": ["@BACK@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "Recall that (Mu and Viswanath 2018) found that the directions with which x has the highest variances encode word frequencies, which are unrelated to word semantics."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-23", "intents": ["@EXT@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "The current work advances the findings of Mu and Viswanath (2018) and improves their post-processing scheme."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-33", "intents": ["@EXT@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "We first briefly review the principal component nulling approach for unsupervised word vector post-processing introduced in (Mu and Viswanath 2018) , upon which our work is based."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-152", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "For ABTT, we set d = 3 for Word2Vec and d = 2 for GloVe, as what has been suggested by Mu and Viswanath (2018) ."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-158", "intents": ["@USE@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "The baseline results (orig. and ABTT) are collected from (Mu and Viswanath 2018) ."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-180", "intents": ["@USE@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "Following (Baroni, Dinu, and Kruszewski 2014; Schnabel et al. 2015; Mu and Viswanath 2018) , we used \"purity\" of clusters (Manning, Raghavan, and Schütze 2008, Section 16.4) as the evaluation criterion."}
{"sent_id": "b2392c74f17fb2c0b6a0f19d16bc99-C001-182", "intents": ["@USE@"], "paper_id": "ABC_b2392c74f17fb2c0b6a0f19d16bc99_4", "text": "We follow previous research (Baroni, Dinu, and Kruszewski 2014; Schnabel et al. 2015; Mu and Viswanath 2018) to set k as the ground-truth number of categories."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "Sentence embeddings have been generated using unsupervised learning approaches (e.g. Hill et al., 2016) , and supervised learning (e.g. Bowman et al., 2016; Conneau et al., 2017) ."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "Some approaches focus on building sentence embeddings for the premises and the hypothesis separately and then combine those using a classifier (e.g. Bowman et al., 2015 Bowman et al., , 2016 Conneau et al., 2017) ."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "Conneau et al. (2017) explore multiple different sentence embedding architectures ranging from LSTM, BiLSTM and intra-attention to convolution neural networks and the performance of these architectures on NLI tasks."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-150", "intents": ["@BACK@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "Conneau et al. (2017) have shown that including all the training data from SNLI and MultiNLI improves significantly the model performance on transfer learning tasks, compared to training the model only on SNLI data."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-21", "intents": ["@MOT@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "Motivated by the success of the architecture of InferSent (Conneau et al., 2017) , we build a hierarchical architecture utilizing bidirectional LSTM (BiLSTM) layers and max pooling."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-51", "intents": ["@MOT@", "@EXT@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "Motivated by the strong results of the BiLSTM max pooling network by Conneau et al. (2017) , we experimented with combining BiLSTM max pooling networks as a hierarchical structure."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-64", "intents": ["@MOT@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "For the transfer learning tasks, described in Section 7, we used training data from both the SNLI and the MultiNLI datasets in order to compare to the results by Conneau et al. (2017) ."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-24", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "We also test our model on a number of transfer learning tasks using the SentEval testing library (Conneau et al., 2017) , and show that our model outperforms the InferSent model on 7 out of 10 and SkipThought on 8 out of 9 tasks, comparing to the scores reported by Conneau et al. (2017) ."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-122", "intents": ["@USE@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "3 We also conducted a linguistic error analysis and compared our results to the results obtained with the InferSent BiLSTM max pooling model of Conneau et al. (2017) (our implementation)."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-147", "intents": ["@USE@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "To better understand how well our model generalizes to different tasks, we conducted additional transfer learning tests using the SentEval sentence embedding evaluation library 5 (Conneau et al., 2017) and compared our results to the results published for InferSent and SkipThought ."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-203", "intents": ["@USE@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "InferSent results obtained with our implementation using the architecture and training set-up described in (Conneau et al., 2017) ."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-124", "intents": ["@SIM@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "4 The scores for our implementation of InferSent are on par or slightly higher than the scores reported by Conneau et al. (2017) using their training setup."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-149", "intents": ["@SIM@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "This allows us to compare our results to the InferSent results which were obtained using a model trained on the same data (Conneau et al., 2017) ."}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "Similarly, the lexical predictability ratio (LPR) of Brooke et al. (2015) is an association measure applicable to any possible syntactic pattern, which is calculated by discounting syntactic predictability from the overall conditional probability for each word given the other words in the phrase."}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "Sakaguchi et al. (2016) demonstrate that improving fluency (closely linked to the use of linguistic formulas) is more important than improving strict grammaticality with respect to native speaker judgments of non-native productions; Brooke et al. (2015) explicitly argue for FS lexicons as a way to identify, track, and improve learner proficiency."}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-55", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "The basis of the calculation of explainedness is the syntax-sensitive LPR association measure of Brooke et al. (2015) , but it is calculated differently depending on the on/off status of the node as well as the status of the nodes in its vicinity."}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-62", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "An initial requirement of any such method is an n-gram frequency threshold, which we set to 1 instance per 10 million words, following Brooke et al. (2015) ."}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-163", "intents": ["@USE@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "In English, we follow Brooke et al. (2015) in using a 890M token filtered portion of the ICWSM blog corpus (Burton et al., 2009 ) tagged with the Tree Tagger (Schmid, 1995) ."}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-166", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "For English, gaps are identified using the same POS regex used in Brooke et al. (2015) , which includes simple nouns and portions thereof, up to a maximum of 4 words."}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-75", "intents": ["@DIF@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "In the segmentation approach of Brooke et al. (2015) , LPR for an entire span is calculated as a product of the individual LPRs, but here we will use the minimum LPR across the words in the sequence:"}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-179", "intents": ["@DIF@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "Other than the inclusion of new languages, our test sets differ from Brooke et al. (2015) in two ways."}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-198", "intents": ["@DIF@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "The LPRseg method of Brooke et al. (2015) consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages."}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-264", "intents": ["@DIF@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "Although the optimization of the lattice is several orders of magnitude more complex than the decomposition heuristics of Brooke et al. (2015) , the time needed to build and optimize the lattice is a fraction of the time required to collect the statistics for LPR calculation, and so the end-to-end runtimes of the two methods are comparable."}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-61", "intents": ["@SIM@", "@EXT@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "Since our primary association measure is an adaption of LPR, our approach in this section mostly follows Brooke et al. (2015) up until the last stage."}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-200", "intents": ["@SIM@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "When only covering is used, the results are fairly similar to Brooke et al. (2015) , which is unsurprising given the extent to which decomposition and covering are related."}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-94", "intents": ["@MOT@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "We also use the concept of hard covering to address the issue of pronouns, based on the observation that specific pronouns often have high LPR values due to pragmatic biases (Brooke et al., 2015) ; for instance, private state verbs like feel tend to have first person singular subjects."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-33", "intents": ["@DIF@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "Our model significantly outperforms the previously mentioned hypergraph model of Lu and Roth (2015) and Muis and Lu (2017) on entity mention recognition for the ACE2004 and ACE2005 corpora."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-205", "intents": ["@DIF@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "We find that our LSTM-flat baseline that ignores embedded entity mentions during training performs worse than Lu and Roth (2015) ; however, our other neural network-based approaches all outperform the previous feature-based approach."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-230", "intents": ["@DIF@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "We suspect that it is because we use pretrained word embeddings 5 trained on PubMed data (Pyysalo et al., 2013) whereas Lu and Roth (2015) did not have access to them."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-265", "intents": ["@DIF@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "We show that our model significantly outperforms a feature based mention hypergraph model (Lu and Roth, 2015) and a recent multigraph model (Muis and Lu, 2017) on the ACE dataset."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-47", "intents": ["@SIM@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "As a result, we do not adopt their parse tree-based representation of nested entities and propose instead a linear time directed hypergraph-based model similar to that of Lu and Roth (2015) ."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-83", "intents": ["@SIM@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "Note that the hypergraph representation of our model is similar to Lu and Roth (2015) ."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-84", "intents": ["@SIM@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "Also, the expressiveness of our model is exactly the same as Lu and Roth (2015) ; Muis and Lu (2017) ."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-183", "intents": ["@SIM@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "We use a strict evaluation metric similar to Lu and Roth (2015) : an entity mention is considered correct if both the mention span and the mention type are exactly correct."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-220", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "We follow the same dataset split as Finkel and Manning (2009); Lu and Roth (2015) ; Muis and Lu (2017) ."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "While most previous efforts for nested entity recognition were limited to named entities, Lu and Roth (2015) addressed the problem of nested entity mention detection where mentions can either be named, nominal or pronominal."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-56", "intents": ["@USE@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "In contrast to the hypergraph representation that we and Lu and Roth (2015) adopt, they learn a multigraph representation and are able to perform exact inference on their structure."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-225", "intents": ["@UNSURE@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "We compare our model with Finkel and Manning (2009) based on a constituency CRF-based parser and the mention hypergraph model by Lu and Roth (2015) and a recent multigraph model by Muis and Lu (2017) ."}
{"sent_id": "e0b72115e1905226d22876e72aa304-C001-53", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_e0b72115e1905226d22876e72aa304_4", "text": "The basic structure of our CKB completion model is similar to that of Li et al. (2016b) ."}
{"sent_id": "e0b72115e1905226d22876e72aa304-C001-141", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_e0b72115e1905226d22876e72aa304_4", "text": "The threshold was determined by using the validation1 data to maximize the accuracy of binary classification for each method, as in (Li et al., 2016b) ."}
{"sent_id": "e0b72115e1905226d22876e72aa304-C001-213", "intents": ["@SIM@"], "paper_id": "ABC_e0b72115e1905226d22876e72aa304_4", "text": "This tendency is similar to the results reported in Li et al. (Li et al., 2016b) ."}
{"sent_id": "e0b72115e1905226d22876e72aa304-C001-150", "intents": ["@DIF@"], "paper_id": "ABC_e0b72115e1905226d22876e72aa304_4", "text": "The bottom two lines show the best performances reported in (Li et al., 2016b) ."}
{"sent_id": "e0b72115e1905226d22876e72aa304-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_e0b72115e1905226d22876e72aa304_4", "text": "Previous model Li et al. (2016b) defined a CKB completion model that estimates a confidence score of an arbitrary triple ⟨t 1 , r, t 2 ⟩. They used a simple neural network model to formulate score(t 1 , r, t 2 ) ∈ R."}
{"sent_id": "e0b72115e1905226d22876e72aa304-C001-234", "intents": ["@BACK@"], "paper_id": "ABC_e0b72115e1905226d22876e72aa304_4", "text": "In particular, Li et al. (2016b) and Socher et al. (2013) proposed a simple KBC model for CKB."}
{"sent_id": "e0b72115e1905226d22876e72aa304-C001-102", "intents": ["@USE@"], "paper_id": "ABC_e0b72115e1905226d22876e72aa304_4", "text": "For the experiments with English, we used the ConceptNet 100K data released by Li et al. (2016b) 1 ."}
{"sent_id": "e0b72115e1905226d22876e72aa304-C001-137", "intents": ["@USE@"], "paper_id": "ABC_e0b72115e1905226d22876e72aa304_4", "text": "CKB completion As baselines, we used the DNN AVG and DNN LSTM models (Li et al., 2016b ) that were described in Section 3.1."}
{"sent_id": "e0b72115e1905226d22876e72aa304-C001-202", "intents": ["@USE@"], "paper_id": "ABC_e0b72115e1905226d22876e72aa304_4", "text": "In Wiki gen, we used triples extracted by using the POS tag sequence pattern for each relation according to Li et al. (2016b) and scored each triple with CKB completion scores."}
{"sent_id": "e0b72115e1905226d22876e72aa304-C001-235", "intents": ["@USE@"], "paper_id": "ABC_e0b72115e1905226d22876e72aa304_4", "text": "The formulations of CKB completion in the two studies are the same, and we evaluated Li et al. (2016b) 's method as a baseline."}
{"sent_id": "e0b72115e1905226d22876e72aa304-C001-123", "intents": ["@UNSURE@"], "paper_id": "ABC_e0b72115e1905226d22876e72aa304_4", "text": "For the test and validation data, we randomly sampled negative examples, as described in Section 4, whose size was the same as the number of positive examples according to (Li et al., 2016b )."}
{"sent_id": "abc19723df6670960705eadbaa6c13-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_abc19723df6670960705eadbaa6c13_4", "text": "Since WordNet is freely available for research, various kinds of WSD studies based on WordNet can be compared with the method of [9] ."}
{"sent_id": "abc19723df6670960705eadbaa6c13-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_abc19723df6670960705eadbaa6c13_4", "text": "[10] reimplemented the method of [9] using a web, which may be a very large corpus, in order to collect example sentences."}
{"sent_id": "abc19723df6670960705eadbaa6c13-C001-160", "intents": ["@USE@"], "paper_id": "ABC_abc19723df6670960705eadbaa6c13_4", "text": "We can observe that in our implementation of the method of [9] , the data sparseness problem is very serious since unambiguous relatives are usually not frequent in the raw corpus."}
{"sent_id": "34346688a7e5166ee7b559ccbfe8e3-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_34346688a7e5166ee7b559ccbfe8e3_4", "text": "Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017) , hybrid models (Mager et al., 2018b; Moeller et al., 2018) , and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018) ."}
{"sent_id": "34346688a7e5166ee7b559ccbfe8e3-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_34346688a7e5166ee7b559ccbfe8e3_4", "text": "We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN) (Kann et al., 2018) ."}
{"sent_id": "34346688a7e5166ee7b559ccbfe8e3-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_34346688a7e5166ee7b559ccbfe8e3_4", "text": "In the dataset from (Kann et al., 2018) , the maximum number of morphemes per word for MX is seven with an average of 2.13; for NH, six with an average of 2.2; for WX, maximum of ten with an average of 3.3; and for YN, the maximum is ten, with an average of 2.13."}
{"sent_id": "34346688a7e5166ee7b559ccbfe8e3-C001-17", "intents": ["@USE@"], "paper_id": "ABC_34346688a7e5166ee7b559ccbfe8e3_4", "text": "We use the datasets introduced by Kann et al. (2018) in an unsupervised fashion (unsegmented words)."}
{"sent_id": "34346688a7e5166ee7b559ccbfe8e3-C001-38", "intents": ["@USE@"], "paper_id": "ABC_34346688a7e5166ee7b559ccbfe8e3_4", "text": "They were constructed so they include both segmentable as well as non- Kann et al. (2018) , for training we do not use the segmented version of the data (our approach is unsupervised)."}
{"sent_id": "34346688a7e5166ee7b559ccbfe8e3-C001-82", "intents": ["@USE@"], "paper_id": "ABC_34346688a7e5166ee7b559ccbfe8e3_4", "text": "We evaluate the different AG setups on the blind test set from Kann et al. (2018) and compare our AG approaches to state-of-the-art unsupervised systems as well as supervised models including the best supervised deep learning models from Kann et al. (2018) ."}
{"sent_id": "34346688a7e5166ee7b559ccbfe8e3-C001-106", "intents": ["@USE@"], "paper_id": "ABC_34346688a7e5166ee7b559ccbfe8e3_4", "text": "An interesting observation is that for YN we only used the words in the training set of Kann et al. (2018) (unsegmented) , without any data augmentation."}
{"sent_id": "34346688a7e5166ee7b559ccbfe8e3-C001-70", "intents": ["@MOT@"], "paper_id": "ABC_34346688a7e5166ee7b559ccbfe8e3_4", "text": "However, since affixes and stems are not distinguished in the training annotations from Kann et al. (2018) , we only consider the first and last morphemes that appear at least five times."}
{"sent_id": "34346688a7e5166ee7b559ccbfe8e3-C001-73", "intents": ["@MOT@"], "paper_id": "ABC_34346688a7e5166ee7b559ccbfe8e3_4", "text": "Since the vocabulary in Kann et al. (2018) for each language is small, and the languages are from the same language family, one data augmentation approach is to train on all languages and test then on each language individually."}
{"sent_id": "34346688a7e5166ee7b559ccbfe8e3-C001-102", "intents": ["@UNSURE@"], "paper_id": "ABC_34346688a7e5166ee7b559ccbfe8e3_4", "text": "To obtain an upper bound, we compare the best AG setup to the best supervised neural methods presented in Kann et al. (2018) for each language."}
{"sent_id": "34346688a7e5166ee7b559ccbfe8e3-C001-107", "intents": ["@DIF@"], "paper_id": "ABC_34346688a7e5166ee7b559ccbfe8e3_4", "text": "For MX and WX, the neural models from Kann et al. (2018) (BestMTT and BestDA), outperform our unsupervised AG-based approaches."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "To this end, various sense-specific word embeddings have been proposed to account for the contextual subtlety of language (Reisinger and Mooney, 2010b,a; Huang et al., 2012; Neelakantan et al., 2015; Tian et al., 2014; Li and Jurafsky, 2015; Arora et al., 2016) ."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "(Reisinger and Mooney, 2010b; Huang et al., 2012; Neelakantan et al., 2015) uses neural networks to learn cluster embeddings in order to matcha polysemous word with its correct sense embeddings."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "This idea of local similarity has been widely used to obtain context sense representation Huang et al., 2012; Le and Mikolov, 2014; Neelakantan et al., 2015) ."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-121", "intents": ["@BACK@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "In (Huang et al., 2012; Neelakantan et al., 2015; , the relevance metric can be seen as the distance (cosine or Euclidean) between the query word and the context cluster center."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-24", "intents": ["@USE@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "We evaluate our approach on various tasks that require contextual understanding of words, combining existing and new test datasets and evaluation metrics: word-sense induction ( (Koeling et al., 2005; Bartunov et al., 2015) ), contextual word similarity ((Huang et al., 2012 ) and a new test set), and relevance detection ( (Arora et al., 2016) and a new test set)."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-158", "intents": ["@USE@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "Following (Huang et al., 2012) , we sort all the n = 2003 test pairs based on predicted similarity score and compare such ranking against the ground-truth ranking indicated by the average human evaluation score."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-81", "intents": ["@DIF@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "However, for embeddings from (Huang et al., 2012) and (Neelakantan et al., 2015) , which all have norm ≈ 1, the choice of α makes little difference."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-117", "intents": ["@DIF@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "Overall Performance Table 5 shows that our method consistently outperforms (Huang et al., 2012; Neelakantan et al., 2015) ."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-162", "intents": ["@DIF@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "We note that in (Huang et al., 2012 ) the similarity between two word-context pairs is the measured using avgSimC, a weighted average of cosine similarities between all possible representation vectors of w 1 and w 2 ."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-186", "intents": ["@DIF@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "One thing to note is that the SCWS Spearman scores of the (Huang et al., 2012) listed here are much smaller than that first reported."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-115", "intents": ["@MOT@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "Since the code in (Huang et al., 2012) allows choosing various distance functions, we pick all and report the best scores."}
{"sent_id": "32e860cdf03df7f6cb58b7f9e85ac0-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_32e860cdf03df7f6cb58b7f9e85ac0_4", "text": "Multiple works in the recent past (Bruni et al., 2014; Lazaridou et al., 2015; Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015; Kottur et al., 2016) have explored using perceptual modalities like vision and sound to learn language embeddings."}
{"sent_id": "32e860cdf03df7f6cb58b7f9e85ac0-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_32e860cdf03df7f6cb58b7f9e85ac0_4", "text": "While Lopopolo and van Miltenburg (2015) show preliminary results on using sound to learn distributional representations, Kiela and Clark (2015) build on ideas from Bruni et al. (2014) to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective."}
{"sent_id": "32e860cdf03df7f6cb58b7f9e85ac0-C001-65", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_32e860cdf03df7f6cb58b7f9e85ac0_4", "text": "AMEN and ASLex (Kiela and Clark, 2015) are subsets of the standard MEN (Bruni et al., 2014) and SimLex (Hill et al., 2015) word similarity datasets consisting of word-pairs that \"can be associated with a distinctive associated sound\"."}
{"sent_id": "32e860cdf03df7f6cb58b7f9e85ac0-C001-145", "intents": ["@BACK@"], "paper_id": "ABC_32e860cdf03df7f6cb58b7f9e85ac0_4", "text": "AMEN and ASLex (Kiela and Clark, 2015) are subsets of the MEN and SimLex-999 datasets for word relatedness grounded in sound."}
{"sent_id": "32e860cdf03df7f6cb58b7f9e85ac0-C001-57", "intents": ["@USE@"], "paper_id": "ABC_32e860cdf03df7f6cb58b7f9e85ac0_4", "text": "We use the freesound database (Font et al., 2013) , also used in prior work (Kiela and Clark, 2015; Lopopolo and van Miltenburg, 2015) to learn the proposed sound-word2vec embeddings."}
{"sent_id": "32e860cdf03df7f6cb58b7f9e85ac0-C001-108", "intents": ["@USE@"], "paper_id": "ABC_32e860cdf03df7f6cb58b7f9e85ac0_4", "text": "We use the openly available implementation for Lopopolo and van Miltenburg (2015) and re-implement Kiela and Clark (2015) and train them on our dataset for a fair comparison of the methods."}
{"sent_id": "32e860cdf03df7f6cb58b7f9e85ac0-C001-99", "intents": ["@DIF@"], "paper_id": "ABC_32e860cdf03df7f6cb58b7f9e85ac0_4", "text": "Our use of language embeddings as an initialization to fine-tune (specialize) from, as opposed to formulating a joint objective with language and audio context (Kiela and Clark, 2015) is driven by the fact that we are interested in embeddings for words grounded in sounds, and not better generic word similarity."}
{"sent_id": "32e860cdf03df7f6cb58b7f9e85ac0-C001-124", "intents": ["@DIF@"], "paper_id": "ABC_32e860cdf03df7f6cb58b7f9e85ac0_4", "text": "We see that specializing the embeddings for sound using our two-stage training outperforms prior work (Kiela and Clark (2015) and Lopopolo and van Miltenburg (2015) ), which did not do specialization."}
{"sent_id": "32e860cdf03df7f6cb58b7f9e85ac0-C001-127", "intents": ["@DIF@"], "paper_id": "ABC_32e860cdf03df7f6cb58b7f9e85ac0_4", "text": "Our approach performs better than Kiela and Clark (2015) ."}
{"sent_id": "32e860cdf03df7f6cb58b7f9e85ac0-C001-141", "intents": ["@DIF@"], "paper_id": "ABC_32e860cdf03df7f6cb58b7f9e85ac0_4", "text": "Lopopolo and van Miltenburg (2015) and Kiela and Clark (2015) perform worse than tag-word2vec with a mean rank of 48.4 and 42.1 respectively."}
{"sent_id": "32e860cdf03df7f6cb58b7f9e85ac0-C001-146", "intents": ["@DIF@"], "paper_id": "ABC_32e860cdf03df7f6cb58b7f9e85ac0_4", "text": "From Table 2, we can see that our embeddings outperform (Kiela and Clark, 2015) on both AMEN and ASLex."}
{"sent_id": "32e860cdf03df7f6cb58b7f9e85ac0-C001-106", "intents": ["@UNSURE@"], "paper_id": "ABC_32e860cdf03df7f6cb58b7f9e85ac0_4", "text": "We compare against previous works Lopopolo and van Miltenburg (2015) and Kiela and Clark (2015) ."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-3", "intents": ["@BACK@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "Predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest, with very promising results being reported across many studies [3, 9, 13, 26] ."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "The widespread use of smart-phones and social media offers new ways of assessing mental well-being, and recent research [1, 2, 3, 5, 9, 10, 13, 14, 22, 23, 26] has started exploring the effectiveness of these modalities for automatically assessing"}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "In order to overcome these issues, previous work [2, 5, 9, 10, 22, 26] has combined the instances {X uj i , y uj i } from different individuals u j and performed evaluation using randomised cross validation (MIXED)."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13, 26] ."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22, 26] , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10, 26] ."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-87", "intents": ["@BACK@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "Tsakalidis et al. [26] monitored the behaviour of 19 individuals over four months."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-97", "intents": ["@BACK@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "Since different users exhibit different mood scores on average [26] , by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-58", "intents": ["@USE@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "In order to examine this effect in both a regression and a classification setting, we have followed the experimental framework by Tsakalidis et al. [26] and Jaques et al. [9] ."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-108", "intents": ["@USE@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "We employed the dataset obtained by Tsakalidis et al. [26] , a pioneering dataset which contains a mix of longitudinal textual and mobile phone usage data for 30 subjects."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-141", "intents": ["@USE@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "In Experiment 1 we follow the setup in [26] : we perform 5-fold CV (MIXED) using SVM (SVR RBF ) and evaluate performance based on R 2 and RM SE."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-191", "intents": ["@USE@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "Experiment 1: Table 7 shows the results based on the evaluation setup of Tsakalidis et al. [26] ."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-207", "intents": ["@USE@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "P3: Results following the evaluation setup in [26] (MIXED), along with the results obtained in the LOIOCV and LOUOCV settings with (+) and without (-) per-user input normalisation."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-99", "intents": ["@UNSURE@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "While we focus on the works of Tsakalidis et al. [26] and Jaques et al. [9] , similar experimental setups were also followed in [10] , using the median of scores to separate the instances and performing five-fold cross-validation, and by Bogomolov et al. in [2] , working on a user-agnostic validation setting on 117 subjects to predict their happiness levels, and in [1] , for the stress level classification task."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-154", "intents": ["@UNSURE@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "Given a mood form and a set of snippets produced by a user before the completion of a mood form, we extracted some commonly used feature sets for every snippet written in English [26] , which were used in all experiments."}
{"sent_id": "ef6f1050651a4c3ac9a53438ac1f87-C001-192", "intents": ["@SIM@"], "paper_id": "ABC_ef6f1050651a4c3ac9a53438ac1f87_4", "text": "In the MIXED cases, the pattern is consistent with [26] , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets)."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-4", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "Koppel and Ordan (2011) performed empirical studies to validate both types of properties using English source texts and other texts translated into English."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-28", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "Koppel and Ordan (2011) perform empirical studies to validate both theories, using a subcorpus extracted from the Europarl (Koehn, 2005) and IHT corpora (Koppel and Ordan, 2011) ."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-51", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "Koppel and Ordan (2011) have built a classifier that can identify the correct source of the translated text (given different possible source languages)."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-90", "intents": ["@BACK@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "The experimental result of Koppel and Ordan (2011) shows that text translated into English holds this property."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-119", "intents": ["@BACK@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "In the past, researchers have used comparable corpora to validate these translation properties (Baroni and Bernardini, 2006; Pastor et al., 2008; Ilisei et al., 2009; Ilisei et al., 2010; Koppel and Ordan, 2011) ."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-121", "intents": ["@BACK@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "Only Koppel and Ordan (Koppel and Ordan, 2011) used English texts translated from multiple source languages."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-127", "intents": ["@BACK@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "Koppel and Ordan (2011) received the highest accuracy (96.7%) among all works noted above."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-62", "intents": ["@SIM@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "We can not compare our findings directly with Koppel and Ordan (2011) even though we use text from the same corpus and similar techniques."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-73", "intents": ["@SIM@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "We trust the source language marker that has been put by the respective translator, as did Lembersky et al.(2011) and Koppel and Ordan (2011) ."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-97", "intents": ["@SIM@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "Our hypothesis is again similar to Koppel and Ordan (2011) , that is, if the classifier's accuracy is close to 20%, then we cannot say that there is an interference effect in translated text."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-138", "intents": ["@SIM@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "We find our results to be compatible with Koppel and Ordan (2011) who used 300 function words."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-64", "intents": ["@DIF@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "Furthermore, instead of the list of 300 function words used by Koppel and Ordan (2011) , we used the 100 most frequent words for each candidate language."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-132", "intents": ["@DIF@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "Even though the classifier for German achieves around 99% accuracy, we cannot compare the result with Koppel and Ordan (Koppel and Ordan, 2011) as the amount of chunks for the classes are different."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-71", "intents": ["@USE@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "We extract a suitable corpus from the Europarl corpus in a way similar to Lembersky et al. (2011) and Koppel and Ordan (2011) ."}
{"sent_id": "cb64ba694c37df9ebc1065a1deac0f-C001-6", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_cb64ba694c37df9ebc1065a1deac0f_5", "text": "To that end, we present results with the CoMiC-EN Content Assessment system (Meurers et al., 2011a) on the dataset published by Mohler et al. (2011) and outline what was necessary to perform this comparison."}
{"sent_id": "cb64ba694c37df9ebc1065a1deac0f-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_cb64ba694c37df9ebc1065a1deac0f_5", "text": "Subsequently, we will zoom into the comparison of two of them, namely CoMiC-EN (Meurers et al., 2011a ) and the one which we call the Texas system (Mohler et al., 2011) and discuss the issues that arise with this endeavor. Returning to the bigger picture, we will explore how such systems could be compared in general, in the belief that meaningful comparison of approaches across research strands will be an important ingredient in advancing this relatively new research field."}
{"sent_id": "cb64ba694c37df9ebc1065a1deac0f-C001-145", "intents": ["@BACK@"], "paper_id": "ABC_cb64ba694c37df9ebc1065a1deac0f_5", "text": "Another recent approach is described by Mohler et al. (2011) , hereafter referred to as the Texas system."}
{"sent_id": "cb64ba694c37df9ebc1065a1deac0f-C001-152", "intents": ["@BACK@"], "paper_id": "ABC_cb64ba694c37df9ebc1065a1deac0f_5", "text": "For evaluating their system, Mohler et al. (2011) collected student responses from an online learning environment."}
{"sent_id": "cb64ba694c37df9ebc1065a1deac0f-C001-174", "intents": ["@BACK@"], "paper_id": "ABC_cb64ba694c37df9ebc1065a1deac0f_5", "text": "After discussing the broad landscape of Short Answer Evaluation systems, the main characteristics and differences, we now turn to a comparison of two concrete systems, namely CoMiC-EN (Meurers et al., 2011a ) and the Texas system Mohler et al. (2011) , to explore what is involved in such a concrete comparison of two systems from different contexts."}
{"sent_id": "cb64ba694c37df9ebc1065a1deac0f-C001-179", "intents": ["@BACK@"], "paper_id": "ABC_cb64ba694c37df9ebc1065a1deac0f_5", "text": "In evaluating the Texas system, Mohler et al. (2011) used a corpus of ten assignments and two exams from an introductory computer science class."}
{"sent_id": "cb64ba694c37df9ebc1065a1deac0f-C001-186", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_cb64ba694c37df9ebc1065a1deac0f_5", "text": "A bias towards correct answers can be observed, which is also mentioned by Mohler et al. (2011) ."}
{"sent_id": "cb64ba694c37df9ebc1065a1deac0f-C001-210", "intents": ["@BACK@"], "paper_id": "ABC_cb64ba694c37df9ebc1065a1deac0f_5", "text": "Therefore, Mohler et al. (2011) employ isotonic regression to map the ranking to the 0-5 scale."}
{"sent_id": "cb64ba694c37df9ebc1065a1deac0f-C001-211", "intents": ["@BACK@"], "paper_id": "ABC_cb64ba694c37df9ebc1065a1deac0f_5", "text": "In terms of performance, Mohler et al. (2011) report that the SVMRank system produces a better correlation measure (r = 0.518) while the SVR system yields a better RMSE (0.978)."}
{"sent_id": "cb64ba694c37df9ebc1065a1deac0f-C001-268", "intents": ["@BACK@"], "paper_id": "ABC_cb64ba694c37df9ebc1065a1deac0f_5", "text": "To that end, we gave an overview of the existing systems and picked two for a concrete comparison on the same data, the CoMiC-EN system (Meurers et al., 2011a ) and the Texas system (Mohler et al., 2011) ."}
{"sent_id": "cb64ba694c37df9ebc1065a1deac0f-C001-223", "intents": ["@EXT@"], "paper_id": "ABC_cb64ba694c37df9ebc1065a1deac0f_5", "text": "We chose Support Vector Regression (SVR) using libSVM 4 since that is one of the methods employed by Mohler et al. (2011) ."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-37", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "In practice, current statistical parsers do not encode LDD directly, as illustrated in Figure 2 , and leave it to post-processing procedures to recover the LDD relation (Johnson, 2002; Nivre et al., 2010) ."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "This figure illustrates the Stanford dependency representation that was used in Rimmel et al. (2009), and Nivre et al. (2010) , indicating below the sentence the long distance dependency that needs to be recovered, but that is not in the representation."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-115", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "Compared to the other statistical dependency parsers, questions (OQ) are not well represented in our training data, since they do not include the additional QB data (Nivre et al., 2010) used to improve the performance of MSTParser and MaltParser."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-140", "intents": ["@BACK@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "Both previous evaluation exercises (Rimell et al., 2009; Nivre et al., 2010) suggest some avenues to relax the matching conditions, and define equivalence classes of representations."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-143", "intents": ["@BACK@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "To relax the requirement of exact match on the definition of arc, a set of equivalence classes between single arcs and paths connecting two nodes indirectly is precisely defined in the post-processing scheme of Nivre et al. (2010) , which applies to the Stanford labelling scheme."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-144", "intents": ["@BACK@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "In Nivre et al. (2010) , the encoding of long-distance dependencies in a dependency parser is categorised as simple, complex, and indirect."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-198", "intents": ["@BACK@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "Automatic and manual results (percent recall) are shown in Table 1 , where we compare our results to the relevant ones of those reported in previous evaluations (Rimell et al., 2009; Nivre et al., 2010; Nguyen et al., 2012) ."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-125", "intents": ["@DIF@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "Unlike Nivre et al. (2010) , we did not use an external part-of-speech tagger to annotate the data of the development set."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-201", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "9 Like the other parsers discussed in Rimell et al. (2009) and Nivre et al. (2010) , the overall performance on these long-distance constructions is much lower than the overall scores for this parser."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-224", "intents": ["@DIF@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "Questions (OQ) are not well represented in our training data, since they do not include the additional QB data (Nivre et al., 2010) used to improve the performance of MSTParser and MaltParser (see Table 4 for comparison of number of errors for each parser)."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-118", "intents": ["@SIM@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "Like the dependency parser in Nivre et al. (2010) , the parser was not trained on the same data or tree representations as those used in the test data."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-132", "intents": ["@SIM@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "Like in previous papers (Rimell et al., 2009; Nivre et al., 2010) , we evaluate the parser on its ability to recover LDDs."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-152", "intents": ["@SIM@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "Following Nivre et al. (2010) , we define a longdistance dependency as simple or complex."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-214", "intents": ["@USE@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "We classify the errors made by our parser on the development set based on Nivre et al. (2010) one which occurs when the parser fails to assign the correct functional relation (e.g., subject, object), while a Sem error is one in which the parser fails to assign the correct semantic relation (e.g., A1, A2)."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-10", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "Recent research has attempted to induce unsupervised bilingual lexicons by aligning monolingual word vector spaces (Zhang et al., 2017a; Conneau et al., 2018; Aldarmaki et al., 2018; Artetxe et al., 2018a; Alvarez-Melis and Jaakkola, 2018; Mukherjee et al., 2018) ."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "As shown in Figure 1a , when the model of Conneau et al. (2018) is applied to English and Italian, the primal model maps the word \"three\" to the Italian word \"tre\", but the dual model maps \"tre\" to \"two\" instead of \"three\"."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "A typical line of work uses adversarial training (Miceli Barone, 2016; Zhang et al., 2017a,b; Conneau et al., 2018) , matching the distributions of source and target word embeddings through generative adversarial networks (Goodfellow et al., 2014) ."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-116", "intents": ["@BACK@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "The baselines include: (1) Procrustes (Conneau et al., 2018) , which learns a linear mapping through Procrustes Analysis (Schönemann, 1966) ."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-119", "intents": ["@BACK@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "(4) GeoMM semi , iterative GeoMM with weak supervision. (5) Adv-C-Procrustes (Conneau et al., 2018) , which refines the mapping learned by Adv-C with iterative Procrustes, which learns the new mapping matrix by constructing a bilingual lexicon iteratively."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-16", "intents": ["@MOT@", "@SIM@", "@DIF@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "In particular, we extend the model of Conneau et al. (2018) by using a cycle consistency loss (Zhou et al., 2016) to regularize two models in opposite directions."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-71", "intents": ["@MOT@", "@EXT@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "In preliminary experiments, we find in adversarial training that the single-direction criterion S(F, X, Y ) by Conneau et al. (2018) does not always work well."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-27", "intents": ["@SIM@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "In this paper, we choose Conneau et al. (2018) as our baseline as it is theoretically attractive and gives strong results on large-scale datasets."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-39", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "We take Conneau et al. (2018) as our baseline, introducing a novel regularizer to enforce cycle consistency."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-70", "intents": ["@USE@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "We follow Conneau et al. (2018) , using an unsupervised criterion to perform model selection."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-81", "intents": ["@USE@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "Our datasets includes: (i) The Multilingual Unsupervised and Supervised Embeddings (MUSE) dataset released by Conneau et al. (2018) ."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-83", "intents": ["@USE@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "We follow the evaluation setups of Conneau et al. (2018) , utilizing cross-domain similarity local scaling (CSLS) for retrieving the translation of given source words."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-84", "intents": ["@USE@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "Following a standard evaluation practice (Vulić and Moens, 2013; Mikolov et al., 2013; Conneau et al., 2018) , we report precision at 1 scores (P@1)."}
{"sent_id": "5a2cd80d7c57e06a51457e53169b49-C001-88", "intents": ["@UNSURE@"], "paper_id": "ABC_5a2cd80d7c57e06a51457e53169b49_5", "text": "We compare our method with Conneau et al. (2018) (Adv-C) under the same settings."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-5", "intents": ["@BACK@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by Niehues et al. (2011) ."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-20", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "In this paper, we base our approach to reordering on bilingual language models (Marino et al., 2006; Niehues et al., 2011) ."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "1 Originally, Marino et al. (2006) used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems (Niehues et al., 2011) ."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-27", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "Also previous contributions to bilingual language modeling (Marino et al., 2006; Niehues et al., 2011) have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to-kens with a rich set of POS tags."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-40", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "Our contributions can be summarized as follows: We argue that the contextual information used in the original bilingual models (Niehues et al., 2011) is insufficient and introduce a simple model that exploits source-side syntax to improve reordering (Sections 2 and 3)."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "This kind of intuition underlies the model of Niehues et al. (2011) , a bilingual LM (BiLM), which defines elementary translation events t 1 , ..., t n as follows:"}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-66", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "Niehues et al. (2011) refer to the defined translation events t i as bilingual tokens and we adopt this terminology."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-85", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "Niehues et al. (2011) also described an alternative variant of the original BiLM, where words are substituted by their POS tags (Figure 2 .a, shaded part)."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-102", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "In this section, we introduce our model which combines the BiLM from Niehues et al. (2011) with source dependency information."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-184", "intents": ["@BACK@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "We consider two variants of BiLM discussed by Niehues et al. (2011) : the standard one, Lex•Lex, and the simplest syntactic one, Pos•Pos."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-245", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "We argued that the very limited contextual information used in the original bilingual models (Niehues et al., 2011) can capture reorderings only to a limited degree and proposed a method to incorporate information from a source dependency tree in bilingual units."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-23", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "We adopt and generalize the approach of Niehues et al. (2011) to investigate several variations of bilingual language models."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-75", "intents": ["@USE@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "Since Niehues et al. (2011) have shown their model to work successfully as an additional feature in combination with commonly used standard phrase-based features, we use their approach as the main point of reference and base our approach on their segmentation method."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-76", "intents": ["@UNSURE@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "In the rest of the text we refer to Niehues et al. (2011) as the original BiLM."}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "An alternative way of dealing with structural differences is to reorder source language sentences to minimize structural divergence with the target language, (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007) ."}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-20", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "For example Wang et al. (2007) introduced a set of rules to decide if a (DE) construction should be reordered or not before translating to English:"}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-25", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "Indeed, Wang et al. (2007) found that the precision of their NP rules is only about 54.6% on a small human-judged set."}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-44", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "Similar to (Wang et al., 2007) , we only consider the majority case when the phrase with (DE) is a noun phrase modifier."}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-49", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "This is implicitly done in the work of Wang et al. (2007) where they use rules to decide if a certain DE and the words next to it will need to be reordered."}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-166", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "Also, we reorder the training data, the tuning and the test sets with the NP rules in (Wang et al., 2007) and compare our results with this second baseline (WANG-NP)."}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-99", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "Features 1-3 are inspired by the rules in (Wang et al., 2007) , and the fourth rule is based on the observation that even though the predicative adjective VA acts as a verb, it actually corresponds to adjectives in English as described in (Xia, 2000) ."}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-182", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "Our approach DE-Annotated reorders the Chinese sentence, which is similar to the approach proposed by Wang et al. (2007) (WANG-NP)."}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-140", "intents": ["@UNSURE@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "Comparing the 2-class accuracy to the (Wang et al., 2007) baseline, we have a 10.9% absolute improvement."}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-63", "intents": ["@EXT@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "2 As a baseline, we use the rules introduced in Wang et al. (2007) to decide if the DEs require reordering or not."}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-65", "intents": ["@EXT@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "So, in order to compare our classifier's performance with the rules in Wang et al. (2007) , we have to map our five-class results into two classes."}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-89", "intents": ["@USE@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "Secondly, we want to incorporate the rules in (Wang et al., 2007) as features in the log-linear classifier."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-3", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "Perhaps the best way to think about this problem is to contrast our work with that of Klein and Manning (2003) ."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003; Klein and Manning, 2003; Matsuzaki et al., 2005) (and others) ."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-13", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003) ."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-19", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "This is exactly what is done by Klein and Manning (2003) ."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-22", "intents": ["@BACK@", "@DIF@", "@MOT@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "The Klein and Manning (2003) parser is an unlexicalized PCFG with various carefully selected context annotations."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-33", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of Johnson (1998) and Klein and Manning (2003) ."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-77", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "Related to this, we further noticed that several of Klein & Manning's (2003) features, such as marking N P s as right recursive or possessive have the property of annotating with the label of the rightmost child (when they are NP and POS respectively)."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-81", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "Another Klein and Manning (2003) feature we try includes the temporal NP feature, where TMP markings in the treebank are retained, and propagated down the head inheritance path of the tree."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-143", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "We also always keep the heir label as a feature, following Klein and Manning (2003 (D, F, E, D, −) , where the first item is the heir of the parent's head."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-209", "intents": ["@BACK@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "While this is not as accurate as the hand-tailored grammar of Klein and Manning (2003) , it is close, and we believe there is room for improvement."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-73", "intents": ["@SIM@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "However, Klein and Manning (2003) find that this hurts performance relative to just marking the NPs, and so our Base feature does not insert."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-139", "intents": ["@SIM@", "@EXT@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "We perform this in a manner similar to Klein and Manning (2003) and Matsuzaki et al. (2005) Our mechanism lays out the unmarkovized intermediate rules in the same way, but we mostly use our clustering scheme to reduce sparsity."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-160", "intents": ["@SIM@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "In order to ease comparison between our work and that of Klein and Manning (2003) , we follow their lead in smoothing no production probabilities save those going from preterminal to nonterminal."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-52", "intents": ["@USE@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "As in (Johnson, 1998) and (Klein and Manning, 2003) , we annotate the Penn treebank nonterminals with various context information."}
{"sent_id": "d785838888358a711fbf07c9dcf430-C001-182", "intents": ["@UNSURE@"], "paper_id": "ABC_d785838888358a711fbf07c9dcf430_5", "text": "Unfortunately, our model does not perform quite as well as those of Klein and Manning (2003) or Matsuzaki et al. (2005) ."}
{"sent_id": "0a7710557d020087035f4a94b5661c-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_0a7710557d020087035f4a94b5661c_5", "text": "In Spanish and Bulgarian projected data extracted by Ganchev et al. (2009) In this paper, we present a dependency parsing algorithm which can train on partial projected parses and can take rich syntactic information as features for learning."}
{"sent_id": "0a7710557d020087035f4a94b5661c-C001-38", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_0a7710557d020087035f4a94b5661c_5", "text": "While Hwa et al. (2005) requires full projected parses to train their parser, Ganchev et al. (2009) and Jiang and Liu (2010) can learn from partially projected trees."}
{"sent_id": "0a7710557d020087035f4a94b5661c-C001-39", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_0a7710557d020087035f4a94b5661c_5", "text": "However, the discriminative training in (Ganchev et al., 2009 ) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse."}
{"sent_id": "0a7710557d020087035f4a94b5661c-C001-190", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_0a7710557d020087035f4a94b5661c_5", "text": "For Bulgarian and Spanish, we used the same test data that was used in the work of Ganchev et al. (2009) ."}
{"sent_id": "0a7710557d020087035f4a94b5661c-C001-43", "intents": ["@EXT@"], "paper_id": "ABC_0a7710557d020087035f4a94b5661c_5", "text": "We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al., 2009 ) for comparison."}
{"sent_id": "0a7710557d020087035f4a94b5661c-C001-172", "intents": ["@USE@"], "paper_id": "ABC_0a7710557d020087035f4a94b5661c_5", "text": "While the Hindi projected treebank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in (Ganchev et al., 2009) ."}
{"sent_id": "0a7710557d020087035f4a94b5661c-C001-173", "intents": ["@USE@"], "paper_id": "ABC_0a7710557d020087035f4a94b5661c_5", "text": "The datasets of Bulgarian and Spanish that contributed to the best accuracies for Ganchev et al. (2009) were used in our work (7 rules dataset for Bulgarian and 3 rules dataset for Spanish)."}
{"sent_id": "0a7710557d020087035f4a94b5661c-C001-204", "intents": ["@UNSURE@"], "paper_id": "ABC_0a7710557d020087035f4a94b5661c_5", "text": "In our work, while creating the data for the baseline by assigning random parents to unconnected words, acyclicity and projectivity con- Table 4 : Comparison of baseline, GNPPA and E-GNPPA with baseline and discriminative model from (Ganchev et al., 2009) for Bulgarian and Spanish."}
{"sent_id": "0a7710557d020087035f4a94b5661c-C001-203", "intents": ["@DIF@"], "paper_id": "ABC_0a7710557d020087035f4a94b5661c_5", "text": "The baseline reported in (Ganchev et al., 2009 ) significantly outperforms our baseline (see Table  4 ) due to the different baselines used in both the works."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-4", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "Second, we discuss the work done by (Barzilay & Lee, 2003) who use clustering of paraphrases to induce rewriting rules."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "Sentence Compression takes an important place for Natural Language Processing (NLP) tasks where specific constraints must be satisfied, such as length in summarization (Barzilay & Lee, 2002; Knight & Marcu, 2002; Shinyama et al., 2002; Barzilay & Lee, 2003; Le Nguyen & Ho, 2004; Unno et al., 2006) , style in text simplification (Marsi & Krahmer, 2005) or sentence simplification for subtitling (Daelemans et al., 2004) ."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "Second, we will discuss the work done by (Barzilay & Lee, 2003) who use clustering of paraphrases to induce rewriting rules."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "Two different approaches have been proposed for Sentence Compression: purely statistical methodologies (Barzilay & Lee, 2003; Le Nguyen & Ho, 2004) and hybrid linguistic/statistic methodologies (Knight & Marcu, 2002; Shinyama et al., 2002; Daelemans et al., 2004; Marsi & Krahmer, 2005; Unno et al., 2006) ."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "As our work is based on the first paradigm, we will focus on the works proposed by (Barzilay & Lee, 2003) and (Le Nguyen & Ho, 2004) ."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-34", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "Comparatively, (Barzilay & Lee, 2003) propose to use the N-gram Overlap metric to capture similarities between sentences and automatically create paraphrase corpora."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-37", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "Unlike (Le Nguyen & Ho, 2004) , one interesting idea proposed by (Barzilay & Lee, 2003 ) is to cluster similar pairs of paraphrases to apply multiplesequence alignment."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-50", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay & Lee, 2003; Dolan & Brockett, 2004) ."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-51", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "However, these unsupervised methodologies show a major drawback by extracting quasi-exact 2 or even exact match pairs of sentences as they rely on classical string similarity measures such as the Edit Distance in the case of (Dolan & Brockett, 2004) and word N-gram overlap for (Barzilay & Lee, 2003) ."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "In particular, it shows systematically better F-Measure and Accuracy measures over all other metrics showing an improvement of (1) at least 2.86% in terms of F-Measure and 3.96% in terms of Accuracy and (2) at most 6.61% in terms of FMeasure and 6.74% in terms of Accuracy compared to the second best metric which is also systematically the word N-gram overlap similarity measure used by (Barzilay & Lee, 2003) ."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "On one hand, as (Barzilay & Lee, 2003) evidence, clusters of paraphrases can lead to better learning of text-totext rewriting rules compared to just pairs of paraphrases."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-102", "intents": ["@DIF@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "These results confirm the observed figures in the previous subsection and reinforce the sight that clustering is a worthless effort for automatic paraphrase corpora construction, contrarily to what (Barzilay & Lee, 2003) suggest."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-185", "intents": ["@DIF@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "Experiments, by using 4 algorithms and through visualization techniques, revealed that clustering is a worthless effort for paraphrase corpora construction, contrary to the literature claims (Barzilay & Lee, 2003) ."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-18", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "Using the described approach, we demonstrate a word-error-rate (WER) reduction of 65% over a do-nothing input baseline, and we improve over a state-of-the-art system (Eskander et al., 2013) which relies heavily on language-specific and manually-selected constraints."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-27", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "The work of Eskander et al. (2013) is the most relevant to the present study: it presents a character-edit classification model (CEC) using the same dataset we use in this paper."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-43", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "To address errors occurring at the end 2 Eskander et al. (2013) also considered a slower, more expensive, and more language-specific method using a morphological tagger that outperformed the CEC model; however, we do not compare to it in this paper."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-75", "intents": ["@BACK@", "@MOT@", "@USE@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "We include a set of basic features inspired by Eskander et al. (2013) in their CEC system and additional features for further improvement."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-103", "intents": ["@BACK@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "Finally, the accuracy Acc metric, used by Eskander et al. (2013) , is a simple string matching metric which enforces a word alignment that pairs words in the reference to those of the output."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-126", "intents": ["@BACK@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "The Acc score reported by Eskander et al. (2013) for CEC+MLE is 91.3% ."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-33", "intents": ["@DIF@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "Moreover, in contrast to Eskander et al. (2013) , it looks beyond the boundary of the current word."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-68", "intents": ["@DIF@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "We further distinguish between the phenomena modeled by our system and by Eskander et al. (2013) ."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-69", "intents": ["@DIF@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "At least 10% of all generated action labels are not handled by Eskander et al. (2013) ."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-28", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "2 Eskander et al. (2013) analyzed the data to identify the seven most common types of errors."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-60", "intents": ["@USE@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "Labels modeled by Eskander et al. (2013) are marked with E , and EP for cases modeled partially, for example, the Insert{A} would only be applied at certain positions such as the end of the word."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-62", "intents": ["@USE@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "The CODA orthography was proposed by Habash et al. (2012) in an attempt to standardize dialectal writing, and we use it as a reference of correct text for spelling correction following the previous work by Eskander et al. (2013) ."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-95", "intents": ["@USE@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "The same speech effect handling was applied by Eskander et al. (2013) ."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-26", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "The state-of-the-art model for this line of work is the combination of the convolutional neural network (CNN) and the attention mechanism (Mullenbach et al. 2018) ."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-31", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "Our Mul-arXiv:1912.00862v1 [cs.CL] 25 Nov 2019 tiResCNN model is composed of five layers: the input layer leverages word embeddings pre-trained by word2vec (Mikolov et al. 2013) ; the multi-filter convolutional layer consists of multiple convolutional filters (Kim 2014); the residual convolutional layer contains multiple residual blocks (He et al. 2016) ; the attention layer keeps the interpretability for the model following (Mullenbach et al. 2018) ; the output layer utilizes the sigmoid function to predict the probability of each ICD code."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-37", "intents": ["@BACK@", "@UNSURE@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "Compared with 5 existing and stateof-the-art models (Perotte et al. 2013; Prakash et al. 2017; Shi et al. 2017; Baumel et al. 2018; Mullenbach et al. 2018) , our model outperformed them in nearly all the evaluation metrics (i.e., macro-and micro-AUC, macro-and micro-F1, precision at K)."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "Mullenbach et al. (2018) incorporated the convolutional neural network (CNN) with per-label attention mechanism."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-148", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "Therefore, some hyper-parameter values were chosen empirically or following prior work (Mullenbach et al. 2018 )."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-172", "intents": ["@BACK@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "CAML & DR-CAML The Convolutional Attention network for Multi-Label classification (CAML) was proposed by Mullenbach et al. (2018) ."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-204", "intents": ["@UNSURE@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "Table 4 , we observed that our model outperformed all the baselines, namely C-MemNN (Prakash et al. 2017 ), C-LSTM-Att (Shi et al. 2017) , CAML and DR-CAML (Mullenbach et al. 2018) , in all evaluation metrics."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-110", "intents": ["@USE@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "Following Mullenbach et al. (2018) , we employed the perlabel attention mechanism to make each ICD code attend to different parts of the document representation H. The attention layer is formalized as:"}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-118", "intents": ["@USE@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "For training, we treated the ICD coding task as a multi-label classification problem following previous work (McCallum 1999; Mullenbach et al. 2018) ."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-125", "intents": ["@USE@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "Following Mullenbach et al. (2018) , we used discharge summaries, split them by patient IDs, and conducted experiments using the full codes as well as the top-50 most frequent codes."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-132", "intents": ["@USE@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "Preprocessing Following previous work (Mullenbach et al. 2018) , the text was tokenized, and each token were transformed into its lowercase."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-152", "intents": ["@USE@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "• CNN, which only has one convolutional filter and is equivalent to the CAML model (Mullenbach et al. 2018 )."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-226", "intents": ["@USE@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "For CAML, we used the optimal hyper-parameter setting reported in their paper (Mullenbach et al. 2018) ."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "These word embeddings have been shown to contain the same biases [3] , due to the source data from which they are trained."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-17", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "First we propose a new version of the Word Embedding Association Tests (WEATs) studied in [3] , designed to demonstrate and quantify bias in word embeddings, which puts them on a firm foundation by using the Linguistic Inquiry and Word Count (LIWC) lexica [17] to systematically detect and measure embedding biases."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-59", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "We first propose a new version of the Word Embedding Association Tests studied in [3] by using the LIWC lexica to systematically detect and measure the biases within the embedding, keeping the tests comparable with the same set of target words."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-110", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "This supports a similar finding for U.S. employment statistics using an independent set of occupations found in [3] ."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-149", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "In this paper, we have introduced the LIWC-WEAT, a set of objective tests extending the association tests in [3] by using the LIWC lexica to measure bias within word embeddings."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-150", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "We found bias in both the associations of gender and race, as first described in [3] , while additionally finding that male names have a slightly higher positive association than female names."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-73", "intents": ["@EXT@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "We begin by using the target words from [3] which were originally used in [8] , allowing us to directly compare our findings with the original WEAT."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-74", "intents": ["@EXT@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "Our approach differs from that of [3] in that while we use the same set of target words in each test, we use an expanded set of attribute words, allowing us to perform a more rigorous, systematic study of the associations found within the word embeddings."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-77", "intents": ["@EXT@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "For each of the original word categories used in [3] , we matched them with their closest equivalent within the LIWC categories, for example matching the word lists for 'career' and 'family' with the 'work' and 'family' LIWC categories."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-89", "intents": ["@EXT@", "@UNSURE@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "As shown in Fig. 1c , we found that the set of male names was more associated with the concept of work, while the female names were more associated with family, mirroring the results found in [3] ."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-81", "intents": ["@USE@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "Taking the list of target European-American and African-American names used in [3] , we tested each of them for their associated with the positive and negative emotion concepts found in [17] by using the methodology described by Eq. 3 in Sec. II-B, replacing the short list of words used to originally represent pleasant and unpleasant attribute sets."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-88", "intents": ["@USE@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "3) Association of Gender with Career and Family : Taking the list of target gendered names used in [3] , we tested each of them for their associated with the career and family concepts using the categories of 'work' and 'family' found in LIWC [17] ."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-83", "intents": ["@SIM@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "This finding supports the association test in [3] , where they also found that European-American names were more pleasant than African-American names."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-87", "intents": ["@SIM@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "The results of our test again support the findings of [3] , with Maths and Science terms being more closely associated with males, while Arts terms are more closely associated with females, as shown in Fig. 1b."}
{"sent_id": "f2925513a7cce2e80ade1f948164d0-C001-127", "intents": ["@SIM@"], "paper_id": "ABC_f2925513a7cce2e80ade1f948164d0_5", "text": "Male and Females names tested in [3] showed a clear distinction in their association with work and family respectively, with our replication of the test in Sec. III-B3 finding the same results."}
{"sent_id": "4cb16f436d910d82c3661052c1fa30-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_4cb16f436d910d82c3661052c1fa30_5", "text": "Most of the previous coreference resolution methods have similar classification phases, implemented either as decision trees (Soon et al., 2001) or as maximum entropy classifiers (Luo et al., 2004) ."}
{"sent_id": "4cb16f436d910d82c3661052c1fa30-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_4cb16f436d910d82c3661052c1fa30_5", "text": "In contrast, globally optimized clustering decisions were reported in (Luo et al., 2004) and (DaumeIII and Marcu, 2005a) , where all clustering possibilities are considered by searching on a Bell tree representation or by using the Learning as Search Optimization (LaSO) framework (DaumeIII and Marcu, 2005b) respectively, but the first search is partial and driven by heuristics and the second one only looks back in text."}
{"sent_id": "4cb16f436d910d82c3661052c1fa30-C001-22", "intents": ["@BACK@", "@DIF@", "@EXT@"], "paper_id": "ABC_4cb16f436d910d82c3661052c1fa30_5", "text": "BESTCUT replaces the bottom-up search in a tree representation (as it was performed in (Luo et al., 2004) ) with the top-down problem of obtaining the best partitioning of a graph."}
{"sent_id": "4cb16f436d910d82c3661052c1fa30-C001-181", "intents": ["@BACK@"], "paper_id": "ABC_4cb16f436d910d82c3661052c1fa30_5", "text": "This only shows that none of them is particularly poor, but it is not a relevant way of comparing methods-the MUC metric has been found too indulgent by researchers ( (Luo et al., 2004) , (Baldwin et al., 1998) Table 4 : Comparison of results between three clusterization algorithms on ACE Phase 2."}
{"sent_id": "4cb16f436d910d82c3661052c1fa30-C001-51", "intents": ["@EXT@"], "paper_id": "ABC_4cb16f436d910d82c3661052c1fa30_5", "text": "We duplicated the statistical model used by (Luo et al., 2004) , with three differences."}
{"sent_id": "4cb16f436d910d82c3661052c1fa30-C001-54", "intents": ["@EXT@"], "paper_id": "ABC_4cb16f436d910d82c3661052c1fa30_5", "text": "Third, as opposed to (Luo et al., 2004) , who represented all numerical features quantized, we translated each numerical feature into a set of binary features that express whether the value is in certain intervals."}
{"sent_id": "4cb16f436d910d82c3661052c1fa30-C001-197", "intents": ["@UNSURE@"], "paper_id": "ABC_4cb16f436d910d82c3661052c1fa30_5", "text": "It is of interest to discuss why our implementation of the Belltree system (Luo et al., 2004 ) is comparable in performance to Link-Best (Ng and Cardie, 2002) ."}
{"sent_id": "4cb16f436d910d82c3661052c1fa30-C001-48", "intents": ["@USE@"], "paper_id": "ABC_4cb16f436d910d82c3661052c1fa30_5", "text": "We created the training examples in the same way as (Luo et al., 2004) , by pairing all mentions of the same type, obtaining their feature vectors and taking the outcome (coreferent/noncoreferent) from the key files."}
{"sent_id": "4cb16f436d910d82c3661052c1fa30-C001-168", "intents": ["@USE@"], "paper_id": "ABC_4cb16f436d910d82c3661052c1fa30_5", "text": "The clusterization algorithms that we implemented to evaluate in comparison with our method are (Luo et al., 2004) 's Belltree and Link-Best (best-first clusterization) from (Ng and Cardie, 2002) ."}
{"sent_id": "4cb16f436d910d82c3661052c1fa30-C001-171", "intents": ["@USE@"], "paper_id": "ABC_4cb16f436d910d82c3661052c1fa30_5", "text": "Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F (Luo et al., 2004) and the MUC P, R and F scores (Vilain et al., 1995) ."}
{"sent_id": "4cb16f436d910d82c3661052c1fa30-C001-189", "intents": ["@USE@"], "paper_id": "ABC_4cb16f436d910d82c3661052c1fa30_5", "text": "Baseline system has the (Luo et al., 2004) features."}
{"sent_id": "715cba53c376e50b76a0966ff16a6a-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_715cba53c376e50b76a0966ff16a6a_5", "text": "Polarity classification, which determine whether the sentiment expressed in a document is positive or negative, is one of the most popular tasks of sentiment analysis (Dasgupta and Ng, 2009 )."}
{"sent_id": "715cba53c376e50b76a0966ff16a6a-C001-18", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_715cba53c376e50b76a0966ff16a6a_5", "text": "Second, sentiment classification systems are typically domain-specific, which makes the expensive process of annotating a large amount of data for each domain and is a bottleneck in building high quality systems (Dasgupta and Ng, 2009 )."}
{"sent_id": "715cba53c376e50b76a0966ff16a6a-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_715cba53c376e50b76a0966ff16a6a_5", "text": "Recently, semi-supervised learning, which uses large amount of unlabeled data together with labeled data to build better learners (Raina et al., 2007; Zhu, 2007) , has drawn more attention in sentiment analysis (Dasgupta and Ng, 2009; Li, et al., 2009) ."}
{"sent_id": "715cba53c376e50b76a0966ff16a6a-C001-27", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_715cba53c376e50b76a0966ff16a6a_5", "text": "Recently, active learning had been applied in sentiment classification (Dasgupta and Ng, 2009) ."}
{"sent_id": "715cba53c376e50b76a0966ff16a6a-C001-55", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_715cba53c376e50b76a0966ff16a6a_5", "text": "Supervised sentiment classification systems are domain-specific and annotating a large scale corpus for each domain is very expensive (Dasgupta and Ng, 2009 )."}
{"sent_id": "715cba53c376e50b76a0966ff16a6a-C001-65", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_715cba53c376e50b76a0966ff16a6a_5", "text": "However, unsupervised learning of sentiment is difficult, partially because of the prevalence of sentimentally ambiguous reviews (Dasgupta and Ng, 2009 )."}
{"sent_id": "715cba53c376e50b76a0966ff16a6a-C001-123", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_715cba53c376e50b76a0966ff16a6a_5", "text": "Following previous work on active learning for SVMs (Dasgupta and Ng, 2009; Tong and Koller, 2002) , we define the uncertainty of a review as its distance from the separating hyperplane."}
{"sent_id": "715cba53c376e50b76a0966ff16a6a-C001-187", "intents": ["@BACK@"], "paper_id": "ABC_715cba53c376e50b76a0966ff16a6a_5", "text": "We compare the classification performance of ADN with five representative classifiers, i.e., Semi-supervised spectral learning (Spectral) (Kamvar et al., 2003) , Transductive SVM (TSVM), Active learning (Active) (Tong and Koller, 2002) , Mine the Easy Classify the Hard (MECH) (Dasgupta and Ng, 2009) , and Deep Belief Networks (DBN) (Hinton, et al., 2006) ."}
{"sent_id": "715cba53c376e50b76a0966ff16a6a-C001-189", "intents": ["@BACK@"], "paper_id": "ABC_715cba53c376e50b76a0966ff16a6a_5", "text": "MECH is a new semi-supervised method for sentiment classification (Dasgupta and Ng, 2009) ."}
{"sent_id": "715cba53c376e50b76a0966ff16a6a-C001-202", "intents": ["@BACK@"], "paper_id": "ABC_715cba53c376e50b76a0966ff16a6a_5", "text": "The results of previous four methods are reported by Dasgupta and Ng (2009) ."}
{"sent_id": "715cba53c376e50b76a0966ff16a6a-C001-78", "intents": ["@SIM@"], "paper_id": "ABC_715cba53c376e50b76a0966ff16a6a_5", "text": "We preprocess these reviews to be classified, which is similar with Dasgupta and Ng (2009) ."}
{"sent_id": "715cba53c376e50b76a0966ff16a6a-C001-129", "intents": ["@EXT@"], "paper_id": "ABC_715cba53c376e50b76a0966ff16a6a_5", "text": "The experimental setting is similar with Dasgupta & Ng (2009) ."}
{"sent_id": "715cba53c376e50b76a0966ff16a6a-C001-167", "intents": ["@EXT@"], "paper_id": "ABC_715cba53c376e50b76a0966ff16a6a_5", "text": "Similar with Dasgupta and Ng (2009), we divide the 2,000 reviews into ten equal-sized folds randomly and test all the algorithms with crossvalidation."}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-18", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "Levy et al. (2017) present a reformulation of RE, where the task is framed as reading comprehension."}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-39", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "Zero-shot relation extraction Levy et al. (2017) propose a novel approach towards achieving this generalization by transforming relations into natural language question templates."}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-179", "intents": ["@BACK@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "All monolingual models' word embeddings were initialised using fastText embeddings trained on each language's Wikipedia and common crawl corpora, 7 except for the comparison experiments described in sub-section 5.1 where GloVe (Pennington et al., 2014) was used for comparability with Levy et al. (2017) ."}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-185", "intents": ["@BACK@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "UnENT UnREL Levy et al. (2017) Faruqui and Kumar (2015) employed a pipeline of machine translation systems to translate to English, then Open RE systems to perform RE on the translated text, followed by crosslingual projection back to source language."}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-56", "intents": ["@USE@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "2 Slot-filling data To extract the contexts for each triple in our dataset we use the distant supervision method described by Levy et al. (2017) ."}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-116", "intents": ["@USE@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "This approach was shown by Levy et al. (2017) to have significantly better paraphrasing abilities than when only one question template or simpler relation descriptions are employed."}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-117", "intents": ["@USE@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "Evaluation Our evaluation methodology follows Levy et al. (2017) ."}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-126", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "Comparison with Levy et al. (2017) In Table  3 , the comparison between the nil-aware machine comprehension framework we employ (Mono) and the results reported by Levy et al. (2017) using the bias-augmented BiDAF model on their dataset (and splits) can be seen."}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-59", "intents": ["@EXT@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "Querification Levy et al. (2017) created 1192 question templates for 120 Wikidata properties."}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-84", "intents": ["@DIF@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "In a set of initial trials (see Table 3 ), we found that this model far outperformed the bias-augmented BiDAF model (Seo et al., 2016) used by Levy et al. (2017)"}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-152", "intents": ["@UNSURE@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "This indicates that it is more difficult to transfer the ability to identify relation paraphrases and entity types through global cues 6 which Levy et al. (2017) suggested are important for generalizing to new rela- 5 We therefore continue the rest of our experiments in the paper using the multilingual fastText embeddings."}
{"sent_id": "7ac01a84ab696e7fa9d0ce336a393e-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_7ac01a84ab696e7fa9d0ce336a393e_5", "text": "They were soon extended with speech modality: speech recordings for the captions of Flickr8k were collected by [3] via crowdsourcing; spoken captions for MSCOCO were generated using Google Text-To-Speech (TTS) by [4] and using Voxygen TTS by [5] ; extensions of these corpora to other languages than English, such as Japanese, were also introduced by [6] ."}
{"sent_id": "7ac01a84ab696e7fa9d0ce336a393e-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_7ac01a84ab696e7fa9d0ce336a393e_5", "text": "This paper focuses on computational models of visually grounded speech that were introduced by [14, 4] ."}
{"sent_id": "7ac01a84ab696e7fa9d0ce336a393e-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_7ac01a84ab696e7fa9d0ce336a393e_5", "text": "Learned representations of such models were analyzed by [11, 7, 4] : [11] introduced novel methods for interpreting the activation patterns of recurrent neural networks (RNN) in a model of visually grounded meaning representation from textual and visual input and showed that RNN pay attention to word tokens belonging to specific lexical categories."}
{"sent_id": "7ac01a84ab696e7fa9d0ce336a393e-C001-83", "intents": ["@BACK@"], "paper_id": "ABC_7ac01a84ab696e7fa9d0ce336a393e_5", "text": "Original implementation by [4] with RHN reports median rank r = 13 on English dataset."}
{"sent_id": "7ac01a84ab696e7fa9d0ce336a393e-C001-19", "intents": ["@DIF@"], "paper_id": "ABC_7ac01a84ab696e7fa9d0ce336a393e_5", "text": "While [11, 7, 4] focused on analyzing speech representations learnt by speech-image neural models from a phonological and semantic point of view, the present work focuses on lexical acquisition and the way speech utterances are segmented into lexical units and processed by a computational model of visually grounded speech."}
{"sent_id": "7ac01a84ab696e7fa9d0ce336a393e-C001-50", "intents": ["@DIF@"], "paper_id": "ABC_7ac01a84ab696e7fa9d0ce336a393e_5", "text": "Contrary to the original model ( [4] ), we used GRU units instead of RHN units."}
{"sent_id": "7ac01a84ab696e7fa9d0ce336a393e-C001-37", "intents": ["@USE@"], "paper_id": "ABC_7ac01a84ab696e7fa9d0ce336a393e_5", "text": "The model we use for our experiments is based on that of [4] ."}
{"sent_id": "7ac01a84ab696e7fa9d0ce336a393e-C001-74", "intents": ["@USE@"], "paper_id": "ABC_7ac01a84ab696e7fa9d0ce336a393e_5", "text": "Spoken COCO dataset was introduced by [4] for English."}
{"sent_id": "7ac01a84ab696e7fa9d0ce336a393e-C001-75", "intents": ["@USE@"], "paper_id": "ABC_7ac01a84ab696e7fa9d0ce336a393e_5", "text": "We followed the same methodology as [4] and generated synthetic speech for each caption in the Japanese STAIR dataset."}
{"sent_id": "7ac01a84ab696e7fa9d0ce336a393e-C001-76", "intents": ["@USE@"], "paper_id": "ABC_7ac01a84ab696e7fa9d0ce336a393e_5", "text": "We created the spoken STAIR dataset so it would follow the exact same train/val/test 5 split as [4] ."}
{"sent_id": "7ac01a84ab696e7fa9d0ce336a393e-C001-57", "intents": ["@EXT@"], "paper_id": "ABC_7ac01a84ab696e7fa9d0ce336a393e_5", "text": "In the original architecture ( [4] ), attention follows the last recurrent layer."}
{"sent_id": "b71321a9252376308d627c439e85b7-C001-19", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_b71321a9252376308d627c439e85b7_5", "text": "Though, unlike semantics in NMT, syntactically-aware NMT has been a relatively hot topic recently, with a number of approaches claiming improvements from using treebank syntax Eriguchi et al., 2016; Nadejde et al., 2017; Bastings et al., 2017; Aharoni and Goldberg, 2017) , our graphs are different from syntactic structures."}
{"sent_id": "b71321a9252376308d627c439e85b7-C001-21", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_b71321a9252376308d627c439e85b7_5", "text": "Luckily, the modeling approach of Bastings et al. (2017) does not make any assumptions about the graph structure, and thus we build on their method."}
{"sent_id": "b71321a9252376308d627c439e85b7-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_b71321a9252376308d627c439e85b7_5", "text": "Bastings et al. (2017) used Graph Convolutional Networks (GCNs) to encode syntactic structure."}
{"sent_id": "b71321a9252376308d627c439e85b7-C001-56", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_b71321a9252376308d627c439e85b7_5", "text": "While GCNs were introduced BiRNN CNN Baseline (Bastings et al., 2017) 14.9 12.6 +Sem 15.6 13.4 +Syn (Bastings et al., 2017) 16.1 13.7 +Syn + Sem 15.8 14.3 for modeling undirected unlabeled graphs (Kipf and Welling, 2016) , in this paper we use a formulation of GCNs for labeled directed graphs, where the direction and the label of an edge are incorporated."}
{"sent_id": "b71321a9252376308d627c439e85b7-C001-65", "intents": ["@BACK@"], "paper_id": "ABC_b71321a9252376308d627c439e85b7_5", "text": "Gates are particularly useful when the graph is predicted BiRNN Baseline (Bastings et al., 2017) 23.3 +Sem 24.5 +Syn (Bastings et al., 2017) 23.9 +Syn + Sem 24.9 and thus may contain errors, i.e., wrong edges."}
{"sent_id": "b71321a9252376308d627c439e85b7-C001-30", "intents": ["@SIM@", "@MOT@", "@USE@"], "paper_id": "ABC_b71321a9252376308d627c439e85b7_5", "text": "As we use exactly the same modeling approach as in the syntactic method of Bastings et al. (2017) , we can easily compare the influence of the types of linguistic structures (i.e., syntax vs. semantics)."}
{"sent_id": "b71321a9252376308d627c439e85b7-C001-57", "intents": ["@USE@"], "paper_id": "ABC_b71321a9252376308d627c439e85b7_5", "text": "In particular, we follow the formulation of and Bastings et al. (2017) for syntactic graphs and apply it to dependency-based semantic-role structures (Hajic et al., 2009 ) (as in Figure 1 )."}
{"sent_id": "b71321a9252376308d627c439e85b7-C001-80", "intents": ["@USE@"], "paper_id": "ABC_b71321a9252376308d627c439e85b7_5", "text": "The settings and the framework (Neural Monkey (Helcl and Libovický, 2017) ) used for experiments are the ones used in Bastings et al. (2017) , which we use as baselines."}
{"sent_id": "b71321a9252376308d627c439e85b7-C001-86", "intents": ["@USE@"], "paper_id": "ABC_b71321a9252376308d627c439e85b7_5", "text": "As in Bastings et al. (2017) , we used the standard attention-based encoder-decoder model as a baseline."}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "We have recently introduced a new transdimensional random field (TRF 1 ) LM [4] , where the whole sentence is modeled as a random field."}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-19", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "Improvements: First, in [4] , the diagonal elements of the Hessian matrices are online estimated during the SA iterations to rescale the gradients, which is shown to benefit the convergence of the training algorithm."}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "As defined in [4] , a trans-dimensional random field model represents the joint probability of the pair (l, x l ) as"}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-48", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "In order to make use of Hessian information in parameter optimization, we use the online estimated Hessian diagonal elements to rescale the gradients in [4] ."}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-52", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "Step I: MCMC sampling: Generate a sample set B (t) with p(l, x l ; λ (t−1) , ζ (t−1) ) as the stationary distribution, using the trans-dimensional mixture sampling method (See Section 3.3 in [4] )."}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "Fig.1 show an example of convergence curves of the SA training algorithm in [4] and the new improved SA."}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-89", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "The improved SA algorithm (in Section 2.2) is used to train the TRF LMs, in conjunction with the trans-dimensional mixture sampling proposed in Section 3.3 of [4] ."}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-90", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "The learning rates of λ and ζ are set as suggested in [4] :"}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-92", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "The class information is also used to accelerate the sampling, and more than one CPU cores are used to parallelize the algorithm, as described in [4] ."}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-171", "intents": ["@BACK@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "Equally importantly, evaluations in this paper and also in [4] have shown that TRF LMs are able to perform as good as NN LMs (either RNN or FNN) on a variety of tasks."}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-41", "intents": ["@EXT@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "In the joint SA training algorithm [4] , we define another form of mixture distribution as follows:"}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-127", "intents": ["@USE@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "In this section, speech recognition and 1000-best list rescoring experiments are conducted as configured in [4] ."}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-133", "intents": ["@UNSURE@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "The TRF LMs are compared with the classic KN n-gram LMs, the RNN LM [3] and the results reported in [4] ."}
{"sent_id": "05b53f9e0a347c4f47d0fd066538c7-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_05b53f9e0a347c4f47d0fd066538c7_5", "text": "In this work, we focus on the recent regression formulation of EFP that aims to predict a real score in the range of [-3,+3 ] to quantify the occurrence possibility of a given event mention (Stanovsky et al., 2017; Rudinger et al., 2018) ."}
{"sent_id": "05b53f9e0a347c4f47d0fd066538c7-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_05b53f9e0a347c4f47d0fd066538c7_5", "text": "There are two major mechanisms that can help the models to identify the cue words and link them to the anchor words, i.e., the syntactic trees (i.e., the dependency trees) and the semantic information (Rudinger et al., 2018) ."}
{"sent_id": "05b53f9e0a347c4f47d0fd066538c7-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_05b53f9e0a347c4f47d0fd066538c7_5", "text": "The meaning of such cue words and their interactions with the anchor words can be captured via their distributed representations (i.e., with word embeddings and long-short term memory networks (LSTM)) (Rudinger et al., 2018) ."}
{"sent_id": "05b53f9e0a347c4f47d0fd066538c7-C001-22", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_05b53f9e0a347c4f47d0fd066538c7_5", "text": "The current state-of-the-art approach for EFP has involved deep learning models (Rudinger et al., 2018 ) that examine both syntactic and semantic information in the modeling process."}
{"sent_id": "05b53f9e0a347c4f47d0fd066538c7-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_05b53f9e0a347c4f47d0fd066538c7_5", "text": "(Qian et al., 2018) employ Generative Adversarial Networks (GANs) for EFP while (Rudinger et al., 2018) utilize LSTMs for both sequential and dependency representations of the input sentences."}
{"sent_id": "05b53f9e0a347c4f47d0fd066538c7-C001-97", "intents": ["@BACK@"], "paper_id": "ABC_05b53f9e0a347c4f47d0fd066538c7_5", "text": "We compare the proposed model with the best reported systems in the literature with linguistic features (Lee et al., 2015; Stanovsky et al., 2017) and deep learning (Rudinger et al., 2018) ."}
{"sent_id": "05b53f9e0a347c4f47d0fd066538c7-C001-52", "intents": ["@USE@"], "paper_id": "ABC_05b53f9e0a347c4f47d0fd066538c7_5", "text": "In the next step, we further abstract (e 1 , e 2 , . . . , e n ) for EFP by feeding them into two layers of bidirectional LSTMs (as in (Rudinger et al., 2018) )."}
{"sent_id": "05b53f9e0a347c4f47d0fd066538c7-C001-80", "intents": ["@USE@"], "paper_id": "ABC_05b53f9e0a347c4f47d0fd066538c7_5", "text": "Finally, similar to (Rudinger et al., 2018) , the feature vector V is fed into a regression model with two layers of feed-forward networks to produce the factuality score."}
{"sent_id": "05b53f9e0a347c4f47d0fd066538c7-C001-81", "intents": ["@USE@"], "paper_id": "ABC_05b53f9e0a347c4f47d0fd066538c7_5", "text": "Following (Rudinger et al., 2018) , we train the proposed model by optimizing the Huber loss with δ = 1 and the Adam optimizer with learning rate = 1.0."}
{"sent_id": "05b53f9e0a347c4f47d0fd066538c7-C001-86", "intents": ["@USE@"], "paper_id": "ABC_05b53f9e0a347c4f47d0fd066538c7_5", "text": "Following the previous work (Stanovsky et al., 2017; Rudinger et al., 2018) , we evaluate the proposed EFP model using four benchmark datasets: FactBack (Saurí and Pustejovsky, 2009 ), UW (Lee et al., 2015) , Meantime (Minard et al., 2016) and UDS-IH2 (Rudinger et al., 2018) ."}
{"sent_id": "05b53f9e0a347c4f47d0fd066538c7-C001-88", "intents": ["@USE@"], "paper_id": "ABC_05b53f9e0a347c4f47d0fd066538c7_5", "text": "For the fourth dataset (i.e., UDS-IH2), we follow the instructions in (Rudinger et al., 2018) to scale the scores to the range of [-3, +3] ."}
{"sent_id": "05b53f9e0a347c4f47d0fd066538c7-C001-57", "intents": ["@DIF@"], "paper_id": "ABC_05b53f9e0a347c4f47d0fd066538c7_5", "text": "Given the hidden representation (h 1 , h 2 , . . . , h n ), it is possible to use the hidden vector corresponding to the anchor word h k as the features to perform factuality prediction (as done in (Rudinger et al., 2018) )."}
{"sent_id": "05b53f9e0a347c4f47d0fd066538c7-C001-99", "intents": ["@EXT@"], "paper_id": "ABC_05b53f9e0a347c4f47d0fd066538c7_5", "text": "Importantly, to achieve a fair comparison, we obtain the actual implementation of the current state-of-the-art EFP models from (Rudinger et al., 2018) , introduce the BERT embeddings as the inputs for those models and compare them with the proposed models (i.e., the rows with \"+BERT\")."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-2", "intents": ["@BACK@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "This paper reports the performances of shallow word-level convolutional neural networks (CNN), our earlier work (2015) [3, 4] , on the eight datasets with relatively large training data that were used for testing the very deep characterlevel CNN in Conneau et al. (2016) [1]."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "Recently, several variations of convolutional neural networks (CNNs) [7] have been shown to achieve high accuracy on text categorization (see e.g., [3, 4, 9, 1] and references therein) in comparison with a number of methods including linear methods, which had long been the state of the art."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "• Our earlier work (2015) [3, 4] : shallow word-level CNNs (taking sequences of words as input), which we abbreviate as word-CNN."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "In [3, 4] the shallow word-CNN was shown to perform well, using training sets (most intensively, 25K documents) that are mostly smaller than those used in [1] ."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "While these results imply that the shallow word-CNN is likely to outperform the deep char-CNN when trained with relatively small training sets such as those used in [3, 4] , the shallow word-CNN is untested on the training sets as large as those used in [1] ."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "We start with briefly reviewing the very deep word-CNN of [1] and the shallow word-CNN of [3, 4] ."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "**SHALLOW WORD-LEVEL CNNS AS IN [3, 4]**"}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "Two types of word-CNN were proposed in [3, 4] , which are illustrated in Figure 1 ."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-19", "intents": ["@MOT@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "Hence, the purpose of this report is to fill the gap by testing the shallow word-CNNs as in [3, 4] on the datasets used in [1] , for direct comparison with the results of very deep char-CNNs reported in [1] ."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-20", "intents": ["@SIM@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "Limitation of work In this work, our new experiments are limited to the shallow word-CNN as in [3, 4] ."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-130", "intents": ["@SIM@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "We observe that, as in our previous work [4] , additional input produced by tv-embeddings led to substantial improvements."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-144", "intents": ["@SIM@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "• The shallow word-CNNs as in [3, 4] generally achieved better error rates than those of the very deep char-CNNs reported in [1] ."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-65", "intents": ["@USE@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "See also the supplementary material of [4] for the representation power analysis."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-109", "intents": ["@USE@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "Tv-embedding training was done as in [4] ; weighted square loss was minimized without regularization while the target regions (adjacent regions) were represented by bow vectors, and the data weights were set so that the negative sampling effect was achieved."}
{"sent_id": "f2b9a5633600cdf787111841bf9ce6-C001-78", "intents": ["@DIF@"], "paper_id": "ABC_f2b9a5633600cdf787111841bf9ce6_5", "text": "In [4] , tv-embedding training was done using unlabeled data as an additional resource; therefore, the proposed models were semi-supervised models."}
{"sent_id": "920f2b94270c0711fcc19ad23dbb0d-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_920f2b94270c0711fcc19ad23dbb0d_6", "text": "2 Wulczyn et al. (2017) estimated that only 17.9% of personal attacks in Wikipedia discussions were followed by moderator actions."}
{"sent_id": "920f2b94270c0711fcc19ad23dbb0d-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_920f2b94270c0711fcc19ad23dbb0d_6", "text": "Krippendorff's (2004) alpha was 0.4762, close to the value (0.45) reported by Wulczyn et al. (2017) for the Wikipedia 'attacks' dataset."}
{"sent_id": "920f2b94270c0711fcc19ad23dbb0d-C001-70", "intents": ["@BACK@"], "paper_id": "ABC_920f2b94270c0711fcc19ad23dbb0d_6", "text": "The Wikipedia 'attacks' dataset (Wulczyn et al., 2017) contains approx."}
{"sent_id": "920f2b94270c0711fcc19ad23dbb0d-C001-91", "intents": ["@BACK@"], "paper_id": "ABC_920f2b94270c0711fcc19ad23dbb0d_6", "text": "DETOX (Wulczyn et al., 2017) was the previous state of the art in comment moderation, in the sense that it had the best reported results on the Wikipedia datasets (Section 2.2), which were in turn the largest previous publicly available dataset of moderated user comments."}
{"sent_id": "920f2b94270c0711fcc19ad23dbb0d-C001-184", "intents": ["@BACK@"], "paper_id": "ABC_920f2b94270c0711fcc19ad23dbb0d_6", "text": "Scores reported by Wulczyn et al. (2017) are shown in brackets."}
{"sent_id": "920f2b94270c0711fcc19ad23dbb0d-C001-199", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_920f2b94270c0711fcc19ad23dbb0d_6", "text": "We also repeated the annotator ensemble experiment of Wulczyn et al. (2017) on 8K randomly chosen comments of W-ATT-TEST (4K comments from random users, 4K comments from banned users)."}
{"sent_id": "920f2b94270c0711fcc19ad23dbb0d-C001-22", "intents": ["@USE@"], "paper_id": "ABC_920f2b94270c0711fcc19ad23dbb0d_6", "text": "Furthermore, we experiment on the 'attacks' dataset of Wulczyn et al. (2017) , approx."}
{"sent_id": "920f2b94270c0711fcc19ad23dbb0d-C001-88", "intents": ["@USE@"], "paper_id": "ABC_920f2b94270c0711fcc19ad23dbb0d_6", "text": "We experimented with an RNN operating on word embeddings, the same RNN enhanced with our attention mechanism (a-RNN), a vanilla convolutional neural network (CNN) also operating on word embeddings, the DETOX system of Wulczyn et al. (2017) , and a baseline that uses word lists."}
{"sent_id": "920f2b94270c0711fcc19ad23dbb0d-C001-30", "intents": ["@UNSURE@"], "paper_id": "ABC_920f2b94270c0711fcc19ad23dbb0d_6", "text": "On both datasets (Gazzetta and Wikipedia comments) and for both scenarios (automatic, semiautomatic), we show that a recurrent neural network (RNN) outperforms the system of Wulczyn et al. (2017) , the previous state of the art for comment moderation, which employed logistic regression or a multi-layer Perceptron (MLP), and represented each comment as a bag of (character or word) n-grams."}
{"sent_id": "920f2b94270c0711fcc19ad23dbb0d-C001-203", "intents": ["@SIM@"], "paper_id": "ABC_920f2b94270c0711fcc19ad23dbb0d_6", "text": "We conclude that RNN and a-RNN are as good as an ensemble of 7 human annotators; CNN is as good as 4 annotators; DETOX is as good as 4 in AUC and 3 annotators in Spearman correlation, which is consistent with the results of Wulczyn et al. (2017) ."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "Crosslingual representations are induced to represent words, phrases, or documents for more than one language, where the representations are constrained to preserve representational similarity or can be transformed between languages (Klementiev et al., 2012; Hermann & Blunsom, 2014) ."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "Examples of such transfer learning tasks are crosslingual sentiment analysis (Wan, 2009) and crosslingual document classification (Klementiev et al., 2012) ."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "Klementiev et al. (2012) used automatically aligned sentences and words to constrain word representations across languages based on the number of times a given word in one language was aligned to a word in another language."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-84", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "Klementiev et al. (2012) use a neural language model to leverage monolingual data."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-89", "intents": ["@BACK@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "They achieve high accuracy on the German → English sub-task of the crosslingual document classification task introduced by Klementiev et al. (2012) ."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-124", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "Like previous work, we evaluate our method on the crosslingual document classification task introduced by Klementiev et al. (2012) ."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-162", "intents": ["@BACK@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "The result table includes previous work as well as the Glossed, the machine translation and the majority class baselines from Klementiev et al. (2012) ."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-27", "intents": ["@MOT@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "Klementiev et al. (2012) ; ; Gouws et al. (2014) all learn their representations using a word-level monolingual objective."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-55", "intents": ["@SIM@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "Following the work of Klementiev et al. (2012) ; Hermann & Blunsom (2014) ; Gouws et al. (2014) we represent each word as a vector and use separate word representations for each language."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-134", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "Like Klementiev et al. (2012) we choose EuroParl v7 (Koehn, 2005) as our bilingual corpus and leverage the English and German parts of the RCV1 and RCV2 corpora as monolingual resources."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-170", "intents": ["@SIM@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "To show that our method achieves high accuracy even with a reduced vocabulary, we discard representations for infrequent terms and report results using our best setup with the same vocabulary size as Klementiev et al. (2012) ."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-189", "intents": ["@SIM@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "Depending on the amount of training data available the accuracy achieved with our models is comparable or greatly improves upon previously reported results for the crosslingual document classification task introduced by Klementiev et al. (2012) ."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-65", "intents": ["@USE@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "Following Klementiev et al. (2012) we split our objective into two sub-objectives, a bilingual objective minimizing the transfer errors and a monolingual objective minimizing the monolingual errors for l 1 and l 2 ."}
{"sent_id": "754ceac25ff3a711ec3737e7eb860b-C001-126", "intents": ["@USE@"], "paper_id": "ABC_754ceac25ff3a711ec3737e7eb860b_6", "text": "We use the original data and the original implementation of the averaged perceptron used by Klementiev et al. (2012) to evaluate the document representations created by our method."}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-2", "intents": ["@BACK@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "This paper gives an Abstract Categorial Grammar (ACG) account of (Kallmeyer and Kuhlmann, 2012)'s process of transformation of the derivation trees of Tree Adjoining Grammar (TAG) into dependency trees."}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "While alternative proposals have succeeded in linking derivation trees to semantic representations using unification (Kallmeyer and Romero, 2004; Kallmeyer and Romero, 2007) or using an encoding (Pogodalla, 2004; Pogodalla, 2009) of TAG into the ACG framework (de Groote, 2001) , only recently (Kallmeyer and Kuhlmann, 2012) has proposed a transformation from standard derivation trees to dependency trees."}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "First, it exhibits the underlying lexical blow up of the yield functions associated with the elementary trees in (Kallmeyer and Kuhlmann, 2012) ."}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "3 The TAG literature typically uses this example, and (Kallmeyer and Kuhlmann, 2012) as well, to show the mismatch between the derivation trees and the expected se- This sentence is usually analyzed in TAG with a derivation tree where the to love component scopes over all the other arguments, and where claims and seems are unrelated, as Fig. 2(a) shows."}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "bol and L yield (X 0 ) = X. Then, the derivation tree, the derived tree, and the yield of Fig. 2 are represented by: Trees (Kallmeyer and Kuhlmann, 2012) 's process to translate derivation trees into dependency trees is a two-step process."}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "According to (Kallmeyer and Kuhlmann, 2012) , such edge reversal is due to the fact that an edge between a complement taking adjunction (CTA) and an initial tree has to be reversed, while the other edges remain unchanged."}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "Moreover, in case an initial tree accepts several adjunction of CTAs, (Kallmeyer and Kuhlmann, 2012) hypothesizes that the farther from the head a CTA is, the higher it is in the dependency tree."}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "In order to do such reversing operations, (Kallmeyer and Kuhlmann, 2012) uses Macro Tree Transducers (MTTs) (Engelfriet and Vogler, 1985) ."}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "The yield of the derived tree resulting from the operations of the derivation tree γ of Fig. 3 defined in (Kallmeyer and Kuhlmann, 2012)"}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-99", "intents": ["@BACK@", "@MOT@", "@SIM@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "This is true both for (Kallmeyer and Kuhlmann, 2012) 's approach and ours."}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "6 This really mimics (Yoshinaka, 2006) 's encoding of (Kallmeyer and Kuhlmann, 2012) MTT rules: . . . are designed in order to indicate that a given adjunction has n adjunctions above it (i.e. which scope over it)."}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-142", "intents": ["@BACK@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "In this paper, we have given an ACG perspective on the transformation of the derivation trees of TAG to the dependency trees proposed in (Kallmeyer and Kuhlmann, 2012) ."}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-148", "intents": ["@BACK@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "The encoding of the second step then made explicit the lexical blow up for the interpretation of the functional symbols of the dependency trees in (Kallmeyer and Kuhlmann, 2012 )'s construct."}
{"sent_id": "f1eae0918a246174b1866ba71d4efc-C001-151", "intents": ["@BACK@"], "paper_id": "ABC_f1eae0918a246174b1866ba71d4efc_6", "text": "Finally, under the assumption of (Kallmeyer and Kuhlmann, 2012) of plausible dependency structures, we get two possible grammatical approaches to the surface-semantics relation that are related but independent: it can be equivalently modeled using either a phrase structure or a dependency model."}
{"sent_id": "6293d300ab46a6d6135ed256005403-C001-29", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_6293d300ab46a6d6135ed256005403_6", "text": "However, to evaluate the semantic relatedness between two triples, Zhang et al. (2014) primarily relied on the text of triples and used search engines, which makes their method difficult to re-implement and in turn limits its application in practice."}
{"sent_id": "6293d300ab46a6d6135ed256005403-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_6293d300ab46a6d6135ed256005403_6", "text": "The most closely related work in this area is our own (Zhang et al., 2014) , which used the triples of SPO as background knowledge."}
{"sent_id": "6293d300ab46a6d6135ed256005403-C001-45", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_6293d300ab46a6d6135ed256005403_6", "text": "Triple graph Zhang et al. (2014) proposed the triple graph as a document representation, where the triples of SPO serve as nodes, and the edges between nodes indicate their semantic relatedness."}
{"sent_id": "6293d300ab46a6d6135ed256005403-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_6293d300ab46a6d6135ed256005403_6", "text": "Relevance evaluation To compute the weight of a edge, Zhang et al. (2014) evaluate the semantic relatedness between two nodes with a search engine-based method."}
{"sent_id": "6293d300ab46a6d6135ed256005403-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_6293d300ab46a6d6135ed256005403_6", "text": "Using r(i, j), Zhang et al. (2014) further define p(i, j), the probability of t i and t j propagating to each other, as shown in Formula 2."}
{"sent_id": "6293d300ab46a6d6135ed256005403-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_6293d300ab46a6d6135ed256005403_6", "text": "After convergence, the relevance weight of t b will be treated as the final relevance to D. There are in total n × n pairs of nodes, and their p(i, j) are stored in a matrix P. Zhang et al. (2014) use W = (w 1 , w 2 , . . . , w n ) to denote the relevance weights of nodes, where w i indicates the relevance of t i to D. At the beginning, each w i of bk-nodes is initialized to 0, and each that of sd-nodes is initialized to its importance to D. Then W is updated to W after every iteration according to Formula 3. They keep updating the weights of both sd-nodes and bk-nodes until convergence and do not distinguish them explicitly."}
{"sent_id": "6293d300ab46a6d6135ed256005403-C001-97", "intents": ["@BACK@"], "paper_id": "ABC_6293d300ab46a6d6135ed256005403_6", "text": "There are three primary modifications to the original model of Zhang et al. (2014) , all of which are shown more powerful in our experiments."}
{"sent_id": "6293d300ab46a6d6135ed256005403-C001-98", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_6293d300ab46a6d6135ed256005403_6", "text": "First of all, the original model (Zhang et al., 2014) does not reset the relevance weight of sdnodes after every iteration."}
{"sent_id": "6293d300ab46a6d6135ed256005403-C001-150", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_6293d300ab46a6d6135ed256005403_6", "text": "Experimental setup Previous research relies on manual annotation to evaluate the ranking performance (Zhang et al., 2014) , which costs a lot, and in which it is difficult to get high consistency."}
{"sent_id": "6293d300ab46a6d6135ed256005403-C001-207", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_6293d300ab46a6d6135ed256005403_6", "text": "This study encodes distributional semantics into the triple-based background knowledge ranking model (Zhang et al., 2014) for better document enrichment."}
{"sent_id": "6293d300ab46a6d6135ed256005403-C001-141", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_6293d300ab46a6d6135ed256005403_6", "text": "Baseline systems As Zhang et al. (2014) argued, it is difficult to use the methods in traditional ranking tasks, such as information retrieval (Manning et al., 2008) and entity linking (Han et al., 2011; Sen, 2012) , as baselines in this task, because our model takes triples as basic input and thus lacks some crucial information such as link structure."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "Studies on automatic humor recognition (Mihalcea and Strapparava, 2005; Yang et al., 2015; Zhang and Liu, 2014; Purandare and Litman, 2006 ) have defined the recognition task as a binary classification task."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-21", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "Among the studies on humor classification, Mihalcea and Strapparava (2005) and Yang et al. (2015) reported high performance on the task."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-26", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "Some humor classification studies (Mihalcea and Strapparava, 2005; Yang et al., 2015; Barbieri and Saggion, 2014 ) have used negative instances from different domains or topics, because non-humorous sentences could not be found or are very challenging to collect in target domains or topics."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "Previous studies (Mihalcea and Strapparava, 2005; Yang et al., 2015; Zhang and Liu, 2014; Purandare and Litman, 2006; Bertero and Fung, 2016) dealt with the humor recognition task as a binary classification task, which was to categorize a given text as humorous or non-humorous."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "Yang et al. (2015) tried to minimize genre differences between humorous and non-humorous texts in order to avoid a chance that a trained model was optimized to distinguish genre differences."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-67", "intents": ["@BACK@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "Yang et al. (2015) collected a corpus of Pun of Day data 1 ."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "In order to reduce the differences between positive and negative instances in the data, Yang et al. (2015) used two constraints when collecting negative instances."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-142", "intents": ["@BACK@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "In this section, we present expeirments that we ran to determine 1) how effective a model trained using 'Pun of Day' data (Pun) is when applied to TED Talk data (Talk), and 2) whether the performance of a model trained using Talk data would be similar to the performance reported in Yang et al. (2015) ."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-57", "intents": ["@DIF@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "Mihalcea and Strapparava (2005) and Yang et al. (2015) borrowed negative instances from different genres such as news websites or proverbs."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-143", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "We reimplemented features developed by Yang et al. (2015) and evaluated those features on Talk data."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-148", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "We followed the experimental setup of Yang et al. (2015) in order to see if the performance of our duplicated features was comparable to their reported performance."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-173", "intents": ["@DIF@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "Yang et al. (2015) borrowed negative instances from different genres such as news websites and proverbs. But, in Talk-to-Talk, both positive and negative instances were from the same genre."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-103", "intents": ["@USE@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "Utilizing the same experimental setup as Mihalcea and Strapparava (2005) and Yang et al. (2015) (50% positive and 50% negative instances), we selected 4,726 sentences from among all collected nonhumorous sentences as negative instances."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-114", "intents": ["@USE@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "Features from Yang et al. (2015) , which we implemented, consisted of (1) two incongruity features, (2) six ambiguity features, (3) four interpersonal effect features, (4) four phonetic features, (5) five k-Nearest Neighbor features, and (6) 300 Word2Vec features."}
{"sent_id": "9684063f991f9a4688d6530fe5a16c-C001-170", "intents": ["@UNSURE@"], "paper_id": "ABC_9684063f991f9a4688d6530fe5a16c_6", "text": "In the experiments described in the preceding section, we weren't able to get results comparable to Yang et al. (2015) when Talk data was used in both train and evaluation data."}
{"sent_id": "74cd12a801d1f8a95f8898a8cef9c0-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_74cd12a801d1f8a95f8898a8cef9c0_6", "text": "Previous experiments with tasks like language modelling [Bengio et al., 2009] , Dependency Parsing, and entailment [Hashimoto et al., 2016] have shown faster convergence and performance gains by following a curriculum training regimen in the order of increasingly complicated syntactic and semantic tasks."}
{"sent_id": "74cd12a801d1f8a95f8898a8cef9c0-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_74cd12a801d1f8a95f8898a8cef9c0_6", "text": "[Hashimoto et al., 2016] propose a hierarchical multitask neural architecture with the lower layers performing syntactic tasks, and the higher layers performing the more involved semantic tasks while using the lower layer predictions."}
{"sent_id": "74cd12a801d1f8a95f8898a8cef9c0-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_74cd12a801d1f8a95f8898a8cef9c0_6", "text": "Like [Hashimoto et al., 2016] , they hypothesize the incorporation of simpler syntactic information into semantic tasks, and provide empirical evidence for the same."}
{"sent_id": "74cd12a801d1f8a95f8898a8cef9c0-C001-71", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_74cd12a801d1f8a95f8898a8cef9c0_6", "text": "Curriculum learning can be seen as a sequence of training criteria [Bengio et al., 2009] , with increasing task or sample difficulty as the training progresses."}
{"sent_id": "74cd12a801d1f8a95f8898a8cef9c0-C001-78", "intents": ["@BACK@"], "paper_id": "ABC_74cd12a801d1f8a95f8898a8cef9c0_6", "text": "In addition to the above tasks, Language Model pretraining has shown significant performance gains as reported by [Howard and Ruder, 2018] ."}
{"sent_id": "74cd12a801d1f8a95f8898a8cef9c0-C001-86", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_74cd12a801d1f8a95f8898a8cef9c0_6", "text": "As noted in earlier efforts [Howard and Ruder, 2018 ] towards finetuning pretrained models for NLP tasks, aggressive finetuning can cause catastrophic forgetting, thus causing the model to simply fit over the target task and forget any capabilities gained during the pretraining stage."}
{"sent_id": "74cd12a801d1f8a95f8898a8cef9c0-C001-107", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_74cd12a801d1f8a95f8898a8cef9c0_6", "text": "We experiment with average pooling and max pooling concatenation over hidden states for semantic prediction, similar to [Howard and Ruder, 2018] , and observe increase in model accuracy by 2.2% on sentiment analysis."}
{"sent_id": "74cd12a801d1f8a95f8898a8cef9c0-C001-114", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_74cd12a801d1f8a95f8898a8cef9c0_6", "text": "We note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [Bengio et al., 2009; Howard and Ruder, 2018] ."}
{"sent_id": "74cd12a801d1f8a95f8898a8cef9c0-C001-116", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_74cd12a801d1f8a95f8898a8cef9c0_6", "text": "As discussed in Section 4.3, for our transfer learning optimization experiments, we segment the optimization of different parameters of our model with different learning rates, in order to limit catastrophic forgetting and interference among the tasks, as proposed by [Howard and Ruder, 2018] ."}
{"sent_id": "74cd12a801d1f8a95f8898a8cef9c0-C001-91", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_74cd12a801d1f8a95f8898a8cef9c0_6", "text": "With this purview, similar to [Howard and Ruder, 2018] , we propose optimizing different layers in our model to different extents, and keep lower step sizes for the deeper pretrained layers while finetuning on a downstream task."}
{"sent_id": "74cd12a801d1f8a95f8898a8cef9c0-C001-95", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_74cd12a801d1f8a95f8898a8cef9c0_6", "text": "Gradual Unfreezing: Similar to [Howard and Ruder, 2018] , rather than updating all the layers together for finetuning, we explore gradual ordered unfreezing of layers."}
{"sent_id": "4588d13c734d1ca0f348e056b1d39e-C001-21", "intents": ["@BACK@", "@SIM@", "@MOT@"], "paper_id": "ABC_4588d13c734d1ca0f348e056b1d39e_6", "text": "Specifically, by extending scalar self-attention models such as those proposed in Lin et al. (2017) , we propose vectorbased multi-head attention, which includes the widely used max pooling, mean pooling, and scalar selfattention itself as special cases."}
{"sent_id": "4588d13c734d1ca0f348e056b1d39e-C001-173", "intents": ["@BACK@"], "paper_id": "ABC_4588d13c734d1ca0f348e056b1d39e_6", "text": "The BiLSTM with self-attention proposed by Lin et al. (2017) achieves better result than CNN and BiLSTM with max pooling."}
{"sent_id": "4588d13c734d1ca0f348e056b1d39e-C001-183", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_4588d13c734d1ca0f348e056b1d39e_6", "text": "The green lines indicate scalar selfattention pooling added on top of the BiLSTMs, same as in Lin et al. (2017) , and the blue lines indicate vector-based attention used in our generalized pooling methods."}
{"sent_id": "4588d13c734d1ca0f348e056b1d39e-C001-213", "intents": ["@BACK@", "@FUT@"], "paper_id": "ABC_4588d13c734d1ca0f348e056b1d39e_6", "text": "Our future work includes exploring more effective MLP to use the structures of multi-head vectors, inspired by the idea from Lin et al. (2017) ."}
{"sent_id": "4588d13c734d1ca0f348e056b1d39e-C001-100", "intents": ["@DIF@"], "paper_id": "ABC_4588d13c734d1ca0f348e056b1d39e_6", "text": "Instead of using AA T − I 2 F to encourage the diversity for scalar attention matrix as in Lin et al. (2017) , we propose the following formula to encourage the diversity for vectorial attention matrices."}
{"sent_id": "4588d13c734d1ca0f348e056b1d39e-C001-174", "intents": ["@DIF@"], "paper_id": "ABC_4588d13c734d1ca0f348e056b1d39e_6", "text": "One of our baseline models using max pooling on BiLSTM achieves accuracies of 65.00% and 82.30% on the Yelp and the Age dataset respectively, which is already better than the self-attention model proposed by Lin et al. (2017) ."}
{"sent_id": "4588d13c734d1ca0f348e056b1d39e-C001-129", "intents": ["@USE@"], "paper_id": "ABC_4588d13c734d1ca0f348e056b1d39e_6", "text": "Age Dataset To compare our models with that of Lin et al. (2017) , we use the same Age dataset in our experiment here, which is an Author Profiling dataset."}
{"sent_id": "4588d13c734d1ca0f348e056b1d39e-C001-133", "intents": ["@USE@"], "paper_id": "ABC_4588d13c734d1ca0f348e056b1d39e_6", "text": "We use the same data split as in Lin et al. (2017) , i.e., 68,485 samples for training, 4,000 for development, and 4,000 for testing."}
{"sent_id": "4588d13c734d1ca0f348e056b1d39e-C001-137", "intents": ["@USE@"], "paper_id": "ABC_4588d13c734d1ca0f348e056b1d39e_6", "text": "We use the same data split as in Lin et al. (2017) , i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-5", "intents": ["@BACK@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "Saffran et al. (1996) assume transitional probability, but Brent (1999a) claims mutual information (MI) is more appropriate."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-7", "intents": ["@BACK@", "@SIM@", "@DIF@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "This paper replicates Brent's (1999a) mutualinformation model on a corpus of childdirected speech in Modern Greek, and introduces a variant model using a global threshold."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "Several models (e.g., Batchelder, 2002; Brent's (1999a) MBDP-1 model; Davis, 2000; de Marcken, 1996; Olivier, 1968) simultaneously address the question of vocabulary acquisition, using previously learned word-candidates to bootstrap later segmentations."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-48", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "Saffran et al. assume implicitly, and Brent (1999a) explicitly, that the proper comparison is local-in Brent, dependent solely on the adjacent pairs of segments."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-58", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "In order to see that, we must examine Brent's (1999a) suggested implementation of Saffran et al. (1996) more closely."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-64", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "Neither Brent's (1999a) implementation of Saffran's et al. (1996) heuristic nor utterance-boundary heuristic can explain how these might be learned."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-81", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "While in its general approach the study reported here replicates the mutual-information and transitional-probability models in Brent (1999a) , it differs slightly in the details of their use."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-83", "intents": ["@BACK@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "4 Secondly, we compare the use of a global threshold (described in more detail in Section 2.3, below) to Brent's (1999a) use of the local context (as described in Section 1.3 above)."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-84", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "Like (Brent, 1999a) , but unlike Saffran et al. (1996) , our model focuses on pairs of segments, not on pairs of syllables."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-116", "intents": ["@BACK@", "@MOT@", "@USE@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "Like Brent (1999a) and indeed most models in the literature, this model assumes (for sake of convenience and simplicity) that the child hears each segment produced within an utterance without error."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-147", "intents": ["@BACK@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "Tables 2 and 3 present the results over the test set for both the global and the local comparisons of the predictability statistics proposed by Saffran et al. (1996) and Brent (1999a) ."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-142", "intents": ["@SIM@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "The second metric, the percentage of word tokens detected, is the same as Brent (1999a) ."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-150", "intents": ["@SIM@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "The findings here confirm Brent's (1999a) contention that mutual information is a better measure of predictability than is transitional probability-at least for the task of identifying words, not just boundaries."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-144", "intents": ["@MOT@", "@EXT@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "The last metric (word type) is slightly more conservative than Brent's (1999a) in that the word type must have been actually spoken in the same utterance (not the same block of 500 utterances) in which it was detected to count as a match."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "As a motivating case, consider an instance of the color reference task from Monroe et al. (2017) shown in the first row of Table 1 ."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-33", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "In particular, we show that incorporating pragmatic reasoning at training time yields improved, state-of-the-art accuracy for listener models on the color reference task from Monroe et al. (2017) , and the effect demonstrated by this improvement is especially large under small training data sizes."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "Prior work has shown that neural network models trained to capture the meanings of utterances can be improved using pragmatic reasoning at test time via the RSA framework (Andreas and Klein, 2016; Monroe et al., 2017; Goodman and Frank, 2016; Frank and Goodman, 2012) ."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "For instance, Monroe et al. (2017) train context-agnostic (i.e. non-pragmatic) neural network models to learn the meanings of color utterances using a corpus of examples of the form shown in the first line of Table 1 ."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "We compare neural nets trained pragmatically and non-pragmatically on a new color-grid reference game corpus as well as the color reference corpus from Monroe et al. (2017) ."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "The color reference game from Monroe et al. (2017) consists of rounds played between a speaker and a listener."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-79", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "Similar to Monroe et al. (2017), we resolve this issue by taking a small set of samples from the pre-trained LSTM applied to each object in a context, to approximate p(U | O), each time l 1 is computed during training and evaluation."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-113", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "For the color reference task, we use the data collected by Monroe et al. (2017) from human play on the color reference task through Amazon Mechanical Turk using the framework of Hawkins (2015)."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-118", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "For model development, we use the train/dev/test split from Monroe et al. (2017) with 15, 665 training, 15, 670 dev, and 15, 659 test rounds."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-120", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "Our use of the CIELAB color space departs from prior work on the color data which used a 54-dimensional Fourier space (Monroe et al., 2017 (Monroe et al., , 2016 Zhang and Lu, 2002) ."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-162", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "Furthermore, relative to state-of-the-art in Monroe et al. (2017) , Table 2 shows that our pragmatically trained model yields improved accuracy over their best \"blended\" pragmatic L e model which computed predictions based on the product of two separate non-pragmatically trained models."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-104", "intents": ["@USE@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "Sample Monroe et al. (2017) )."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-143", "intents": ["@USE@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "We follow Monroe et al. (2017) for language model hyper-parameters, with embedding and LSTM layers of size 100."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-149", "intents": ["@USE@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "We generally use speaker rationality α = 8.0 based on dev set tuning, and we follow Monroe et al. (2017) for other hyper-parameters-with embedding size of 100 and LSTM size of 100 in our meaning functions."}
{"sent_id": "3395c9ed8ad9f2d048bf8ebf950d16-C001-123", "intents": ["@EXT@"], "paper_id": "ABC_3395c9ed8ad9f2d048bf8ebf950d16_6", "text": "Following Monroe et al. (2017) , we preprocess the tokens by lowercasing, splitting off punctuation, and replacing tokens that appear only once with [unk] ."}
{"sent_id": "a3dbc3362016cdcfc0c4da429b98cc-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_a3dbc3362016cdcfc0c4da429b98cc_6", "text": "Figure 1: Generated example on ROTOWIRE by using Conditional Copy (CC) as baseline (Wiseman et al., 2017) ."}
{"sent_id": "a3dbc3362016cdcfc0c4da429b98cc-C001-81", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_a3dbc3362016cdcfc0c4da429b98cc_6", "text": "The word embeddings for each type of information are trainable and randomly initialized before training following Wiseman et al. (2017) ."}
{"sent_id": "a3dbc3362016cdcfc0c4da429b98cc-C001-189", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_a3dbc3362016cdcfc0c4da429b98cc_6", "text": "We chose Conditional Copy (CC) model as our baseline, which is the best model in Wiseman et al. (2017) ."}
{"sent_id": "a3dbc3362016cdcfc0c4da429b98cc-C001-197", "intents": ["@BACK@"], "paper_id": "ABC_a3dbc3362016cdcfc0c4da429b98cc_6", "text": "We refer the readers to Wiseman et al. (2017) 's paper for more detailed information on templates."}
{"sent_id": "a3dbc3362016cdcfc0c4da429b98cc-C001-213", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_a3dbc3362016cdcfc0c4da429b98cc_6", "text": "Results were obtained using Wiseman et al. (2017) 's trained extractive evaluation models with relexicalization (Li and Wan, 2018) ."}
{"sent_id": "a3dbc3362016cdcfc0c4da429b98cc-C001-219", "intents": ["@BACK@"], "paper_id": "ABC_a3dbc3362016cdcfc0c4da429b98cc_6", "text": "In addition, we compare our model with delayed copy model (DEL) (Li and Wan, 2018) along with gold text, template system (TEM), conditional copy (CC) (Wiseman et al., 2017) and NCP+CC (NCP) (Puduppully et al., 2019) ."}
{"sent_id": "a3dbc3362016cdcfc0c4da429b98cc-C001-221", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_a3dbc3362016cdcfc0c4da429b98cc_6", "text": "Since its result in Li and Wan (2018) 's paper was evaluated by IE model trained by Wiseman et al. (2017) and \"relexicalization\" by Li and Wan (2018) , we adopted the corresponding IE model and re-implement \"relexicalization\" as suggested by Li and Wan (2018) for fair comparison."}
{"sent_id": "a3dbc3362016cdcfc0c4da429b98cc-C001-232", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_a3dbc3362016cdcfc0c4da429b98cc_6", "text": "We compared our full model with gold texts, template-based system, CC (Wiseman et al., 2017) and NCP+CC (NCP) (Puduppully et al., 2019) ."}
{"sent_id": "a3dbc3362016cdcfc0c4da429b98cc-C001-196", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_a3dbc3362016cdcfc0c4da429b98cc_6", "text": "Also we implemented a template system same as the one used in Wiseman et al. (2017) which outputted eight sentences: an introductory sentence (two teams' points and who win), six top players' statistics (ranked by their points) and a conclusion sentence."}
{"sent_id": "a3dbc3362016cdcfc0c4da429b98cc-C001-175", "intents": ["@USE@"], "paper_id": "ABC_a3dbc3362016cdcfc0c4da429b98cc_6", "text": "We conducted experiments on ROTOWIRE (Wiseman et al., 2017) ."}
{"sent_id": "a3dbc3362016cdcfc0c4da429b98cc-C001-178", "intents": ["@USE@"], "paper_id": "ABC_a3dbc3362016cdcfc0c4da429b98cc_6", "text": "In this paper, we followed the data split introduced in Wiseman et al. (2017)"}
{"sent_id": "8c26fb4c81c121103c1d5851edb41e-C001-25", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_8c26fb4c81c121103c1d5851edb41e_6", "text": "We conduct our experiments on Trafficking-10k (Tong et al., 2017) , a dataset of escort ads for which anti-trafficking experts assigned each sample one of seven ordered labels ranging from \"1: Very Unlikely (to come from traffickers)\" to \"7: Very Likely\"."}
{"sent_id": "8c26fb4c81c121103c1d5851edb41e-C001-26", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_8c26fb4c81c121103c1d5851edb41e_6", "text": "Our proposed model significantly outperforms previously published models (Tong et al., 2017) on Trafficking-10k as well as a variety of baseline ordinal regression models."}
{"sent_id": "8c26fb4c81c121103c1d5851edb41e-C001-37", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_8c26fb4c81c121103c1d5851edb41e_6", "text": "Closest to our work is the Human Trafficking Deep Network (HTDN) (Tong et al., 2017) ."}
{"sent_id": "8c26fb4c81c121103c1d5851edb41e-C001-41", "intents": ["@BACK@", "@DIF@", "@EXT@"], "paper_id": "ABC_8c26fb4c81c121103c1d5851edb41e_6", "text": "As in the work of E. Tong et al. (2017) , we pre-train word embeddings using a skip-gram model (Mikolov et al., 2013b) applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon."}
{"sent_id": "8c26fb4c81c121103c1d5851edb41e-C001-61", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_8c26fb4c81c121103c1d5851edb41e_6", "text": "Unfortunately, the escort ads contain a plethora of emojis, acronyms, and (sometimes deliberate) typographical errors that are not encountered in more standard text data, which suggests that it is likely better to learn word embeddings from scratch on a large collection of escort ads instead of using previously published embeddings (Tong et al., 2017) ."}
{"sent_id": "8c26fb4c81c121103c1d5851edb41e-C001-104", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_8c26fb4c81c121103c1d5851edb41e_6", "text": "We use raw texts scraped from Backpage and TNABoard to pre-train the word embeddings, and use the same labeled texts E. Tong et al. (2017) used to conduct model comparisons."}
{"sent_id": "8c26fb4c81c121103c1d5851edb41e-C001-110", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_8c26fb4c81c121103c1d5851edb41e_6", "text": "It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection 6 (Tong et al., 2017) ."}
{"sent_id": "8c26fb4c81c121103c1d5851edb41e-C001-126", "intents": ["@BACK@"], "paper_id": "ABC_8c26fb4c81c121103c1d5851edb41e_6", "text": "data shuffle and split) 10-fold crossvalidation (CV) on Trafficking-10k, except for HTDN, whose result is read from the original paper (Tong et al., 2017) 7 ."}
{"sent_id": "8c26fb4c81c121103c1d5851edb41e-C001-162", "intents": ["@BACK@"], "paper_id": "ABC_8c26fb4c81c121103c1d5851edb41e_6", "text": "Traffickers often avoid using explicit keywords when advertising victims, but instead use acronyms, intentional typos, and emojis (Tong et al., 2017) ."}
{"sent_id": "8c26fb4c81c121103c1d5851edb41e-C001-164", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_8c26fb4c81c121103c1d5851edb41e_6", "text": "To make matters worse, traffickers change their dictionaries over time and regularly switch to new emojis to replace certain keywords (Tong et al., 2017) ."}
{"sent_id": "8c26fb4c81c121103c1d5851edb41e-C001-184", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_8c26fb4c81c121103c1d5851edb41e_6", "text": "Our ORNN achieved the state-of-the-art performance on Trafficking-10K (Tong et al., 2017) , outperforming all baseline ordinal regression models as well as improving the classification accuracy over the Human Trafficking Deep Network (Tong et al., 2017) ."}
{"sent_id": "8c26fb4c81c121103c1d5851edb41e-C001-98", "intents": ["@USE@"], "paper_id": "ABC_8c26fb4c81c121103c1d5851edb41e_6", "text": "Then we present a detailed comparison of our proposed model with commonly used ordinal regression models as well as the previous state-of-the-art classification model by E. Tong et al. (2017) ."}
{"sent_id": "8c26fb4c81c121103c1d5851edb41e-C001-121", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_8c26fb4c81c121103c1d5851edb41e_6", "text": "To compare our model with the previous stateof-the-art classification model for escort ads, the Human Trafficking Deep Network (HTDN) (Tong et al., 2017) , we also polarize the true and predicted labels into two classes, \"1-4: Unlikely\" and \"5-7: Likely\"; then we compute the binary classification accuracy (Acc.) as well as the weighted binary classification accuracy (Wt. Acc.) The text data need to be vectorized before they can be fed into the baseline models (whereas vectorization is built into ORNN)."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-121", "intents": ["@USE@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "We applied the monolingual sentence alignment algorithm of Barzilay and Elhadad (2003) ."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-141", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "Like Barzilay and Elhadad (2003) , we performed 200 iterations in Boostexter."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-172", "intents": ["@USE@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "As can be seen from Table 2 , by applying the sentence alignment algorithm of Barzilay and Elhadad (2003) we were able to extract only 5% of all reference alignments, while precision was below 30%."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-129", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "Barzilay and Elhadad (2003) additionally considered every word starting with a capital letter inside a sentence to be a proper name."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-133", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "We adapted the hierarchical completelink clustering method of Barzilay and Elhadad (2003) : While the authors claimed to have set a specific number of clusters, we believe this is not generally possible in hierarchical agglomerative clustering."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-136", "intents": ["@BACK@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "Barzilay and Elhadad (2003) used the boosting tool Boostexter (Schapire and Singer, 2000) ."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-176", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "In conclusion, none of the three approaches (adapted algorithm of Barzilay and Elhadad (2003) , two baselines \"First sentence\" and \"Word in common\") performed well on our test set."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-192", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "In terms of domain, Barzilay and Elhadad (2003) used city descriptions from an encyclopedia for their experiments."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-218", "intents": ["@BACK@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "For example, named entity recognition, a preprocessing step to clustering, is harder for German than for English, the language Barzilay and Elhadad (2003) worked with."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-220", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "The domain of our corpus was also broader than that of Barzilay and Elhadad (2003) , who used city descriptions from an encyclopedia for their experiments."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-170", "intents": ["@EXT@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "Adapted algorithm of Barzilay and Elhadad (2003) 27.7% 5.0% 8.5% Baseline I: First sentence 88.1% 4.8% 9.3% Baseline II: Word in common 2.2% 8.2% 3.5% Table 2 : Alignment results on test set 1. Aligning only the first sentence of each text (\"First sentence\") 2."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-152", "intents": ["@SIM@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "We set the skip penalty to 0.001 conforming to the value of Barzilay and Elhadad (2003) ."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-216", "intents": ["@SIM@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "Since all of our data was from the same language, we applied the monolingual sentence alignment approach of Barzilay and Elhadad (2003) ."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-180", "intents": ["@DIF@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "Compared with the results of Barzilay and Elhadad (2003) , who achieved 77% precision at 55.8% recall for their data, our alignment scores were considerably lower (27.7% precision, 5% recall)."}
{"sent_id": "febb64368c09d03932742fc557f3d3-C001-183", "intents": ["@DIF@"], "paper_id": "ABC_febb64368c09d03932742fc557f3d3_6", "text": "While Barzilay and Elhadad (2003) aligned English/Simple English texts, we dealt with German/Simple German data."}
{"sent_id": "5a6684d978c0dbcfaabb4bc2314aeb-C001-4", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5a6684d978c0dbcfaabb4bc2314aeb_6", "text": "To alleviate this problem, we extract hierarchical rules from weighted alignment matrix (Liu et al., 2009) ."}
{"sent_id": "5a6684d978c0dbcfaabb4bc2314aeb-C001-28", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_5a6684d978c0dbcfaabb4bc2314aeb_6", "text": "Note that given a source phrase, there would be plenty of \"potential\" candidate target phrases in weighted matrices (Liu et al., 2009 )."}
{"sent_id": "5a6684d978c0dbcfaabb4bc2314aeb-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_5a6684d978c0dbcfaabb4bc2314aeb_6", "text": "A weighted alignment matrix (Liu et al., 2009) m is a J × I matrix to encode the probabilities of n-best alignments of the same sentence pair."}
{"sent_id": "5a6684d978c0dbcfaabb4bc2314aeb-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_5a6684d978c0dbcfaabb4bc2314aeb_6", "text": "To calculate the lexical weights, Liu et al. (2009) adapt p m (j, i) as the fractional count count(f j , e i )."}
{"sent_id": "5a6684d978c0dbcfaabb4bc2314aeb-C001-64", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_5a6684d978c0dbcfaabb4bc2314aeb_6", "text": "Since we can calculate relative frequencies and lexical weights of phrase pairs as in Liu et al. (2009) , we only focus on the calculation of variable rules."}
{"sent_id": "5a6684d978c0dbcfaabb4bc2314aeb-C001-20", "intents": ["@USE@"], "paper_id": "ABC_5a6684d978c0dbcfaabb4bc2314aeb_6", "text": "Since Liu et al. (2009) show that weighted alignment matrix provides an elegant solution to these two drawbacks, we apply it to the hierarchical phrase-based model (Chiang, 2005) and the tree-to-string model Huang et al., 2006) ."}
{"sent_id": "5a6684d978c0dbcfaabb4bc2314aeb-C001-105", "intents": ["@USE@"], "paper_id": "ABC_5a6684d978c0dbcfaabb4bc2314aeb_6", "text": "We follow Liu et al. (2009) to calculate relative frequencies using the product of inside and outside probabilities."}
{"sent_id": "5a6684d978c0dbcfaabb4bc2314aeb-C001-119", "intents": ["@USE@"], "paper_id": "ABC_5a6684d978c0dbcfaabb4bc2314aeb_6", "text": "We follow Liu et al. (2009) to prune rule table using a threshold of frequency."}
{"sent_id": "5a6684d978c0dbcfaabb4bc2314aeb-C001-136", "intents": ["@USE@"], "paper_id": "ABC_5a6684d978c0dbcfaabb4bc2314aeb_6", "text": "We follow Liu et al. (2009) to use p s2t × p t2s as the probabilities of an alignment pair."}
{"sent_id": "d70e69bb3eaa6b46ee3b7110126129-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_d70e69bb3eaa6b46ee3b7110126129_6", "text": "Following this property of finding relations or analogies, one popular example of gender bias is the word association between man to computer programmer as woman to homemaker (Bolukbasi et al., 2016) ."}
{"sent_id": "d70e69bb3eaa6b46ee3b7110126129-C001-18", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d70e69bb3eaa6b46ee3b7110126129_6", "text": "While gender bias has been studied, detected and partially addressed for standard word embeddings techniques (Bolukbasi et al., 2016; Zhao et al., 2018a; Gonen and Goldberg, 2019) , it is not the case for the latest techniques of contextualized word embeddings."}
{"sent_id": "d70e69bb3eaa6b46ee3b7110126129-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_d70e69bb3eaa6b46ee3b7110126129_6", "text": "Only just recently, Zhao et al. (2019) present a first analysis on the topic based on the proposed methods in Bolukbasi et al. (2016) ."}
{"sent_id": "d70e69bb3eaa6b46ee3b7110126129-C001-38", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d70e69bb3eaa6b46ee3b7110126129_6", "text": "Those biases are reflected in the cooccurrence statistics, and therefore learned into word embeddings trained in those corpora, amplifying them (Bolukbasi et al., 2016; Caliskan et al., 2017) ."}
{"sent_id": "d70e69bb3eaa6b46ee3b7110126129-C001-46", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d70e69bb3eaa6b46ee3b7110126129_6", "text": "The transformations proposed by both Bolukbasi et al. (2016) and Zhao et al. (2018b) are downstream task-agnostic."}
{"sent_id": "d70e69bb3eaa6b46ee3b7110126129-C001-82", "intents": ["@BACK@"], "paper_id": "ABC_d70e69bb3eaa6b46ee3b7110126129_6", "text": "We compare our results to previous results from debiased and non-debiased word embeddings (Bolukbasi et al., 2016) ."}
{"sent_id": "d70e69bb3eaa6b46ee3b7110126129-C001-91", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_d70e69bb3eaa6b46ee3b7110126129_6", "text": "Similarly to Bolukbasi et al. (2016) , figure 1 shows that the first eigenvalue is significantly larger than the rest and that there is also a single direction describing the majority of variance in these vectors, still the difference between the percentage of variances is less in case of contextualized embeddings, which may refer that there is less bias in such embeddings."}
{"sent_id": "d70e69bb3eaa6b46ee3b7110126129-C001-100", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_d70e69bb3eaa6b46ee3b7110126129_6", "text": "We got direct bias of 0.03, compared to 0.08 from standard word2vec embeddings described in Bolukbasi et al. (2016) ."}
{"sent_id": "d70e69bb3eaa6b46ee3b7110126129-C001-59", "intents": ["@DIF@"], "paper_id": "ABC_d70e69bb3eaa6b46ee3b7110126129_6", "text": "To address these questions, we adapt and contrast with the evaluation measures proposed by Bolukbasi et al. (2016) and Gonen and Goldberg (2019) ."}
{"sent_id": "d70e69bb3eaa6b46ee3b7110126129-C001-66", "intents": ["@USE@"], "paper_id": "ABC_d70e69bb3eaa6b46ee3b7110126129_6", "text": "To perform our analysis we used a set of lists from previous work (Bolukbasi et al., 2016; Gonen and Goldberg, 2019) ."}
{"sent_id": "d70e69bb3eaa6b46ee3b7110126129-C001-72", "intents": ["@USE@"], "paper_id": "ABC_d70e69bb3eaa6b46ee3b7110126129_6", "text": "A note to be considered, is that the lists we used in our experiments (and obtained from Bolukbasi et al. (2016) and Gonen and Goldberg (2019) ) may contain words that are missing in our corpus and so we can not obtain contextualized embeddings for them."}
{"sent_id": "d70e69bb3eaa6b46ee3b7110126129-C001-79", "intents": ["@EXT@"], "paper_id": "ABC_d70e69bb3eaa6b46ee3b7110126129_6", "text": "In this section, we adapt gender bias measures for word embedding methods from previous work (Bolukbasi et al., 2016) and (Gonen and Goldberg, 2019) to be applicable to contextualized word embeddings."}
{"sent_id": "d70e69bb3eaa6b46ee3b7110126129-C001-98", "intents": ["@SIM@"], "paper_id": "ABC_d70e69bb3eaa6b46ee3b7110126129_6", "text": "We applied the definition of direct bias from Bolukbasi et al. (2016) on the ELMo representations of the professional words in these sentences."}
{"sent_id": "3ec2dc9530699f55b8a4c234532daf-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_3ec2dc9530699f55b8a4c234532daf_6", "text": "This task aims to select the best-matched response from a set of candidates, given the context of a conversation which is composed of multiple utterances (Lowe et al., 2015; Lowe et al., 2017; Wu et al., 2017 )."}
{"sent_id": "3ec2dc9530699f55b8a4c234532daf-C001-23", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_3ec2dc9530699f55b8a4c234532daf_6", "text": "This approach has the advantage of providing informative and fluent responses because they select a proper response for the current conversation from a repository by means of response selection algorithms (Lowe et al., 2015; Lowe et al., 2017 EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EA  EB  EB  EB  EB  EB   E0  E1  E2  E3  E4  E5  E18  E19  E6  E7  E8  E9  E10  E11  E17  E12  E13  E14  E15  E16  E20  E21  E22  E23  E24  E25 [ E0  E0  E0  E0  E0  E0  E0  E0  E1  E1  E1  E1  E1  E1  E0  E1  E1  E1  E1  E1  E0  E1  E1  E1  E1  E1 Speaker Embeddings + + + + + + + + + + + + + + + + + + + + + + + + + + Figure 1 : The input representation of SA-BERT."}
{"sent_id": "3ec2dc9530699f55b8a4c234532daf-C001-96", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_3ec2dc9530699f55b8a4c234532daf_6", "text": "We tested SA-BERT on five public multi-turn response selection datasets, Ubuntu Dialogue Corpus V1 (Lowe et al., 2015) , Ubuntu Dialogue Corpus V2 (Lowe et al., 2017) , Douban Conversation Corpus (Wu et al., 2017) , E-commerce Dialogue Corpus (Zhang et al., 2018b) and DSTC 8-Track 2-Subtask 2 Corpus (Seokhwan Kim, 2019)."}
{"sent_id": "3ec2dc9530699f55b8a4c234532daf-C001-120", "intents": ["@USE@"], "paper_id": "ABC_3ec2dc9530699f55b8a4c234532daf_6", "text": "We used the same evaluation metrics as those used in previous work (Lowe et al., 2015; Lowe et al., 2017; Wu et al., 2017; Zhang et al., 2018b; Seokhwan Kim, 2019) ."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "(ii) The local approach might suffer from the data sparseness issue of unseen words/features, the difficulty of calibrating, and the failure to induce the underlying similarity structures at high levels of abstraction for EL (due to the extensive reliance on the hand-designed coarse features) (Sun et al., 2015; Francis-Landau et al., 2016) ."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-30", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "In practice, the features automatically induced by NN are combined with the discrete features in the local approach to extend their coverage for EL (Sun et al., 2015; Francis-Landau et al., 2016) ."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-34", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "Our work is an extension of (Francis-Landau et al., 2016) which only considers the local similarities."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-62", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "In the next step, we apply the convolution operation over w to generate the hidden vector sequence, that is then transformed by a non-linear function G and pooled by the sum function (Francis-Landau et al., 2016) ."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-72", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "We employ the local similarities φ local (m i , p ij ) from (Francis-Landau et al., 2016) , the state-of-the-art neural network model for EL."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "The representative features include the anchor text counts from Wikipedia, the string match indications with the title of the Wikipedia candidate pages, or the information about the shape of the queries for candidate generations (Francis-Landau et al., 2016) ."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "The intuition for this computation is that the similarities at different levels of contexts might help to enforce the potential topic compatibility between the contexts of the entity mentions and target candidates for EL (Francis-Landau et al., 2016) ."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-116", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "Note that every parameter and resource in this work is either taken from the previous work (Nguyen and Grishman, 2016b; Francis-Landau et al., 2016) or selected by the development data."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-131", "intents": ["@BACK@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "These systems include the neural network model in (Francis-Landau et al., 2016) , the joint model for entity analysis in (Durrett and Klein, 2014) and the AIDA-light system with two-stage mapping in (Nguyen et al., 2014b) 6 ."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-133", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "We also include the performance of the systems on the December 2014 Wikipedia dump that was used and provided by (Francis-Landau et al., 2016) for further and compatible comparison."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-149", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "Eventually, we expect that the proposed model with global coherence features would be more robust to domain shifts than the local approach (Francis-Landau et al., 2016) ."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-157", "intents": ["@BACK@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "Table 3 compares Global-RNN with the neural network EL model in (Francis-Landau et al., 2016) , the best reported model on the ACE dataset in the literature 8 ."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-160", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "Second and most importantly, Global-RNN is consistently better than the model with only local features in (Francis-Landau et al., 2016) over all the target domains (although it is less pronounced in the cts domain)."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-190", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "Sun et al. (2015) employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while Francis-Landau et al. (2016) combine CNN-based representations with sparse features to improve the performance."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-100", "intents": ["@USE@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "Following (Francis-Landau et al., 2016), we evaluate the models on 4 different entity linking datasets: i) ACE (Bentivogli et al., 2010 ): This corpus is from the 2005 evaluation of NIST."}
{"sent_id": "c2a956a6ae0fb1ab338da01e5a5645-C001-106", "intents": ["@USE@"], "paper_id": "ABC_c2a956a6ae0fb1ab338da01e5a5645_6", "text": "For all the datasets, we use the standard data splits (for training data, test data and development data) as the previous works for comparable comparison (Francis-Landau et al., 2016)."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010) , the recent work of (Poon and Domingos, 2009 ) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "More detailed discussion of relation between the Markov Logic Network (MLN) approach of (Poon and Domingos, 2009 ) and our non-parametric method is presented in Section 3."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-28", "intents": ["@BACK@", "@UNSURE@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus (Poon and Domingos, 2009 )."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-45", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "Unlike (Poon and Domingos, 2009 ), we do not use the lambda calculus formalism to define our task but rather treat it as an instance of frame-semantic parsing, or a specific type of semantic role labeling (Gildea and Jurafsky, 2002) ."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-47", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "Second, lambda calculus is a considerably more powerful formalism than the predicate-argument structure used in frame semantics, normally supporting quantification and logical connectors (for example, negation and disjunction), neither of which is modeled by our model or in (Poon and Domingos, 2009) ."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-77", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "The work of (Poon and Domingos, 2009 ) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) , selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-82", "intents": ["@BACK@", "@DIF@", "@MOT@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "In order to overcome this problem, (Poon and Domingos, 2009 ) group parameters and impose local normalization constraints within each group."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-219", "intents": ["@BACK@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "In addition to the MLN model (Poon and Domingos, 2009 ), another unsupervised method has been proposed in (Goldwasser et al., 2011) ."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-23", "intents": ["@DIF@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of (Poon and Domingos, 2009) which have to perform model selection and use heuristics to penalize more complex models of semantics."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-34", "intents": ["@USE@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "In both cases, we follow (Poon and Domingos, 2009 ) in using the corpus of biomedical abstracts."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-181", "intents": ["@USE@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "We used the GENIA corpus (Kim et al., 2003) , a dataset of 1999 biomedical abstracts, and a set of questions produced by (Poon and Domingos, 2009) ."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-192", "intents": ["@USE@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "We now turn to the QA task and compare our model (USP-BAYES) with the results of baselines considered in (Poon and Domingos, 2009 )."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-68", "intents": ["@SIM@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "Third, as in (Poon and Domingos, 2009 ), we do not model polysemy as we assume that each syntactic fragment corresponds to a single semantic class."}
{"sent_id": "2b836473cf682ed474b7cda1800f84-C001-71", "intents": ["@SIM@"], "paper_id": "ABC_2b836473cf682ed474b7cda1800f84_6", "text": "As in some of the recent work on learning semantic representations (Eisenstein et al., 2009; Poon and Domingos, 2009 ), we assume that dependency structures are provided for every sentence."}
{"sent_id": "7895613ddd09696bbee4143c4359b0-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_7895613ddd09696bbee4143c4359b0_6", "text": "Recent work has shown that we can learn better visually grounded representations of sentences by training image-sentence ranking models on multiple languages (Gella et al., 2017; Kádár et al., 2018) ."}
{"sent_id": "7895613ddd09696bbee4143c4359b0-C001-40", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_7895613ddd09696bbee4143c4359b0_6", "text": "We adopt the model architecture and training procedure of Kádár et al. (2018) for the task of matching images with sentences."}
{"sent_id": "7895613ddd09696bbee4143c4359b0-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_7895613ddd09696bbee4143c4359b0_6", "text": "In our experiments, the < a, b > pairs are either image-caption pairs < i, c > or caption-caption pairs < c a , c b > (following Gella et al. (2017) ; Kádár et al. (2018) )."}
{"sent_id": "7895613ddd09696bbee4143c4359b0-C001-113", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_7895613ddd09696bbee4143c4359b0_6", "text": "This finding contradicts the conclusion of Kádár et al. (2018) , who claimed that the aligned and disjoint conditions lead to comparable performance."}
{"sent_id": "7895613ddd09696bbee4143c4359b0-C001-114", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_7895613ddd09696bbee4143c4359b0_6", "text": "This is most likely because the disjoint setting in Kádár et al. (2018) is artificial, in the sense that they used different 50% subsets of M30K."}
{"sent_id": "7895613ddd09696bbee4143c4359b0-C001-119", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_7895613ddd09696bbee4143c4359b0_6", "text": "Summary First we reproduced the findings of Kádár et al. (2018) showing that bilingual joint training improves over monolingual and using c2c loss further improves performance."}
{"sent_id": "7895613ddd09696bbee4143c4359b0-C001-188", "intents": ["@BACK@"], "paper_id": "ABC_7895613ddd09696bbee4143c4359b0_6", "text": "More recently, there has been a focus on solving this task using multilingual data (Gella et al., 2017; Kádár et al., 2018) in the Multi30K dataset ; an extension of the popular Flickr30K dataset into German, French, and Czech."}
{"sent_id": "7895613ddd09696bbee4143c4359b0-C001-198", "intents": ["@BACK@"], "paper_id": "ABC_7895613ddd09696bbee4143c4359b0_6", "text": "Their results were improved by the approach presented in Kádár et al. (2018) , who has also shown that the multilingual models outperform bilingual models, and that image-caption retrieval performance in languages with less resources can be improved with data from higher-resource languages."}
{"sent_id": "7895613ddd09696bbee4143c4359b0-C001-199", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_7895613ddd09696bbee4143c4359b0_6", "text": "We largely follow Kádár et al. (2018) , however, our main interest lies in learning multimodal and bilingual representations in the scenario where the images do not come from the same data set i.e.: the data is presented is two sets of image-caption tuples rather than image-caption-caption triples."}
{"sent_id": "7895613ddd09696bbee4143c4359b0-C001-205", "intents": ["@BACK@"], "paper_id": "ABC_7895613ddd09696bbee4143c4359b0_6", "text": "Previous work has demonstrated improved imagesentence ranking performance when training models jointly on multiple languages (Gella et al., 2017; Kádár et al., 2018) ."}
{"sent_id": "7895613ddd09696bbee4143c4359b0-C001-47", "intents": ["@USE@"], "paper_id": "ABC_7895613ddd09696bbee4143c4359b0_6", "text": "For the caption-caption objective, we follow Kádár et al. (2018) and generate a sentence pair data set by taking all pairs of sentences that belong to the same image and are written in different languages: 5 English and 5 German captions result in 25 English-German pairs."}
{"sent_id": "7895613ddd09696bbee4143c4359b0-C001-71", "intents": ["@USE@"], "paper_id": "ABC_7895613ddd09696bbee4143c4359b0_6", "text": "Our implementation, training protocol and parameter settings are based on the existing codebase of Kádár et al. (2018) ."}
{"sent_id": "7895613ddd09696bbee4143c4359b0-C001-101", "intents": ["@SIM@"], "paper_id": "ABC_7895613ddd09696bbee4143c4359b0_6", "text": "These results reproduce the findings of Kádár et al. (2018) ."}
{"sent_id": "d3672a2d7129beef6703598f1558c4-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_d3672a2d7129beef6703598f1558c4_6", "text": "Text segmentation deals with automatically breaking down the structure of text into such topically contiguous segments, i.e., it aims to identify the points of topic shift (Hearst 1994; Choi 2000; Brants, Chen, and Tsochantaridis 2002; Riedl and Biemann 2012; Du, Buntine, and Johnson 2013; Glavaš, Nanni, and Ponzetto 2016; Koshorek et al. 2018) ."}
{"sent_id": "d3672a2d7129beef6703598f1558c4-C001-25", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_d3672a2d7129beef6703598f1558c4_6", "text": "Similarly, a recently proposed state-of-the-art supervised neural segmentation model (Koshorek et al. 2018 ) directly learns to predict binary sentence-level segmentation decisions and has no explicit mechanism for modeling coherence."}
{"sent_id": "d3672a2d7129beef6703598f1558c4-C001-122", "intents": ["@BACK@"], "paper_id": "ABC_d3672a2d7129beef6703598f1558c4_6", "text": "We compare CATS against the state-ofthe-art neural segmentation model of Koshorek et al. (2018) and against GRAPHSEG (Glavaš, Nanni, and Ponzetto 2016) , the state-of-the-art unsupervised text segmentation model."}
{"sent_id": "d3672a2d7129beef6703598f1558c4-C001-149", "intents": ["@BACK@"], "paper_id": "ABC_d3672a2d7129beef6703598f1558c4_6", "text": "The improved performance that TLT-TS has with respect to the model of Koshorek et al. (2018) is consistent with improvements that Transformer-based architectures yield in comparison with models based on recurrent components in other NLP tasks (Vaswani et al. 2017; Devlin et al. 2018) ."}
{"sent_id": "d3672a2d7129beef6703598f1558c4-C001-157", "intents": ["@BACK@"], "paper_id": "ABC_d3672a2d7129beef6703598f1558c4_6", "text": "Moreover, Koshorek et al. (2018) report human performance on the WIKI-50 dataset of 14.97, which is a mere one P k point better than the performance of our coherence-aware CATS model."}
{"sent_id": "d3672a2d7129beef6703598f1558c4-C001-188", "intents": ["@BACK@", "@DIF@", "@MOT@"], "paper_id": "ABC_d3672a2d7129beef6703598f1558c4_6", "text": "Finally, Koshorek et al. (2018) identify Wikipedia as a free large-scale source of manually segmented texts that can be used to train a supervised segmentation model."}
{"sent_id": "d3672a2d7129beef6703598f1558c4-C001-28", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_d3672a2d7129beef6703598f1558c4_6", "text": "Similar to (Koshorek et al. 2018) , CATS' main learning objective is a binary sentence-level segmentation prediction."}
{"sent_id": "d3672a2d7129beef6703598f1558c4-C001-113", "intents": ["@SIM@", "@EXT@"], "paper_id": "ABC_d3672a2d7129beef6703598f1558c4_6", "text": "Analogous to the WIKI-50 dataset created by Koshorek et al. (2018) from English (EN) Wikipedia, we created WIKI-50-CS, WIKI-50-FI, and WIKI-50-TR datasets consisting of 50 randomly selected pages from Czech (CS), Finnish (FI), and Turkish (TR) Wikipedia, respectively."}
{"sent_id": "d3672a2d7129beef6703598f1558c4-C001-118", "intents": ["@USE@"], "paper_id": "ABC_d3672a2d7129beef6703598f1558c4_6", "text": "Following previous work (Riedl and Biemann 2012; Glavaš, Nanni, and Ponzetto 2016; Koshorek et al. 2018) , we also adopt the standard text segmentation measure P k (Beeferman, Berger, and Lafferty 1999) as our evaluation metric."}
{"sent_id": "d3672a2d7129beef6703598f1558c4-C001-120", "intents": ["@USE@"], "paper_id": "ABC_d3672a2d7129beef6703598f1558c4_6", "text": "Following (Glavaš, Nanni, and Ponzetto 2016; Koshorek et al. 2018) , we set k to the half of the average ground truth segment size of the dataset."}
{"sent_id": "d3672a2d7129beef6703598f1558c4-C001-152", "intents": ["@UNSURE@"], "paper_id": "ABC_d3672a2d7129beef6703598f1558c4_6", "text": "This would suggest that TLT-TS and CATS offer more robust domain transfer than the recurrent model of Koshorek et al. (2018) ."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "Such a scheme facilitates the detection of decision discussions (Fernández et al., 2008) , and by indicating which utterances contain particular types of information, it also aids their summarization."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "Only very recent research has specifically investigated the automatic detection of decisions, namely (Hsueh and Moore, 2007) and (Fernández et al., 2008) ."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "Unlike Hsueh and Moore (2007), Fernández et al. (2008) made an attempt at modelling the structure of decision-making dialogue."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "Note that (Purver et al., 2007) had previously pursued the same basic approach as Fernández et al. (2008) in order to detect action items."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "The reader can find a comparison between these annotations and our own manual transcript annotations in (Fernández et al., 2008) ."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-194", "intents": ["@BACK@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "7 Conclusion (Fernández et al., 2008) described an approach to decision detection in multi-party meetings and demonstrated how it could work relatively well in an off-line system."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-198", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "In this paper then, we have taken the same basic approach to decision detection as Fernández et al. (2008) , but changed the way in which it is implemented so that it can work effectively in realtime."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-45", "intents": ["@DIF@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "While both Hsueh and Moore (2007), and Fernández et al. (2008) attempted off-line decision detection, in this paper, we attempt real-time decision detection."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-46", "intents": ["@DIF@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "We take the same basic approach as Fernández et al. (2008) , and make changes to its implementation so that it can work effectively in real-time."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-178", "intents": ["@DIF@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "Hence, while Fernández et al. (2008) demonstrated that the hierarchical classification approach could improve off-line decision detection, we have demonstrated here that it can also improve realtime decision detection."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-62", "intents": ["@USE@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "We use the same annotation scheme as (Fernández et al., 2008 ) in order to model decision-making dialogue."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-150", "intents": ["@USE@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "For 1 and 2, we use the same lenient-match metric as (Fernández et al., 2008; Hsueh and Moore, 2007) , which allows a margin of 20 seconds preceding and following a hypothesized DDA."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-152", "intents": ["@USE@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "For 3, we follow (Fernández et al., 2008; Purver et al., 2007) and use a windowed metric that divides the dialogue into 30-second windows and evaluates on a per window basis."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "The PeerRead dataset (Kang et al., 2018) is an excellent resource towards research and study on this very impactful and crucial problem."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "For more details on the dataset creation and the task, we request the readers to refer to Kang et al. (2018) ."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-41", "intents": ["@USE@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "We carry our current investigations on a portion of the recently released PeerRead dataset (Kang et al., 2018) ."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-94", "intents": ["@USE@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "We employ a Multi-Layer Perceptron (MLP Predict) to take the joint paper+review representations x pr as input to get the final (Kang et al., 2018) , RMSE→Root Mean Squared Error."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-95", "intents": ["@USE@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "CNN variant as in (Kang et al., 2018 ) is used as the comparing system."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-115", "intents": ["@USE@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "To compare with Kang et al. (2018) , we keep the experimental setup (train vs test ratio) identical and re-implement their codes to generate the comparing figures."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-121", "intents": ["@USE@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "We train the model with SGD optimizer, set momentum as 0.9 (Kang et al., 2018 ) is feature-based and considers only paper, and not the reviews."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-43", "intents": ["@DIF@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "Our approach achieves significant performance improvement over the two tasks defined in Kang et al. (2018) ."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-116", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "However, Kang et al. (2018) performed Task 2 on ICLR 2017 dataset with handcrafted features, and Task 1 in a deep learning setting."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-132", "intents": ["@DIF@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "With only using review+sentiment information, we are still able to outperform Kang et al. (2018) by a margin of 11% in terms of RMSE."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-137", "intents": ["@DIF@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "For Task 2, we observe that the handcrafted feature-based system by Kang et al. (2018) performs inferior compared to the baselines."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-148", "intents": ["@DIF@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "The reason is that the work reported in Kang et al. (2018) relies on elementary handcrafted features extracted only from the paper; does not consider the review features whereas we include the review features along with the sentiment information in our deep neural architecture."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-149", "intents": ["@DIF@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "However, we also find that our approach with only Review+Sentiment performs inferior to the Paper+Review method in Kang et al. (2018) for ACL 2017."}
{"sent_id": "dbb0178b572c2a451853737910ac86-C001-52", "intents": ["@MOT@"], "paper_id": "ABC_dbb0178b572c2a451853737910ac86_7", "text": "One motivation of our work stems from the finding that aspect scores for certain factors like Impact, Originality, Soundness/Correctness which are seemingly central to the merit of the paper, often have very low correlation with the final recommendation made by the reviewers as is made evident in Kang et al. (2018) ."}
{"sent_id": "5095f2af3f0c51283c8fbee08a17ac-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_5095f2af3f0c51283c8fbee08a17ac_7", "text": "Recently, an attention based system (Zhong et al., 2019) utilizing both documentlevel and entity-level information achieved stateof-the-art results on WIKIHOP data set, proving that techniques like co-attention and self-attention widely employed in single-document RC tasks are also useful in multi-document RC tasks."}
{"sent_id": "5095f2af3f0c51283c8fbee08a17ac-C001-75", "intents": ["@BACK@"], "paper_id": "ABC_5095f2af3f0c51283c8fbee08a17ac_7", "text": "Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (Seo et al., 2016; Xiong et al., 2016) , and recently was applied to multiple-hop reading comprehension (Zhong et al., 2019) ."}
{"sent_id": "5095f2af3f0c51283c8fbee08a17ac-C001-33", "intents": ["@USE@"], "paper_id": "ABC_5095f2af3f0c51283c8fbee08a17ac_7", "text": "In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network (Zhong et al., 2019) , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities;"}
{"sent_id": "5095f2af3f0c51283c8fbee08a17ac-C001-40", "intents": ["@USE@"], "paper_id": "ABC_5095f2af3f0c51283c8fbee08a17ac_7", "text": "The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (Dhingra et al., 2018; Song et al., 2018; De Cao et al., 2018; Zhong et al., 2019; Kundu et al., 2018) ."}
{"sent_id": "5095f2af3f0c51283c8fbee08a17ac-C001-43", "intents": ["@USE@"], "paper_id": "ABC_5095f2af3f0c51283c8fbee08a17ac_7", "text": "The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model (Zhong et al., 2019) because they show the effectiveness of attention mechanisms."}
{"sent_id": "5095f2af3f0c51283c8fbee08a17ac-C001-71", "intents": ["@USE@"], "paper_id": "ABC_5095f2af3f0c51283c8fbee08a17ac_7", "text": "We use simple exact match strategy (De Cao et al., 2018; Zhong et al., 2019) to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention."}
{"sent_id": "5095f2af3f0c51283c8fbee08a17ac-C001-77", "intents": ["@USE@"], "paper_id": "ABC_5095f2af3f0c51283c8fbee08a17ac_7", "text": "We follow the implementation of coattention in (Zhong et al., 2019) ."}
{"sent_id": "5095f2af3f0c51283c8fbee08a17ac-C001-88", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_5095f2af3f0c51283c8fbee08a17ac_7", "text": "We expect S ca carries query-aware contextual information of supporting documents as shown by Zhong et al. (2019) ."}
{"sent_id": "5095f2af3f0c51283c8fbee08a17ac-C001-91", "intents": ["@USE@"], "paper_id": "ABC_5095f2af3f0c51283c8fbee08a17ac_7", "text": "Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information (Zhong et al., 2019) ."}
{"sent_id": "5095f2af3f0c51283c8fbee08a17ac-C001-37", "intents": ["@DIF@"], "paper_id": "ABC_5095f2af3f0c51283c8fbee08a17ac_7", "text": "Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in (Zhong et al., 2019) 1 , without using pretrained contextual ELMo embedding (Peters et al., 2018) ."}
{"sent_id": "5095f2af3f0c51283c8fbee08a17ac-C001-97", "intents": ["@DIF@"], "paper_id": "ABC_5095f2af3f0c51283c8fbee08a17ac_7", "text": "Our context encoding module is different from the one used in Zhong et al. (2019) in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model."}
{"sent_id": "5095f2af3f0c51283c8fbee08a17ac-C001-98", "intents": ["@DIF@"], "paper_id": "ABC_5095f2af3f0c51283c8fbee08a17ac_7", "text": "2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while Zhong et al. (2019) first do self-attention on entity word sequences to get a sequence of entity vectors in each documents."}
{"sent_id": "5095f2af3f0c51283c8fbee08a17ac-C001-179", "intents": ["@DIF@"], "paper_id": "ABC_5095f2af3f0c51283c8fbee08a17ac_7", "text": "We show that our proposed HDE graph based model improves the state-of-the-art accuracy on development set from 67.1% (Kundu et al., 2018) to 68.1%, on the blind test set from 70.6% (Zhong et al., 2019) to 70.9%."}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "Of particular interest to this work is the work by Sukhbaatar et al. (2015) , on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning and goal-oriented dialogue tasks ."}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "End-to-End Memory Networks: Building on top of memory networks , Sukhbaatar et al. (2015) ing the memory position supervision and making the model trainable in an end-to-end fashion, through the advent of supporting memories and a memory access controller."}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "To enhance the model's ability to cope with more challenging tasks requiring multiple supporting facts from the memory, Sukhbaatar et al. (2015) further extended the model by stacking multiple memory layers (also known as \"hops\"), in which case the output of the k th hop is taken as input to the (k + 1) th hop:"}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "In Sukhbaatar et al. (2015) , two types of weight tying were explored for N2N, namely adjacent (\"ADJ\") and layer-wise (\"LW\")."}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "Table 1 : Accuracy (%) reported in (Sukhbaatar et al., 2015) on a selected subset of the 20 bAbI 10k tasks."}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-17", "intents": ["@MOT@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "While N2Ns generally work well with either weight tying approach, as reported in Sukhbaatar et al. (2015) , the performance is uneven on some difficult tasks."}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-78", "intents": ["@USE@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "Moreover, following Sukhbaatar et al. (2015) , we add a linear mapping H 2 R d⇥d to the update connection between memory hops, but in our case, down-weight it by 1 z, resulting in:"}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-97", "intents": ["@USE@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "Training Details: Following Sukhbaatar et al. (2015), we hold out 10% of the bAbI training set to form a development set."}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-100", "intents": ["@USE@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "Following Sukhbaatar et al. (2015) , linear start is employed in all our experiments for the first 20 epochs."}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-103", "intents": ["@USE@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "Also following Sukhbaatar et al. (2015) , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to"}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-106", "intents": ["@USE@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "Consistent with other published results over bAbI (Sukhbaatar et al., 2015; Seo et al., 2017) , we repeat training 30 times for each task, and select the model which performs best on the development set."}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-110", "intents": ["@USE@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "We benchmark against other memory network based models: (1) N2N with ADJ and LW (Sukhbaatar et al., 2015) ; (2) DMN (Kumar et al., 2016) and its improved version DMN+ (Xiong et al., 2016) ; and (3) GN2N (Liu and Perez, 2017) ."}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-143", "intents": ["@USE@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "As a large variance can be observed due to how sensitive memory-based models are to parameter initialisation, following (Sukhbaatar et al., 2015) and (Liu and Perez, 2017) , we repeat each training 10 times using the Table 4 : Per-response accuracy on the Dialog bAbI tasks."}
{"sent_id": "7a2f56cb4bbcd09ba35934ca76c9a9-C001-152", "intents": ["@USE@"], "paper_id": "ABC_7a2f56cb4bbcd09ba35934ca76c9a9_7", "text": "In terms of baselines, we benchmark against other memory network-based models: 4 (1) N2N (Sukhbaatar et al., 2015) ; and (2) GN2N (Sukhbaatar et al., 2015) ."}
{"sent_id": "a301586ed006905275ab42c5e40d88-C001-25", "intents": ["@USE@"], "paper_id": "ABC_a301586ed006905275ab42c5e40d88_7", "text": "Following Seo et al. (2019) , we concatenate both sparse and dense vectors to encode every phrase in Wikipedia and use maximum similarity search to find the closest candidate phrase to answer each question."}
{"sent_id": "a301586ed006905275ab42c5e40d88-C001-127", "intents": ["@USE@"], "paper_id": "ABC_a301586ed006905275ab42c5e40d88_7", "text": "While the loss above is an unbiased estimator, in practice, we adopt early loss summation as suggested by Seo et al. (2019) for larger gradient signals in early training."}
{"sent_id": "a301586ed006905275ab42c5e40d88-C001-128", "intents": ["@USE@"], "paper_id": "ABC_a301586ed006905275ab42c5e40d88_7", "text": "Additionally, we also add dense-only loss that omits the sparse logits (i.e. original loss in Seo et al. (2019) ) to the final loss, in which case we find that we obtain higher-quality dense phrase representations."}
{"sent_id": "a301586ed006905275ab42c5e40d88-C001-131", "intents": ["@USE@"], "paper_id": "ABC_a301586ed006905275ab42c5e40d88_7", "text": "To each paragraph x, we concatenate the paragraph x neg which was paired with the question whose dense representation h neg is most similar to the original dense question representation h , following Seo et al. (2019) ."}
{"sent_id": "a301586ed006905275ab42c5e40d88-C001-150", "intents": ["@USE@"], "paper_id": "ABC_a301586ed006905275ab42c5e40d88_7", "text": "We use the same storage reduction and search techniques by Seo et al. (2019) ."}
{"sent_id": "a301586ed006905275ab42c5e40d88-C001-182", "intents": ["@USE@"], "paper_id": "ABC_a301586ed006905275ab42c5e40d88_7", "text": "Table 4 shows the outputs of three OpenQA models: DrQA (Chen et al., 2017) , DENSPI (Seo et al., 2019) , and our DENSPI+COSPR."}
{"sent_id": "a301586ed006905275ab42c5e40d88-C001-28", "intents": ["@DIF@"], "paper_id": "ABC_a301586ed006905275ab42c5e40d88_7", "text": "Notably, our method significantly outperforms DENSPI (Seo et al., 2019) , the previous end-toend QA model, by more than 4% with negligible drop in inference speed."}
{"sent_id": "a301586ed006905275ab42c5e40d88-C001-117", "intents": ["@DIF@"], "paper_id": "ABC_a301586ed006905275ab42c5e40d88_7", "text": "Unlike Seo et al. (2019) , each phrase's sparse embedding is also trained, so it needs to be considered in the loss function."}
{"sent_id": "a301586ed006905275ab42c5e40d88-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_a301586ed006905275ab42c5e40d88_7", "text": "To mitigate this problem, Seo et al. (2019) propose to learn query-agnostic representations of phrases in Wikipedia and retrieve phrases that best answers a question."}
{"sent_id": "a301586ed006905275ab42c5e40d88-C001-72", "intents": ["@BACK@"], "paper_id": "ABC_a301586ed006905275ab42c5e40d88_7", "text": "Refer to Seo et al. (2019) for details; we mostly reuse its architecture."}
{"sent_id": "a301586ed006905275ab42c5e40d88-C001-41", "intents": ["@MOT@"], "paper_id": "ABC_a301586ed006905275ab42c5e40d88_7", "text": "While Seo et al. (2019) have shown that encoding both dense and sparse representations for each phrase could keep the lexically important words of a phrase to some extent, their sparse representations are based on static tf-idf vectors which have globally the same weight for each n-gram."}
{"sent_id": "a301586ed006905275ab42c5e40d88-C001-161", "intents": ["@EXT@"], "paper_id": "ABC_a301586ed006905275ab42c5e40d88_7", "text": "We evaluate the effectiveness of COSPR by augmenting DENSPI (Seo et al., 2019) with contextualized sparse representations (DENSPI+COSPR)."}
{"sent_id": "5177188d88391f08325262dbdefabf-C001-22", "intents": ["@MOT@"], "paper_id": "ABC_5177188d88391f08325262dbdefabf_7", "text": "The context of usage is even more crucial for characterizing meanings of ambiguous or polysemous words: a definition that does not take disambiguating context into account will be of limited use (Gadetsky et al., 2018) ."}
{"sent_id": "5177188d88391f08325262dbdefabf-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_5177188d88391f08325262dbdefabf_7", "text": "3 Definition modeling as a sequence-to-sequence task Gadetsky et al. (2018) remarked that words are often ambiguous or polysemous, and thus generating a correct definition requires that we either use sense-level representations, or that we disambiguate the word embedding of the definiendum."}
{"sent_id": "5177188d88391f08325262dbdefabf-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_5177188d88391f08325262dbdefabf_7", "text": "The disambiguation that Gadetsky et al. (2018) proposed was based on a contextual cue-ie."}
{"sent_id": "5177188d88391f08325262dbdefabf-C001-149", "intents": ["@BACK@"], "paper_id": "ABC_5177188d88391f08325262dbdefabf_7", "text": "In the dataset of Gadetsky et al. (2018) (henceforth D Gad ), each example consists of a definiendum, the definientia for one of its meanings and a contextual cue sentence."}
{"sent_id": "5177188d88391f08325262dbdefabf-C001-101", "intents": ["@SIM@"], "paper_id": "ABC_5177188d88391f08325262dbdefabf_7", "text": "Being simple and natural, the SELECT approach resembles architectures like that of Gadetsky et al. (2018) and : the full encoder is dedicated to altering the embedding of the definiendum on the basis of its context; in that, the encoder may be seen as a dedicated contextualization sub-module."}
{"sent_id": "5177188d88391f08325262dbdefabf-C001-177", "intents": ["@SIM@"], "paper_id": "ABC_5177188d88391f08325262dbdefabf_7", "text": "Like Gadetsky et al. (2018) , we conclude that disambiguating the definiendum, when done correctly, improves performances: our best performing contex-tual model outranks the non-contextual variant by 5 to 6 points."}
{"sent_id": "5177188d88391f08325262dbdefabf-C001-221", "intents": ["@SIM@"], "paper_id": "ABC_5177188d88391f08325262dbdefabf_7", "text": "This was demonstrated by comparison both to the previous contextualized model by Gadetsky et al. (2018) and to the Transformerbased SELECT variation of our model, which differs from the proposed architecture only in the context encoding pipeline."}
{"sent_id": "5177188d88391f08325262dbdefabf-C001-179", "intents": ["@DIF@"], "paper_id": "ABC_5177188d88391f08325262dbdefabf_7", "text": "Note also that we do not rely on taskspecific external resources (unlike Noraset et al., 2017; or on pre-training (unlike Gadetsky et al., 2018) ."}
{"sent_id": "5177188d88391f08325262dbdefabf-C001-206", "intents": ["@FUT@"], "paper_id": "ABC_5177188d88391f08325262dbdefabf_7", "text": "Another possibility would be to pre-train the model, as was done by Gadetsky et al. (2018) : in our case in particular, the encoder could be trained for POS-tagging or lemmatization."}
{"sent_id": "7f2622701e1f6c8492ec627b6ac32b-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_7f2622701e1f6c8492ec627b6ac32b_7", "text": "Recently, Guided Source Separation (GSS) enhancement on the test data was shown to significantly improve the performance of an acoustic model, which had been trained with a large amount of unprocessed and simulated noisy data [13] ."}
{"sent_id": "7f2622701e1f6c8492ec627b6ac32b-C001-40", "intents": ["@DIF@"], "paper_id": "ABC_7f2622701e1f6c8492ec627b6ac32b_7", "text": "This compares very favorably with the recently published topline in [13] , where the single-system best result, i.e., the WER without system combination, was 45.1 % and 47.3 % on DEV and EVAL, respectively, using an augmented training data set of 4500 hrs total."}
{"sent_id": "7f2622701e1f6c8492ec627b6ac32b-C001-141", "intents": ["@DIF@"], "paper_id": "ABC_7f2622701e1f6c8492ec627b6ac32b_7", "text": "For the single array track, the proposed system without RNN LM rescoring achieves 16 % (11 %) relative WER reduction on the DEV (EVAL) set when compared with System8 in [13] (row one in Table 4 )."}
{"sent_id": "7f2622701e1f6c8492ec627b6ac32b-C001-143", "intents": ["@DIF@"], "paper_id": "ABC_7f2622701e1f6c8492ec627b6ac32b_7", "text": "For the multi array track, the proposed system without RNN LM rescoring achieved 6 % (7 %) relative WER reduction on the DEV (EVAL) set when compared with System16 in [13] (row six in Table 4 )."}
{"sent_id": "7f2622701e1f6c8492ec627b6ac32b-C001-63", "intents": ["@USE@"], "paper_id": "ABC_7f2622701e1f6c8492ec627b6ac32b_7", "text": "It follows the approach presented in [13] , which was shown to outperform the baseline version."}
{"sent_id": "7f2622701e1f6c8492ec627b6ac32b-C001-87", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_7f2622701e1f6c8492ec627b6ac32b_7", "text": "Therefore, the temporal context was only used for dereverberation and the mixture model parameter estimation, while for the estimation of covariance matrices for beamforming the context was dropped and only the original segment length was considered [13] ."}
{"sent_id": "7f2622701e1f6c8492ec627b6ac32b-C001-135", "intents": ["@USE@"], "paper_id": "ABC_7f2622701e1f6c8492ec627b6ac32b_7", "text": "To facilitate comparison with the recently published top-line in [13] (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table 4 ."}
{"sent_id": "7f2622701e1f6c8492ec627b6ac32b-C001-136", "intents": ["@USE@"], "paper_id": "ABC_7f2622701e1f6c8492ec627b6ac32b_7", "text": "As explained in Section 5.1, we opted for [13] instead of [14] as baseline because the former system is stronger."}
{"sent_id": "7f2622701e1f6c8492ec627b6ac32b-C001-140", "intents": ["@USE@"], "paper_id": "ABC_7f2622701e1f6c8492ec627b6ac32b_7", "text": "To have a fair comparison, the results are compared with the single-system performance reported in [13] ."}
{"sent_id": "7f2622701e1f6c8492ec627b6ac32b-C001-146", "intents": ["@USE@"], "paper_id": "ABC_7f2622701e1f6c8492ec627b6ac32b_7", "text": "Finally, cleaning up the training set not only boosted the recognition performance, but managed to do so using a fraction of the training data in [13] , as shown in Table 5 ."}
{"sent_id": "7f2622701e1f6c8492ec627b6ac32b-C001-156", "intents": ["@USE@"], "paper_id": "ABC_7f2622701e1f6c8492ec627b6ac32b_7", "text": "The first row corresponds to the GSS configuration in [14] while the second one corresponds to the GSS configuration in [13] ."}
{"sent_id": "7f2622701e1f6c8492ec627b6ac32b-C001-161", "intents": ["@USE@"], "paper_id": "ABC_7f2622701e1f6c8492ec627b6ac32b_7", "text": "Consequently, we have chosen system [13] as baseline in this study since is using the stronger GSS configuration."}
{"sent_id": "7f2622701e1f6c8492ec627b6ac32b-C001-84", "intents": ["@MOT@"], "paper_id": "ABC_7f2622701e1f6c8492ec627b6ac32b_7", "text": "However, having such a large temporal context may become problematic when the speakers are moving, because the estimated spatial covariance matrix can become outdated due to the movement [13] ."}
{"sent_id": "24506b0aa7a859eb8744e390f9fb60-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_24506b0aa7a859eb8744e390f9fb60_7", "text": "Meanwhile, several previous works (Carreras, 2007; Koo and Collins, 2010) have shown that grandchild interactions provide important information for dependency parsing."}
{"sent_id": "24506b0aa7a859eb8744e390f9fb60-C001-15", "intents": ["@MOT@"], "paper_id": "ABC_24506b0aa7a859eb8744e390f9fb60_7", "text": "Consequently, the existing most powerful parser (Koo and Collins, 2010 ) is limited to third-order parts, which requires O(n 4 ) time and O(n 3 ) space."}
{"sent_id": "24506b0aa7a859eb8744e390f9fb60-C001-33", "intents": ["@USE@"], "paper_id": "ABC_24506b0aa7a859eb8744e390f9fb60_7", "text": "Following Koo and Collins (2010) , we refer to these augmented structures as g-spans."}
{"sent_id": "24506b0aa7a859eb8744e390f9fb60-C001-58", "intents": ["@USE@"], "paper_id": "ABC_24506b0aa7a859eb8744e390f9fb60_7", "text": "Following previous works (McDonald and Pereira, 2006; Koo and Collins, 2010) , the fourthorder parser captures not only features associated with corresponding fourth-order grand-trisibling parts, but also the features of relevant lower-order parts that are enclosed in its factorization."}
{"sent_id": "24506b0aa7a859eb8744e390f9fb60-C001-65", "intents": ["@USE@"], "paper_id": "ABC_24506b0aa7a859eb8744e390f9fb60_7", "text": "The second set of features is defined as backed-off features (Koo and Collins, 2010) for grand-tri-sibling part (g, s, r, m, t)-the 4-gram (g, r, m, t), which never exist in any lower-order part."}
{"sent_id": "24506b0aa7a859eb8744e390f9fb60-C001-71", "intents": ["@USE@"], "paper_id": "ABC_24506b0aa7a859eb8744e390f9fb60_7", "text": "Following Koo and Collins (2010) , two versions of POS tags are used for any features involve POS: one using is normal POS tags and another is a coarsened version of the POS tags."}
{"sent_id": "24506b0aa7a859eb8744e390f9fb60-C001-99", "intents": ["@USE@"], "paper_id": "ABC_24506b0aa7a859eb8744e390f9fb60_7", "text": "We compare our method to first-order and secondorder sibling dependency parsers (McDonald and Pereira, 2006) , and two third-order graphbased parsers (Koo and Collins, 2010) ."}
{"sent_id": "24506b0aa7a859eb8744e390f9fb60-C001-102", "intents": ["@DIF@"], "paper_id": "ABC_24506b0aa7a859eb8744e390f9fb60_7", "text": "Our results are also better than the results of the two third-order graph-based dependency parsing models in Koo and Collins (2010) ."}
{"sent_id": "24506b0aa7a859eb8744e390f9fb60-C001-109", "intents": ["@DIF@"], "paper_id": "ABC_24506b0aa7a859eb8744e390f9fb60_7", "text": "Here we compare our method to an implement of the third-order grand-sibling parser -whose parsing performance on CTB is not reported in Koo and Collins (2010) , and the dynamic programming transition-based parser of Huang and Sagae (2010) ."}
{"sent_id": "4f1a48dc79b9a099783d7e63741883-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_4f1a48dc79b9a099783d7e63741883_7", "text": "Nonetheless, several properties distinguish our corpus of peer reviews from other types of reviews: 1) The helpfulness of our peer reviews is directly rated using a discrete scale from one to five instead of being defined as a function of binary votes (e.g. the percentage of \"helpful\" votes (Kim et al., 2006) ); 2) Peer reviews frequently refer to the related students' papers, thus review analysis needs to take into account paper topics; 3) Within the context of education, peer-review helpfulness often has a writing specific semantics, e.g. improving revision likelihood; 4) In general, peer-review corpora collected from classrooms are of a much smaller size compared to online product reviews."}
{"sent_id": "4f1a48dc79b9a099783d7e63741883-C001-71", "intents": ["@DIF@"], "paper_id": "ABC_4f1a48dc79b9a099783d7e63741883_7", "text": "Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews (Kim et al., 2006) where product scores are significantly correlated with product-review helpfulness."}
{"sent_id": "4f1a48dc79b9a099783d7e63741883-C001-73", "intents": ["@DIF@"], "paper_id": "ABC_4f1a48dc79b9a099783d7e63741883_7", "text": "When comparing the performance between predicting helpfulness ratings versus ranking, we observe r ≈ r s consistently for our peer reviews, while Kim et al. (2006) reported r < r s for product reviews."}
{"sent_id": "4f1a48dc79b9a099783d7e63741883-C001-95", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_4f1a48dc79b9a099783d7e63741883_7", "text": "Given only 267 peer reviews in our case compared to more than ten thousand product reviews (Kim et al., 2006) , this is an important consideration."}
{"sent_id": "4f1a48dc79b9a099783d7e63741883-C001-100", "intents": ["@DIF@"], "paper_id": "ABC_4f1a48dc79b9a099783d7e63741883_7", "text": "We also found that SVM regression does not favor ranking over predicting helpfulness as in (Kim et al., 2006) ."}
{"sent_id": "4f1a48dc79b9a099783d7e63741883-C001-98", "intents": ["@DIF@"], "paper_id": "ABC_4f1a48dc79b9a099783d7e63741883_7", "text": "More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews (Kim et al., 2006; Danescu-Niculescu-Mizil et al., 2009) , have no predictive power for peer reviews."}
{"sent_id": "4f1a48dc79b9a099783d7e63741883-C001-42", "intents": ["@USE@"], "paper_id": "ABC_4f1a48dc79b9a099783d7e63741883_7", "text": "As shown in Table 1 , we first mine generic linguistic features from reviews and papers based on the results of syntactic analysis of the texts, aiming to replicate the feature sets used by Kim et al. (2006) ."}
{"sent_id": "4f1a48dc79b9a099783d7e63741883-C001-62", "intents": ["@USE@"], "paper_id": "ABC_4f1a48dc79b9a099783d7e63741883_7", "text": "Following Kim et al. (2006) , we train our helpfulness model using SVM regression with a radial basis function kernel provided by SVM light (Joachims, 1999) ."}
{"sent_id": "4f1a48dc79b9a099783d7e63741883-C001-96", "intents": ["@USE@"], "paper_id": "ABC_4f1a48dc79b9a099783d7e63741883_7", "text": "Though our absolute quantitative results are not directly comparable to the results of Kim et al. (2006) , we indirectly compared them by analyzing the utility of features in isolation and combined."}
{"sent_id": "4f1a48dc79b9a099783d7e63741883-C001-74", "intents": ["@SIM@"], "paper_id": "ABC_4f1a48dc79b9a099783d7e63741883_7", "text": "4 Finally, we observed a similar feature redundancy effect as Kim et al. (2006) did, in that simply combining all features does not improve the model's performance."}
{"sent_id": "206b65ee4e69a01e8a0892dc0f2b30-C001-12", "intents": ["@MOT@"], "paper_id": "ABC_206b65ee4e69a01e8a0892dc0f2b30_7", "text": "When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Plank and Moschitti, 2013) ."}
{"sent_id": "206b65ee4e69a01e8a0892dc0f2b30-C001-18", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_206b65ee4e69a01e8a0892dc0f2b30_7", "text": "In fact, Plank and Moschitti (2013) only use the 10-bit cluster prefix in their study."}
{"sent_id": "206b65ee4e69a01e8a0892dc0f2b30-C001-61", "intents": ["@MOT@"], "paper_id": "ABC_206b65ee4e69a01e8a0892dc0f2b30_7", "text": "Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality (Plank and Moschitti, 2013) ."}
{"sent_id": "206b65ee4e69a01e8a0892dc0f2b30-C001-63", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_206b65ee4e69a01e8a0892dc0f2b30_7", "text": "Therefore, following the settings of Plank and Moschitti (2013) , we will only assume entity boundaries and not rely on the gold standard information in the experiments."}
{"sent_id": "206b65ee4e69a01e8a0892dc0f2b30-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_206b65ee4e69a01e8a0892dc0f2b30_7", "text": "The only study explicitly targeting this problem so far is by Plank and Moschitti (2013) who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels."}
{"sent_id": "206b65ee4e69a01e8a0892dc0f2b30-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_206b65ee4e69a01e8a0892dc0f2b30_7", "text": "The application of word representations such as word clusters in domain adaptation of RE (Plank and Moschitti, 2013 ) is motivated by its successes in semi-supervised methods (Chan and Roth, 2010; Sun et al., 2011) where word representations help to reduce data-sparseness of lexical information in the training data."}
{"sent_id": "206b65ee4e69a01e8a0892dc0f2b30-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_206b65ee4e69a01e8a0892dc0f2b30_7", "text": "As noted in Plank and Moschitti (2013) , the distributions of relations as well as the vocabularies of the domains are quite different."}
{"sent_id": "206b65ee4e69a01e8a0892dc0f2b30-C001-36", "intents": ["@USE@"], "paper_id": "ABC_206b65ee4e69a01e8a0892dc0f2b30_7", "text": "Following Plank and Moschitti (2013) , we assume that we only have labeled data in a single source domain but no labeled as well as unlabeled target data."}
{"sent_id": "206b65ee4e69a01e8a0892dc0f2b30-C001-76", "intents": ["@USE@"], "paper_id": "ABC_206b65ee4e69a01e8a0892dc0f2b30_7", "text": "We use the ACE 2005 corpus for DA experiments (as in Plank and Moschitti (2013) )."}
{"sent_id": "206b65ee4e69a01e8a0892dc0f2b30-C001-78", "intents": ["@USE@"], "paper_id": "ABC_206b65ee4e69a01e8a0892dc0f2b30_7", "text": "We follow the standard practices on ACE (Plank and Moschitti, 2013) and use news (the union of bn and nw) as the source domain and bc, cts and wl as our target domains."}
{"sent_id": "206b65ee4e69a01e8a0892dc0f2b30-C001-100", "intents": ["@USE@"], "paper_id": "ABC_206b65ee4e69a01e8a0892dc0f2b30_7", "text": "For word clusters, we experiment with two possibilities: (i) only using a single prefix length of 10 (as Plank and Moschitti (2013) did) (denoted by WC10) and (ii) applying multiple prefix lengths of 4, 6, 8, 10 together with the full string (denoted by WC)."}
{"sent_id": "206b65ee4e69a01e8a0892dc0f2b30-C001-102", "intents": ["@DIF@"], "paper_id": "ABC_206b65ee4e69a01e8a0892dc0f2b30_7", "text": "Our baseline performance is worse than that of Plank and Moschitti (2013) only on the target domain cts and better in the other cases."}
{"sent_id": "206b65ee4e69a01e8a0892dc0f2b30-C001-113", "intents": ["@DIF@"], "paper_id": "ABC_206b65ee4e69a01e8a0892dc0f2b30_7", "text": "(v): Finally, the in-domain performance is also improved consistently demonstrating the robustness of word representations (Plank and Moschitti, 2013) ."}
{"sent_id": "91e869971f139d90e36f73b1089877-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_91e869971f139d90e36f73b1089877_7", "text": "BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011) , summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; Stajner et al., 2015; Xu et al., 2016) , i.e. the rewriting of a sentence as one or more simpler sentences."}
{"sent_id": "91e869971f139d90e36f73b1089877-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_91e869971f139d90e36f73b1089877_7", "text": "Indeed, focusing on lexical simplification, Xu et al. (2016) argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used."}
{"sent_id": "91e869971f139d90e36f73b1089877-C001-78", "intents": ["@BACK@"], "paper_id": "ABC_91e869971f139d90e36f73b1089877_7", "text": "In addition to BLEU, 7 we also experiment with (1) iBLEU (Sun and Zhou, 2012) which was recently used for TS (Xu et al., 2016; and which takes into account the BLEU scores of the output against the input and against the references; (2) the Flesch-Kincaid Grade Level (FK; Kincaid et al., 1975 ), computed at the system level, which estimates the readability of the text with a lower value indicating higher 4 Examples are taken from Siddharthan (2006) ."}
{"sent_id": "91e869971f139d90e36f73b1089877-C001-23", "intents": ["@USE@"], "paper_id": "ABC_91e869971f139d90e36f73b1089877_7", "text": "First, we experiment with the most common set, proposed by Xu et al. (2016) , evaluating a variety of system outputs, as well as HSplit."}
{"sent_id": "91e869971f139d90e36f73b1089877-C001-56", "intents": ["@USE@"], "paper_id": "ABC_91e869971f139d90e36f73b1089877_7", "text": "We use the complex side of the test corpus of Xu et al. (2016) ."}
{"sent_id": "91e869971f139d90e36f73b1089877-C001-92", "intents": ["@USE@"], "paper_id": "ABC_91e869971f139d90e36f73b1089877_7", "text": "10 We further include Moses (Koehn et al., 2007) and SBMT-SARI (Xu et al., 2016) , a syntax-based MT system tuned against SARI, and the identity function (outputs are same as inputs)."}
{"sent_id": "91e869971f139d90e36f73b1089877-C001-87", "intents": ["@USE@"], "paper_id": "ABC_91e869971f139d90e36f73b1089877_7", "text": "In one (\"Standard Reference Setting\", §4.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by Xu et al. (2016) (yielding BLEU-8ref, iBLEU-8ref and SARI-8ref)."}
{"sent_id": "91e869971f139d90e36f73b1089877-C001-96", "intents": ["@USE@"], "paper_id": "ABC_91e869971f139d90e36f73b1089877_7", "text": "We use the evaluation benchmark provided by Sulem et al. (2018b) , 11 including system outputs and human evaluation scores corresponding to the first 70 sentences of the test corpus of Xu et al. (2016) , and extend it to apply to HSplit as well."}
{"sent_id": "91e869971f139d90e36f73b1089877-C001-46", "intents": ["@MOT@"], "paper_id": "ABC_91e869971f139d90e36f73b1089877_7", "text": "While BLEU is standardly used for TS evaluation (e.g., Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Ma and Sun, 2017 ), only few works tested its correlation with human judgments."}
{"sent_id": "91e869971f139d90e36f73b1089877-C001-58", "intents": ["@EXT@"], "paper_id": "ABC_91e869971f139d90e36f73b1089877_7", "text": "This corpus enriches the set of references focused on lexical operations that were collected by Xu et al. (2016) for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (Narayan et al., 2017) ."}
{"sent_id": "91e869971f139d90e36f73b1089877-C001-119", "intents": ["@SIM@"], "paper_id": "ABC_91e869971f139d90e36f73b1089877_7", "text": "12 The high scores obtained for Identity, also observed by Xu et al. (2016) , indicate that BLEU is a not a good predictor for relative simplicity to the input."}
{"sent_id": "53ed85f4bfa634656062ad6ba342d2-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_53ed85f4bfa634656062ad6ba342d2_7", "text": "Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000) , English (Yamada and Matsumoto, 2003) , Turkish (Oflazer, 2003) , and Swedish (Nivre et al., 2004) ."}
{"sent_id": "53ed85f4bfa634656062ad6ba342d2-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_53ed85f4bfa634656062ad6ba342d2_7", "text": "Moreover, the deterministic dependency parser of Yamada and Matsumoto (2003) , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of Collins (1997) and Charniak (2000) ."}
{"sent_id": "53ed85f4bfa634656062ad6ba342d2-C001-16", "intents": ["@USE@", "@SIM@", "@DIF@", "@EXT@"], "paper_id": "ABC_53ed85f4bfa634656062ad6ba342d2_7", "text": "The parser described in this paper is similar to that of Yamada and Matsumoto (2003) in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank."}
{"sent_id": "53ed85f4bfa634656062ad6ba342d2-C001-87", "intents": ["@USE@"], "paper_id": "ABC_53ed85f4bfa634656062ad6ba342d2_7", "text": "This permits us to make exact comparisons with the parser of Yamada and Matsumoto (2003) , but also the parsers of Collins (1997) and Charniak (2000) , which are evaluated on the same data set in Yamada and Matsumoto (2003) ."}
{"sent_id": "53ed85f4bfa634656062ad6ba342d2-C001-104", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_53ed85f4bfa634656062ad6ba342d2_7", "text": "The proportion of non-root words that are assigned the correct head (Yamada and Matsumoto, 2003) ."}
{"sent_id": "53ed85f4bfa634656062ad6ba342d2-C001-107", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_53ed85f4bfa634656062ad6ba342d2_7", "text": "The proportion of root words that are analyzed as such (Yamada and Matsumoto, 2003) ."}
{"sent_id": "53ed85f4bfa634656062ad6ba342d2-C001-110", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_53ed85f4bfa634656062ad6ba342d2_7", "text": "The proportion of sentences whose unlabeled dependency structure is completely correct (Yamada and Matsumoto, 2003) ."}
{"sent_id": "53ed85f4bfa634656062ad6ba342d2-C001-118", "intents": ["@USE@"], "paper_id": "ABC_53ed85f4bfa634656062ad6ba342d2_7", "text": "Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with Collins (1997) (Model 3) , Charniak (2000) , and Yamada and Matsumoto (2003) ."}
{"sent_id": "53ed85f4bfa634656062ad6ba342d2-C001-123", "intents": ["@USE@"], "paper_id": "ABC_53ed85f4bfa634656062ad6ba342d2_7", "text": "Secondly, since 5 The information in the first three rows is taken directly from Yamada and Matsumoto (2003) ."}
{"sent_id": "53ed85f4bfa634656062ad6ba342d2-C001-22", "intents": ["@DIF@"], "paper_id": "ABC_53ed85f4bfa634656062ad6ba342d2_7", "text": "As far as we know, this makes it different from all previous systems for dependency parsing applied to the Penn Treebank (Eisner, 1996; Yamada and Matsumoto, 2003) , although there are systems that extract labeled grammatical relations based on shallow parsing, e.g. Buchholz (2002) ."}
{"sent_id": "53ed85f4bfa634656062ad6ba342d2-C001-119", "intents": ["@DIF@"], "paper_id": "ABC_53ed85f4bfa634656062ad6ba342d2_7", "text": "5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of Yamada and Matsumoto (2003) ."}
{"sent_id": "53ed85f4bfa634656062ad6ba342d2-C001-121", "intents": ["@DIF@"], "paper_id": "ABC_53ed85f4bfa634656062ad6ba342d2_7", "text": "First of all, the part-of-speech tagger used for preprocessing in our experiments has a lower accuracy than the one used by Yamada and Matsumoto (2003) (96.1% vs. 97.1%) ."}
{"sent_id": "53ed85f4bfa634656062ad6ba342d2-C001-137", "intents": ["@DIF@"], "paper_id": "ABC_53ed85f4bfa634656062ad6ba342d2_7", "text": "6 Although none of the previous results on labeling accuracy is strictly comparable to ours, it nevertheless seems fair to conclude that the (Yamada and Matsumoto, 2003) labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is not."}
{"sent_id": "bb74dd634a8fc5cdb2f4f3294b6bc5-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7", "text": "[2] ever proposed a approach to close the goal."}
{"sent_id": "bb74dd634a8fc5cdb2f4f3294b6bc5-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7", "text": "[4] extend the work of [2] with a neural network based approach."}
{"sent_id": "bb74dd634a8fc5cdb2f4f3294b6bc5-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7", "text": "[2] improved the sentiment classification by involving knowledge."}
{"sent_id": "bb74dd634a8fc5cdb2f4f3294b6bc5-C001-52", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7", "text": "Previous classical paper [2] chose the sentiment classification as the learning target because it could be regarded as a large task as well as a group of related sub-tasks in the different domains."}
{"sent_id": "bb74dd634a8fc5cdb2f4f3294b6bc5-C001-58", "intents": ["@USE@"], "paper_id": "ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7", "text": "As the previous work [2] , this paper also uses Naïve Bayes as the knowledge can be presented by the probability."}
{"sent_id": "bb74dd634a8fc5cdb2f4f3294b6bc5-C001-77", "intents": ["@USE@"], "paper_id": "ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7", "text": "We use the same formula below as in the LSC [2] ."}
{"sent_id": "f54235664f013f0fec918222be9198-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_f54235664f013f0fec918222be9198_7", "text": "While Klein and Manning's approach may be described as an \"all-substrings\" approach to unsupervised parsing, an even richer model consists of an \"all-subtrees\" approach to unsupervised parsing, called U-DOP (Bod 2006) ."}
{"sent_id": "f54235664f013f0fec918222be9198-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_f54235664f013f0fec918222be9198_7", "text": "Moreover, DOP*'s estimation procedure is very efficient, while the EM training procedure for UML-DOP proposed in Bod (2006) is particularly time consuming and can only operate by randomly sampling trees."}
{"sent_id": "f54235664f013f0fec918222be9198-C001-22", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_f54235664f013f0fec918222be9198_7", "text": "While we do not achieve as high an f-score as the UML-DOP model in Bod (2006) , we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in Bod (2006) ."}
{"sent_id": "f54235664f013f0fec918222be9198-C001-69", "intents": ["@DIF@"], "paper_id": "ABC_f54235664f013f0fec918222be9198_7", "text": "This is a huge reduction compared to Bod (2006) where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work."}
{"sent_id": "f54235664f013f0fec918222be9198-C001-129", "intents": ["@DIF@"], "paper_id": "ABC_f54235664f013f0fec918222be9198_7", "text": "While a similar result was obtained in Bod (2006) , the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006): 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction."}
{"sent_id": "f54235664f013f0fec918222be9198-C001-76", "intents": ["@USE@"], "paper_id": "ABC_f54235664f013f0fec918222be9198_7", "text": "Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006) ."}
{"sent_id": "f54235664f013f0fec918222be9198-C001-83", "intents": ["@USE@"], "paper_id": "ABC_f54235664f013f0fec918222be9198_7", "text": "To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Manning (2002, 2004) and Bod (2006) : Penn's WSJ10 which contains 7422 sentences ≤ 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences ≤ 10 words after removing punctuation."}
{"sent_id": "f54235664f013f0fec918222be9198-C001-89", "intents": ["@USE@"], "paper_id": "ABC_f54235664f013f0fec918222be9198_7", "text": "All trees in the test set were binarized beforehand, in the same way as in Bod (2006) ."}
{"sent_id": "f54235664f013f0fec918222be9198-C001-93", "intents": ["@USE@"], "paper_id": "ABC_f54235664f013f0fec918222be9198_7", "text": "Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006) , the CCM model in Klein and Manning (2002) , the DMV dependency model in Klein and Manning (2004) It should be kept in mind that an exact comparison can only be made between U-DOP* and UML-DOP in table 1, since these two models were tested on 90%/10% splits, while the other models were applied to the full WSJ10, NEGRA10 and CTB10 corpora."}
{"sent_id": "f54235664f013f0fec918222be9198-C001-45", "intents": ["@EXT@"], "paper_id": "ABC_f54235664f013f0fec918222be9198_7", "text": "We will use the same allsubtrees methodology as in Bod (2006) , but now by applying the efficient and consistent DOP*-based estimator."}
{"sent_id": "d576de5c19d7ff62a143b0d4d56135-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_d576de5c19d7ff62a143b0d4d56135_7", "text": "An important goal in argument mining is to understand the structure in argumentative text (Persing and Ng, 2016; Peldszus and Stede, 2015; Stab and Gurevych, 2016; Nguyen and Litman, 2016) ."}
{"sent_id": "d576de5c19d7ff62a143b0d4d56135-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_d576de5c19d7ff62a143b0d4d56135_7", "text": "This can be labeled as a 'micro' approach to argument mining (Stab and Gurevych, 2016) ."}
{"sent_id": "d576de5c19d7ff62a143b0d4d56135-C001-36", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_d576de5c19d7ff62a143b0d4d56135_7", "text": "Linking to the first argument component can provide a competitive baseline heuristic (Peldszus and Stede, 2015; Stab and Gurevych, 2016) ."}
{"sent_id": "d576de5c19d7ff62a143b0d4d56135-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_d576de5c19d7ff62a143b0d4d56135_7", "text": "Various authors have also proposed to jointly model link extraction with other subtasks from the argument mining pipeline, using either an Integer Linear Programming (ILP) framework (Persing and Ng, 2016; Stab and Gurevych, 2016) or directly feeding previous subtask predictions into a tree-based parser."}
{"sent_id": "d576de5c19d7ff62a143b0d4d56135-C001-176", "intents": ["@BACK@"], "paper_id": "ABC_d576de5c19d7ff62a143b0d4d56135_7", "text": "Conversely, the ILP Joint Model (Stab and Gurevych, 2016) provides constraints by sharing prediction information between the base classifiers."}
{"sent_id": "d576de5c19d7ff62a143b0d4d56135-C001-18", "intents": ["@USE@"], "paper_id": "ABC_d576de5c19d7ff62a143b0d4d56135_7", "text": "Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau and Moens, 2009; Cohen, 1987; Peldszus and Stede, 2015; Stab and Gurevych, 2016) ."}
{"sent_id": "d576de5c19d7ff62a143b0d4d56135-C001-23", "intents": ["@USE@"], "paper_id": "ABC_d576de5c19d7ff62a143b0d4d56135_7", "text": "First, the left side of the figure presents the raw text of a paragraph in a persuasive essay (Stab and Gurevych, 2016) , with the ACs contained in square brackets."}
{"sent_id": "d576de5c19d7ff62a143b0d4d56135-C001-49", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_d576de5c19d7ff62a143b0d4d56135_7", "text": "We evaluate our models on the corpora of Stab and Gurevych (2016) and Peldszus (2014) , and compare our results with the results of the aformentioned authors."}
{"sent_id": "d576de5c19d7ff62a143b0d4d56135-C001-124", "intents": ["@USE@"], "paper_id": "ABC_d576de5c19d7ff62a143b0d4d56135_7", "text": "We follow the work of Stab and Gurevych (2016) and focus on three different types of features to represent our ACs: (1) Bag-of-Words of the AC; (2) Embedding representation based on GloVe embeddings (Pennington et al., 2014) , which uses average, max, and min pooling across the token embeddings; (3) Structural features: Whether or not the AC is the first AC in a paragraph, and whether the AC is in an opening, body, or closing paragraph."}
{"sent_id": "d576de5c19d7ff62a143b0d4d56135-C001-150", "intents": ["@USE@"], "paper_id": "ABC_d576de5c19d7ff62a143b0d4d56135_7", "text": "We test the effectiveness of our proposed model on a dataset of persuasive essays (PEC) (Stab and Gurevych, 2016) , as well as a dataset of microtexts (MTC) (Peldszus, 2014) ."}
{"sent_id": "d576de5c19d7ff62a143b0d4d56135-C001-174", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_d576de5c19d7ff62a143b0d4d56135_7", "text": "We implement and compare four types of neural models: 1) The previously described joint model from In both corpora we compare against the following previously proposed models: Base Classifier (Stab and Gurevych, 2016 ) is a feature-rich, taskspecific (AC type or link extraction) SVM classifier."}
{"sent_id": "d576de5c19d7ff62a143b0d4d56135-C001-224", "intents": ["@USE@"], "paper_id": "ABC_d576de5c19d7ff62a143b0d4d56135_7", "text": "We evaluate our models on two corpora: a corpus of persuasive essays (Stab and Gurevych, 2016) , and a corpus of microtexts (Peldszus, 2014) ."}
{"sent_id": "d576de5c19d7ff62a143b0d4d56135-C001-209", "intents": ["@MOT@"], "paper_id": "ABC_d576de5c19d7ff62a143b0d4d56135_7", "text": "The popular method of averaging embeddings (which is used by Stab and Gurevych (2016) in their system) is in fact the worst method, although its performance is still competitive with the previous state-of-the-art."}
{"sent_id": "586a0f40a9299ef2753d2b0575eff8-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_586a0f40a9299ef2753d2b0575eff8_7", "text": "4.3.pdf and Rilof, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013a) or a joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014) ."}
{"sent_id": "586a0f40a9299ef2753d2b0575eff8-C001-28", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_586a0f40a9299ef2753d2b0575eff8_7", "text": "Given a sentence, for every token in that sentence, we want to predict if the current token is an event trigger: i.e, does it express some event in the pre-defined event set or not (Li et al., 2013b) ?"}
{"sent_id": "586a0f40a9299ef2753d2b0575eff8-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_586a0f40a9299ef2753d2b0575eff8_7", "text": "The state-of-the-art systems for event detection on the ACE 2005 dataset have followed the traditional feature-based approach with rich hand-designed feature sets, and statistical classifiers such as MaxEnt and perceptron for structured prediction in a joint architecture (Hong et al., 2011; Li et al., 2013b) ."}
{"sent_id": "586a0f40a9299ef2753d2b0575eff8-C001-15", "intents": ["@MOT@"], "paper_id": "ABC_586a0f40a9299ef2753d2b0575eff8_7", "text": "Although this approach has achieved the top performance (Hong et al., 2011; Li et al., 2013b) , it suffers from at least two issues:"}
{"sent_id": "586a0f40a9299ef2753d2b0575eff8-C001-54", "intents": ["@USE@"], "paper_id": "ABC_586a0f40a9299ef2753d2b0575eff8_7", "text": "For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaning 529 documents (14,849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b) ."}
{"sent_id": "586a0f40a9299ef2753d2b0575eff8-C001-69", "intents": ["@USE@"], "paper_id": "ABC_586a0f40a9299ef2753d2b0575eff8_7", "text": "Table 3 compares the performance of CNN1 and the feature-based systems in a more realistic setting, where entity mentions and types are acquired from an automatic high-performing name tagger and information extraction system (Li et al., 2013b) ."}
{"sent_id": "586a0f40a9299ef2753d2b0575eff8-C001-65", "intents": ["@DIF@"], "paper_id": "ABC_586a0f40a9299ef2753d2b0575eff8_7", "text": "As we can see from the table, considering the systems that only use sentence level information, CNN1 significantly outperforms the MaxEnt classifier as well as the joint beam search with local features from Li et al. (2013b) (an improvement of 1.6% in F1 score), and performs comparably with the joint beam search approach using both local and global features (Li et al., 2013b) ."}
{"sent_id": "586a0f40a9299ef2753d2b0575eff8-C001-67", "intents": ["@DIF@"], "paper_id": "ABC_586a0f40a9299ef2753d2b0575eff8_7", "text": "More interestingly, when the entity type information is incorporated into CNN1, we obtain CNN2 that still only needs sentence level information but achieves the stateof-the-art performance for this task (an improvement of 1.5% over the best system with only sentence level information (Li et al., 2013b) )."}
{"sent_id": "586a0f40a9299ef2753d2b0575eff8-C001-82", "intents": ["@DIF@"], "paper_id": "ABC_586a0f40a9299ef2753d2b0575eff8_7", "text": "First, rather than relying on the symbolic and concrete forms (i.e words, types etc) to construct features as the traditional feature-based systems (Ji and Grishman, 2008; Li et al., 2013b) do, CNNs automatically induce their features from word embeddings, the general distributed representation of words that is shared across domains."}
{"sent_id": "2b10893f03b4f5eaac0fe06b4d6115-C001-86", "intents": ["@USE@"], "paper_id": "ABC_2b10893f03b4f5eaac0fe06b4d6115_7", "text": "In order to compare the performance of our system with others, we also used the dataset of Tu and Roth (2012) , which contains 1,348 sentences taken from different parts of the British National Corpus."}
{"sent_id": "2b10893f03b4f5eaac0fe06b4d6115-C001-130", "intents": ["@USE@"], "paper_id": "ABC_2b10893f03b4f5eaac0fe06b4d6115_7", "text": "Moreover, Support Vector Machines (SVM) (Cortes and Vapnik, 1995) results are also reported to compare the performance of our methods with that of Tu and Roth (2012) ."}
{"sent_id": "2b10893f03b4f5eaac0fe06b4d6115-C001-141", "intents": ["@USE@"], "paper_id": "ABC_2b10893f03b4f5eaac0fe06b4d6115_7", "text": "We also compared our results with the rule-based results available for Wiki50 and also with the 5-fold cross validation results of Tu and Roth (2012) ."}
{"sent_id": "2b10893f03b4f5eaac0fe06b4d6115-C001-149", "intents": ["@USE@"], "paper_id": "ABC_2b10893f03b4f5eaac0fe06b4d6115_7", "text": "In order to compare the performance of our system with others, we evaluated it on the Tu&Roth dataset (Tu and Roth, 2012) ."}
{"sent_id": "2b10893f03b4f5eaac0fe06b4d6115-C001-100", "intents": ["@BACK@"], "paper_id": "ABC_2b10893f03b4f5eaac0fe06b4d6115_7", "text": "One example is Tu and Roth (2012) , where the authors examined a verbparticle combination only if the verbal components were formed with one of the previously given six verbs (i.e. make, take, have, give, do, get)."}
{"sent_id": "2b10893f03b4f5eaac0fe06b4d6115-C001-102", "intents": ["@BACK@"], "paper_id": "ABC_2b10893f03b4f5eaac0fe06b4d6115_7", "text": "As Table 3 shows, the six verbs used by Tu and Roth (2012) are responsible for only 50 VPCs on the Wiki50 corpus, so it covers only 11.16% of all gold standard VPCs."}
{"sent_id": "2b10893f03b4f5eaac0fe06b4d6115-C001-105", "intents": ["@BACK@"], "paper_id": "ABC_2b10893f03b4f5eaac0fe06b4d6115_7", "text": "Furthermore, 127 different verbal component occurred in Wiki50, but the verbs have and do -which are used by Tu and Roth (2012) -do not appear in the corpus as verbal component of VPCs."}
{"sent_id": "2b10893f03b4f5eaac0fe06b4d6115-C001-175", "intents": ["@BACK@"], "paper_id": "ABC_2b10893f03b4f5eaac0fe06b4d6115_7", "text": "A striking difference between the Tu & Roth database and Wiki50 is that while Tu and Roth (2012) included the verbs do and have in their data, they do not occur at all among the VPCs collected from Wiki50."}
{"sent_id": "2b10893f03b4f5eaac0fe06b4d6115-C001-132", "intents": ["@SIM@"], "paper_id": "ABC_2b10893f03b4f5eaac0fe06b4d6115_7", "text": "As Tu and Roth (2012) presented only the accuracy scores on the Tu & Roth dataset, we also employed an accuracy score as an evaluation metric on this dataset, where positive and negative examples were also marked."}
{"sent_id": "2b10893f03b4f5eaac0fe06b4d6115-C001-164", "intents": ["@DIF@"], "paper_id": "ABC_2b10893f03b4f5eaac0fe06b4d6115_7", "text": "Moreover, the results obtained with our machine learning approach on the Tu&Roth dataset outperformed those reported in Tu and Roth (2012) ."}
{"sent_id": "2b10893f03b4f5eaac0fe06b4d6115-C001-211", "intents": ["@DIF@"], "paper_id": "ABC_2b10893f03b4f5eaac0fe06b4d6115_7", "text": "Our method yielded better results than those got using the dependency parsers on the Wiki50 corpus and the method reported in (Tu and Roth, 2012) on the Tu&Roth dataset."}
{"sent_id": "c4a9b122e8f1b9e98197743c94fea2-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_c4a9b122e8f1b9e98197743c94fea2_7", "text": "Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008) , dual decomposition , or multi-commodity flows (Martins et al., 2009 (Martins et al., , 2011 ."}
{"sent_id": "c4a9b122e8f1b9e98197743c94fea2-C001-38", "intents": ["@BACK@", "@DIF@", "@MOT@"], "paper_id": "ABC_c4a9b122e8f1b9e98197743c94fea2_7", "text": "This opens the door for larger subproblems (such as the combination of trees and head automata in instead of a many-components approach (Martins et al., 2011) , while still enjoying faster convergence."}
{"sent_id": "c4a9b122e8f1b9e98197743c94fea2-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_c4a9b122e8f1b9e98197743c94fea2_7", "text": "Past work in dependency parsing considered either (i) a few \"large\" components, such as trees and head automata (Smith and Eisner, 2008; , or (ii) many \"small\" components, coming from a multi-commodity flow formulation (Martins et al., 2009 (Martins et al., , 2011 )."}
{"sent_id": "c4a9b122e8f1b9e98197743c94fea2-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_c4a9b122e8f1b9e98197743c94fea2_7", "text": "The AD 3 algorithm (Martins et al., 2011) alternates among the following iterative updates:"}
{"sent_id": "c4a9b122e8f1b9e98197743c94fea2-C001-71", "intents": ["@BACK@", "@MOT@", "@EXT@"], "paper_id": "ABC_c4a9b122e8f1b9e98197743c94fea2_7", "text": "While closed-form solutions have been developed for some specialized components (Martins et al., 2011) , this problem is in general more difficult than the one arising in the subgradient algorithm."}
{"sent_id": "c4a9b122e8f1b9e98197743c94fea2-C001-20", "intents": ["@USE@"], "paper_id": "ABC_c4a9b122e8f1b9e98197743c94fea2_7", "text": "Instead, we adapt AD 3 , the dual decomposition algorithm proposed by Martins et al. (2011) , to handle third-order features, by introducing specialized head automata."}
{"sent_id": "c4a9b122e8f1b9e98197743c94fea2-C001-32", "intents": ["@USE@"], "paper_id": "ABC_c4a9b122e8f1b9e98197743c94fea2_7", "text": "Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in Martins et al. (2011) , in addition to third-order features for grand-and tri-siblings ."}
{"sent_id": "c4a9b122e8f1b9e98197743c94fea2-C001-34", "intents": ["@USE@"], "paper_id": "ABC_c4a9b122e8f1b9e98197743c94fea2_7", "text": "In this paper, we employ alternating directions dual decomposition (AD 3 ; Martins et al., 2011) ."}
{"sent_id": "c4a9b122e8f1b9e98197743c94fea2-C001-50", "intents": ["@USE@"], "paper_id": "ABC_c4a9b122e8f1b9e98197743c94fea2_7", "text": "Following Martins et al. (2011) , the problem of obtaining the best-scored tree can be written as follows:"}
{"sent_id": "c4a9b122e8f1b9e98197743c94fea2-C001-107", "intents": ["@USE@"], "paper_id": "ABC_c4a9b122e8f1b9e98197743c94fea2_7", "text": "Each score σ HB (m, h, h ) is obtained via features that look at the heads of consecutive words (as in Martins et al. (2011) )."}
{"sent_id": "c4a9b122e8f1b9e98197743c94fea2-C001-110", "intents": ["@USE@"], "paper_id": "ABC_c4a9b122e8f1b9e98197743c94fea2_7", "text": "We handle arbitrary siblings as in Martins et al. (2011) , defining O(L 3 ) component functions of the form f ASIB h,m,s (z h,m , z h,s ) = σ ASIB (h, m, s)."}
{"sent_id": "c4a9b122e8f1b9e98197743c94fea2-C001-21", "intents": ["@DIF@"], "paper_id": "ABC_c4a9b122e8f1b9e98197743c94fea2_7", "text": "• We make our parser substantially faster than the many-components approach of Martins et al. (2011) ."}
{"sent_id": "c4a9b122e8f1b9e98197743c94fea2-C001-127", "intents": ["@SIM@"], "paper_id": "ABC_c4a9b122e8f1b9e98197743c94fea2_7", "text": "By looking at the two bottom blocks, we observe that our parser has slightly better accuracies than recent projective parsers, with comparable speed levels (with the exception of the highly optimized vine cascade approach of Rush and Petrov, 2012 Martins et al. (2010 Martins et al. ( , 2011 , , Rush and Petrov (2012) , Zhang and McDonald (2012) ."}
{"sent_id": "07b062d569749924fa6ee1b2223411-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_07b062d569749924fa6ee1b2223411_7", "text": "One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005) ."}
{"sent_id": "07b062d569749924fa6ee1b2223411-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_07b062d569749924fa6ee1b2223411_7", "text": "In Clark and Curran (2004b) we use cluster computing resources to solve this problem."}
{"sent_id": "07b062d569749924fa6ee1b2223411-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_07b062d569749924fa6ee1b2223411_7", "text": "2 The CCG Parser Clark and Curran (2004b) describes the CCG parser."}
{"sent_id": "07b062d569749924fa6ee1b2223411-C001-62", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_07b062d569749924fa6ee1b2223411_7", "text": "In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem."}
{"sent_id": "07b062d569749924fa6ee1b2223411-C001-99", "intents": ["@BACK@"], "paper_id": "ABC_07b062d569749924fa6ee1b2223411_7", "text": "A feature forest is essentially a packed chart with only the feature information retained (see Miyao and Tsujii (2002) and Clark and Curran (2004b) for the details)."}
{"sent_id": "07b062d569749924fa6ee1b2223411-C001-101", "intents": ["@BACK@"], "paper_id": "ABC_07b062d569749924fa6ee1b2223411_7", "text": "For the log-linear parsing model in Clark and Curran (2004b) , the inside-outside algorithm is used to calculate feature expectations, which are then used by the BFGS algorithm to optimise the likelihood function."}
{"sent_id": "07b062d569749924fa6ee1b2223411-C001-19", "intents": ["@USE@"], "paper_id": "ABC_07b062d569749924fa6ee1b2223411_7", "text": "We use a lexicalized phrase-structure parser, the CCG parser of Clark and Curran (2004b) , together with a DP-based decoder."}
{"sent_id": "07b062d569749924fa6ee1b2223411-C001-78", "intents": ["@USE@"], "paper_id": "ABC_07b062d569749924fa6ee1b2223411_7", "text": "In this paper, Y is the set of possible CCG derivations and GEN(x) enumerates the set of derivations for sentence x. We use the same feature representation Φ(x, y) as in Clark and Curran (2004b) , to allow comparison with the log-linear model."}
{"sent_id": "07b062d569749924fa6ee1b2223411-C001-124", "intents": ["@USE@"], "paper_id": "ABC_07b062d569749924fa6ee1b2223411_7", "text": "We applied the same normal-form restrictions used in Clark and Curran (2004b) : categories can only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints."}
{"sent_id": "07b062d569749924fa6ee1b2223411-C001-152", "intents": ["@USE@"], "paper_id": "ABC_07b062d569749924fa6ee1b2223411_7", "text": "Following Clark and Curran (2004b) , accuracy is measured using F-score over the goldstandard predicate-argument dependencies in CCGbank."}
{"sent_id": "07b062d569749924fa6ee1b2223411-C001-26", "intents": ["@MOT@"], "paper_id": "ABC_07b062d569749924fa6ee1b2223411_7", "text": "Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train."}
{"sent_id": "07b062d569749924fa6ee1b2223411-C001-139", "intents": ["@DIF@"], "paper_id": "ABC_07b062d569749924fa6ee1b2223411_7", "text": "In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM."}
{"sent_id": "c870d761c6fcd24de73f5bf98a9fd3-C001-12", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_c870d761c6fcd24de73f5bf98a9fd3_7", "text": "The work described in this paper uses probabilistic latent variable models to describe patterns of syntactic interaction, building on the selectional preference models ofÓ Séaghdha (2010) and Ritter et al. (2010) and the lexical substitution models of Dinu and Lapata (2010) ."}
{"sent_id": "c870d761c6fcd24de73f5bf98a9fd3-C001-38", "intents": ["@USE@"], "paper_id": "ABC_c870d761c6fcd24de73f5bf98a9fd3_7", "text": "In particular, we follow recent work (Dinu and Lapata, 2010; Ó Séaghdha, 2010; Ritter et al., 2010) in assuming a latent variable model that associates contexts with distributions over a shared set of variables and associates each variable with a distribution over the vocabulary of word types:"}
{"sent_id": "c870d761c6fcd24de73f5bf98a9fd3-C001-119", "intents": ["@USE@"], "paper_id": "ABC_c870d761c6fcd24de73f5bf98a9fd3_7", "text": "We use two measures to evaluate performance: Generalised Averaged Precision (Kishida, 2005 ) and Kendall's τ b rank correlation coefficient, which were used for this task by Thater et al. (2010) and Dinu and Lapata (2010) , respectively."}
{"sent_id": "c870d761c6fcd24de73f5bf98a9fd3-C001-141", "intents": ["@USE@"], "paper_id": "ABC_c870d761c6fcd24de73f5bf98a9fd3_7", "text": "For the window-based context model we follow Dinu and Lapata (2010) in treating each word within five words of a target as a member of its context set."}
{"sent_id": "c870d761c6fcd24de73f5bf98a9fd3-C001-144", "intents": ["@USE@"], "paper_id": "ABC_c870d761c6fcd24de73f5bf98a9fd3_7", "text": "The choice of 70 for scaling Wikipedia counts is adopted from Dinu and Lapata (2010) , who used the same factor for the comparably sized English Gigaword corpus."}
{"sent_id": "c870d761c6fcd24de73f5bf98a9fd3-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_c870d761c6fcd24de73f5bf98a9fd3_7", "text": "As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs."}
{"sent_id": "c870d761c6fcd24de73f5bf98a9fd3-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_c870d761c6fcd24de73f5bf98a9fd3_7", "text": "A non-generative alternative is one that estimates the similarity of the latent variable distributions associated with seeing n and o in context C. The principle that similarity between topic distributions corresponds to semantic similarity is well-known in document modelling and was proposed in the context of lexical substitution by Dinu and Lapata (2010) ."}
{"sent_id": "c870d761c6fcd24de73f5bf98a9fd3-C001-55", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_c870d761c6fcd24de73f5bf98a9fd3_7", "text": "However, Thater et al. (2010) and Dinu and Lapata (2010) both observe that contextualising both o and n can degrade performance; in view of this we actually compare P (z|o, C) with P (z|n) and make the further simplifying assumption that P (z|n) ∝ P (n|z)."}
{"sent_id": "c870d761c6fcd24de73f5bf98a9fd3-C001-164", "intents": ["@DIF@"], "paper_id": "ABC_c870d761c6fcd24de73f5bf98a9fd3_7", "text": "As remarked in Section 3.1, Dinu and Lapata (2010) use a slightly different formulation of P (z|C, o)."}
{"sent_id": "c870d761c6fcd24de73f5bf98a9fd3-C001-165", "intents": ["@DIF@"], "paper_id": "ABC_c870d761c6fcd24de73f5bf98a9fd3_7", "text": "Using the window-based context model our formulation (5) outperforms (7) for both training corpora; the Dinu and Lapata (2010) Table 6 : Performance by part of speech Table 6 gives a breakdown of performance by target part of speech for the BNC+Wikipedia-trained W5 and W5 + T ↔ C models, as well as figures provided by previous researchers."}
{"sent_id": "c870d761c6fcd24de73f5bf98a9fd3-C001-167", "intents": ["@SIM@"], "paper_id": "ABC_c870d761c6fcd24de73f5bf98a9fd3_7", "text": "As remarked above, previous researchers have used the corpus in slightly different ways; we believe that the results of Dinu and Lapata (2010) are fully comparable, while those of Thater et al. (2010) were attained on a slightly smaller dataset with parameters set through cross-validation."}
{"sent_id": "8dd8c0e61010d97d0ddae6d81a9067-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_8dd8c0e61010d97d0ddae6d81a9067_7", "text": "While recent work on generation in restricted domains, such as (Belz, 2007) , has shown promising results there remains much room for improvement particularly for broad coverage and robust generators, like those of Nakanishi et al. (2005) and Cahill and van Genabith (2006) , which do not rely on handcrafted grammars and thus can easily be ported to new languages."}
{"sent_id": "8dd8c0e61010d97d0ddae6d81a9067-C001-16", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_8dd8c0e61010d97d0ddae6d81a9067_7", "text": "In the LFG-based generation algorithm presented by Cahill and van Genabith (2006) complex named entities (i.e. those consisting of more than one word token) and other multi-word units can be fragmented in the surface realization."}
{"sent_id": "8dd8c0e61010d97d0ddae6d81a9067-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_8dd8c0e61010d97d0ddae6d81a9067_7", "text": "These rules can be handcrafted grammar rules, such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005) , created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al., 2005; Cahill and van Genabith, 2006) ."}
{"sent_id": "8dd8c0e61010d97d0ddae6d81a9067-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_8dd8c0e61010d97d0ddae6d81a9067_7", "text": "The generation model of (Cahill and van Genabith, 2006) maximises the probability of a tree given an f-structure (Eqn. 1), and the string generated is the yield of the highest probability tree."}
{"sent_id": "8dd8c0e61010d97d0ddae6d81a9067-C001-74", "intents": ["@BACK@"], "paper_id": "ABC_8dd8c0e61010d97d0ddae6d81a9067_7", "text": "To solve the problem, Cahill and van Genabith (2006) apply an automatic generation grammar transformation to their training data: they automatically label CFG nodes with additional case information and the model now learns the new improved generation rules of Tables 4 and 5 . Note how the additional case labelling subverts the problematic independence assumptions of the probability model and communicates the fact that a subject NP has to be realised as nominative case from the S → NP-nom VP production, via the intermediate NP-nom → PRP-nom, down to the lexical production PRP-nom → she."}
{"sent_id": "8dd8c0e61010d97d0ddae6d81a9067-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_8dd8c0e61010d97d0ddae6d81a9067_7", "text": "The automatic generation grammar transform presented in (Cahill and van Genabith, 2006) provides a solution to coarse-grained and (in fact) inappropriate independence assumptions in the basic generation model."}
{"sent_id": "8dd8c0e61010d97d0ddae6d81a9067-C001-18", "intents": ["@USE@"], "paper_id": "ABC_8dd8c0e61010d97d0ddae6d81a9067_7", "text": "We take the generator of (Cahill and van Genabith, 2006) as our baseline generator."}
{"sent_id": "8dd8c0e61010d97d0ddae6d81a9067-C001-122", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_8dd8c0e61010d97d0ddae6d81a9067_7", "text": "As in (Cahill and van Genabith, 2006) fstructures are generated from the (now altered) treebank and from this data, along with the treebank trees, the PCFG-based grammar, which is used for training the generation model, is extracted."}
{"sent_id": "8dd8c0e61010d97d0ddae6d81a9067-C001-159", "intents": ["@USE@"], "paper_id": "ABC_8dd8c0e61010d97d0ddae6d81a9067_7", "text": "In Table 10 , Baseline gives the results of the generation algorithm of (Cahill and van Genabith, 2006) ."}
{"sent_id": "8dd8c0e61010d97d0ddae6d81a9067-C001-92", "intents": ["@DIF@"], "paper_id": "ABC_8dd8c0e61010d97d0ddae6d81a9067_7", "text": "This additional conditioning has the effect of making the choice of generation rules sensitive to the history of the generation process, and, we argue, provides a simpler, more uniform, general, intuitive and natural probabilistic generation model obviating the need for CFG-grammar transforms in the original proposal of (Cahill and van Genabith, 2006) ."}
{"sent_id": "8dd8c0e61010d97d0ddae6d81a9067-C001-99", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_8dd8c0e61010d97d0ddae6d81a9067_7", "text": "F-Struct Feats Grammar Rules Note, that for our example the effect of the uniform additional conditioning on mother grammatical function has the same effect as the generation grammar transform of (Cahill and van Genabith, 2006 ), but without the need for the gram- mar transform."}
{"sent_id": "8dd8c0e61010d97d0ddae6d81a9067-C001-101", "intents": ["@DIF@"], "paper_id": "ABC_8dd8c0e61010d97d0ddae6d81a9067_7", "text": "In addition, uniform conditioning on mother grammatical function is more general than the case-phenomena specific generation grammar transform of (Cahill and van Genabith, 2006) , in that it applies to each and every sub-part of a recursive input f-structure driving generation, making available relevant generation history (context) to guide local generation decisions."}
{"sent_id": "a7d6441ad365994edb41209e6405e0-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_a7d6441ad365994edb41209e6405e0_8", "text": "Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015) , POS tagging (Ling et al., 2015; Wang et al., 2015) , transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016) , fine-grained sentiment analysis (Liu et al., 2015) , syntactic chunking (Huang et al., 2015) , and semantic role labeling (Zhou and Xu, 2015) ."}
{"sent_id": "a7d6441ad365994edb41209e6405e0-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_a7d6441ad365994edb41209e6405e0_8", "text": "Previous work on using deep learning-based methods for POS tagging has focused either on a single language (Collobert et al., 2011; Wang et al., 2015) or a small set of languages (Ling et al., 2015; Santos and Zadrozny, 2014 )."}
{"sent_id": "a7d6441ad365994edb41209e6405e0-C001-18", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_a7d6441ad365994edb41209e6405e0_8", "text": "These levels of representation were previously introduced in different efforts (Chrupała, 2013; Zhang et al., 2015; Ling et al., 2015; Santos and Zadrozny, 2014; Gillick et al., 2016; Kim et al., 2015) , but a comparative evaluation was missing."}
{"sent_id": "a7d6441ad365994edb41209e6405e0-C001-41", "intents": ["@USE@"], "paper_id": "ABC_a7d6441ad365994edb41209e6405e0_8", "text": "Our basic bi-LSTM tagging model is a context bi-LSTM taking as input word embeddings w. We incorporate subtoken information using an hierarchical bi-LSTM architecture (Ling et al., 2015; Ballesteros et al., 2015 )."}
{"sent_id": "a7d6441ad365994edb41209e6405e0-C001-86", "intents": ["@SIM@"], "paper_id": "ABC_a7d6441ad365994edb41209e6405e0_8", "text": "We examined simple RNNs and confirm the finding of Ling et al. (2015) that they performed worse than their LSTM counterparts."}
{"sent_id": "a7d6441ad365994edb41209e6405e0-C001-118", "intents": ["@DIF@"], "paper_id": "ABC_a7d6441ad365994edb41209e6405e0_8", "text": "Bi-LSTMs for POS tagging are also reported in Wang et al. (2015) , however, they only explore word embeddings, orthographic information and evaluate on WSJ only."}
{"sent_id": "1fd85a350d9ec7ac12151cfe4412e4-C001-4", "intents": ["@BACK@"], "paper_id": "ABC_1fd85a350d9ec7ac12151cfe4412e4_8", "text": "State-of-the-art NLG models are built using recurrent neural network (RNN) based sequence to sequence models (Dušek and Jurcicek, 2016a) ."}
{"sent_id": "1fd85a350d9ec7ac12151cfe4412e4-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_1fd85a350d9ec7ac12151cfe4412e4_8", "text": "Recent advances have been in the direction of developing a fully trainable context aware NLG model (Dušek and Jurcicek, 2016a) ."}
{"sent_id": "1fd85a350d9ec7ac12151cfe4412e4-C001-70", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_1fd85a350d9ec7ac12151cfe4412e4_8", "text": "This reranker is based on n-gram precision scores and promotes responses having phrase overlaps with user utterances (Dušek and Jurcicek, 2016a )."}
{"sent_id": "1fd85a350d9ec7ac12151cfe4412e4-C001-35", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_1fd85a350d9ec7ac12151cfe4412e4_8", "text": "We evaluate our model on the Alex Context natural language generation (NLG) dataset of Dušek and Jurcicek (2016a) and demonstrate that our model outperforms the RNNbased model of Dušek and Jurcicek (2016a) (TGen model) in automatic metrics."}
{"sent_id": "1fd85a350d9ec7ac12151cfe4412e4-C001-64", "intents": ["@USE@"], "paper_id": "ABC_1fd85a350d9ec7ac12151cfe4412e4_8", "text": "Model proposed by Dušek and Jurcicek (2016a) serves as a baseline sequence to sequence generation model (TGen model) for SDS which takes into account the context."}
{"sent_id": "1fd85a350d9ec7ac12151cfe4412e4-C001-81", "intents": ["@USE@"], "paper_id": "ABC_1fd85a350d9ec7ac12151cfe4412e4_8", "text": "We implement the N-gram match reranker as given by Dušek and Jurcicek (2016a) ."}
{"sent_id": "1fd85a350d9ec7ac12151cfe4412e4-C001-134", "intents": ["@USE@"], "paper_id": "ABC_1fd85a350d9ec7ac12151cfe4412e4_8", "text": "For the dataset which we have used (Dušek and Jurcicek, 2016a) , there are 19 such classes of DA types and slot-value combinations."}
{"sent_id": "1fd85a350d9ec7ac12151cfe4412e4-C001-138", "intents": ["@USE@"], "paper_id": "ABC_1fd85a350d9ec7ac12151cfe4412e4_8", "text": "The weighted reranking penalties of all the n-best responses are subtracted from their log-probabilities similar to Dušek and Jurcicek (2016a) ."}
{"sent_id": "1fd85a350d9ec7ac12151cfe4412e4-C001-156", "intents": ["@USE@"], "paper_id": "ABC_1fd85a350d9ec7ac12151cfe4412e4_8", "text": "The studies in this work are performed on Alex Context natural language generation (NLG) dataset (Dušek and Jurcicek, 2016a )."}
{"sent_id": "1fd85a350d9ec7ac12151cfe4412e4-C001-161", "intents": ["@USE@"], "paper_id": "ABC_1fd85a350d9ec7ac12151cfe4412e4_8", "text": "Data is delexicalized and split into training, validation and test sets as done by Dušek and Jurcicek (2016a) ."}
{"sent_id": "1fd85a350d9ec7ac12151cfe4412e4-C001-180", "intents": ["@USE@"], "paper_id": "ABC_1fd85a350d9ec7ac12151cfe4412e4_8", "text": "The performance of the proposed ConvSeq2Seq model for NLG is compared with that of TGen model (Dušek and Jurcicek, 2016a) ."}
{"sent_id": "1fd85a350d9ec7ac12151cfe4412e4-C001-185", "intents": ["@USE@"], "paper_id": "ABC_1fd85a350d9ec7ac12151cfe4412e4_8", "text": "Our model has also been evaluated using the metric script \"mtevalv11b.pl\" (version 11b) to compare our results with those stated in (Dušek and Jurcicek, 2016a) ."}
{"sent_id": "1fd85a350d9ec7ac12151cfe4412e4-C001-211", "intents": ["@SIM@"], "paper_id": "ABC_1fd85a350d9ec7ac12151cfe4412e4_8", "text": "BLEU and NIST scores of the TGen model given in Table 2 match with that represented in (Dušek and Jurcicek, 2016a) ."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "One way to remedy this fundamental problem is to refine model output iteratively (Lee et al., 2018; Ghazvininejad et al., 2019) ."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-17", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "Unlike the masked language models (Devlin et al., 2019; Ghazvininejad et al., 2019) where the model only predicts the masked words, the DisCo transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-19", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "This decoding algorithm allows for predicting all tokens with different context in each iteration and terminates when the output prediction converges, contrasting with the constant number of iterations (Ghazvininejad et al., 2019) ."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "Similar to masked language models for contextual word representations (Devlin et al., 2019; Liu et al., 2019 ), a con- ditional masked language model (CMLM, Ghazvininejad et al. (2019) ) predicts randomly masked target tokens Y mask given a source text X and the rest of the target tokens Y obs ."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "2 CMLMs have proven successful in parallel decoding for machine translation (Ghazvininejad et al., 2019) , video captioning (Yang et al., 2019a) , and speech recognition (Nakayama et al., 2019) ."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-84", "intents": ["@BACK@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "Mask-predict is an iterative inference algorithm introduced in Ghazvininejad et al. (2019) to decode a conditional masked language model (CMLM)."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-39", "intents": ["@DIF@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "We introduce the 2 BERT (Devlin et al., 2019 ) masks a token with probability 0.15 while CMLMs (Ghazvininejad et al., 2019) sample the number of masked tokens uniformly from [1, N ]."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-61", "intents": ["@SIM@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "For each Y n in Y where |Y | = N , we uniformly sample the number of visible tokens from [0, N − 1], and then we randomly choose that number of tokens from Y \\ Y n as Y n obs , similarly to CMLMs (Ghazvininejad et al., 2019) ."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-226", "intents": ["@SIM@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "As in this work, several prior work proposed methods to iteratively refine output predictions (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019; Mansimov et al., 2019) ."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-101", "intents": ["@USE@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "Following Ghazvininejad et al. (2019) , we apply length beam."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-108", "intents": ["@USE@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "Notice that all for-loops are parallelizable except the one over iterations t. In the subsequent experiments, we use length beam size of 5 (Ghazvininejad et al., 2019) unless otherwise noted."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-123", "intents": ["@USE@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "We evaluate performance with BLEU scores (Papineni et al., 2002) for all directions except that we use SacreBLEU (Post, 2018) 5 in en→zh again for fair comparison with prior work (Ghazvininejad et al., 2019) ."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-136", "intents": ["@USE@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "CMLM As discussed earlier, we can generate a translation with mask-predict from a CMLM (Ghazvininejad et al., 2019) ."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-147", "intents": ["@USE@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "Hyperparameters We generally follow the hyperparameters for a transformer base (Vaswani et al., 2017; Ghazvininejad et al., 2019) : 6 layers for both the encoder and decoder, 8 attention heads, 512 model dimensions, and 2048 hidden dimensions."}
{"sent_id": "6d5a52c29e4f91bc17502e250c9187-C001-162", "intents": ["@USE@"], "paper_id": "ABC_6d5a52c29e4f91bc17502e250c9187_8", "text": "First, our re-implementations of CMLM + Mask-Predict outperform Ghazvininejad et al. (2019) (e.g. 31.24 vs. 30.53 in de→en with 10 steps)."}
{"sent_id": "b87a8d14f1c2016caa7538aa08a33f-C001-14", "intents": ["@USE@"], "paper_id": "ABC_b87a8d14f1c2016caa7538aa08a33f_8", "text": "One important avenue in this work is to understand the structure in argumentative text (Persing & Ng, 2016; Peldszus & Stede, 2015; Stab & Gurevych, 2016; Nguyen & Litman, 2016) ."}
{"sent_id": "b87a8d14f1c2016caa7538aa08a33f-C001-22", "intents": ["@USE@"], "paper_id": "ABC_b87a8d14f1c2016caa7538aa08a33f_8", "text": "Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau & Moens, 2009; Cohen, 1987; Peldszus & Stede, 2015; Stab & Gurevych, 2016) Figure 1: An example of argument structure with four ACs."}
{"sent_id": "b87a8d14f1c2016caa7538aa08a33f-C001-32", "intents": ["@USE@"], "paper_id": "ABC_b87a8d14f1c2016caa7538aa08a33f_8", "text": "First, the left side of the figure presents the raw text of a paragraph in a persuasive essay (Stab & Gurevych, 2016) , with the ACs contained in square brackets."}
{"sent_id": "b87a8d14f1c2016caa7538aa08a33f-C001-38", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_b87a8d14f1c2016caa7538aa08a33f_8", "text": "Linking to the first argument component can provide a competitive baseline heuristic (Peldszus & Stede, 2015; Stab & Gurevych, 2016) ."}
{"sent_id": "b87a8d14f1c2016caa7538aa08a33f-C001-51", "intents": ["@USE@"], "paper_id": "ABC_b87a8d14f1c2016caa7538aa08a33f_8", "text": "We evaluate our models on the corpora of Stab & Gurevych (2016) and Peldszus (2014) , and compare our results with the results of the aformentioned authors."}
{"sent_id": "b87a8d14f1c2016caa7538aa08a33f-C001-116", "intents": ["@USE@"], "paper_id": "ABC_b87a8d14f1c2016caa7538aa08a33f_8", "text": "We follow the work of Stab & Gurevych (2016) and focus on three different types of features to represent our ACs: 1) Bag-of-Words of the AC; 2) Embedding representation based on GloVe embeddings (Pennington et al., 2014) ; 3) Structural features: Whether or not the AC is the first AC in a paragraph, and Whether the AC is in an opening, body, or closing paragraph."}
{"sent_id": "b87a8d14f1c2016caa7538aa08a33f-C001-145", "intents": ["@USE@"], "paper_id": "ABC_b87a8d14f1c2016caa7538aa08a33f_8", "text": "We test the effectiveness of our proposed model on a dataset of persuasive essays (Stab & Gurevych, 2016) , as well as a dataset of microtexts (Peldszus, 2014) ."}
{"sent_id": "b87a8d14f1c2016caa7538aa08a33f-C001-176", "intents": ["@USE@", "@BACK@", "@MOT@"], "paper_id": "ABC_b87a8d14f1c2016caa7538aa08a33f_8", "text": "Conversely, the ILP Joint Model (Stab & Gurevych, 2016) provides constrains by sharing prediction information between the base classifier."}
{"sent_id": "b87a8d14f1c2016caa7538aa08a33f-C001-215", "intents": ["@USE@"], "paper_id": "ABC_b87a8d14f1c2016caa7538aa08a33f_8", "text": "We evaluate our models on two corpora: a corpus of persuasive essays (Stab & Gurevych, 2016) , and a corpus of microtexts (Peldszus, 2014) ."}
{"sent_id": "b87a8d14f1c2016caa7538aa08a33f-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_b87a8d14f1c2016caa7538aa08a33f_8", "text": "Various authors have also proposed to jointly model link extraction with other subtasks from the argumentation mining pipeline, using either an Integer Linear Programming (ILP) framework (Persing & Ng, 2016; Stab & Gurevych, 2016) or directly feeding previous subtask predictions into another model."}
{"sent_id": "b87a8d14f1c2016caa7538aa08a33f-C001-203", "intents": ["@MOT@"], "paper_id": "ABC_b87a8d14f1c2016caa7538aa08a33f_8", "text": "The popular method of averaging embeddings (which is used by Stab & Gurevych (2016) in their system) is in fact the worst method, although its performance is still competitive with the previous state-of-the-art."}
{"sent_id": "dc6d4eb1870ed5b0bbcbbf6686e5be-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_dc6d4eb1870ed5b0bbcbbf6686e5be_8", "text": "A crucial component is temporal relation (TempRel; e.g., before or after) extraction (Mani et al., 2006; Bethard et al., 2007; Do et al., 2012; Mirza and Tonelli, 2016; Ning et al., 2017 Ning et al., , 2018a ."}
{"sent_id": "dc6d4eb1870ed5b0bbcbbf6686e5be-C001-40", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_dc6d4eb1870ed5b0bbcbbf6686e5be_8", "text": "It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (Chambers, 2013; Ning et al., 2017) ."}
{"sent_id": "dc6d4eb1870ed5b0bbcbbf6686e5be-C001-48", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_dc6d4eb1870ed5b0bbcbbf6686e5be_8", "text": "Two recent TempRel extraction systems (Mirza and Tonelli, 2016; Ning et al., 2017 ) also reported their performances on TB-Dense (F) and on TempEval-3 (P) separately."}
{"sent_id": "dc6d4eb1870ed5b0bbcbbf6686e5be-C001-76", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_dc6d4eb1870ed5b0bbcbbf6686e5be_8", "text": "where {r m 3 } is selected based on the general transitivity proposed in (Ning et al., 2017) ."}
{"sent_id": "dc6d4eb1870ed5b0bbcbbf6686e5be-C001-105", "intents": ["@BACK@"], "paper_id": "ABC_dc6d4eb1870ed5b0bbcbbf6686e5be_8", "text": "While incorporating transitivity constraints in inference is widely used, Ning et al. (2017) proposed to incorporate these constraints in the learning phase as well."}
{"sent_id": "dc6d4eb1870ed5b0bbcbbf6686e5be-C001-79", "intents": ["@USE@"], "paper_id": "ABC_dc6d4eb1870ed5b0bbcbbf6686e5be_8", "text": "Note that Algorithm 1 is only for the learning step of TempRel extraction; as for the inference step of this task, we consistently adopt the standard method by solving Eq. (1), as was done by (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012; Ning et al., 2017) ."}
{"sent_id": "dc6d4eb1870ed5b0bbcbbf6686e5be-C001-106", "intents": ["@USE@"], "paper_id": "ABC_dc6d4eb1870ed5b0bbcbbf6686e5be_8", "text": "One of the algorithms proposed in Ning et al. (2017) is based on Chang et al. (2012) 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of Ning et al. (2017) ."}
{"sent_id": "dc6d4eb1870ed5b0bbcbbf6686e5be-C001-128", "intents": ["@USE@"], "paper_id": "ABC_dc6d4eb1870ed5b0bbcbbf6686e5be_8", "text": "System 7 can also be considered as a reproduction of Ning et al. (2017) (see the discussion in Sec. 5 for details)."}
{"sent_id": "dc6d4eb1870ed5b0bbcbbf6686e5be-C001-101", "intents": ["@DIF@"], "paper_id": "ABC_dc6d4eb1870ed5b0bbcbbf6686e5be_8", "text": "While System 7 can be regarded as a reproduction of Ning et al. (2017) , the original paper of Ning et al. (2017) achieved an overall score of P=43.0, R=46.4, F=44.7 and an awareness score of P=42.6, R=44.0, and F=43.3, and the proposed System 9 is also better than Ning et al. (2017) on all metrics."}
{"sent_id": "dc6d4eb1870ed5b0bbcbbf6686e5be-C001-107", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_dc6d4eb1870ed5b0bbcbbf6686e5be_8", "text": "Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: Ning et al. (2017) tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework."}
{"sent_id": "304773c64de1f0906f0246f2aa0d29-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_304773c64de1f0906f0246f2aa0d29_8", "text": "To extract opinion targets, pervious approaches usually relied on opinion words which are the words used to express the opinions (Hu and Liu, 2004a; Popescu and Etzioni, 2005; Liu et al., 2005; Wang and Wang, 2008; Qiu et al., 2011; Liu et al., 2012) ."}
{"sent_id": "304773c64de1f0906f0246f2aa0d29-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_304773c64de1f0906f0246f2aa0d29_8", "text": "To resolve these problems, Liu et al. (2012) formulated identifying opinion relations between words as an monolingual alignment process."}
{"sent_id": "304773c64de1f0906f0246f2aa0d29-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_304773c64de1f0906f0246f2aa0d29_8", "text": "(Liu et al., 2012) formulated identifying opinion relations between words as an alignment process."}
{"sent_id": "304773c64de1f0906f0246f2aa0d29-C001-33", "intents": ["@MOT@"], "paper_id": "ABC_304773c64de1f0906f0246f2aa0d29_8", "text": "Although (Liu et al., 2012) had proved the effectiveness of WAM, they mainly performed experiments on the dataset with medium size."}
{"sent_id": "304773c64de1f0906f0246f2aa0d29-C001-85", "intents": ["@MOT@"], "paper_id": "ABC_304773c64de1f0906f0246f2aa0d29_8", "text": "We notice these two methods ( (Liu et al., 2012) and (Liu et al., 2013) ) only performed experiments on the corpora with a medium size."}
{"sent_id": "304773c64de1f0906f0246f2aa0d29-C001-90", "intents": ["@USE@"], "paper_id": "ABC_304773c64de1f0906f0246f2aa0d29_8", "text": "To extract opinion targets from reviews, we adopt the framework proposed by (Liu et al., 2012) , which is a graph-based extraction framework and has two main components as follows."}
{"sent_id": "304773c64de1f0906f0246f2aa0d29-C001-122", "intents": ["@USE@"], "paper_id": "ABC_304773c64de1f0906f0246f2aa0d29_8", "text": "Similar to (Liu et al., 2012) , every sentence in reviews is replicated to generate a parallel sentence pair, and the word alignment algorithm is applied to the monolingual scenario to align a noun/noun phase with its modifiers."}
{"sent_id": "304773c64de1f0906f0246f2aa0d29-C001-167", "intents": ["@USE@"], "paper_id": "ABC_304773c64de1f0906f0246f2aa0d29_8", "text": "Then, similar to (Liu et al., 2012) , the association between an opinion target candidate and its modifier is estimated as follows."}
{"sent_id": "304773c64de1f0906f0246f2aa0d29-C001-172", "intents": ["@USE@"], "paper_id": "ABC_304773c64de1f0906f0246f2aa0d29_8", "text": "In the second component, we adopt a graph-based algorithm used in (Liu et al., 2012) to compute the confidence of each opinion target candidate, and the candidates with higher confidence than the threshold will be extracted as the opinion targets."}
{"sent_id": "304773c64de1f0906f0246f2aa0d29-C001-184", "intents": ["@USE@"], "paper_id": "ABC_304773c64de1f0906f0246f2aa0d29_8", "text": "Similar to (Liu et al., 2012) , we set each item in"}
{"sent_id": "304773c64de1f0906f0246f2aa0d29-C001-248", "intents": ["@USE@"], "paper_id": "ABC_304773c64de1f0906f0246f2aa0d29_8", "text": "To further prove the effectiveness of our combination, we compare PSWAM with some state-of-the-art methods, including Hu (Hu and Liu, 2004a) , which extracted frequent opinion target words based on association mining rules, DP (Qiu et al., 2011) , which extracted opinion targets through syntactic patterns, and LIU (Liu et al., 2012) , which fulfilled this task by using unsupervised WAM."}
{"sent_id": "304773c64de1f0906f0246f2aa0d29-C001-92", "intents": ["@SIM@"], "paper_id": "ABC_304773c64de1f0906f0246f2aa0d29_8", "text": "In this paper, we assume opinion targets to be nouns or noun phrases, and opinion words may be adjectives or verbs, which are usually adopted by (Hu and Liu, 2004a; Qiu et al., 2011; Wang and Wang, 2008; Liu et al., 2012) ."}
{"sent_id": "304773c64de1f0906f0246f2aa0d29-C001-194", "intents": ["@SIM@"], "paper_id": "ABC_304773c64de1f0906f0246f2aa0d29_8", "text": "This collection was also used in (Liu et al., 2012) ."}
{"sent_id": "9426b2faf2ba633033c7dfcee4118b-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9426b2faf2ba633033c7dfcee4118b_8", "text": "While relatively new, the seq2seq approach has achieved state-of-the-art results in not only its original application -machine translation - (Luong et al., 2015b; Jean et al., 2015a; Luong et al., 2015a; Jean et al., 2015b; Luong & Manning, 2015) , but also image caption generation , and constituency parsing (Vinyals et al., 2015a) ."}
{"sent_id": "9426b2faf2ba633033c7dfcee4118b-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_9426b2faf2ba633033c7dfcee4118b_8", "text": "Recent work such as Jean et al., 2015a; Luong et al., 2015a; Vinyals et al., 2015a) has found that it is crucial to empower seq2seq models with the attention mechanism."}
{"sent_id": "9426b2faf2ba633033c7dfcee4118b-C001-132", "intents": ["@BACK@"], "paper_id": "ABC_9426b2faf2ba633033c7dfcee4118b_8", "text": "Since the parsing task maps from a sequence of English words to a sequence of parsing tags (Vinyals et al., 2015a) , only the encoder can be shared with an English→German translation task."}
{"sent_id": "9426b2faf2ba633033c7dfcee4118b-C001-19", "intents": ["@DIF@"], "paper_id": "ABC_9426b2faf2ba633033c7dfcee4118b_8", "text": "We show that syntactic parsing and image caption generation improves the translation quality between English (Sutskever et al., 2014) and (right) constituent parsing (Vinyals et al., 2015a) ."}
{"sent_id": "9426b2faf2ba633033c7dfcee4118b-C001-173", "intents": ["@DIF@"], "paper_id": "ABC_9426b2faf2ba633033c7dfcee4118b_8", "text": "A closer look into these models reveal that there seems to be an architectural difference: Vinyals et al. (2015a) use 3-layer LSTM with 256 cells and 512-dimensional embeddings; whereas our models use 4-layer LSTM with 1000 cells and 1000-dimensional embeddings."}
{"sent_id": "9426b2faf2ba633033c7dfcee4118b-C001-109", "intents": ["@USE@"], "paper_id": "ABC_9426b2faf2ba633033c7dfcee4118b_8", "text": "1. a small corpus -the widely used Penn Tree Bank (PTB) dataset (Marcus et al., 1993) and, 2. a large corpus -the high-confidence (HC) parse trees provided by Vinyals et al. (2015a) ."}
{"sent_id": "9426b2faf2ba633033c7dfcee4118b-C001-111", "intents": ["@USE@"], "paper_id": "ABC_9426b2faf2ba633033c7dfcee4118b_8", "text": "Note also that the parse trees have been linearized following Vinyals et al. (2015a) ."}
{"sent_id": "9426b2faf2ba633033c7dfcee4118b-C001-158", "intents": ["@USE@"], "paper_id": "ABC_9426b2faf2ba633033c7dfcee4118b_8", "text": "Instead of using the small Penn Tree Bank corpus, we consider a large parsing resource, the high-confidence (HC) corpus, which is provided by Vinyals et al. (2015a) ."}
{"sent_id": "9426b2faf2ba633033c7dfcee4118b-C001-168", "intents": ["@USE@"], "paper_id": "ABC_9426b2faf2ba633033c7dfcee4118b_8", "text": "Our models are compared against the best attention-based systems in (Vinyals et al., 2015a) , including the state-of-the-art result of 92.8 F 1 ."}
{"sent_id": "9426b2faf2ba633033c7dfcee4118b-C001-138", "intents": ["@SIM@"], "paper_id": "ABC_9426b2faf2ba633033c7dfcee4118b_8", "text": "For parsing, as Vinyals et al. (2015a) have shown that attention is crucial to achieve good parsing performance when training on the small PTB corpus, we do not set a high bar for our attention-free systems in this setup (better performances are reported in Section 4.3.3)."}
{"sent_id": "9426b2faf2ba633033c7dfcee4118b-C001-171", "intents": ["@SIM@"], "paper_id": "ABC_9426b2faf2ba633033c7dfcee4118b_8", "text": "7 Second, our baseline system can obtain a very competitive F 1 score of 92.2, rivaling Vinyals et al. (2015a) 's systems."}
{"sent_id": "9426b2faf2ba633033c7dfcee4118b-C001-176", "intents": ["@SIM@"], "paper_id": "ABC_9426b2faf2ba633033c7dfcee4118b_8", "text": "At the mixing ratio of 0.05, we obtain a non-negligible boost of 0.2 F 1 over the baseline and with 92.4 F 1 , our multi-task system is on par with the best single system reported in (Vinyals et al., 2015a) ."}
{"sent_id": "74568758fe5fef3727d94e7597f305-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_74568758fe5fef3727d94e7597f305_8", "text": "The first of these, lexical cohesion, may be used for either linear segmentation (Morris and Hirst, 1991; Hearst, 1997) or hierarchical segmentation (Yarri, 1997; Choi, 2000) ."}
{"sent_id": "74568758fe5fef3727d94e7597f305-C001-47", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_74568758fe5fef3727d94e7597f305_8", "text": "Hearst (1994 Hearst ( , 1997 in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores."}
{"sent_id": "74568758fe5fef3727d94e7597f305-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_74568758fe5fef3727d94e7597f305_8", "text": "Using a vector space method without singular value decomposition, Hearst (1997) reports an F-measure of .70 when detecting topic shifts between paragraphs."}
{"sent_id": "74568758fe5fef3727d94e7597f305-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_74568758fe5fef3727d94e7597f305_8", "text": "Thus previous work suggests that the Hearst (1997) method is superior to that of Foltz et al. (1998) , having roughly twice the accuracy indicated by F-measure."}
{"sent_id": "74568758fe5fef3727d94e7597f305-C001-192", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_74568758fe5fef3727d94e7597f305_8", "text": "Recall that in monologue, Hearst (1997) reports a much larger F-measure than Foltz et al. (1998) , .70 vs. .33, albeit on different data sets."}
{"sent_id": "74568758fe5fef3727d94e7597f305-C001-101", "intents": ["@USE@"], "paper_id": "ABC_74568758fe5fef3727d94e7597f305_8", "text": "Hearst (1994 Hearst ( , 1997 was replicated using the JTextTile (Choi, 1999 ) Java software."}
{"sent_id": "74568758fe5fef3727d94e7597f305-C001-143", "intents": ["@DIF@"], "paper_id": "ABC_74568758fe5fef3727d94e7597f305_8", "text": "On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by Hearst (1997) , .70."}
{"sent_id": "e3dd013c944cf8dcb6ff90124a0e01-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_e3dd013c944cf8dcb6ff90124a0e01_8", "text": "More recently, a number of novel insertionbased architectures have been developed for sequence generation (Gu et al., 2019; Stern et al., 2019; Welleck et al., 2019) ."}
{"sent_id": "e3dd013c944cf8dcb6ff90124a0e01-C001-21", "intents": ["@SIM@"], "paper_id": "ABC_e3dd013c944cf8dcb6ff90124a0e01_8", "text": "Some of them also match the quality of state-of-the-art left-to-right models (Stern et al., 2019) ."}
{"sent_id": "e3dd013c944cf8dcb6ff90124a0e01-C001-35", "intents": ["@USE@"], "paper_id": "ABC_e3dd013c944cf8dcb6ff90124a0e01_8", "text": "We use one such insertion-based model, the Insertion Transformer (Stern et al., 2019) , for our empirical study."}
{"sent_id": "e3dd013c944cf8dcb6ff90124a0e01-C001-49", "intents": ["@USE@"], "paper_id": "ABC_e3dd013c944cf8dcb6ff90124a0e01_8", "text": "We note that Stern et al. (2019) also experimented with a number of other architectural variants, but we use the baseline version of the model described above in our experiments for simplicity."}
{"sent_id": "e3dd013c944cf8dcb6ff90124a0e01-C001-69", "intents": ["@USE@"], "paper_id": "ABC_e3dd013c944cf8dcb6ff90124a0e01_8", "text": "Having defined the target distribution, we take the slot loss L for insertions within a particular slot to be the KL-divergence between the oracle distribution q oracle and the model distribution p. Substituting L in for the slot loss within the training framework of Stern et al. (2019) then gives the full sequence generation loss, which we can use to train an Insertion Transformer under any oracle policy rather than just the specific one they propose."}
{"sent_id": "e3dd013c944cf8dcb6ff90124a0e01-C001-117", "intents": ["@USE@"], "paper_id": "ABC_e3dd013c944cf8dcb6ff90124a0e01_8", "text": "We follow Stern et al. (2019) and use a uniform roll-in policy when sampling partial outputs at training time in which we first select a subset size uniformly at random, then select a random subset of the output of that size."}
{"sent_id": "e3dd013c944cf8dcb6ff90124a0e01-C001-154", "intents": ["@USE@"], "paper_id": "ABC_e3dd013c944cf8dcb6ff90124a0e01_8", "text": "We perform a sweep over temperatures τ ∈ {0.5, 1, 2} and EOS penalties ∈ {0, 0.5, 1, 1.5, . . . , 8} (Stern et al., 2019)"}
{"sent_id": "e3dd013c944cf8dcb6ff90124a0e01-C001-169", "intents": ["@USE@"], "paper_id": "ABC_e3dd013c944cf8dcb6ff90124a0e01_8", "text": "The uniform loss proposed by Stern et al. (2019) serves as a strong baseline for both language pairs, coming within 0.6 points of the original Transformer for En-De at 26.72 BLEU, and attaining a respectable score of 33.1 BLEU on En-Zh."}
{"sent_id": "e3dd013c944cf8dcb6ff90124a0e01-C001-176", "intents": ["@DIF@"], "paper_id": "ABC_e3dd013c944cf8dcb6ff90124a0e01_8", "text": "On the other hand, we note that while the soft left-to-right and right-to-left losses perform substantially better than the hard loss employed in the original work by Stern et al. (2019) , performance does suffer when using parallel decoding for those models, which is generally untrue of the other orderings."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "Hence distant supervision is often coupled with learning methods that allow for this sort of noise, e.g., by introducing latent variables for each entity mention (Hoffmann et al., 2011; Riedel et al., 2010; Surdeanu et al., 2012) ; by carefully selecting the entity mentions from contexts likely to include specific KB facts (Wu and Weld, 2010) ; or by careful filtering of the KB strings used as seeds (Movshovitz-Attias and Cohen, 2012) ."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "Another recently-introduced approach to reducing the noise in distant supervision is to combine distant labeling with label propagation (LP) (Bing et al., 2015; Bing et al., 2016) ."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "Depending on the LP method used, agreement with seed labels can be imposed as a hard constraint (Zhu et al., 2003) or a soft constraint (Lin and Cohen, 2010; Talukdar and Cohen, 2014) ."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "An extension of this approach (Bing et al., 2016) learned to classify NP pairs as relations, using a more complex graph structure."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "In (Bing et al., 2016) , relation extraction was performed on an \"entity centric\" corpus, where each document is primarily concerned with a particular \"title entity\", and the first argument of each relation is always the title entity: hence relation extraction can be viewed as classification, where an entity mention is labeled with its slot filling role, i.e., its relation to the title entity."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-197", "intents": ["@BACK@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "MultiR (Hoffmann et al., 2011) and MIML-RE (Surdeanu et al., 2012) extend this approach to support multi- ple relations expressed by different sentences in a bag."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-202", "intents": ["@BACK@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "Document structure was previously explored by (Bing et al., 2016) , which used the structure to enrich an LP graph by adding coupling edges between mentions in the same section of particular documents."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-207", "intents": ["@BACK@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "In the classic bootstrap learning scheme (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Bunescu and Mooney, 2007) , a small number of seed instances are used to extract new patterns from a large corpus, which are then used to extract more instances."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-103", "intents": ["@USE@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "We use an existing multi-class label propagation method, namely, MultiRankWalk (MRW) (Lin and Cohen, 2010) , which is a graph-based SSL method related to personalized PageRank (PPR) (Haveliwala et al., 2003) (aka random walk with restart (Tong et al., 2006) )."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-114", "intents": ["@USE@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "We use SVMs (Chang and Lin, 2001) and discard singleton features, as well as the most frequent 5% of features (as a stop-wording variant)."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-125", "intents": ["@USE@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "This data was originally generated in (Bing et al., 2016) ."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-137", "intents": ["@USE@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "The first is MultiR (Hoffmann et al., 2011) which models each relation mention separately and aggregates their labels using a deterministic OR."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-143", "intents": ["@USE@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "We also compare with DIEBOLDS (Bing et al., 2016) , which uses LP on a graph containing entity mention pairs."}
{"sent_id": "59b6eaca400342159b867d018d4042-C001-164", "intents": ["@USE@"], "paper_id": "ABC_59b6eaca400342159b867d018d4042_8", "text": "The results for DIEBOLDS are from (Bing et al., 2016) ."}
{"sent_id": "a0bd41c3653073dd79e19d3ddc8d14-C001-4", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_a0bd41c3653073dd79e19d3ddc8d14_8", "text": "We present a deep learning system that advances the state of the art for the MIMIC-III dataset, achieving a new best micro F1-measure of 55.85%, significantly outperforming the previous best result (Mullenbach et al., 2018) ."}
{"sent_id": "a0bd41c3653073dd79e19d3ddc8d14-C001-45", "intents": ["@DIF@", "@EXT@", "@SIM@", "@USE@"], "paper_id": "ABC_a0bd41c3653073dd79e19d3ddc8d14_8", "text": "Our approach is most similar to the current state-of-the-art model by Mullenbach et al. (2018) ."}
{"sent_id": "a0bd41c3653073dd79e19d3ddc8d14-C001-185", "intents": ["@DIF@"], "paper_id": "ABC_a0bd41c3653073dd79e19d3ddc8d14_8", "text": "This is due to the architecture differences, such as the use of multi-view CNN and 6 The authors of CAML (Mullenbach et al., 2018) reported micro F1-Proc=60.9%, micro F1-Diag=52.4%, micro F1=53.9%, P@8=70.9% and P@15=56.1% on the Dis set."}
{"sent_id": "a0bd41c3653073dd79e19d3ddc8d14-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_a0bd41c3653073dd79e19d3ddc8d14_8", "text": "Mullenbach et al. (2018) presented a model capable of predicting full codes for both ICD-9 diagnoses and procedures composed of shared embedding and CNN layers between all codes and an individual attention layer for each code."}
{"sent_id": "a0bd41c3653073dd79e19d3ddc8d14-C001-148", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_a0bd41c3653073dd79e19d3ddc8d14_8", "text": "CAML (Mullenbach et al., 2018) has achieved the best state-of-the-art results on MIMIC-III."}
{"sent_id": "a0bd41c3653073dd79e19d3ddc8d14-C001-154", "intents": ["@BACK@"], "paper_id": "ABC_a0bd41c3653073dd79e19d3ddc8d14_8", "text": "The most widely used metric for evaluating ICD code prediction is micro F1-score (Perotte et al., 2013; Wang et al., 2018; Mullenbach et al., 2018; Kavuluru et al., 2015) ."}
{"sent_id": "a0bd41c3653073dd79e19d3ddc8d14-C001-155", "intents": ["@BACK@"], "paper_id": "ABC_a0bd41c3653073dd79e19d3ddc8d14_8", "text": "New studies have reported results on macro F1-score, precision@n, and AUC of ROC as well (Wang et al., 2018; Mullenbach et al., 2018) ."}
{"sent_id": "a0bd41c3653073dd79e19d3ddc8d14-C001-58", "intents": ["@SIM@"], "paper_id": "ABC_a0bd41c3653073dd79e19d3ddc8d14_8", "text": "Also, some of the hospital stays do not have discharge summaries; following previous studies for automated coding, we only consider those that do (Perotte et al., 2013; Baumel et al., 2018; Mullenbach et al., 2018; Wang et al., 2018) ."}
{"sent_id": "a0bd41c3653073dd79e19d3ddc8d14-C001-59", "intents": ["@SIM@"], "paper_id": "ABC_a0bd41c3653073dd79e19d3ddc8d14_8", "text": "We have three sets for our experiments: one including only the discharge summaries, which allows us to compare our results with previous studies on this corpus (Mullenbach et al., 2018; Perotte et al., 2013) , hereon the Dis set; one on the concatenation of all patient notes, hereon, Full set; and one on another set which includes only discharge summary samples with the 50 most frequent codes, hereon, Dis-50 set, for comparison to previous studies (Mullenbach et al., 2018; Wang et al., 2018) ."}
{"sent_id": "a0bd41c3653073dd79e19d3ddc8d14-C001-182", "intents": ["@SIM@"], "paper_id": "ABC_a0bd41c3653073dd79e19d3ddc8d14_8", "text": "We evaluate the baseline models on the Dis set and Dis-50 sets and provide micro F1 scores for diagnosis and procedure codes for comparability with previous studies (Mullenbach et al., 2018; Perotte et al., 2013) ."}
{"sent_id": "a0bd41c3653073dd79e19d3ddc8d14-C001-235", "intents": ["@SIM@"], "paper_id": "ABC_a0bd41c3653073dd79e19d3ddc8d14_8", "text": "For instance, we trained the model on all ground-truth codes equally, similarly to previous approaches (Baumel et al., 2018; Wang et al., 2018; Mullenbach et al., 2018; Perotte et al., 2013) ."}
{"sent_id": "a0bd41c3653073dd79e19d3ddc8d14-C001-61", "intents": ["@USE@"], "paper_id": "ABC_a0bd41c3653073dd79e19d3ddc8d14_8", "text": "3 We follow the train, test, and development splits publicly shared by the recent study on this dataset (Mullenbach et al., 2018) ."}
{"sent_id": "a0bd41c3653073dd79e19d3ddc8d14-C001-141", "intents": ["@USE@"], "paper_id": "ABC_a0bd41c3653073dd79e19d3ddc8d14_8", "text": "We compare our approach with four baselines: flat and hierarchical SVMs (Perotte et al., 2013), LEAM (Wang et al., 2018) , and CAML (Mullenbach et al., 2018) ."}
{"sent_id": "e5ef75cd497dd94b4cf818291707df-C001-32", "intents": ["@USE@"], "paper_id": "ABC_e5ef75cd497dd94b4cf818291707df_8", "text": "Second, although the feature set is fundamentally a combination of those used in previous works (Zhang and Clark, 2010; Huang and Sagae, 2010) , to integrate them in a single incremental framework is not straightforward."}
{"sent_id": "e5ef75cd497dd94b4cf818291707df-C001-49", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_e5ef75cd497dd94b4cf818291707df_8", "text": "Among the many recent works on joint segmentation and POS tagging for Chinese, the linear-time incremental models by Zhang and Clark (2008) and Zhang and Clark (2010) largely inspired our model."}
{"sent_id": "e5ef75cd497dd94b4cf818291707df-C001-60", "intents": ["@USE@"], "paper_id": "ABC_e5ef75cd497dd94b4cf818291707df_8", "text": "Particularly, we change the role of the shift action and additionally use the append action, inspired by the character-based actions used in the joint segmentation and POS tagging model by Zhang and Clark (2010) ."}
{"sent_id": "e5ef75cd497dd94b4cf818291707df-C001-66", "intents": ["@USE@"], "paper_id": "ABC_e5ef75cd497dd94b4cf818291707df_8", "text": "Following Zhang and Clark (2010) , the POS tag is assigned to the word when its first character is shifted, and the word-tag pairs observed in the training data and the closed-set tags (Xia, 2000) are used to prune unlikely derivations."}
{"sent_id": "e5ef75cd497dd94b4cf818291707df-C001-75", "intents": ["@USE@"], "paper_id": "ABC_e5ef75cd497dd94b4cf818291707df_8", "text": "We can first think of using the number of shifted characters as the step index, as Zhang and Clark (2010) does."}
{"sent_id": "e5ef75cd497dd94b4cf818291707df-C001-92", "intents": ["@USE@"], "paper_id": "ABC_e5ef75cd497dd94b4cf818291707df_8", "text": "The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010) , both of which are used as baseline models in our experiment."}
{"sent_id": "e5ef75cd497dd94b4cf818291707df-C001-94", "intents": ["@USE@"], "paper_id": "ABC_e5ef75cd497dd94b4cf818291707df_8", "text": "The list of the features used in our joint model is presented in Table 1 , where S01-S05, W01-W21, and T01-05 are taken from Zhang and Clark (2010) , and P01-P28 are taken from Huang and Sagae (2010) ."}
{"sent_id": "e5ef75cd497dd94b4cf818291707df-C001-111", "intents": ["@USE@"], "paper_id": "ABC_e5ef75cd497dd94b4cf818291707df_8", "text": "These features will also be used in our reimplementation of the model by Zhang and Clark (2010) ."}
{"sent_id": "e5ef75cd497dd94b4cf818291707df-C001-150", "intents": ["@USE@"], "paper_id": "ABC_e5ef75cd497dd94b4cf818291707df_8", "text": "• SegTag: our reimplementation of the joint segmentation and POS tagging model by Zhang and Clark (2010) ."}
{"sent_id": "e5ef75cd497dd94b4cf818291707df-C001-157", "intents": ["@USE@"], "paper_id": "ABC_e5ef75cd497dd94b4cf818291707df_8", "text": "tagging (Zhang and Clark, 2008; Zhang and Clark, 2010) and dependency parsing (Huang and Sagae, 2010) ."}
{"sent_id": "e5ef75cd497dd94b4cf818291707df-C001-189", "intents": ["@USE@"], "paper_id": "ABC_e5ef75cd497dd94b4cf818291707df_8", "text": "\"Zhang '10\" is the incremental model by Zhang and Clark (2010) ."}
{"sent_id": "e5ef75cd497dd94b4cf818291707df-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_e5ef75cd497dd94b4cf818291707df_8", "text": "More recently, Zhang and Clark (2010) proposed an efficient character-based decoder for their word-based model."}
{"sent_id": "e5ef75cd497dd94b4cf818291707df-C001-88", "intents": ["@DIF@"], "paper_id": "ABC_e5ef75cd497dd94b4cf818291707df_8", "text": "Theoretically, the computational time is greater than that with the character-based joint segmentation and tagging model by Zhang and Clark (2010) by a factor of"}
{"sent_id": "a0730efd9575800ba779516af1f440-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_a0730efd9575800ba779516af1f440_8", "text": "Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013) , or the prediction of the compositionality of MWE types (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014) ."}
{"sent_id": "a0730efd9575800ba779516af1f440-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_a0730efd9575800ba779516af1f440_8", "text": "The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003) , Bannard (2006) ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014) ."}
{"sent_id": "a0730efd9575800ba779516af1f440-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_a0730efd9575800ba779516af1f440_8", "text": "In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011; Salehi and Cook, 2013) , or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006) ."}
{"sent_id": "a0730efd9575800ba779516af1f440-C001-26", "intents": ["@SIM@"], "paper_id": "ABC_a0730efd9575800ba779516af1f440_8", "text": "The work that is perhaps most closely related to this paper is that of Salehi and Cook (2013) and Salehi et al. (2014) , who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score."}
{"sent_id": "a0730efd9575800ba779516af1f440-C001-77", "intents": ["@SIM@"], "paper_id": "ABC_a0730efd9575800ba779516af1f440_8", "text": "As mentioned above, we evaluate our method over the same two datasets as Salehi and Cook (2013) (which were later used, in addition to a third dataset of German noun compounds, in Salehi et al. (2014) ): (1) 90 binary English noun compounds (ENCs, e.g. spelling bee or swimming pool); and (2) 160 English verb particle constructions (EVPCs, e.g. stand up and give away)."}
{"sent_id": "a0730efd9575800ba779516af1f440-C001-78", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_a0730efd9575800ba779516af1f440_8", "text": "Our results are not directly comparable with those of Salehi and Cook (2013) and Salehi et al. (2014) , however, who evaluated in terms of a regression task, modelling the overall compositionality of the MWE."}
{"sent_id": "a0730efd9575800ba779516af1f440-C001-28", "intents": ["@USE@"], "paper_id": "ABC_a0730efd9575800ba779516af1f440_8", "text": "The basis of the similarity calculation is unsupervised, using either string similarity (Salehi and Cook, 2013) or distributional similarity (Salehi et al., 2014) ."}
{"sent_id": "a0730efd9575800ba779516af1f440-C001-30", "intents": ["@USE@"], "paper_id": "ABC_a0730efd9575800ba779516af1f440_8", "text": "To benchmark our method, we use two of the same datasets as these two papers, and repurpose the best-performing methods of Salehi and Cook (2013) and Salehi et al. (2014) for classification of the compositionality of each MWE component."}
{"sent_id": "a0730efd9575800ba779516af1f440-C001-60", "intents": ["@USE@"], "paper_id": "ABC_a0730efd9575800ba779516af1f440_8", "text": "Our approach is to take whatever translations happen to exist in Wiktionary for a given MWE, and where there are translations in that language for the component of interest, use the LCSbased method of Salehi and Cook (2013) to measure the string similarity between the translation of the MWE and the translation of the components."}
{"sent_id": "a0730efd9575800ba779516af1f440-C001-92", "intents": ["@USE@"], "paper_id": "ABC_a0730efd9575800ba779516af1f440_8", "text": "We also compare our method with: (1) \"LCS\", the string similarity-based method of Salehi and Cook (2013) , in which 54 languages are used; (2) \"DS\", the monolingual distributional similarity method of Salehi et al. (2014) ; (3) \"DS+DSL2\", the multilingual distributional similarity method of Salehi et al. (2014) , including supervised language selection for a given dataset, based on crossvalidation; and (4) \"LCS+DS+DSL2\", whereby the first three methods are combined using a supervised support vector regression model."}
{"sent_id": "a0730efd9575800ba779516af1f440-C001-99", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_a0730efd9575800ba779516af1f440_8", "text": "Overall, the simple unsupervised methods proposed in this paper are comparable with the unsupervised and supervised state-of-the-art methods of Salehi and Cook (2013) and Salehi et al. (2014) , with ITAG achieving the highest F-score for the ENC dataset and for the verb components of the EVPC dataset."}
{"sent_id": "a0730efd9575800ba779516af1f440-C001-101", "intents": ["@USE@"], "paper_id": "ABC_a0730efd9575800ba779516af1f440_8", "text": "When we combine each of our proposed methods with the string and distributional similarity methods of Salehi and Cook (2013) and Salehi et al. (2014) , we see substantial improvements over the comparable combined method of \"LCS+DS+DSL2\" in most cases, demonstrating both the robustness of the proposed methods and their complementarity with the earlier methods."}
{"sent_id": "a0730efd9575800ba779516af1f440-C001-61", "intents": ["@DIF@"], "paper_id": "ABC_a0730efd9575800ba779516af1f440_8", "text": "Unlike Salehi and Cook (2013) , however, we do not use development data to select the optimal set of languages in a supervised manner, and instead simply take the average of the string similarity scores across the available languages."}
{"sent_id": "155920441b8e81dff4e2b8e110383d-C001-34", "intents": ["@USE@"], "paper_id": "ABC_155920441b8e81dff4e2b8e110383d_8", "text": "Here, we also try to mimic the word2vec (Mikolov et al., 2013) embeddings (i.e. that are the expected outputs of the model) to learn the rare word representations with a complex morphology."}
{"sent_id": "155920441b8e81dff4e2b8e110383d-C001-82", "intents": ["@USE@"], "paper_id": "ABC_155920441b8e81dff4e2b8e110383d_8", "text": "For training, we use the pre-trained word2vec (Mikolov et al., 2013) vectors in order to minimize the cost between the learned and pre-trained vectors with the following objective function:"}
{"sent_id": "155920441b8e81dff4e2b8e110383d-C001-113", "intents": ["@USE@"], "paper_id": "ABC_155920441b8e81dff4e2b8e110383d_8", "text": "For the pre-trained word vectors, we used the word vectors of dimension 300 that were obtained by training word2vec (Mikolov et al., 2013) ."}
{"sent_id": "155920441b8e81dff4e2b8e110383d-C001-156", "intents": ["@USE@"], "paper_id": "ABC_155920441b8e81dff4e2b8e110383d_8", "text": "For English, we used the syntactic relations section provided in the Google analogy dataset (Mikolov et al., 2013) that involves 10675 questions."}
{"sent_id": "155920441b8e81dff4e2b8e110383d-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_155920441b8e81dff4e2b8e110383d_8", "text": "Classical word representation models such as word2vec (Mikolov et al., 2013) have been successful in learning word representations for frequent words."}
{"sent_id": "155920441b8e81dff4e2b8e110383d-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_155920441b8e81dff4e2b8e110383d_8", "text": "Bojanowski et al. (2017) introduce an extension to word2vec (Mikolov et al., 2013) by representing each word in terms of the vector representations of its n-grams, which was earlier applied by Schütze (1993) that learns the representations of fourgrams by applying singular value decomposition (SVD)."}
{"sent_id": "155920441b8e81dff4e2b8e110383d-C001-141", "intents": ["@DIF@"], "paper_id": "ABC_155920441b8e81dff4e2b8e110383d_8", "text": "However, our model performs better than both fasttext (Bojanowski et al., 2017) and word2vec (Mikolov et al., 2013) on Turkish despite the highly agglutinative morphological structure of the language."}
{"sent_id": "155920441b8e81dff4e2b8e110383d-C001-161", "intents": ["@DIF@"], "paper_id": "ABC_155920441b8e81dff4e2b8e110383d_8", "text": "The results show that our model outperforms both word2vec (Mikolov et al., 2013) and fasttext (Bojanowski et al., 2017) on both Turkish and English languages."}
{"sent_id": "155920441b8e81dff4e2b8e110383d-C001-216", "intents": ["@DIF@"], "paper_id": "ABC_155920441b8e81dff4e2b8e110383d_8", "text": "Our morpheme-based model morph2vec learns better word representations for morphologically complex words compared to the word-based model word2vec (Mikolov et al., 2013) , character-based model char2vec (Cao and Rei, 2016) , and the character n-gram level model fasttext (Bojanowski et al., 2017) ."}
{"sent_id": "bb5e6e32d7e507bc6d943719c02902-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_bb5e6e32d7e507bc6d943719c02902_8", "text": "Sparsification of the RNN is usually performed either at the level of individual weights (unstructured sparsification) [13, 11, 1] or at the level of neurons [14] (structured sparsification -removing weights by groups corresponding to neurons)."}
{"sent_id": "bb5e6e32d7e507bc6d943719c02902-C001-126", "intents": ["@BACK@"], "paper_id": "ABC_bb5e6e32d7e507bc6d943719c02902_8", "text": "In [1] , SparseVD is adapted to the RNNs."}
{"sent_id": "bb5e6e32d7e507bc6d943719c02902-C001-175", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_bb5e6e32d7e507bc6d943719c02902_8", "text": "This configuration corresponds to a model of Chirkova et al. [1] ."}
{"sent_id": "bb5e6e32d7e507bc6d943719c02902-C001-17", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_bb5e6e32d7e507bc6d943719c02902_8", "text": "We propose to add an intermediate level of sparsification between individual weights [1] and neurons [14] -gates (see fig. 1 , left)."}
{"sent_id": "bb5e6e32d7e507bc6d943719c02902-C001-22", "intents": ["@USE@"], "paper_id": "ABC_bb5e6e32d7e507bc6d943719c02902_8", "text": "We implement the idea for LSTM in two frameworks: pruning [14] and Bayesian sparsification [1] and observe that resulting gate structures (which gates are constant and which are not) vary for different NLP tasks."}
{"sent_id": "bb5e6e32d7e507bc6d943719c02902-C001-47", "intents": ["@USE@"], "paper_id": "ABC_bb5e6e32d7e507bc6d943719c02902_8", "text": "We rely on Sparse Variational Dropout [10, 1] to sparsify individual weights."}
{"sent_id": "bb5e6e32d7e507bc6d943719c02902-C001-56", "intents": ["@USE@"], "paper_id": "ABC_bb5e6e32d7e507bc6d943719c02902_8", "text": "In the Bayesian framework, we perform an evaluation on the text classification (datasets IMDb [6] and AGNews [17]) and language modeling (dataset PTB, character and word level tasks) following [1] ."}
{"sent_id": "bb5e6e32d7e507bc6d943719c02902-C001-58", "intents": ["@USE@"], "paper_id": "ABC_bb5e6e32d7e507bc6d943719c02902_8", "text": "Here we regularize and sparsify all layers following [1] ."}
{"sent_id": "bb5e6e32d7e507bc6d943719c02902-C001-129", "intents": ["@USE@"], "paper_id": "ABC_bb5e6e32d7e507bc6d943719c02902_8", "text": "To estimate the expectation in (1), we sample weights from the approximate posterior distribution in the same way as in [1] ."}
{"sent_id": "bb5e6e32d7e507bc6d943719c02902-C001-157", "intents": ["@USE@"], "paper_id": "ABC_bb5e6e32d7e507bc6d943719c02902_8", "text": "Since in text classification tasks, usually only a small number of input words are important, we use additional multiplicative weights to sparsify the input vocabulary following Chirkova et al. [1] ."}
{"sent_id": "bb5e6e32d7e507bc6d943719c02902-C001-161", "intents": ["@USE@"], "paper_id": "ABC_bb5e6e32d7e507bc6d943719c02902_8", "text": "Models for the text classification and the character-level LM are trained in the same setting as in [1] (we used the code provided by the authors)."}
{"sent_id": "22253d7b7cd43697b99909e09e7ebb-C001-96", "intents": ["@BACK@"], "paper_id": "ABC_22253d7b7cd43697b99909e09e7ebb_8", "text": "In [17] a dynamical model of language change was proposed, based on a spin glass model for syntactic parameters and language interactions."}
{"sent_id": "22253d7b7cd43697b99909e09e7ebb-C001-98", "intents": ["@BACK@"], "paper_id": "ABC_22253d7b7cd43697b99909e09e7ebb_8", "text": "In the case of syntactic parameters behaving as independent variables, in the low temperature regime (see [17] for a discussion of the interpretation of the temperature parameter in this model) the dynamics converges rapidly towards an equilibrium state where all the spin variables corresponding to a given syntactic feature for the various languages align to the value most prevalent in the initial configuration."}
{"sent_id": "22253d7b7cd43697b99909e09e7ebb-C001-100", "intents": ["@BACK@"], "paper_id": "ABC_22253d7b7cd43697b99909e09e7ebb_8", "text": "Using syntactic data from [6] , [7] , which record explicit entailment relation between different parameter, it was shown in [17] , for small graph examples, that in the presence of relations the dynamics settles on equilibrium states that are not necessarily given by completely aligned spins."}
{"sent_id": "22253d7b7cd43697b99909e09e7ebb-C001-109", "intents": ["@BACK@"], "paper_id": "ABC_22253d7b7cd43697b99909e09e7ebb_8", "text": "In the presence of entailment relations between different syntactic variables, it was shown in [17] that the Hamiltonian should be modified by a term that introduces the relations as a Lagrange multiplier."}
{"sent_id": "22253d7b7cd43697b99909e09e7ebb-C001-119", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_22253d7b7cd43697b99909e09e7ebb_8", "text": "Given an initial condition x 0 ∈ {0, 1} n L and the datum (J e ) e∈E(G L ) of the strengths of the interaction energies along the edges, the same method used in [17] , based on the standard Metropolis-Hastings algorithm, can be used to study the dynamics in this setting, with a similar behavior."}
{"sent_id": "22253d7b7cd43697b99909e09e7ebb-C001-141", "intents": ["@BACK@"], "paper_id": "ABC_22253d7b7cd43697b99909e09e7ebb_8", "text": "Consider the very small example, with just two entailed syntactic variables and four languages, discussed in [17] , where the chosen languages are L = { 1 , 2 , 3 , 4 } = {English, Welsh, Russian, Bulgarian} and the two syntactic parameters are {x 1 , x 2 } = {StrongDeixis, StrongAnaphoricity}. Since we have an entailment relation, the possible values of the variables x i are now ternary, x i ( ) ∈ {0, −1, +1}, that is, we consider here codes C ⊂ F n 3 ."}
{"sent_id": "22253d7b7cd43697b99909e09e7ebb-C001-140", "intents": ["@BACK@"], "paper_id": "ABC_22253d7b7cd43697b99909e09e7ebb_8", "text": "One can see already in a very simple example, and using the dynamical system in the form described in [17] , that the dynamics in the space of code parameters now does not need to move towards the δ = 0 line."}
{"sent_id": "22253d7b7cd43697b99909e09e7ebb-C001-147", "intents": ["@BACK@"], "paper_id": "ABC_22253d7b7cd43697b99909e09e7ebb_8", "text": "In the cases with high temperature and either high or low entailment energy, it is shown in [17] that one can have equilibrium states like"}
{"sent_id": "22253d7b7cd43697b99909e09e7ebb-C001-102", "intents": ["@USE@"], "paper_id": "ABC_22253d7b7cd43697b99909e09e7ebb_8", "text": "When we interpret the dynamics of the model considered in [17] in terms of codes and the space of code parameters, the initial datum of the set of languages L at the vertices of the graph, with its given list of syntactic binary variables, determines a code C L ."}
{"sent_id": "22253d7b7cd43697b99909e09e7ebb-C001-145", "intents": ["@USE@"], "paper_id": "ABC_22253d7b7cd43697b99909e09e7ebb_8", "text": "We consider in this case the same dynamical system used in [17] to model the case with entailment, which is a modification of the Ising model to a coupling of an Ising and a Potts model with q = 3 at the vertices of the graph."}
{"sent_id": "22253d7b7cd43697b99909e09e7ebb-C001-158", "intents": ["@USE@"], "paper_id": "ABC_22253d7b7cd43697b99909e09e7ebb_8", "text": "The example mentioned above is too simple and artificial to be significant, but we can analyze a more general situation, where we consider the full syntactic data of [6] , [7] , with all the entailment relations taken into account, and the same interaction energies along the edges as in [17] , taken from the data of [16] , which can be regarded as roughly proportional to a measure of the amount of bilingualism."}
{"sent_id": "22253d7b7cd43697b99909e09e7ebb-C001-161", "intents": ["@DIF@"], "paper_id": "ABC_22253d7b7cd43697b99909e09e7ebb_8", "text": "a lot more complicated than the simple examples discussed in [17] ."}
{"sent_id": "7adc4bb66b9173ccee2adc4b64c945-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_7adc4bb66b9173ccee2adc4b64c945_8", "text": "The recently available Stanford Sentiment Treebank (Socher et al., 2013) renders manually annotated, real-valued sentiment scores for all phrases in parse trees."}
{"sent_id": "7adc4bb66b9173ccee2adc4b64c945-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_7adc4bb66b9173ccee2adc4b64c945_8", "text": "Much recent work considers sentiment analysis from a semantic-composition perspective (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2012; Socher et al., 2013) , which achieved the state-of-the-art performance."}
{"sent_id": "7adc4bb66b9173ccee2adc4b64c945-C001-65", "intents": ["@BACK@"], "paper_id": "ABC_7adc4bb66b9173ccee2adc4b64c945_8", "text": "The more recent work of (Socher et al., 2012; Socher et al., 2013) proposed models based on recursive neural networks that do not rely on any heuristic rules."}
{"sent_id": "7adc4bb66b9173ccee2adc4b64c945-C001-123", "intents": ["@BACK@"], "paper_id": "ABC_7adc4bb66b9173ccee2adc4b64c945_8", "text": "More details can be found in (Socher et al., 2013) ."}
{"sent_id": "7adc4bb66b9173ccee2adc4b64c945-C001-181", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_7adc4bb66b9173ccee2adc4b64c945_8", "text": "Data As described earlier, the Stanford Sentiment Treebank (Socher et al., 2013) has manually annotated, real-valued sentiment values for all phrases in parse trees."}
{"sent_id": "7adc4bb66b9173ccee2adc4b64c945-C001-37", "intents": ["@USE@"], "paper_id": "ABC_7adc4bb66b9173ccee2adc4b64c945_8", "text": "Next, we evaluate a recently proposed composition model (Socher, 2013) that relies on both the negator and the argument."}
{"sent_id": "7adc4bb66b9173ccee2adc4b64c945-C001-68", "intents": ["@USE@"], "paper_id": "ABC_7adc4bb66b9173ccee2adc4b64c945_8", "text": "In principle neural network is able to fit very complicated functions (Mitchell, 1997) , and in this paper, we adapt the state-of-the-art approach described in (Socher et al., 2013) to help understand the behavior of negators specifically."}
{"sent_id": "7adc4bb66b9173ccee2adc4b64c945-C001-117", "intents": ["@USE@"], "paper_id": "ABC_7adc4bb66b9173ccee2adc4b64c945_8", "text": "For the former, we adopt the recursive neural tensor network (RNTN) proposed recently by Socher et al. (2013) , which has showed to achieve the state-of-the-art performance in sentiment analysis."}
{"sent_id": "7adc4bb66b9173ccee2adc4b64c945-C001-151", "intents": ["@USE@"], "paper_id": "ABC_7adc4bb66b9173ccee2adc4b64c945_8", "text": "Inference and learning in PSTN follow a forwardbackward propagation process similar to that in (Socher et al., 2013) , and for completeness, we depict the details as follows."}
{"sent_id": "7adc4bb66b9173ccee2adc4b64c945-C001-154", "intents": ["@USE@"], "paper_id": "ABC_7adc4bb66b9173ccee2adc4b64c945_8", "text": "During learning, following the method used by the RNTN model in (Socher et al., 2013) , PSTN also aims to minimize the cross-entropy error between the predicted distribution y i ∈ R m×1 at node i and the target distribution t i ∈ R m×1 at that node."}
{"sent_id": "7adc4bb66b9173ccee2adc4b64c945-C001-161", "intents": ["@USE@"], "paper_id": "ABC_7adc4bb66b9173ccee2adc4b64c945_8", "text": "Note that the gradient calculations for the V, W, W label , L are the same as that of presented in (Socher et al., 2013) ."}
{"sent_id": "7adc4bb66b9173ccee2adc4b64c945-C001-176", "intents": ["@USE@"], "paper_id": "ABC_7adc4bb66b9173ccee2adc4b64c945_8", "text": "The original RNTN and the PSTN predict 5-class sentiment for each negated phrase; we map the output to real-valued scores based on the scale that Socher et al. (2013) used to map real-valued sentiment scores to sentiment categories."}
{"sent_id": "7adc4bb66b9173ccee2adc4b64c945-C001-191", "intents": ["@USE@"], "paper_id": "ABC_7adc4bb66b9173ccee2adc4b64c945_8", "text": "The split of training and test data is same as specified in (Socher et al., 2013) ."}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "One promising approach for addressing this problem is to model argument sharing across multiple predicates (Iida et al., 2015; Ouchi et al., 2015; Ouchi et al., 2017) ."}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-20", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "More recently, as an end-to-end model considering multi-predicate dependencies, Ouchi et al. (2017) used Grid RNN to incorporate intermediate representations of the prediction for one predicate generated by an RNN layer into the inputs of the RNN layer for another predicate."}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-129", "intents": ["@BACK@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "Grid RNN of Ouchi et al. (2017) is a state-of-the-art end-to-end model, designed to capture interactions among multiple predicate-argument relations."}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-188", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "For an end-to-end neural model, Ouchi et al. (2017) used a Grid RNN to capture multiple predicate interactions."}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-31", "intents": ["@USE@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "In this study, we follow the setting of Iida et al. (2015) , Ouchi et al. (2017) , and Matsubayashi and Inui (2017) , and focus only on analyzing arguments in a target sentence."}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-32", "intents": ["@USE@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "In addition, we exclude argument instances that are in the same bunsetsu, a base phrase unit in Japanese, as the target predicate, following Ouchi et al. (2017) , which we will compare with the results in experiments."}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-40", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "Our proposed models extend end-to-end style SRL systems using deep bi-RNN (Zhou and Xu, 2015; He et al., 2017; Ouchi et al., 2017) to combine mechanisms that consider multiple predicate interactions."}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-48", "intents": ["@USE@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "In addition, we use the residual connections (He et al., 2016) following Ouchi et al. (2017) ."}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-105", "intents": ["@USE@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "In order to strictly compare the impact of our extensions to the method used for integrating multiple pieces of predicate information in the state-of-the-art end-to-end model, in addition to our base model, we replicated the method of Ouchi et al. (2017) by modifying Equations (1) of our base model as follows:"}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-134", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "We thus trained our GRID model with Ouchi et al. (2017) 's settings (d r = 32 and K = 8) and the best performing hyperparameters; however, we were not able to reproduce the reported gain from Grid RNN (see the row of \"GRID (d r = 32, K = 8)\" in Table 1 )."}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-56", "intents": ["@DIF@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "In contrast to the Grid RNN model of Ouchi et al. (2017) , where the information of multiple predicates propagates through the RNNs, our interaction layers use pooling and attention mechanisms to directly associate the label prediction information for a target (predicate, word) pair with that for words strongly related to the target pair, without being disturbed by word order and distance."}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-107", "intents": ["@DIF@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "The performance of this replicated model may not be strictly the same as that reported in Ouchi et al. (2017) due to discrepancies in the embeddings of inputs, hyperparameters (a training batch size, a hidden unit size, etc.), and training strategy (an optimizing algorithm, a regularization method, an early stopping method, etc.)."}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-131", "intents": ["@DIF@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "Unlike the results reported in Ouchi et al. (2017) , the GRID model in our experiment did not clearly outperform the model without the grid architecture, i.e., the Base model."}
{"sent_id": "d76946b009d67613326a8e7650ad36-C001-132", "intents": ["@DIF@"], "paper_id": "ABC_d76946b009d67613326a8e7650ad36_8", "text": "We first suspected that this might have resulted from the difference in dimensionality d r of RNN hidden states: d r = 32 in Ouchi et al. (2017) , whereas d r = 256 in our experiments."}
{"sent_id": "2cedb1a0f0c0fbb9bd95d5b54e4967-C001-12", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_2cedb1a0f0c0fbb9bd95d5b54e4967_8", "text": "Only few approaches have attempted comprehension on multiparty dialogue Ma, Jurczyk, and Choi [2018] ."}
{"sent_id": "2cedb1a0f0c0fbb9bd95d5b54e4967-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_2cedb1a0f0c0fbb9bd95d5b54e4967_8", "text": "Unlike the above tasks where documents and queries are written in a similar writing style, the multiparty dialogue reading comprehension task introduced by Ma, Jurczyk, and Choi [2018] has a very different writing style between dialogues and queries."}
{"sent_id": "2cedb1a0f0c0fbb9bd95d5b54e4967-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_2cedb1a0f0c0fbb9bd95d5b54e4967_8", "text": "The rest of the plot summaries were collected by Ma, Jurczyk, and Choi [2018] ."}
{"sent_id": "2cedb1a0f0c0fbb9bd95d5b54e4967-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_2cedb1a0f0c0fbb9bd95d5b54e4967_8", "text": "The single variable task from Ma, Jurczyk, and Choi [2018] consists a dialogue passage p, a query q which is from plot summary of the dialogue passage and an answer a. In this 1 https://github.com/emorynlp/character-mining task, a query q replaces only one character entity with an unknown variable x and the machine is asked to infer the replaced character entity (answer a) from all the possible entities appear in the dialogue passage p. This task is evaluated by computing the accuracy of predictions (see Section )."}
{"sent_id": "2cedb1a0f0c0fbb9bd95d5b54e4967-C001-41", "intents": ["@MOT@"], "paper_id": "ABC_2cedb1a0f0c0fbb9bd95d5b54e4967_8", "text": "In the previous work of Ma, Jurczyk, and Choi [2018] , they used a random data split where 1,187 of 1,349 queries in the development set and 1,207 of 1,353 queries in the test set are generated from the same plot summaries as some queries in the training set with only masking the different character entities which makes the model can see the right answer in the training set."}
{"sent_id": "2cedb1a0f0c0fbb9bd95d5b54e4967-C001-118", "intents": ["@MOT@", "@USE@", "@DIF@"], "paper_id": "ABC_2cedb1a0f0c0fbb9bd95d5b54e4967_8", "text": "Adding a CNN can achieve even lower accuracy because passing sequences to the CNN only keeps important information after the pooling operation, but for dialogue data, most of the time the replaced entity needs to be decided by Ma, Jurczyk, and Choi [2018] are not helpful for these tasks on our data split because dialogues contain so many informal expressions and the size of the corpus is small."}
{"sent_id": "2cedb1a0f0c0fbb9bd95d5b54e4967-C001-14", "intents": ["@USE@"], "paper_id": "ABC_2cedb1a0f0c0fbb9bd95d5b54e4967_8", "text": "Inspired by various options of analytic models and the potential of the dialogue processing market, we extend the corpus presented by Ma, Jurczyk, and Choi [2018] for comprehensive predictions of personal entities in multiparty dialogue and develop deep learning models to make robust inference on their contexts."}
{"sent_id": "2cedb1a0f0c0fbb9bd95d5b54e4967-C001-39", "intents": ["@USE@"], "paper_id": "ABC_2cedb1a0f0c0fbb9bd95d5b54e4967_8", "text": "Table 1 shows the statistical data of the corpus from Ma, Jurczyk, and Choi [2018] ."}
{"sent_id": "2cedb1a0f0c0fbb9bd95d5b54e4967-C001-40", "intents": ["@USE@"], "paper_id": "ABC_2cedb1a0f0c0fbb9bd95d5b54e4967_8", "text": "Based on the above corpus we created a new data split different from Ma, Jurczyk, and Choi [2018] 's data split."}
{"sent_id": "2cedb1a0f0c0fbb9bd95d5b54e4967-C001-44", "intents": ["@USE@"], "paper_id": "ABC_2cedb1a0f0c0fbb9bd95d5b54e4967_8", "text": "We propose three tasks, one is from Ma, Jurczyk, and Choi [2018] , and another two tasks are new tasks designed by us."}
{"sent_id": "2cedb1a0f0c0fbb9bd95d5b54e4967-C001-75", "intents": ["@USE@"], "paper_id": "ABC_2cedb1a0f0c0fbb9bd95d5b54e4967_8", "text": "Based on Ma, Jurczyk, and Choi [2018] , we first use CNN to extract the gram-level features of utterances and then use @ent04 asks @ent00 how someone could get a hold of @ent00 's credit card number and @ent00 is surprised at how much was spent ."}
{"sent_id": "2cedb1a0f0c0fbb9bd95d5b54e4967-C001-96", "intents": ["@USE@"], "paper_id": "ABC_2cedb1a0f0c0fbb9bd95d5b54e4967_8", "text": "This method is the SOTA method last year in Ma, Jurczyk, and Choi [2018] 's data split which is also selected as one of our experimental methods."}
{"sent_id": "2cedb1a0f0c0fbb9bd95d5b54e4967-C001-18", "intents": ["@EXT@"], "paper_id": "ABC_2cedb1a0f0c0fbb9bd95d5b54e4967_8", "text": "Distinguished from the previous work that only focused on a single variable per passage Ma, Jurczyk, and Choi [2018] , we propose two new passage completion tasks on multiparty dialogue which increase the task complexity by replacing more character mentions with variables with a better motivated data split."}
{"sent_id": "d7dba136667d6058bf46d6ede3f2ef-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_d7dba136667d6058bf46d6ede3f2ef_8", "text": "Most recently, several approaches based on cross-lingual entity embeddings (Hao et al., 2016; Chen et al., 2017; Sun, Hu, and Li, 2017) or graph neural networks Xu et al., 2019; Wu et al., 2019) have been proposed for this task."}
{"sent_id": "d7dba136667d6058bf46d6ede3f2ef-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_d7dba136667d6058bf46d6ede3f2ef_8", "text": "In particular, Xu et al. (2019) introduces the topic entity graph to capture the local context information of an entity within the KG, and further tackles this task as a graph matching problem by proposing a graph matching network."}
{"sent_id": "d7dba136667d6058bf46d6ede3f2ef-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_d7dba136667d6058bf46d6ede3f2ef_8", "text": "Different with those methods that still follow previous works that rely on learned entity embeddings to rank alignments, Xu et al. (2019) views this task as a graph matching problem and further proposes a graph matching neural network that additionally considers the matching information of an entity's neighborhood to perform the prediction."}
{"sent_id": "d7dba136667d6058bf46d6ede3f2ef-C001-137", "intents": ["@BACK@"], "paper_id": "ABC_d7dba136667d6058bf46d6ede3f2ef_8", "text": "As concluded in Xu et al. (2019) , the node-level matching layer has a significant impact on the matching performance, since it captures the local entity matching information."}
{"sent_id": "d7dba136667d6058bf46d6ede3f2ef-C001-24", "intents": ["@USE@"], "paper_id": "ABC_d7dba136667d6058bf46d6ede3f2ef_8", "text": "In particular, we analyze the results of Xu et al. (2019) and find that nearly 8% of the alignments are many-to-one mappings."}
{"sent_id": "d7dba136667d6058bf46d6ede3f2ef-C001-92", "intents": ["@USE@"], "paper_id": "ABC_d7dba136667d6058bf46d6ede3f2ef_8", "text": "We analyze the alignment results of three baseline methods, i.e., Wang et al. (2018) , Xu et al. (2019) and Wu et al. (2019) ."}
{"sent_id": "d7dba136667d6058bf46d6ede3f2ef-C001-105", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_d7dba136667d6058bf46d6ede3f2ef_8", "text": "In this paper, we use the state-of-the-art alignment model (Xu et al., 2019) as our baseline method and propose two ways to enhance this model by incorporating easy assignment information."}
{"sent_id": "d7dba136667d6058bf46d6ede3f2ef-C001-197", "intents": ["@USE@"], "paper_id": "ABC_d7dba136667d6058bf46d6ede3f2ef_8", "text": "We compare our approach against existing alignment methods: JE (Hao et al., 2016) , MTransE (Chen et al., 2017) , JAPE (Sun, Hu, and Li, 2017) , IPTransE (Zhu et al., 2017) , BootEA (Sun, Hu, and Li, 2017) , GCN , GM (Xu et al., 2019) and RDGCN (Wu et al., 2019) ."}
{"sent_id": "d7dba136667d6058bf46d6ede3f2ef-C001-202", "intents": ["@USE@"], "paper_id": "ABC_d7dba136667d6058bf46d6ede3f2ef_8", "text": "For the configurations of the alignment model, we use the same settings as Xu et al. (2019) ."}
{"sent_id": "d7dba136667d6058bf46d6ede3f2ef-C001-131", "intents": ["@EXT@"], "paper_id": "ABC_d7dba136667d6058bf46d6ede3f2ef_8", "text": "In particular, we introduce two ways to enhance this baseline model by explicitly integrating the easy assignment information into two layers of Xu et al. (2019) :"}
{"sent_id": "d7dba136667d6058bf46d6ede3f2ef-C001-130", "intents": ["@DIF@"], "paper_id": "ABC_d7dba136667d6058bf46d6ede3f2ef_8", "text": "In contrast to Xu et al. (2019) that only takes two topic graphs as input, we can utilize additional information such as easy assignments found in previous decoding steps to resolve hard assignments."}
{"sent_id": "00a2e4d0cacfb1fb7098bd324d960a-C001-3", "intents": ["@USE@"], "paper_id": "ABC_00a2e4d0cacfb1fb7098bd324d960a_9", "text": "Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018) ."}
{"sent_id": "00a2e4d0cacfb1fb7098bd324d960a-C001-109", "intents": ["@USE@"], "paper_id": "ABC_00a2e4d0cacfb1fb7098bd324d960a_9", "text": "The top row ( †) represents MoS scores reported in Yang et al. (2018) as a baseline."}
{"sent_id": "00a2e4d0cacfb1fb7098bd324d960a-C001-118", "intents": ["@USE@"], "paper_id": "ABC_00a2e4d0cacfb1fb7098bd324d960a_9", "text": "Moreover, the top row of Table 3 shows the perplexity of AWD-LSTM with MoS reported in Yang et al. (2018) for comparison."}
{"sent_id": "00a2e4d0cacfb1fb7098bd324d960a-C001-229", "intents": ["@USE@"], "paper_id": "ABC_00a2e4d0cacfb1fb7098bd324d960a_9", "text": "We compare AWD-LSTM-DOC with AWD-LSTM (Merity et al., 2018) and AWD-LSTMMoS (Yang et al., 2018) ."}
{"sent_id": "00a2e4d0cacfb1fb7098bd324d960a-C001-274", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_00a2e4d0cacfb1fb7098bd324d960a_9", "text": "We proposed Direct Output Connection (DOC), a generalization method of MoS introduced by Yang et al. (2018) ."}
{"sent_id": "00a2e4d0cacfb1fb7098bd324d960a-C001-25", "intents": ["@MOT@"], "paper_id": "ABC_00a2e4d0cacfb1fb7098bd324d960a_9", "text": "However, Yang et al. (2018) proved that existing RNN language models have low expressive power due to the Softmax bottleneck, which means the output matrix of RNN language models is low rank when we interpret the training of RNN language models as a matrix factorization problem."}
{"sent_id": "00a2e4d0cacfb1fb7098bd324d960a-C001-26", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_00a2e4d0cacfb1fb7098bd324d960a_9", "text": "To solve the Softmax bottleneck, Yang et al. (2018) proposed Mixture of Softmaxes (MoS), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed-length vector."}
{"sent_id": "00a2e4d0cacfb1fb7098bd324d960a-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_00a2e4d0cacfb1fb7098bd324d960a_9", "text": "Yang et al. (2018) also argued that rank(A ) is as high as vocabulary size V based on the following two assumptions:"}
{"sent_id": "00a2e4d0cacfb1fb7098bd324d960a-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_00a2e4d0cacfb1fb7098bd324d960a_9", "text": "In summary, Yang et al. (2018) indicated that D h N is much smaller than rank(A) because its scale is usually 10 2 and vocabulary size V is at least 10 4 ."}
{"sent_id": "00a2e4d0cacfb1fb7098bd324d960a-C001-63", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_00a2e4d0cacfb1fb7098bd324d960a_9", "text": "To construct a high-rank matrix, Yang et al. (2018) proposed Mixture of Softmaxes (MoS)."}
{"sent_id": "00a2e4d0cacfb1fb7098bd324d960a-C001-265", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_00a2e4d0cacfb1fb7098bd324d960a_9", "text": "As described in Section 3, Yang et al. (2018) interpreted training language modeling as matrix factorization and improved performance by computing multiple probability distributions."}
{"sent_id": "00a2e4d0cacfb1fb7098bd324d960a-C001-42", "intents": ["@EXT@"], "paper_id": "ABC_00a2e4d0cacfb1fb7098bd324d960a_9", "text": "3 Language Modeling as Matrix Factorization Yang et al. (2018) indicated that the training of language models can be interpreted as a matrix 2 Actually, we apply a bias term in addition to the weight matrix but we omit it to simplify the following discussion."}
{"sent_id": "00a2e4d0cacfb1fb7098bd324d960a-C001-142", "intents": ["@SIM@"], "paper_id": "ABC_00a2e4d0cacfb1fb7098bd324d960a_9", "text": "In contrast, AWD-LSTM-MoS (Yang et al., 2018) and AWD-LSTM-DOC outputted matrices whose ranks equal the vocabulary size."}
{"sent_id": "6a054953660e465151e4d8a2223a76-C001-21", "intents": ["@USE@"], "paper_id": "ABC_6a054953660e465151e4d8a2223a76_9", "text": "In this work we focus on vector spaces that directly weight a co-occurrence matrix and report results for SVD, GloVe and SGNS from the study of Levy et al. (2015) for comparison."}
{"sent_id": "6a054953660e465151e4d8a2223a76-C001-41", "intents": ["@USE@"], "paper_id": "ABC_6a054953660e465151e4d8a2223a76_9", "text": "This can be generalised to an additional cutoff parameter k (neg) following Levy et al. (2015) , giving our third PMI variant (abbreviated as SPMI): 2"}
{"sent_id": "6a054953660e465151e4d8a2223a76-C001-95", "intents": ["@USE@"], "paper_id": "ABC_6a054953660e465151e4d8a2223a76_9", "text": "We also compare them to the best scores reported by Levy et al. (2015) for their model (PMI and SVD), word2vec-SGNS (Mikolov et al., 2013) and GloVe (Pennington et al., 2014 )-see Figure 3a , where only the betterperforming SPMI and SCPMI are shown."}
{"sent_id": "6a054953660e465151e4d8a2223a76-C001-99", "intents": ["@USE@"], "paper_id": "ABC_6a054953660e465151e4d8a2223a76_9", "text": "With lognSCPMI and 1SCPMI, the heuristics follow Levy et al. (2015) ."}
{"sent_id": "6a054953660e465151e4d8a2223a76-C001-101", "intents": ["@USE@"], "paper_id": "ABC_6a054953660e465151e4d8a2223a76_9", "text": "On the right, our heuristic in comparison to the best and average results together with the models selected using the recommendations presented in Levy et al. (2015) ."}
{"sent_id": "6a054953660e465151e4d8a2223a76-C001-112", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_6a054953660e465151e4d8a2223a76_9", "text": "This paper presents a systematic study of cooccurrence quantification focusing on the selection of parameters presented in Levy et al. (2015) ."}
{"sent_id": "6a054953660e465151e4d8a2223a76-C001-121", "intents": ["@USE@"], "paper_id": "ABC_6a054953660e465151e4d8a2223a76_9", "text": "* Results reported by Levy et al. (2015) ."}
{"sent_id": "6a054953660e465151e4d8a2223a76-C001-25", "intents": ["@MOT@"], "paper_id": "ABC_6a054953660e465151e4d8a2223a76_9", "text": "Can the findings of Levy et al. (2015) be directly applied to models with a few thousand dimensions?"}
{"sent_id": "6a054953660e465151e4d8a2223a76-C001-52", "intents": ["@MOT@"], "paper_id": "ABC_6a054953660e465151e4d8a2223a76_9", "text": "Another issue with PMI is its bias towards rare events (Levy et al., 2015) ; one way of solving this issue is to weight the value by the co-occurrence frequency (Evert, 2005) :"}
{"sent_id": "6a054953660e465151e4d8a2223a76-C001-107", "intents": ["@DIF@"], "paper_id": "ABC_6a054953660e465151e4d8a2223a76_9", "text": "As Figure 3b shows, our heuristics give performance close to the optimum for any dimensionality, with a large improvement over both an average parameter setting and the parameters suggested by Levy et al. (2015) in a high-dimensional setting."}
{"sent_id": "9885b924f6b0806844d4e70d857a35-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9885b924f6b0806844d4e70d857a35_9", "text": "Most previous research on user geolocation has focused either on text-based classification approaches (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2014) or, to a lesser extent, network-based regression approaches (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015) ."}
{"sent_id": "9885b924f6b0806844d4e70d857a35-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_9885b924f6b0806844d4e70d857a35_9", "text": "As shown by Rahimi et al. (2015) , geolocation predictions from text can be used as a backoff for disconnected users, but there has been little work that has investigated a more integrated text-and network-based approach to user geolocation."}
{"sent_id": "9885b924f6b0806844d4e70d857a35-C001-17", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_9885b924f6b0806844d4e70d857a35_9", "text": "Our contributions are as follows: (1) we propose the use of Modified Adsorption (Talukdar and Crammer, 2009) as a baseline networkbased geolocation model, and show that it outperforms previous network-based approaches (Jurgens, 2013; Rahimi et al., 2015) ; (2) we demonstrate that removing \"celebrity\" nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we integrate textbased geolocation priors into Modified Adsorption, and show that our unified geolocation model outperforms both text-only and network-only approaches, and achieves state-of-the-art results over three standard datasets."}
{"sent_id": "9885b924f6b0806844d4e70d857a35-C001-93", "intents": ["@DIF@"], "paper_id": "ABC_9885b924f6b0806844d4e70d857a35_9", "text": "Our baseline network-based model of MAD-B outperforms the text-based models and also previous network-based models (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015) ."}
{"sent_id": "9885b924f6b0806844d4e70d857a35-C001-96", "intents": ["@DIF@"], "paper_id": "ABC_9885b924f6b0806844d4e70d857a35_9", "text": "It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification (Rahimi et al., 2015) ."}
{"sent_id": "9885b924f6b0806844d4e70d857a35-C001-38", "intents": ["@USE@"], "paper_id": "ABC_9885b924f6b0806844d4e70d857a35_9", "text": "We evaluate our models over three pre-existing geotagged Twitter datasets: (1) GEOTEXT (Eisen-stein et al., 2010), (2) TWITTER-US (Roller et al., 2012) , and (3) TWITTER-WORLD (Han et al., 2012) ."}
{"sent_id": "9885b924f6b0806844d4e70d857a35-C001-83", "intents": ["@USE@"], "paper_id": "ABC_9885b924f6b0806844d4e70d857a35_9", "text": "The label for the dongle node is based on a textbased l 1 regularised logistic regression model, using the method of Rahimi et al. (2015) ."}
{"sent_id": "9885b924f6b0806844d4e70d857a35-C001-90", "intents": ["@SIM@"], "paper_id": "ABC_9885b924f6b0806844d4e70d857a35_9", "text": "The results are also compared with prior work on network-based geolocation using label propagation (LP) (Rahimi et al., 2015) , text-based classification models (Han et al., 2012; Wing and Baldridge, 2011; Rahimi et al., 2015; Cha et al., 2015) , textbased graphical models (Ahmed et al., 2013) , and network-text hybrid models (LP-LR) (Rahimi et al., 2015) ."}
{"sent_id": "4cc18724e62db32e748838080cbfd0-C001-27", "intents": ["@DIF@"], "paper_id": "ABC_4cc18724e62db32e748838080cbfd0_9", "text": "• Exhaustive experimental evaluation on dialog act datasets, outperforming state-of-theart deep CNN (Lee and Dernoncourt, 2016) and RNN variants (Khanpour et al., 2016; Ortega and Vu, 2017 )."}
{"sent_id": "4cc18724e62db32e748838080cbfd0-C001-103", "intents": ["@DIF@"], "paper_id": "ABC_4cc18724e62db32e748838080cbfd0_9", "text": "Unlike prior approaches (Lee and Dernoncourt, 2016; Ortega and Vu, 2017 ) that rely on pre-trained word embeddings, we learn the projection weights on the fly during training, i.e word embeddings (or vocabularies) do not need to be stored."}
{"sent_id": "4cc18724e62db32e748838080cbfd0-C001-113", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_4cc18724e62db32e748838080cbfd0_9", "text": "We compare our model against a majority class baseline and Naive Bayes classifier (Lee and Dernoncourt, 2016) ."}
{"sent_id": "4cc18724e62db32e748838080cbfd0-C001-123", "intents": ["@DIF@"], "paper_id": "ABC_4cc18724e62db32e748838080cbfd0_9", "text": "Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN (Lee and Dernoncourt, 2016) and RNN variants (Khanpour et al., 2016; Ortega and Vu, 2017) ."}
{"sent_id": "4cc18724e62db32e748838080cbfd0-C001-139", "intents": ["@DIF@"], "paper_id": "ABC_4cc18724e62db32e748838080cbfd0_9", "text": "Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Ortega and Vu, 2017) ."}
{"sent_id": "4cc18724e62db32e748838080cbfd0-C001-94", "intents": ["@USE@"], "paper_id": "ABC_4cc18724e62db32e748838080cbfd0_9", "text": "We use the train, validation and test splits as defined in (Lee and Dernoncourt, 2016; Ortega and Vu, 2017) ."}
{"sent_id": "4cc18724e62db32e748838080cbfd0-C001-117", "intents": ["@USE@"], "paper_id": "ABC_4cc18724e62db32e748838080cbfd0_9", "text": "We also compare our performance against prior work using HMMs (Stolcke et al., 2000) and recent deep learning methods like CNN (Lee and Dernoncourt, 2016) , RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017) ."}
{"sent_id": "4cc18724e62db32e748838080cbfd0-C001-118", "intents": ["@BACK@"], "paper_id": "ABC_4cc18724e62db32e748838080cbfd0_9", "text": "To the best of our knowledge, (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Tran et al., 2017) are the latest approaches in dialog act classification, which also reported on the same data splits."}
{"sent_id": "5fe12a1a43957faded5722f698eb41-C001-31", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_5fe12a1a43957faded5722f698eb41_9", "text": "The word embedding model is retrofitted over WordNet's synonym pairs (for details, please refer to Paetzold and Specia (2017) )."}
{"sent_id": "5fe12a1a43957faded5722f698eb41-C001-28", "intents": ["@USE@"], "paper_id": "ABC_5fe12a1a43957faded5722f698eb41_9", "text": "For generating substitution candidates, we utilize the method proposed by Paetzold and Specia (2017) , which was recently shown to be the state-of-art method for generating substitution candidates."}
{"sent_id": "5fe12a1a43957faded5722f698eb41-C001-55", "intents": ["@USE@"], "paper_id": "ABC_5fe12a1a43957faded5722f698eb41_9", "text": "As baseline features, we use the same n-gram probability features as in Paetzold and Specia (2017) , who also employ a neural network to rank substitution candidates."}
{"sent_id": "5fe12a1a43957faded5722f698eb41-C001-56", "intents": ["@USE@"], "paper_id": "ABC_5fe12a1a43957faded5722f698eb41_9", "text": "As in Paetzold and Specia (2017) , the features were extracted using the SubIMDB corpus (Paetzold and Specia, 2015) ."}
{"sent_id": "5fe12a1a43957faded5722f698eb41-C001-83", "intents": ["@USE@"], "paper_id": "ABC_5fe12a1a43957faded5722f698eb41_9", "text": "Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity (Paetzold and Specia, 2017) ."}
{"sent_id": "5fe12a1a43957faded5722f698eb41-C001-90", "intents": ["@USE@"], "paper_id": "ABC_5fe12a1a43957faded5722f698eb41_9", "text": "The first baseline is the Neural Substitution Ranking (NSR) approach described in (Paetzold and Specia, 2017) , which employs a multi-layer perceptron neural network."}
{"sent_id": "5fe12a1a43957faded5722f698eb41-C001-99", "intents": ["@USE@"], "paper_id": "ABC_5fe12a1a43957faded5722f698eb41_9", "text": "All the three models employ the n-gram probability features extracted from the SubIMDB corpus (Paetzold and Specia, 2015) , as described in (Paetzold and Specia, 2017) , and are trained using the LexMTurk dataset."}
{"sent_id": "5fe12a1a43957faded5722f698eb41-C001-107", "intents": ["@USE@", "@SIM@", "@DIF@"], "paper_id": "ABC_5fe12a1a43957faded5722f698eb41_9", "text": "We follow the Unsupervised Boundary Ranking Substitution Selection method described in Paetzold and Specia (2017) , which ranks candidates according to how well they fit the context of the target word, and discards 50% of the worst ranking candidates."}
{"sent_id": "0bd3236100730487986ade49af24b9-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_0bd3236100730487986ade49af24b9_9", "text": "It is usually de ned as a classi cation problem where for a text and target pair, the stance of the author of the text for that target is expected as a classi cation output from the set: {Favor, Against, Neither} [12] ."}
{"sent_id": "0bd3236100730487986ade49af24b9-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_0bd3236100730487986ade49af24b9_9", "text": "Like sentiment analysis, stance detection systems can be valuable components of information retrieval and other text analysis systems [12] ."}
{"sent_id": "0bd3236100730487986ade49af24b9-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_0bd3236100730487986ade49af24b9_9", "text": "e authors state that their approach achieves stateof-the art performance rates [1] on SemEval 2016 Twi er Stance Detection corpus [12] ."}
{"sent_id": "0bd3236100730487986ade49af24b9-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_0bd3236100730487986ade49af24b9_9", "text": "Lastly, in [12] , SemEval 2016's aforementioned shared task on Twi er Stance Detection is described."}
{"sent_id": "0bd3236100730487986ade49af24b9-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_0bd3236100730487986ade49af24b9_9", "text": "It is emphasized in the related literature that unigram-based methods are reliable for the stance detection task [16] and similarly unigram-based models have been used as baseline models in studies such as [12] ."}
{"sent_id": "0bd3236100730487986ade49af24b9-C001-79", "intents": ["@DIF@"], "paper_id": "ABC_0bd3236100730487986ade49af24b9_9", "text": "Yet, completely the opposite pa ern is observed in stance detection results of baseline systems given in [12] , i.e., be er FMeasure rates have been obtained for the Against class when compared with the Favor class [12] ."}
{"sent_id": "0bd3236100730487986ade49af24b9-C001-80", "intents": ["@DIF@"], "paper_id": "ABC_0bd3236100730487986ade49af24b9_9", "text": "Some of the baseline systems reported in [12] are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classi ed as belonging to Favor or Against classes."}
{"sent_id": "0bd3236100730487986ade49af24b9-C001-81", "intents": ["@DIF@"], "paper_id": "ABC_0bd3236100730487986ade49af24b9_9", "text": "Another di erence is that the data sets in [12] have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set."}
{"sent_id": "0bd3236100730487986ade49af24b9-C001-85", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_0bd3236100730487986ade49af24b9_9", "text": "We have also evaluated SVM classi ers which use only bigrams as features, as ngram-based classi ers have been reported to perform be er for the stance detection problem [12] ."}
{"sent_id": "250a88831a4911f76acca3c9d318de-C001-11", "intents": ["@USE@"], "paper_id": "ABC_250a88831a4911f76acca3c9d318de_9", "text": "The algorithm, which is an extension of Sassano's (2004) , allows us to chunk morphemes into base phrases and decide dependency relations of the phrases in a strict left-toright manner."}
{"sent_id": "250a88831a4911f76acca3c9d318de-C001-52", "intents": ["@USE@"], "paper_id": "ABC_250a88831a4911f76acca3c9d318de_9", "text": "The algorithm that we propose is based on (Sassano, 2004) , which is considered to be a simple form of shift-reduce parsing."}
{"sent_id": "250a88831a4911f76acca3c9d318de-C001-57", "intents": ["@USE@"], "paper_id": "ABC_250a88831a4911f76acca3c9d318de_9", "text": "The flow of the algorithm, which has the same structure as Sassano's (2004) , is controlled with a stack that holds IDs for modifier morphemes."}
{"sent_id": "250a88831a4911f76acca3c9d318de-C001-76", "intents": ["@USE@"], "paper_id": "ABC_250a88831a4911f76acca3c9d318de_9", "text": "We have designed rather simple features based on the common feature set (Uchimoto et al., 1999; Kudo and Matsumoto, 2002; Sassano, 2004) for bunsetsu-based parsers."}
{"sent_id": "250a88831a4911f76acca3c9d318de-C001-83", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_250a88831a4911f76acca3c9d318de_9", "text": "The system with the previous method employs the algorithm (Sassano, 2004 ) with the voted perceptron."}
{"sent_id": "250a88831a4911f76acca3c9d318de-C001-97", "intents": ["@USE@", "@DIF@", "@MOT@"], "paper_id": "ABC_250a88831a4911f76acca3c9d318de_9", "text": "We implemented a parser that employs the algorithm of (Sassano, 2004) with the commonly used features and runs with VP instead of SVM, which Sassano (2004) originally used."}
{"sent_id": "250a88831a4911f76acca3c9d318de-C001-100", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_250a88831a4911f76acca3c9d318de_9", "text": "To enable us to compare them we gave bunsetsu chunked sentences by our parser to the parser of (Sassano, 2004) in the Kyoto University Corpus. And then we received results from the parser of (Sassano, 2004) , which are bunsetsu-based dependency structures, and converted them to morpheme-based structures that follow the scheme we propose in this paper."}
{"sent_id": "250a88831a4911f76acca3c9d318de-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_250a88831a4911f76acca3c9d318de_9", "text": "In addition, most of algorithms of Japanese dependency parsing, e.g., (Sekine et al., 2000; Sassano, 2004) , assume the three constraints below."}
{"sent_id": "250a88831a4911f76acca3c9d318de-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_250a88831a4911f76acca3c9d318de_9", "text": "Most of the modern dependency parsers for Japanese require bunsetsu chunking (base phrase chunking) before dependency parsing (Sekine et al., 2000; Kudo and Matsumoto, 2002; Sassano, 2004) ."}
{"sent_id": "250a88831a4911f76acca3c9d318de-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_250a88831a4911f76acca3c9d318de_9", "text": "See (Sassano, 2004) for further details."}
{"sent_id": "1042e7b6ef7b73f29ad75b193f9e3b-C001-8", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_1042e7b6ef7b73f29ad75b193f9e3b_9", "text": "To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: -aligned parallel corpora to project POS annotations to target languages (Yarowsky et al., 2001; Agić et al., 2015; Fang and Cohn, 2016) , -noisy tag dictionaries for type-level approximation of full supervision (Li et al., 2012) , -combination of projection and type constraints (Das and Petrov, 2011; Täckström et al., 2013) , -rapid annotation of seed training data ."}
{"sent_id": "1042e7b6ef7b73f29ad75b193f9e3b-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_1042e7b6ef7b73f29ad75b193f9e3b_9", "text": "Dictionaries are a useful source for distant supervision (Li et al., 2012; Täckström et al., 2013) ."}
{"sent_id": "1042e7b6ef7b73f29ad75b193f9e3b-C001-120", "intents": ["@BACK@"], "paper_id": "ABC_1042e7b6ef7b73f29ad75b193f9e3b_9", "text": "There is a large 33-point accuracy gap between the scores of Li et al. (2012) , where the dictionaries are large, and the other languages in Figure 4 , with smaller dictionaries."}
{"sent_id": "1042e7b6ef7b73f29ad75b193f9e3b-C001-126", "intents": ["@BACK@", "@DIF@", "@EXT@"], "paper_id": "ABC_1042e7b6ef7b73f29ad75b193f9e3b_9", "text": "Most successful work on low-resource POS tagging is based on projection (Yarowsky et al., 2001) , tag dictionaries (Li et al., 2012) , annotation of seed training data or even more recently some combination of these, e.g., via multi-task learning (Fang and not Li et al. (2012) Figure 4: The performance of LI with our dictionary data over EM iterations, separate for the languages from Li et al. (2012) and all the remaining languages in Table 1 . Cohn, 2016; Kann et al., 2018) ."}
{"sent_id": "1042e7b6ef7b73f29ad75b193f9e3b-C001-41", "intents": ["@USE@"], "paper_id": "ABC_1042e7b6ef7b73f29ad75b193f9e3b_9", "text": "We evaluate two dictionary sources, motivated by ease of accessibility to many languages: WIK-TIONARY, a word type dictionary that maps tokens to one of the 12 Universal POS tags (Li et al., 2012; Petrov et al., 2012) ; and UNIMORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016) ."}
{"sent_id": "1042e7b6ef7b73f29ad75b193f9e3b-C001-42", "intents": ["@USE@"], "paper_id": "ABC_1042e7b6ef7b73f29ad75b193f9e3b_9", "text": "For Wiktionary, we use the freely available dictionaries from Li et al. (2012) and ."}
{"sent_id": "1042e7b6ef7b73f29ad75b193f9e3b-C001-56", "intents": ["@USE@"], "paper_id": "ABC_1042e7b6ef7b73f29ad75b193f9e3b_9", "text": "-LI: Wiktionary supervision (Li et al., 2012) ."}
{"sent_id": "1042e7b6ef7b73f29ad75b193f9e3b-C001-90", "intents": ["@SIM@"], "paper_id": "ABC_1042e7b6ef7b73f29ad75b193f9e3b_9", "text": "On the test sets (Table 4 , right) DSDS reaches 87.2 over 8 test languages intersecting Li et al. (2012) and Agić et al. (2016) ."}
{"sent_id": "1042e7b6ef7b73f29ad75b193f9e3b-C001-119", "intents": ["@DIF@"], "paper_id": "ABC_1042e7b6ef7b73f29ad75b193f9e3b_9", "text": "This is in slight contrast to 50 iterations that Li et al. (2012) recommend, although selecting 50 does not dramatically hurt the scores; ii) Our replication falls ∼5 points short of their 84.9 accuracy."}
{"sent_id": "26743b7d006e485be1b850a4424a5f-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_26743b7d006e485be1b850a4424a5f_9", "text": "Xiang et al. (2013) formalized the problem as classifying each IP node (roughly corresponds to S and SBAR in Penn Treebank) in the phrase structure."}
{"sent_id": "26743b7d006e485be1b850a4424a5f-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_26743b7d006e485be1b850a4424a5f_9", "text": "The probability model of (Xiang et al., 2013) is formulated as MaxEnt model:"}
{"sent_id": "26743b7d006e485be1b850a4424a5f-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_26743b7d006e485be1b850a4424a5f_9", "text": "Xiang et al. (2013) grouped their features into four types: tree label features, lexical features, empty category features and conjunction features as shown in Table 1 ."}
{"sent_id": "26743b7d006e485be1b850a4424a5f-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_26743b7d006e485be1b850a4424a5f_9", "text": "As it has annotations for pro and trace, we show our method has substantial improvements over the state-of-the-art machine learning-based method (Xiang et al., 2013) for Chinese empty category detection as well as linguistically-motivated manually written rule-based method similar to (Campbell, 2004 )."}
{"sent_id": "26743b7d006e485be1b850a4424a5f-C001-91", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_26743b7d006e485be1b850a4424a5f_9", "text": "In the gold parse condition, the two baselines, the rule-based method (RULE) and the modified (Xiang et al., 2013) method, achieved the F-measure of 62.6% and 68.6% respectively."}
{"sent_id": "26743b7d006e485be1b850a4424a5f-C001-37", "intents": ["@USE@"], "paper_id": "ABC_26743b7d006e485be1b850a4424a5f_9", "text": "We also use Xiang et al's (2013) model as another baseline."}
{"sent_id": "26743b7d006e485be1b850a4424a5f-C001-49", "intents": ["@USE@"], "paper_id": "ABC_26743b7d006e485be1b850a4424a5f_9", "text": "Table 2 shows the accuracies of Japanese empty category detection, using the original and our modification of the (Xiang et al., 2013) with ablation test."}
{"sent_id": "26743b7d006e485be1b850a4424a5f-C001-82", "intents": ["@USE@"], "paper_id": "ABC_26743b7d006e485be1b850a4424a5f_9", "text": "We evaluated them using the word-position-level identification metrics described in (Xiang et al., 2013) ."}
{"sent_id": "26743b7d006e485be1b850a4424a5f-C001-46", "intents": ["@EXT@"], "paper_id": "ABC_26743b7d006e485be1b850a4424a5f_9", "text": "As the features for (Xiang et al., 2013) were developed for Chinese Penn Treebank, we modify their features for Keyaki Treebank: First, the traversal order is changed from post-order (bottom-up) to pre-order (top-down)."}
{"sent_id": "7d80c3cc15453ddeaea72dcb9c04f9-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_7d80c3cc15453ddeaea72dcb9c04f9_9", "text": "Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn * First two authors contributed equally to the paper. and surface form relations such as \"was born in\" extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations (Riedel et al., 2013; Fan et al., 2014; ."}
{"sent_id": "7d80c3cc15453ddeaea72dcb9c04f9-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_7d80c3cc15453ddeaea72dcb9c04f9_9", "text": "A universal schema is defined as the union of all OpenIE-like surface form patterns found in text and fixed canonical relations that exist in a knowledge base (Riedel et al., 2013) ."}
{"sent_id": "7d80c3cc15453ddeaea72dcb9c04f9-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_7d80c3cc15453ddeaea72dcb9c04f9_9", "text": "In matrix factorization for universal schema, Riedel et al. (2013) construct a sparse binary matrix of size |P| × |R| whose rows are indexed by entity-pairs (a, b) ∈ P and columns by surface form and Freebase relations s ∈ R. Subsequently, generalized PCA (Collins et al., 2001 ) is used to find a rank-k factorization, i.e., with relation factors r ∈ R |R|×k and entity-pair factors p ∈ R |P|×k , the probability of a relation s and two entities a and b is:"}
{"sent_id": "7d80c3cc15453ddeaea72dcb9c04f9-C001-77", "intents": ["@EXT@"], "paper_id": "ABC_7d80c3cc15453ddeaea72dcb9c04f9_9", "text": "Furthermore, we isolate the entity factorization in Riedel et al. (2013) by viewing it as tensor factorization."}
{"sent_id": "7d80c3cc15453ddeaea72dcb9c04f9-C001-79", "intents": ["@EXT@"], "paper_id": "ABC_7d80c3cc15453ddeaea72dcb9c04f9_9", "text": "Although not explored in isolation by Riedel et al. (2013) , model E can be used on its own to predict relations between entities, even if they have not been observed to be in a relation."}
{"sent_id": "7d80c3cc15453ddeaea72dcb9c04f9-C001-114", "intents": ["@EXT@", "@MOT@", "@USE@"], "paper_id": "ABC_7d80c3cc15453ddeaea72dcb9c04f9_9", "text": "As the direct combination of a pairwise model (Eq. 1) with an entity model (Eq. 5), we consider the FE model from Riedel et al. (2013) , i.e., the additive combination of the two: P (s(a, b)) = σ(r s · e ab + r s,1 · e a + r s,2 · e b ) (6) Both the matrix factorization model F and entity model E can de defined as special cases of this model, by setting r s,1/2 or r s to zero, respectively."}
{"sent_id": "7d80c3cc15453ddeaea72dcb9c04f9-C001-83", "intents": ["@MOT@"], "paper_id": "ABC_7d80c3cc15453ddeaea72dcb9c04f9_9", "text": "Although matrix factorization performs well for universal schema (Riedel et al., 2013) , it is not robust to sparse data and does not capture latent entity types that can be crucial for accurate relation extraction."}
{"sent_id": "7d80c3cc15453ddeaea72dcb9c04f9-C001-123", "intents": ["@USE@"], "paper_id": "ABC_7d80c3cc15453ddeaea72dcb9c04f9_9", "text": "As by Riedel et al. (2013) , we use a Bayesian personalized ranking objective (Rendle et al., 2009 ) to estimate parameters, i.e., for each observed training fact, we sample an unobserved fact for the same relation, and maximize their relative ranking using AdaGrad."}
{"sent_id": "7d80c3cc15453ddeaea72dcb9c04f9-C001-147", "intents": ["@USE@"], "paper_id": "ABC_7d80c3cc15453ddeaea72dcb9c04f9_9", "text": "Following the experiment setup of Riedel et al. (2013) , we instantiate the universal schema matrix over entity pairs and text/Freebase relations for New York Times data, and compare the performance using average precision of the presented models."}
{"sent_id": "7d80c3cc15453ddeaea72dcb9c04f9-C001-148", "intents": ["@USE@"], "paper_id": "ABC_7d80c3cc15453ddeaea72dcb9c04f9_9", "text": "Table 1 summarizes the performance of our models, as compared to existing approaches (see Riedel et al. (2013) for an overview)."}
{"sent_id": "a557739447131395f7a76d87a4cd19-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_a557739447131395f7a76d87a4cd19_9", "text": "It is usually defined as a classification problem where for a text and target pair, the stance of the author of the text for that target is expected as a classification output from the set: {Favor, Against, Neither} [12] ."}
{"sent_id": "a557739447131395f7a76d87a4cd19-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_a557739447131395f7a76d87a4cd19_9", "text": "The authors state that their approach achieves state-ofthe art performance rates [1] on SemEval 2016 Twitter Stance Detection corpus [12] ."}
{"sent_id": "a557739447131395f7a76d87a4cd19-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_a557739447131395f7a76d87a4cd19_9", "text": "Lastly, in [12] , Se-mEval 2016's aforementioned shared task on Twitter Stance Detection is described."}
{"sent_id": "a557739447131395f7a76d87a4cd19-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_a557739447131395f7a76d87a4cd19_9", "text": "Also provided are the results of the evaluations of 19 systems participating in two subtasks (one with training data set provided and the other without an annotated data set) of the shared task [12] ."}
{"sent_id": "a557739447131395f7a76d87a4cd19-C001-83", "intents": ["@BACK@"], "paper_id": "ABC_a557739447131395f7a76d87a4cd19_9", "text": "We have also evaluated SVM classifiers which use only bigrams as features, as ngram-based classifiers have been reported to perform better for the stance detection problem [12] ."}
{"sent_id": "a557739447131395f7a76d87a4cd19-C001-77", "intents": ["@DIF@"], "paper_id": "ABC_a557739447131395f7a76d87a4cd19_9", "text": "Yet, completely the opposite pattern is observed in stance detection results of baseline systems given in [12] , i.e., better F-Measure rates have been obtained for the Against class when compared with the Favor class [12] ."}
{"sent_id": "a557739447131395f7a76d87a4cd19-C001-78", "intents": ["@DIF@"], "paper_id": "ABC_a557739447131395f7a76d87a4cd19_9", "text": "Some of the baseline systems reported in [12] are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classified as belonging to Favor or Against classes."}
{"sent_id": "a557739447131395f7a76d87a4cd19-C001-79", "intents": ["@DIF@"], "paper_id": "ABC_a557739447131395f7a76d87a4cd19_9", "text": "Another difference is that the data sets in [12] have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set."}
{"sent_id": "b8244f9337456f1f90a576b2398680-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_b8244f9337456f1f90a576b2398680_9", "text": "More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003) ."}
{"sent_id": "b8244f9337456f1f90a576b2398680-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_b8244f9337456f1f90a576b2398680_9", "text": "For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while Sudo et al. (2003) allow any subtree within the dependency parse to act as an extraction pattern."}
{"sent_id": "b8244f9337456f1f90a576b2398680-C001-35", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_b8244f9337456f1f90a576b2398680_9", "text": "This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by Sudo et al. (2003) ."}
{"sent_id": "b8244f9337456f1f90a576b2398680-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_b8244f9337456f1f90a576b2398680_9", "text": "Subtrees: The final model to be considered is the subtree model (Sudo et al., 2003) ."}
{"sent_id": "b8244f9337456f1f90a576b2398680-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_b8244f9337456f1f90a576b2398680_9", "text": "Sudo et al. (2003) compared three models (SVO, chains and subtrees) on two IE scenarios using a entity extraction task."}
{"sent_id": "b8244f9337456f1f90a576b2398680-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_b8244f9337456f1f90a576b2398680_9", "text": "However, Stevenson and Greenwood (2006) also found that the coverage of the chain model was significantly worse than the subtree model, although Sudo et al. (2003) found that in some cases their performance could not be distinguished."}
{"sent_id": "b8244f9337456f1f90a576b2398680-C001-88", "intents": ["@BACK@"], "paper_id": "ABC_b8244f9337456f1f90a576b2398680_9", "text": "Sudo et al. (2003) found that it was important to find the appropriate balance between these two factors."}
{"sent_id": "b8244f9337456f1f90a576b2398680-C001-125", "intents": ["@BACK@"], "paper_id": "ABC_b8244f9337456f1f90a576b2398680_9", "text": "In addition, Sudo et al. (2003) only generated subtrees which appeared in at least three documents."}
{"sent_id": "b8244f9337456f1f90a576b2398680-C001-126", "intents": ["@BACK@"], "paper_id": "ABC_b8244f9337456f1f90a576b2398680_9", "text": "Kudo et al. (2005) and Sudo et al. (2003) both used the rightmost extension algorithm to generate subtrees."}
{"sent_id": "b8244f9337456f1f90a576b2398680-C001-77", "intents": ["@USE@"], "paper_id": "ABC_b8244f9337456f1f90a576b2398680_9", "text": "We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by Sudo et al. (2003) ."}
{"sent_id": "b8244f9337456f1f90a576b2398680-C001-132", "intents": ["@USE@"], "paper_id": "ABC_b8244f9337456f1f90a576b2398680_9", "text": "The value of β in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by Sudo et al. (2003) ."}
{"sent_id": "b8244f9337456f1f90a576b2398680-C001-100", "intents": ["@DIF@"], "paper_id": "ABC_b8244f9337456f1f90a576b2398680_9", "text": "However, this is not necessary since Sudo et al. (2003) showed that adequate knowledge about document relevance could be obtained automatically using an IR system."}
{"sent_id": "6580dc2f7316cea4e0933ff515a704-C001-3", "intents": ["@BACK@"], "paper_id": "ABC_6580dc2f7316cea4e0933ff515a704_9", "text": "Recently, McDonald et al. (2005b) formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic time relative to the length of the sentence."}
{"sent_id": "6580dc2f7316cea4e0933ff515a704-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_6580dc2f7316cea4e0933ff515a704_9", "text": "Building upon the theoretical work of (Chu and Liu, 1965; Edmonds, 1967) , McDonald et al. (2005b) present a quadratic-time dependency parsing algorithm that is just 0.7% less accurate than \"full-fledged\" chart parsing (which, in the case of dependency parsing, runs in time O(n 3 ) (Eisner, 1996) )."}
{"sent_id": "6580dc2f7316cea4e0933ff515a704-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_6580dc2f7316cea4e0933ff515a704_9", "text": "In this section, we review dependency parsing formulated as a maximum spanning tree problem (McDonald et al., 2005b) , which can be solved in quadratic time, and then present its adaptation and novel application to phrase-based decoding."}
{"sent_id": "6580dc2f7316cea4e0933ff515a704-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_6580dc2f7316cea4e0933ff515a704_9", "text": "While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999) , McDonald et al. (2005b) show O(n 2 )-time parsing is possible if trees are not required to be projective."}
{"sent_id": "6580dc2f7316cea4e0933ff515a704-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_6580dc2f7316cea4e0933ff515a704_9", "text": "That is not the case: dependency accuracy for nonprojective parsing is 90.2% for English (McDonald et al., 2005b) , only 0.7% lower than a projective parser (McDonald et al., 2005a ) that uses the same set of features and learning algorithm."}
{"sent_id": "6580dc2f7316cea4e0933ff515a704-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_6580dc2f7316cea4e0933ff515a704_9", "text": "In the case of dependency parsing for Czech, (McDonald et al., 2005b) even outperforms projective parsing, and was one of the top systems in the CoNLL-06 shared task in multilingual dependency parsing."}
{"sent_id": "6580dc2f7316cea4e0933ff515a704-C001-24", "intents": ["@EXT@"], "paper_id": "ABC_6580dc2f7316cea4e0933ff515a704_9", "text": "Adapting the approach of McDonald et al. (2005b) for machine translation, we incrementally build dependency structure left-toright in time O(n 2 ) during decoding."}
{"sent_id": "6580dc2f7316cea4e0933ff515a704-C001-57", "intents": ["@USE@"], "paper_id": "ABC_6580dc2f7316cea4e0933ff515a704_9", "text": "We now formalize weighted non-projective dependency parsing similarly to (McDonald et al., 2005b) and then describe a modified and more efficient version that can be integrated into a phrasebased decoder."}
{"sent_id": "6580dc2f7316cea4e0933ff515a704-C001-205", "intents": ["@USE@"], "paper_id": "ABC_6580dc2f7316cea4e0933ff515a704_9", "text": "In this paper, we presented a non-projective dependency parser whose time-complexity of O(n 2 ) improves upon the cubic time implementation of (McDonald et al., 2005b) , and does so with little loss in dependency accuracy (.25% to .34%)."}
{"sent_id": "6580dc2f7316cea4e0933ff515a704-C001-104", "intents": ["@MOT@"], "paper_id": "ABC_6580dc2f7316cea4e0933ff515a704_9", "text": "Since i and j are both free variables, feature computation in (McDonald et al., 2005b) takes time O(n 3 ), even though parsing itself takes O(n 2 ) time."}
{"sent_id": "6580dc2f7316cea4e0933ff515a704-C001-109", "intents": ["@DIF@"], "paper_id": "ABC_6580dc2f7316cea4e0933ff515a704_9", "text": "The O(n 3 ) non-projective parser of (McDonald et al., 2005b ) is slightly more accurate than our version, though ours runs in O(n 2 ) time. \"Local classifier\" refers to non-projective dependency parsing without removing loops as a post-processing step."}
{"sent_id": "6580dc2f7316cea4e0933ff515a704-C001-141", "intents": ["@DIF@"], "paper_id": "ABC_6580dc2f7316cea4e0933ff515a704_9", "text": "Table 4 shows that the accuracy of our truly O(n 2 ) parser is only .25% to .34% worse than the O(n 3 ) implementation of (McDonald et al., 2005b) ."}
{"sent_id": "6580dc2f7316cea4e0933ff515a704-C001-154", "intents": ["@DIF@"], "paper_id": "ABC_6580dc2f7316cea4e0933ff515a704_9", "text": "The training data consists of about 28 million English words and 23.3 million 5 Note that our results on WSJ are not exactly the same as those reported in (McDonald et al., 2005b ), since we used slightly different head finding rules."}
{"sent_id": "e41a1adcb5c9d91f2130bd249ed598-C001-131", "intents": ["@DIF@"], "paper_id": "ABC_e41a1adcb5c9d91f2130bd249ed598_9", "text": "The largest improvement is the MBN model which outperforms the results reported in [15] by as much as 23.2 percentage points on R@10."}
{"sent_id": "14fa8c3b947667244d30dd30dae89a-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_14fa8c3b947667244d30dd30dae89a_9", "text": "Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies."}
{"sent_id": "14fa8c3b947667244d30dd30dae89a-C001-90", "intents": ["@BACK@"], "paper_id": "ABC_14fa8c3b947667244d30dd30dae89a_9", "text": "Chen and Manning (2014) fine-tune word embeddings in supervised training, consistent with Socher et al. (2013) ."}
{"sent_id": "14fa8c3b947667244d30dd30dae89a-C001-32", "intents": ["@USE@"], "paper_id": "ABC_14fa8c3b947667244d30dd30dae89a_9", "text": "We take Chen and Manning (2014) , which uses the arc-standard transition system (Nivre, 2008) ."}
{"sent_id": "14fa8c3b947667244d30dd30dae89a-C001-40", "intents": ["@USE@"], "paper_id": "ABC_14fa8c3b947667244d30dd30dae89a_9", "text": "Following Chen and Manning (2014) , training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched AdaGrad (Duchi et al., 2011) ."}
{"sent_id": "14fa8c3b947667244d30dd30dae89a-C001-50", "intents": ["@USE@"], "paper_id": "ABC_14fa8c3b947667244d30dd30dae89a_9", "text": "We take the Neural model of Chen and Manning (2014) as another baseline (Figure 1(b) )."}
{"sent_id": "14fa8c3b947667244d30dd30dae89a-C001-58", "intents": ["@USE@"], "paper_id": "ABC_14fa8c3b947667244d30dd30dae89a_9", "text": "We use the arc-standard features Φ e as Chen and Manning (2014) , which is also based on the arc-eager templates of Zhang and Nivre (2011) , similar to those of the baseline model L."}
{"sent_id": "14fa8c3b947667244d30dd30dae89a-C001-86", "intents": ["@USE@"], "paper_id": "ABC_14fa8c3b947667244d30dd30dae89a_9", "text": "Following Chen and Manning (2014) , we use the pre-trained word embedding released by Collobert et al. (2011) , and set h = 200 for the hidden layer size, λ = 10 −8 for L2 regularization, and α = 0.01 for the initial learning rate of Adagrad."}
{"sent_id": "14fa8c3b947667244d30dd30dae89a-C001-104", "intents": ["@USE@"], "paper_id": "ABC_14fa8c3b947667244d30dd30dae89a_9", "text": "Note also tat for all experiments, the POS and label embedding features of Chen and Manning (2014) are fine-tuned, consistent with their original method."}
{"sent_id": "14fa8c3b947667244d30dd30dae89a-C001-106", "intents": ["@USE@"], "paper_id": "ABC_14fa8c3b947667244d30dd30dae89a_9", "text": "We test the effect of dropout (Hinton et al., 2012) during training, using a default ratio of 0.5 according to Chen and Manning (2014) ."}
{"sent_id": "14fa8c3b947667244d30dd30dae89a-C001-111", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_14fa8c3b947667244d30dd30dae89a_9", "text": "Our logistic regression linear parser and re-implementation of Chen and Manning (2014) give comparable accuracies to the perceptron ZPar 2 and Stanford NN Parser 3 , respectively."}
{"sent_id": "14fa8c3b947667244d30dd30dae89a-C001-133", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_14fa8c3b947667244d30dd30dae89a_9", "text": "In particular, the method of Chen and Manning (2014) is the same as our NN baseline."}
{"sent_id": "14fa8c3b947667244d30dd30dae89a-C001-118", "intents": ["@SIM@"], "paper_id": "ABC_14fa8c3b947667244d30dd30dae89a_9", "text": "Our parser enjoys the fast speed of deterministic parsers, and in particular the baseline NN parser (Chen and Manning, 2014) ."}
{"sent_id": "7be8bcb17980dee5e94df9faec8183-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_7be8bcb17980dee5e94df9faec8183_9", "text": "Whereas early approaches to unsupervised text segmentation measured the coherence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000) , more recent methods (Misra et al., 2009; Riedl and Biemann, 2012) addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics."}
{"sent_id": "7be8bcb17980dee5e94df9faec8183-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_7be8bcb17980dee5e94df9faec8183_9", "text": "More recent models (Misra et al., 2009; Riedl and Biemann, 2012) employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003) ."}
{"sent_id": "7be8bcb17980dee5e94df9faec8183-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_7be8bcb17980dee5e94df9faec8183_9", "text": "Misra et al. (2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas Riedl and Biemann (2012) introduced TopicTiling, an LDA-driven extension of Hearst's TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors)."}
{"sent_id": "7be8bcb17980dee5e94df9faec8183-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_7be8bcb17980dee5e94df9faec8183_9", "text": "Riedl and Biemann (2012) show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009 ) and that it is also faster than other LDA-based methods (Misra et al., 2009 )."}
{"sent_id": "7be8bcb17980dee5e94df9faec8183-C001-85", "intents": ["@BACK@"], "paper_id": "ABC_7be8bcb17980dee5e94df9faec8183_9", "text": "4 Both LDA-based models (Misra et al., 2009; Riedl and Biemann, 2012) and GRAPHSEG rely on corpus-derived word representations."}
{"sent_id": "7be8bcb17980dee5e94df9faec8183-C001-105", "intents": ["@BACK@"], "paper_id": "ABC_7be8bcb17980dee5e94df9faec8183_9", "text": "Despite the fact that they use different documents for training the topic models from those used for evaluating segmentation quality, the evaluation is still tainted because snippets from the original documents appear in multiple artificial documents -some of which belong to the the training set and others to the test set, as admitted by Riedl and Biemann (2012) and this is why their reported performance on this dataset is overestimated."}
{"sent_id": "7be8bcb17980dee5e94df9faec8183-C001-22", "intents": ["@SIM@"], "paper_id": "ABC_7be8bcb17980dee5e94df9faec8183_9", "text": "Like the majority of TS methods (Hearst, 1994; Brants et al., 2002; Misra et al., 2009; Riedl and Biemann, 2012) , in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009 )."}
{"sent_id": "7be8bcb17980dee5e94df9faec8183-C001-98", "intents": ["@SIM@"], "paper_id": "ABC_7be8bcb17980dee5e94df9faec8183_9", "text": "cause other models, e.g., TopicTiling (Riedl and Biemann, 2012) , also have parameters (e.g., number of topics for the topic model) which are optimized using cross-validation."}
{"sent_id": "7be8bcb17980dee5e94df9faec8183-C001-88", "intents": ["@USE@"], "paper_id": "ABC_7be8bcb17980dee5e94df9faec8183_9", "text": "This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of Riedl and Biemann (2012) and (2) domain-specific embeddings for the GRAPHSEG algorithm."}
{"sent_id": "7be8bcb17980dee5e94df9faec8183-C001-92", "intents": ["@USE@"], "paper_id": "ABC_7be8bcb17980dee5e94df9faec8183_9", "text": "Following Riedl and Biemann (2012) , we set k to half of the document length divided by the number of gold segments."}
{"sent_id": "7be8bcb17980dee5e94df9faec8183-C001-102", "intents": ["@DIF@"], "paper_id": "ABC_7be8bcb17980dee5e94df9faec8183_9", "text": "GRAPHSEG performs competitively, outperforming all methods but (Fragkou et al., 2004) and domain-adapted versions of LDA-based models (Misra et al., 2009; Riedl and Biemann, 2012) ."}
{"sent_id": "7be8bcb17980dee5e94df9faec8183-C001-110", "intents": ["@DIF@"], "paper_id": "ABC_7be8bcb17980dee5e94df9faec8183_9", "text": "This result contrasts previous findings (Misra et al., 2009; Riedl and Biemann, 2012) in which the performance boost was credited to the indomain trained topics and supports our hypothesis that the performance boost of the LDA-based methods' with in-domain trained topics originates from information leakage between different portions of the synthetic Choi dataset."}
{"sent_id": "8e738a8f52e5931a92c9e4577a1ad3-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_8e738a8f52e5931a92c9e4577a1ad3_9", "text": "Unlike the dataset of Specia et al. (2012) , sentences in their dataset contain at least one complex word, but they might contain more than one complex word."}
{"sent_id": "8e738a8f52e5931a92c9e4577a1ad3-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_8e738a8f52e5931a92c9e4577a1ad3_9", "text": "English lexical simplification datasets (Specia et al., 2012; De Belder and Moens, 2012) do not have this problem because both of them use a balanced corpus of English (Sharoff, 2006) ."}
{"sent_id": "8e738a8f52e5931a92c9e4577a1ad3-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_8e738a8f52e5931a92c9e4577a1ad3_9", "text": "De Belder and Moens (2012) allow ties in simplification ranking and report considerably higher agreement among annotators than Specia et al. (2012) ."}
{"sent_id": "8e738a8f52e5931a92c9e4577a1ad3-C001-50", "intents": ["@DIF@"], "paper_id": "ABC_8e738a8f52e5931a92c9e4577a1ad3_9", "text": "Spearman's score of this work was lower than that of Specia et al. (2012) by 0.064."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-3", "intents": ["@BACK@", "@USE@", "@EXT@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "Recently, Wang et al. (2018) proposed a novel reconstruction-based approach to alleviating dropped pronoun (DP) translation problems for neural machine translation models."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "A number of approaches have been investigated for DP translation (Le Nagard and Koehn, 2010; Xiang et al., 2013; Wang et al., 2016 Wang et al., , 2018 ."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "Wang et al. (2018) is a pioneering work to model DP translation for neural machine trans- * Zhaopeng Tu is the corresponding author of the paper."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "As shown in Figure 1 , Wang et al. (2018) introduced two independent reconstructors with their own parameters, which reconstruct the DPannotated source sentence from the encoder and decoder hidden states, respectively."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "As shown in Table 1 , Wang et al. (2016 Wang et al. ( , 2018 explored to predict the exact DP words 1 , the accuracy of which is only 66% in F1-score."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-84", "intents": ["@USE@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "To compare our work with the results reported by previous work (Wang et al., 2018) , we conducted experiments on their released Chinese⇒English TV Subtitle corpus."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-87", "intents": ["@USE@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "We implemented our models on the code repository released by Wang et al. (2018) ."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-95", "intents": ["@USE@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "\"Separate-Recs⇒(+DPs)\" (Row 3) is the best model reported in Wang et al. (2018) , which we employed as another strong baseline."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-25", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "Experimental results on a largescale Chinese-English subtitle corpus show that the two modifications can accumulatively improve translation performance, and the best result is +1.5 BLEU points better than that reported by Wang et al. (2018) ."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-79", "intents": ["@DIF@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "Different from Wang et al. (2018), we reconstruct DPP-annotated source sentence, which is predicted by an external model."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-89", "intents": ["@DIF@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "It should be emphasized that we did not use the pre-train strategy as done in Wang et al. (2018) , since we found training from scratch achieved a better performance in the shared reconstructor setting."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-101", "intents": ["@DIF@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by Wang et al. (2018) (Row 3) ."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-111", "intents": ["@DIF@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "We can see that the proposed model outperforms both the strong baseline and the best model reported in Wang et al. (2018) ."}
{"sent_id": "ab6f114b2ce4e62e6d8a639e8183eb-C001-103", "intents": ["@SIM@"], "paper_id": "ABC_ab6f114b2ce4e62e6d8a639e8183eb_9", "text": "Similar to Wang et al. (2018) , the proposed approach improves BLEU scores at the cost of decreased training and decoding speed, which is due to the large number of newly introduced parameters resulting from the incorporation of reconstructors into the NMT model."}
{"sent_id": "8abb7b77fd6996a905395de9693d42-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_8abb7b77fd6996a905395de9693d42_9", "text": "In our previous work (Papegnies et al., 2019) , we proposed a radically different method that completely ignores the textual content of the messages, and relies only on a graph-based modeling of the conversation."}
{"sent_id": "8abb7b77fd6996a905395de9693d42-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_8abb7b77fd6996a905395de9693d42_9", "text": "Additional references on abusive message detection and conversational network modeling can be found in (Papegnies et al., 2019) ."}
{"sent_id": "8abb7b77fd6996a905395de9693d42-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_8abb7b77fd6996a905395de9693d42_9", "text": "In this section, we summarize the content-based method from (Papegnies et al., 2017b ) (Section 2.1) and the graph-based method from (Papegnies et al., 2019 ) (Section 2.2)."}
{"sent_id": "8abb7b77fd6996a905395de9693d42-C001-188", "intents": ["@BACK@"], "paper_id": "ABC_8abb7b77fd6996a905395de9693d42_9", "text": "The Graph-Based TF are discussed in depth in our previous article (Papegnies et al., 2019) ."}
{"sent_id": "8abb7b77fd6996a905395de9693d42-C001-38", "intents": ["@EXT@", "@USE@"], "paper_id": "ABC_8abb7b77fd6996a905395de9693d42_9", "text": "For this purpose, we take advantage of the content- (Papegnies et al., 2017b) and graph-based (Papegnies et al., 2019 ) methods that we previously developed."}
{"sent_id": "8abb7b77fd6996a905395de9693d42-C001-204", "intents": ["@EXT@"], "paper_id": "ABC_8abb7b77fd6996a905395de9693d42_9", "text": "We take advantage of the methods that we previously developed to leverage message content (Papegnies et al., 2017a) and interactions between users (Papegnies et al., 2019) , and create a new method using both types of information simultaneously."}
{"sent_id": "8abb7b77fd6996a905395de9693d42-C001-53", "intents": ["@USE@"], "paper_id": "ABC_8abb7b77fd6996a905395de9693d42_9", "text": "Existing methods refers to our previous work described in (Papegnies et al., 2017b ) (content-based method) and (Papegnies et al., 2019) (graph-based method), whereas the contribution presented in this article appears on the right side (fusion strategies)."}
{"sent_id": "8abb7b77fd6996a905395de9693d42-C001-105", "intents": ["@USE@"], "paper_id": "ABC_8abb7b77fd6996a905395de9693d42_9", "text": "In this work, we use exactly the same measures as in (Papegnies et al., 2019) ."}
{"sent_id": "8abb7b77fd6996a905395de9693d42-C001-129", "intents": ["@USE@"], "paper_id": "ABC_8abb7b77fd6996a905395de9693d42_9", "text": "The dataset is the same as in our previous publications (Papegnies et al., 2017b (Papegnies et al., , 2019 ."}
{"sent_id": "8abb7b77fd6996a905395de9693d42-C001-139", "intents": ["@USE@"], "paper_id": "ABC_8abb7b77fd6996a905395de9693d42_9", "text": "We use the values matching the best performance, obtained during the greedy search of the parameter space performed in (Papegnies et al., 2019) ."}
{"sent_id": "8abb7b77fd6996a905395de9693d42-C001-146", "intents": ["@USE@"], "paper_id": "ABC_8abb7b77fd6996a905395de9693d42_9", "text": "Table 1 presents the Precision, Recall and F -measure scores obtained on the Abuse class, for both baselines (Content-based (Papegnies et al., 2017b) and Graph-based (Papegnies et al., 2019) ) and all three proposed fusion strategies (Early Fusion, Late Fusion and Hybrid Fusion)."}
{"sent_id": "8abb7b77fd6996a905395de9693d42-C001-81", "intents": ["@MOT@"], "paper_id": "ABC_8abb7b77fd6996a905395de9693d42_9", "text": "It completely ignores the content of the messages, and only focuses on the dynamics of the conversation, based on the interactions between its participants (Papegnies et al., 2019) ."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-3", "intents": ["@USE@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "Our shared task submission adopts the bidirectional LSTM-CNN model of Chiu and Nichols (2016), as it has been shown to perform well on both newswire and Web texts."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-15", "intents": ["@USE@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "Our shared task submission is based on the model of Chiu and Nichols (2016) , a hybrid model of bidirectional long short-term memory (BLSTM) networks and convolutional neural networks (CNN) that automatically learns both character-and word-level features, and which holds the current state-of-the-art on both newswire texts (CoNLL 2003) and diverse corpora including Web texts (OntoNotes 5.0)."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-21", "intents": ["@USE@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "In Section 2, we describe the adaptations made to Chiu and Nichols (2016) 's model."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-33", "intents": ["@USE@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "Our system is based on the BLSTM-CNN model of Chiu and Nichols (2016) , and, unless otherwise noted, follows their training and tagging methodology, which the reader is referred to for more details."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-42", "intents": ["@USE@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "The neural embeddings of Collobert et al. (2011) were chosen because Chiu and Nichols (2016) reported them to be the highest performing on both CoNLL-2003 and OntoNotes 5.0 datasets."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-48", "intents": ["@USE@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "Following Chiu and Nichols (2016) , we use a CNN to extract features from 25 dim."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-62", "intents": ["@USE@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "To generate lexicon features, we apply the partial matching algorithm of Chiu and Nichols (2016) to the input text, as shown in Figure 2 ."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-69", "intents": ["@USE@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "Following Chiu and Nichols (2016), we used different symbols for word-level capitalization feature each assigned a randomly initialized embedding: allCaps, upperInitial, lowercase, mixedCaps and noinfo."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-87", "intents": ["@USE@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "We follow the training and inference methodology of Chiu and Nichols (2016) , training our neural network to maximize the sentence-level log-likelihood from Collobert et al. (2011) ."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-207", "intents": ["@USE@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "Our system adopts the architecture of Chiu and Nichols (2016) , which combined BLSTMs to maximize context over the tagged word sequence and word-level CNNs to automatically generate characterlevel features with a partial-matching lexicon to achieve the state-of-the-art for NER on both CoNLL 2003 and OntoNotes datasets."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-211", "intents": ["@USE@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "In this paper, we described the DeepNNNER entry to the WNUT 2016 Shared Task #2: Named Entity Recognition in Twitter, which adopted the BLSTM-CNN model of Chiu and Nichols (2016) ."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "Recently, neural network models -especially those that use recursive models -have shown that state of the art performance can be achieved with little feature engineering (Collobert et al., 2011; Santos et al., 2015; Chiu and Nichols, 2016) ."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-190", "intents": ["@BACK@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "For a more detailed survey, see (Chiu and Nichols, 2016) ."}
{"sent_id": "05bf376f0a18cf313ead7189b029b6-C001-19", "intents": ["@EXT@"], "paper_id": "ABC_05bf376f0a18cf313ead7189b029b6_9", "text": "Our primary contribution is adapting the model of Chiu and Nichols (2016) to Twitter data by developing a text normalization method to effectively apply word embeddings to large vocabulary Web texts and automatically constructing lexicons for the shared task's target NE classes from publicly-available sources."}
{"sent_id": "79ac70221fa28e577876425cad627f-C001-12", "intents": ["@DIF@"], "paper_id": "ABC_79ac70221fa28e577876425cad627f_10", "text": "Compared to meta-learning baselines and recent approaches which use language supervision as a more fundamental bottleneck in a model [1] , we find this simple auxiliary training objective results in learned representations that generalize better to new concepts."}
{"sent_id": "79ac70221fa28e577876425cad627f-C001-19", "intents": ["@DIF@"], "paper_id": "ABC_79ac70221fa28e577876425cad627f_10", "text": "[13] use METEOR scores between captions as a similarity metric for specializing embeddings for image retrieval, but do not directly Figure 1 : Building on prototype networks [26] , we propose few-shot classification models whose learned representations are constrained to predict natural language descriptions of the task during training, in contrast to models [1] which explicitly use language as a bottleneck for classification."}
{"sent_id": "79ac70221fa28e577876425cad627f-C001-113", "intents": ["@DIF@"], "paper_id": "ABC_79ac70221fa28e577876425cad627f_10", "text": "This differs slightly from [1] , who use different numbers of epochs per model and did not specify how they were chosen; otherwise, the training and evaluation process is the same."}
{"sent_id": "79ac70221fa28e577876425cad627f-C001-115", "intents": ["@DIF@", "@SIM@", "@USE@"], "paper_id": "ABC_79ac70221fa28e577876425cad627f_10", "text": "We recreated the ShapeWorld dataset using the same code as [1] , except generating 4x as many test tasks (4000 vs 1000) for more stable confidence intervals."}
{"sent_id": "79ac70221fa28e577876425cad627f-C001-116", "intents": ["@DIF@"], "paper_id": "ABC_79ac70221fa28e577876425cad627f_10", "text": "Note that results for both L3 and Baseline (Meta) are 3-4 points lower than the scores of the corresponding implementations in [1] ."}
{"sent_id": "79ac70221fa28e577876425cad627f-C001-23", "intents": ["@SIM@"], "paper_id": "ABC_79ac70221fa28e577876425cad627f_10", "text": "Our work is most similar to [1] , which we describe and compare to later."}
{"sent_id": "79ac70221fa28e577876425cad627f-C001-42", "intents": ["@SIM@"], "paper_id": "ABC_79ac70221fa28e577876425cad627f_10", "text": "With this component, we call our approach language-shaped learning (LSL) (Figure 1 )."}
{"sent_id": "79ac70221fa28e577876425cad627f-C001-44", "intents": ["@SIM@"], "paper_id": "ABC_79ac70221fa28e577876425cad627f_10", "text": "LSL is similar to another recent model for this setting: Learning with Latent Language (L3) [1] , which proposes to use language not only as a supervision source, but as a bottleneck for classification ( Figure 1 )."}
{"sent_id": "79ac70221fa28e577876425cad627f-C001-100", "intents": ["@SIM@"], "paper_id": "ABC_79ac70221fa28e577876425cad627f_10", "text": "A.1 ShapeWorld f θ . Like [1] , f θ starts with features extracted from the last convolutional layer of a fixed ImageNetpretrained VGG-19 network [25] ."}
{"sent_id": "79ac70221fa28e577876425cad627f-C001-109", "intents": ["@SIM@"], "paper_id": "ABC_79ac70221fa28e577876425cad627f_10", "text": "Like [1] , a total of 10 descriptions per task are sampled at test time."}
{"sent_id": "79ac70221fa28e577876425cad627f-C001-55", "intents": ["@USE@"], "paper_id": "ABC_79ac70221fa28e577876425cad627f_10", "text": "First, we use the ShapeWorld [20] dataset devised by [1] , which consists of 9000 training, 1000 validation, and 4000 test tasks ( Figure 2 )."}
{"sent_id": "79ac70221fa28e577876425cad627f-C001-59", "intents": ["@USE@"], "paper_id": "ABC_79ac70221fa28e577876425cad627f_10", "text": "Model details are identical to [1] for easy comparison."}
{"sent_id": "ecb6e93a5254b86ef49a5ffd0a52a0-C001-28", "intents": ["@USE@"], "paper_id": "ABC_ecb6e93a5254b86ef49a5ffd0a52a0_10", "text": "The second subtask can be stated as follows (Brill and Moore, 2000) : Given an alphabet Σ, a word list D of strings ∈ Σ * , and a string r / ∈ D and ∈ Σ * , find w ∈ D such that w is the most likely correction."}
{"sent_id": "ecb6e93a5254b86ef49a5ffd0a52a0-C001-142", "intents": ["@USE@"], "paper_id": "ABC_ecb6e93a5254b86ef49a5ffd0a52a0_10", "text": "7 In order to rank the words as candidate corrections for a misspelling r, P L (r|w) and P P HL (r|w) are calculated for each word in the word list using the algorithm described in Brill and Moore (2000) ."}
{"sent_id": "ecb6e93a5254b86ef49a5ffd0a52a0-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_ecb6e93a5254b86ef49a5ffd0a52a0_10", "text": "The spelling error model proposed by Brill and Moore (2000) allows generic string edit operations up to a certain length."}
{"sent_id": "ecb6e93a5254b86ef49a5ffd0a52a0-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_ecb6e93a5254b86ef49a5ffd0a52a0_10", "text": "Brill and Moore (2000) estimate the probability of each edit from a corpus of spelling errors."}
{"sent_id": "ecb6e93a5254b86ef49a5ffd0a52a0-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_ecb6e93a5254b86ef49a5ffd0a52a0_10", "text": "They show that including pronunciation information improves performance as compared to Brill and Moore (2000) ."}
{"sent_id": "ecb6e93a5254b86ef49a5ffd0a52a0-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_ecb6e93a5254b86ef49a5ffd0a52a0_10", "text": "1 Brill and Moore (2000) allow all edit operations α → β where Σ is the alphabet and α, β ∈ Σ * , with a constraint on the length of α and β."}
{"sent_id": "ecb6e93a5254b86ef49a5ffd0a52a0-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_ecb6e93a5254b86ef49a5ffd0a52a0_10", "text": "This error model over letters, called P L , is approximated by Brill and Moore (2000) as shown in Figure 1 by considering only the pair of partitions of w and r with the maximum product of the probabilities of individual substitutions."}
{"sent_id": "ecb6e93a5254b86ef49a5ffd0a52a0-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_ecb6e93a5254b86ef49a5ffd0a52a0_10", "text": "The method, which is described in detail in Brill and Moore (2000) , involves aligning the letters in pairs of words and misspellings, expanding each alignment with up to N neighboring alignments, and calculating the probability of each α → β alignment."}
{"sent_id": "c4e2a9322471fb5988a5bd737fa51e-C001-24", "intents": ["@USE@"], "paper_id": "ABC_c4e2a9322471fb5988a5bd737fa51e_10", "text": "We evaluate and compare our proposed system both on our new multi-target UK election dataset, as well as on the benchmarking dataset for single-target dependent sentiment (Dong et al., 2014) ."}
{"sent_id": "c4e2a9322471fb5988a5bd737fa51e-C001-114", "intents": ["@USE@"], "paper_id": "ABC_c4e2a9322471fb5988a5bd737fa51e_10", "text": "Data set: We evaluate and compare our proposed system to the state-of-the-art baselines on a benchmarking corpus (Dong et al., 2014 ) that has been used by several previous studies (Vo and Zhang, 2015; Tang et al., 2016a; Zhang et al., 2016) ."}
{"sent_id": "c4e2a9322471fb5988a5bd737fa51e-C001-148", "intents": ["@USE@"], "paper_id": "ABC_c4e2a9322471fb5988a5bd737fa51e_10", "text": "We report our experimental results in Table 2 on the single-target benchmarking corpus (Dong et al., 2014) , with three model categories: 1) tweet-level target-independent models, 2) targetdependent models without considering the 'sametarget-multi-appearance' scenario and 3) targetdependent models incorporating the 'same-targetmulti-appearance' scenario."}
{"sent_id": "c4e2a9322471fb5988a5bd737fa51e-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_c4e2a9322471fb5988a5bd737fa51e_10", "text": "Recent work (Dong et al., 2014) used recursive neural networks and adaptively chose composition functions to combine child feature vectors according to their dependency type, to reflect sentiment signal propagation to the target."}
{"sent_id": "c4e2a9322471fb5988a5bd737fa51e-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_c4e2a9322471fb5988a5bd737fa51e_10", "text": "Despite not fully capturing semantic and syntactic information given the target entity, they show a much better performance than Dong et al. (2014) , indicating useful signals in relation to the target can be drawn from such context representation."}
{"sent_id": "c4e2a9322471fb5988a5bd737fa51e-C001-151", "intents": ["@BACK@"], "paper_id": "ABC_c4e2a9322471fb5988a5bd737fa51e_10", "text": "Interestingly they also perform better than some of the targetdependent baseline systems, namely SVM-dep (Jiang et al., 2011) , Recursive NN and AdaRNN (Dong et al., 2014) , showing the difficulty of fully extracting and incorporating target information in tweets."}
{"sent_id": "c4e2a9322471fb5988a5bd737fa51e-C001-154", "intents": ["@BACK@"], "paper_id": "ABC_c4e2a9322471fb5988a5bd737fa51e_10", "text": "The adaptive recursive neural network, namely AdaRNN (Dong et al., 2014) , adaptively selects composition functions based on the input data and thus performs better than a standard recursive neural network model (Recursive NN (Dong et al., 2014) )."}
{"sent_id": "c4e2a9322471fb5988a5bd737fa51e-C001-90", "intents": ["@DIF@"], "paper_id": "ABC_c4e2a9322471fb5988a5bd737fa51e_10", "text": "As many as 3,713 tweets have more than a single entity mention (target) per tweet, which makes the task different from 2015 Semeval 10 subtask C (Rosenthal et al., 2015) and a target-dependent benchmarking dataset of Dong et al. (2014) where each tweet has only one target annotated and thus one sentiment label assigned."}
{"sent_id": "c4e2a9322471fb5988a5bd737fa51e-C001-119", "intents": ["@UNSURE@"], "paper_id": "ABC_c4e2a9322471fb5988a5bd737fa51e_10", "text": "While it isn't clear if Dong et al. (2014) and Tang et al. (2016a) have considered this realistic same-target-multiappearance scenario, Vo et al. (2015) and Zhang et al. (2016) do not take it into account when extracting target-dependent contexts."}
{"sent_id": "e831e058f208542af16c1ea236d2c9-C001-5", "intents": ["@DIF@"], "paper_id": "ABC_e831e058f208542af16c1ea236d2c9_10", "text": "These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data [Charniak and Johnson 2001] , and 20.44% percent relative error reduction in F-score over the latest best result where punctuation is excluded from the training and testing data [Johnson and Charniak 2004] ."}
{"sent_id": "e831e058f208542af16c1ea236d2c9-C001-18", "intents": ["@DIF@"], "paper_id": "ABC_e831e058f208542af16c1ea236d2c9_10", "text": "These steps result in a significant improvement in F-score over the earlier best result reported in [Charniak and Johnson 2001] , where punctuation is included in both the training and testing data of the Switchboard corpus, and a significant error reduction in F-score over the latest best result [Johnson and Charniak 2004] , where punctuation is ignored in both the training and testing data of the Switchboard corpus."}
{"sent_id": "e831e058f208542af16c1ea236d2c9-C001-85", "intents": ["@DIF@"], "paper_id": "ABC_e831e058f208542af16c1ea236d2c9_10", "text": "We relax the definition for rough copy, because more than 94% of all edits have both reparandum and repair, while the rough copy defined in [Charniak and Johnson 2001] only covers 77.66% of such instances."}
{"sent_id": "e831e058f208542af16c1ea236d2c9-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_e831e058f208542af16c1ea236d2c9_10", "text": "Because of the availability of the Switchboard corpus [Godfrey et al. 1992] and other conversational telephone speech (CTS) corpora, there has been an increasing interest in improving the performance of identifying the edited regions for parsing disfluent sentences [Charniak and Johnson 2001 , Johnson and Charniak 2004 , Liu et al. 2005 ."}
{"sent_id": "e831e058f208542af16c1ea236d2c9-C001-72", "intents": ["@BACK@"], "paper_id": "ABC_e831e058f208542af16c1ea236d2c9_10", "text": "In [Charniak and Johnson 2001] , identifying edited regions is considered as a classification problem, where each word is classified either as edited or normal."}
{"sent_id": "e831e058f208542af16c1ea236d2c9-C001-110", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_e831e058f208542af16c1ea236d2c9_10", "text": "We used the exactly same training and testing data from the Switchboard corpus as in [Charniak and Johnson 2001] ."}
{"sent_id": "e831e058f208542af16c1ea236d2c9-C001-117", "intents": ["@SIM@"], "paper_id": "ABC_e831e058f208542af16c1ea236d2c9_10", "text": "These results are comparable with the results from [Charniak & Johnson 2001] , i.e., 95.2%, 67.8%, and 79.2% for precision, recall, and f-score, correspondingly."}
{"sent_id": "e831e058f208542af16c1ea236d2c9-C001-109", "intents": ["@MOT@"], "paper_id": "ABC_e831e058f208542af16c1ea236d2c9_10", "text": "Since the original code from [Charniak and Johnson 2001] is not available, we conducted our first experiment to replicate the result of their baseline system described in section 3."}
{"sent_id": "e831e058f208542af16c1ea236d2c9-C001-55", "intents": ["@USE@"], "paper_id": "ABC_e831e058f208542af16c1ea236d2c9_10", "text": "We take as our baseline system the work by [Charniak and Johnson 2001] ."}
{"sent_id": "e831e058f208542af16c1ea236d2c9-C001-65", "intents": ["@USE@"], "paper_id": "ABC_e831e058f208542af16c1ea236d2c9_10", "text": "We re-implement the boosting algorithm reported by [Charniak and Johnson 2001]"}
{"sent_id": "e831e058f208542af16c1ea236d2c9-C001-144", "intents": ["@UNSURE@"], "paper_id": "ABC_e831e058f208542af16c1ea236d2c9_10", "text": "We observed a 43.98% relative error reduction on F-scores for the baseline with punctuation in both training and testing [Charniak and Johnson 2001] ."}
{"sent_id": "b1c9b8e24916b136948610383f8ea2-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_b1c9b8e24916b136948610383f8ea2_10", "text": "Hence, there has been a massive increase in work on MT systems that involve more than two languages (Dong et al., 2015; Firat et al., 2016a; Cheng et al., 2017; Johnson et al., 2017; Chen et al., 2017 Neubig and Hu, 2018) etc."}
{"sent_id": "b1c9b8e24916b136948610383f8ea2-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_b1c9b8e24916b136948610383f8ea2_10", "text": "In addition, MNMT systems will be compact, because a single model handles translations for multiple languages (Johnson et al., 2017) ."}
{"sent_id": "b1c9b8e24916b136948610383f8ea2-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_b1c9b8e24916b136948610383f8ea2_10", "text": "Zeroshot translation: Translating between language pairs without parallel corpora (Johnson et al., 2017) ."}
{"sent_id": "b1c9b8e24916b136948610383f8ea2-C001-92", "intents": ["@BACK@"], "paper_id": "ABC_b1c9b8e24916b136948610383f8ea2_10", "text": "In fact, the number of parameters is only a small multiple of the compact model (the multiplication factor accounts for the language embedding size) (Johnson et al., 2017) , but the language embeddings can directly impact the model parameters instead of the weak influence that language tags have."}
{"sent_id": "b1c9b8e24916b136948610383f8ea2-C001-102", "intents": ["@BACK@"], "paper_id": "ABC_b1c9b8e24916b136948610383f8ea2_10", "text": "Each of these methods provide gains over Johnson et al. (2017) , and combining all gave the best results."}
{"sent_id": "b1c9b8e24916b136948610383f8ea2-C001-109", "intents": ["@BACK@"], "paper_id": "ABC_b1c9b8e24916b136948610383f8ea2_10", "text": "Mini-batches can be comprised of a mix of samples from different language pairs (Johnson et al., 2017) or the training schedule can cycle through mini-batches consisting of a language pair only (Firat et al., 2016a) ."}
{"sent_id": "b1c9b8e24916b136948610383f8ea2-C001-168", "intents": ["@BACK@"], "paper_id": "ABC_b1c9b8e24916b136948610383f8ea2_10", "text": "Zero-shot NMT was first demonstrated by Johnson et al. (2017) ."}
{"sent_id": "b1c9b8e24916b136948610383f8ea2-C001-263", "intents": ["@BACK@"], "paper_id": "ABC_b1c9b8e24916b136948610383f8ea2_10", "text": "As there are always vocabulary overlaps between different domains, there are no zero-shot translation (Johnson et al., 2017) settings in domain adaptation."}
{"sent_id": "b1c9b8e24916b136948610383f8ea2-C001-286", "intents": ["@FUT@"], "paper_id": "ABC_b1c9b8e24916b136948610383f8ea2_10", "text": "The compact MNMT models can handle code-mixed input, but code-mixed output remains an open problem (Johnson et al., 2017) ."}
{"sent_id": "ffd65a1a02c852a2670b471fb4b110-C001-99", "intents": ["@UNSURE@"], "paper_id": "ABC_ffd65a1a02c852a2670b471fb4b110_10", "text": "We conclude from Model Accuracy Random 0.50 NN (Van de Cruys, 2014) 0.68 NN+WK (Wang et al., 2018) 0.76 Fine-tuned BERT 0.89 these results that distributional data does provide a strong cue for semantic plausibility in the supervised setting of Wang et al. (2018) ."}
{"sent_id": "ffd65a1a02c852a2670b471fb4b110-C001-19", "intents": ["@DIF@"], "paper_id": "ABC_ffd65a1a02c852a2670b471fb4b110_10", "text": "We show that in the original supervised setting a distributional model, namely a novel application of BERT (Devlin et al., 2019) , significantly outperforms the best existing method which has access to manually labeled physical features (Wang et al., 2018) ."}
{"sent_id": "ffd65a1a02c852a2670b471fb4b110-C001-22", "intents": ["@USE@"], "paper_id": "ABC_ffd65a1a02c852a2670b471fb4b110_10", "text": "We create a training set by parsing and extracting attested s-v-o triples from English Wikipedia, and we provide a baseline for training on this dataset and evaluating on Wang et al. (2018) 's physical plausibility task."}
{"sent_id": "ffd65a1a02c852a2670b471fb4b110-C001-24", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_ffd65a1a02c852a2670b471fb4b110_10", "text": "arXiv:1911.05689v1 [cs.CL] 13 Nov 2019 Wang et al. (2018) present the semantic plausibility dataset that we use for evaluation in this work, and they show that distributional methods fail on this dataset."}
{"sent_id": "ffd65a1a02c852a2670b471fb4b110-C001-49", "intents": ["@USE@"], "paper_id": "ABC_ffd65a1a02c852a2670b471fb4b110_10", "text": "We use Wang et al. (2018) 's physical plausibility dataset for evaluation."}
{"sent_id": "ffd65a1a02c852a2670b471fb4b110-C001-56", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_ffd65a1a02c852a2670b471fb4b110_10", "text": "We follow the same evaluation procedure as previous work and perform cross validation on the 3,062 labeled triples (Wang et al., 2018) ."}
{"sent_id": "ffd65a1a02c852a2670b471fb4b110-C001-97", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_ffd65a1a02c852a2670b471fb4b110_10", "text": "For the supervised setting, we follow the same evaluation procedure as Wang et al. (2018) : we perform 10-fold cross validation on the dataset of 3,062 s-v-o triples, and report the mean accuracy of running this procedure 20 times all with the same model initialization (Table 3) ."}
{"sent_id": "ffd65a1a02c852a2670b471fb4b110-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_ffd65a1a02c852a2670b471fb4b110_10", "text": "Current approaches to selectional preference are distributional (Erk et al., 2010; Van de Cruys, 2014) and have shown limited performance in capturing semantic plausibility (Wang et al., 2018) ."}
{"sent_id": "ffd65a1a02c852a2670b471fb4b110-C001-75", "intents": ["@SIM@"], "paper_id": "ABC_ffd65a1a02c852a2670b471fb4b110_10", "text": "We reproduce the results of Wang et al. (2018) using GloVe embeddings and the same hyperparameter settings."}
{"sent_id": "ffd65a1a02c852a2670b471fb4b110-C001-66", "intents": ["@EXT@"], "paper_id": "ABC_ffd65a1a02c852a2670b471fb4b110_10", "text": "For evaluation, we split Wang et al. (2018)'s 3,062 triples into equal sized validation and test sets."}
{"sent_id": "183cf87042a3ad2180ead67555d247-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_183cf87042a3ad2180ead67555d247_10", "text": "Recently, pre-trained language representation models such as GPT (Radford et al., 2018 (Radford et al., , 2019 , ELMo (Peters et al., 2018) , BERT (Devlin et al., 2019) and XLNet have achieved promising results in NLP tasks, including reading comprehension (Rajpurkar et al., 2016) , natural language inference (Bowman et al., 2015; Williams et al., 2018) and sentiment classification (Socher et al., 2013) ."}
{"sent_id": "183cf87042a3ad2180ead67555d247-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_183cf87042a3ad2180ead67555d247_10", "text": "Thus contextual language representation based on pre-trained models including CoVe (McCann et al., 2017) , ELMo (Peters et al., 2018) , GPT (Radford et al., 2018 (Radford et al., , 2019 and BERT (Devlin et al., 2019) becomes prevalent recently."}
{"sent_id": "183cf87042a3ad2180ead67555d247-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_183cf87042a3ad2180ead67555d247_10", "text": "Various pre-training tasks were explored including traditional NLP tasks like machine translation (Mc-Cann et al., 2017) and language model (Peters et al., 2018; Radford et al., 2018 Radford et al., , 2019 , or other tasks such as masked language model and next sentence prediction (Devlin et al., 2019) ."}
{"sent_id": "183cf87042a3ad2180ead67555d247-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_183cf87042a3ad2180ead67555d247_10", "text": "With the advent of BERT (Devlin et al., 2019) achieving state-of-the-art performances on various NLP tasks, many variants of BERT have been proposed."}
{"sent_id": "183cf87042a3ad2180ead67555d247-C001-12", "intents": ["@MOT@"], "paper_id": "ABC_183cf87042a3ad2180ead67555d247_10", "text": "Although pre-trained language representation models have achieved transformative performance, the pre-training tasks like masked language model and next sentence prediction (Devlin et al., 2019) neglect to consider the linguistic knowledge."}
{"sent_id": "183cf87042a3ad2180ead67555d247-C001-64", "intents": ["@DIF@"], "paper_id": "ABC_183cf87042a3ad2180ead67555d247_10", "text": "Compared with the vanilla pretrained models like BERT (Devlin et al., 2019) , our model enriches the input sequence with its linguistic knowledge including part-of-speech tags and sentiment polarity labels, and utilizes a modified masked language model to capture the relationship between sentence-level sentiment labels and word-level knowledge in addition to context dependency."}
{"sent_id": "183cf87042a3ad2180ead67555d247-C001-141", "intents": ["@DIF@"], "paper_id": "ABC_183cf87042a3ad2180ead67555d247_10", "text": "Due to the sparsity of aspect terms compared with aspect categories, our model improved a larger margin on the task of aspect category sentiment classification than the (Devlin et al., 2019) ."}
{"sent_id": "183cf87042a3ad2180ead67555d247-C001-76", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_183cf87042a3ad2180ead67555d247_10", "text": "During pre-training, Label-aware masked language model (LA-MLM) and next sentence prediction (NSP) are adopted as the pre-training tasks where the setting of NSP is identical to the one proposed by Devlin et al. (2019) ."}
{"sent_id": "183cf87042a3ad2180ead67555d247-C001-95", "intents": ["@USE@"], "paper_id": "ABC_183cf87042a3ad2180ead67555d247_10", "text": "We follow the fine-tuning setting of the existing work (Devlin et al., 2019; : Sentence-level Sentiment Classification: The input of this task is a text sequence ([CLS], x 1 , x 2 , · · · , x n , [SEP])."}
{"sent_id": "183cf87042a3ad2180ead67555d247-C001-107", "intents": ["@USE@"], "paper_id": "ABC_183cf87042a3ad2180ead67555d247_10", "text": "Since our method can adapt to all the BERTstyle pre-training models, we used vanilla BERT (Devlin et al., 2019) as the base framework to construct Transformer blocks in this paper and leave the exploration of other models like RoBERTa as future work."}
{"sent_id": "183cf87042a3ad2180ead67555d247-C001-145", "intents": ["@USE@"], "paper_id": "ABC_183cf87042a3ad2180ead67555d247_10", "text": "Note that we directly used the results of BERT on SST-2, MNLI, QNLI and MRPC which are reported by Devlin et al. (2019) and reimplemented the BERT model fine-tuned on the rest of the tasks by ourselves."}
{"sent_id": "e3b9c00d792bcddb6eea449179e61e-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_e3b9c00d792bcddb6eea449179e61e_10", "text": "Their output exhibits a disproportionate replication of common n-grams and full captions seen in the training set [9, 11, 26] ."}
{"sent_id": "e3b9c00d792bcddb6eea449179e61e-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_e3b9c00d792bcddb6eea449179e61e_10", "text": "The subjectivity in what defines a good caption, has made it difficult to identify a single metric for the overall quality of Image Captioning models [5, 26] ."}
{"sent_id": "e3b9c00d792bcddb6eea449179e61e-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_e3b9c00d792bcddb6eea449179e61e_10", "text": "All these approaches unfortunately have a strong focus on replicating common n-grams from the ground-truth captions [5] and do not take into account the richness and diversity of human expression [9, 26] ."}
{"sent_id": "e3b9c00d792bcddb6eea449179e61e-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_e3b9c00d792bcddb6eea449179e61e_10", "text": "More recently, this concept has been employed as the focus for training and evaluation [26, 29] , and it has been proposed that improving caption diversity leads to more human-like captions [26] ."}
{"sent_id": "e3b9c00d792bcddb6eea449179e61e-C001-46", "intents": ["@USE@"], "paper_id": "ABC_e3b9c00d792bcddb6eea449179e61e_10", "text": "─ novelty -percentage of generated captions where exact duplicates are not found in the training set [11, 26, 29 ] ─ diversity -percentage of distinct captions (where duplicates count as a single distinct caption) out of the total number of generated captions [11] ─ vocabulary size -number of unique words used in generated captions [26]"}
{"sent_id": "e3b9c00d792bcddb6eea449179e61e-C001-136", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_e3b9c00d792bcddb6eea449179e61e_10", "text": "Note that [12, 29] use different data splits, while our models and [26] use the Karpathy 5k splits [17] ."}
{"sent_id": "e3b9c00d792bcddb6eea449179e61e-C001-64", "intents": ["@DIF@"], "paper_id": "ABC_e3b9c00d792bcddb6eea449179e61e_10", "text": "We take a slightly different approach from both the joint training in [22] and recent applications of GAN training in Image Captioning [9, 26] ."}
{"sent_id": "e3b9c00d792bcddb6eea449179e61e-C001-127", "intents": ["@DIF@"], "paper_id": "ABC_e3b9c00d792bcddb6eea449179e61e_10", "text": "The vocabulary size also increases but is lower than in [26] ."}
{"sent_id": "e3b9c00d792bcddb6eea449179e61e-C001-168", "intents": ["@DIF@"], "paper_id": "ABC_e3b9c00d792bcddb6eea449179e61e_10", "text": "Another example of GAN training is [26] where the Discriminator classifies whether a multi-sample set of captions are human-written or generated."}
{"sent_id": "5596207b89d917db38c04af49c08aa-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_5596207b89d917db38c04af49c08aa_10", "text": "Benefiting from the availability of large-scale benchmark datasets such as SQuAD (Rajpurkar et al., 2016) , the attention-based neural networks has spread to machine comprehension and question answering tasks to allow the model to attend over past output vectors (Wang & Jiang, 2017; Seo et al., 2017; Xiong et al., 2017; Hu et al., 2017; Pan et al., 2017) ."}
{"sent_id": "5596207b89d917db38c04af49c08aa-C001-47", "intents": ["@DIF@"], "paper_id": "ABC_5596207b89d917db38c04af49c08aa_10", "text": "Different from the commonly used approaches that every single model has exactly one question and passage encoder (Seo et al., 2017; Hu et al., 2017) , our encoder layers simultaneously calculate multiple question and passage representations, for the purpose of serving different parts of attention functions of different phases."}
{"sent_id": "5596207b89d917db38c04af49c08aa-C001-111", "intents": ["@SIM@"], "paper_id": "ABC_5596207b89d917db38c04af49c08aa_10", "text": "Our directly available baseline is one implementation of MReader, re-named as Iterative Aligner which has very similar results as those of MReader (Hu et al., 2017) 71.1 / 79.5 71.3 / 79.7 75.6 / 82.8 75.9 / 82.9 MReader (Hu et al., 2017) N As shown in Table 3 , in the single model setting, our model PhaseCond is clearly more effective than all the single-layered models (BiDAF and RNET) and multi-layered models (MReader and Iterative Aligner)."}
{"sent_id": "9e0a44722390d0508fbe56785701e6-C001-35", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_9e0a44722390d0508fbe56785701e6_10", "text": "The work of Shwartz et al. (2016) , that we closely follow, is also using both semantic and syntactic features, by combining the dependency paths between entities, with word embedding representations of both the entities and the lemmas in the dependency paths."}
{"sent_id": "9e0a44722390d0508fbe56785701e6-C001-46", "intents": ["@USE@"], "paper_id": "ABC_9e0a44722390d0508fbe56785701e6_10", "text": "This is the approach that Shwartz et al. (2016) and the current work follow."}
{"sent_id": "9e0a44722390d0508fbe56785701e6-C001-98", "intents": ["@USE@"], "paper_id": "ABC_9e0a44722390d0508fbe56785701e6_10", "text": "Following Shwartz et al. (2016) , we use a 4:1 negative to positive ratio."}
{"sent_id": "9e0a44722390d0508fbe56785701e6-C001-191", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_9e0a44722390d0508fbe56785701e6_10", "text": "This type of features has also been adopted by various systems including Mintz et al. (2009) and Shwartz et al. (2016) , and we wanted to establish a basis for its effectiveness across multiple relations."}
{"sent_id": "9e0a44722390d0508fbe56785701e6-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_9e0a44722390d0508fbe56785701e6_10", "text": "A recent paper (Shwartz et al., 2016) proposed HypeNET, a new method for RE that integrated dependency path information with distributional semantic vector representation of the entities."}
{"sent_id": "9e0a44722390d0508fbe56785701e6-C001-79", "intents": ["@EXT@"], "paper_id": "ABC_9e0a44722390d0508fbe56785701e6_10", "text": "The bottom half of Figure 1 presents the distant supervision generation process adapted from Shwartz et al. (2016) to work with our data."}
{"sent_id": "9e0a44722390d0508fbe56785701e6-C001-101", "intents": ["@MOT@"], "paper_id": "ABC_9e0a44722390d0508fbe56785701e6_10", "text": "To discover the effectiveness of the approach of Shwartz et al. (2016) , we wanted to separate HypeNET's neural architecture from its input features and use those features with different (and simpler) classifiers."}
{"sent_id": "9e0a44722390d0508fbe56785701e6-C001-103", "intents": ["@DIF@"], "paper_id": "ABC_9e0a44722390d0508fbe56785701e6_10", "text": "As our goal was to generate discrete features to be used with more traditional classifiers, we opted for using Brown clusters (Brown et al., 1992) instead of the 50-dimensional GloVe vectors (Pennington et al., 2014) used by Shwartz et al. (2016) ."}
{"sent_id": "9e0a44722390d0508fbe56785701e6-C001-183", "intents": ["@DIF@"], "paper_id": "ABC_9e0a44722390d0508fbe56785701e6_10", "text": "In Shwartz et al. (2016) , the grouping was performed by the mean pooling layer; in the case of the fastText-based system, we simply concatenate the feature tokens from all the supports and feed them into the single hidden layer."}
{"sent_id": "9e0a44722390d0508fbe56785701e6-C001-156", "intents": ["@UNSURE@"], "paper_id": "ABC_9e0a44722390d0508fbe56785701e6_10", "text": "In order to quantify the effect of the new method, we manually annotated 1,000 instance of distant supervision examples produced by our new method and the original method used by Shwartz et al. (2016) ."}
{"sent_id": "f8fc3634684ff37ab3d29cee910443-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_f8fc3634684ff37ab3d29cee910443_10", "text": "For example, systems have learned to commentate simulated robot soccer games by learning from sample sportscasts (Chen and Mooney, 2008; Liang et al., 2009; Börschinger et al., 2011) , or understand navigation instructions by learning from action traces produced when following the directions (Chen and Mooney, 2011; Tellex et al., 2011) ."}
{"sent_id": "f8fc3634684ff37ab3d29cee910443-C001-202", "intents": ["@BACK@"], "paper_id": "ABC_f8fc3634684ff37ab3d29cee910443_10", "text": "A number of approaches (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010; Börschinger et al., 2011) assume training data consisting of a set of sentences each associated with a small set of MRs, one of which is usually the correct meaning of the sentence."}
{"sent_id": "f8fc3634684ff37ab3d29cee910443-C001-206", "intents": ["@BACK@"], "paper_id": "ABC_f8fc3634684ff37ab3d29cee910443_10", "text": "As previously discussed, Börschinger et al. (2011) use a PCFG generative model and also train it on ambiguous data using EM."}
{"sent_id": "f8fc3634684ff37ab3d29cee910443-C001-47", "intents": ["@EXT@"], "paper_id": "ABC_f8fc3634684ff37ab3d29cee910443_10", "text": "Our approach extends that of Börschinger et al. (2011) , which in turn was inspired by a series of previous techniques (Lu et al., 2008; Liang et al., 2009; following the idea of constructing correspondences between NL and MR in a single probabilistic generative framework."}
{"sent_id": "f8fc3634684ff37ab3d29cee910443-C001-113", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_f8fc3634684ff37ab3d29cee910443_10", "text": "We basically follow the scheme of Börschinger et al. (2011) , but instead of generating NL words from each atomic MR, words are generated from each lexeme MR, Figure 6 : Summary of the rule generation process."}
{"sent_id": "f8fc3634684ff37ab3d29cee910443-C001-186", "intents": ["@EXT@"], "paper_id": "ABC_f8fc3634684ff37ab3d29cee910443_10", "text": "Our approach improves on Börschinger et al. (2011) 's method in the following ways:"}
{"sent_id": "f8fc3634684ff37ab3d29cee910443-C001-229", "intents": ["@EXT@"], "paper_id": "ABC_f8fc3634684ff37ab3d29cee910443_10", "text": "Our model enhances Börschinger et al. (2011) 's approach to reducing the problem of grounded learning of semantic parsers to PCFG induction."}
{"sent_id": "f8fc3634684ff37ab3d29cee910443-C001-81", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f8fc3634684ff37ab3d29cee910443_10", "text": "2 Like Börschinger et al. (2011) , our approach learns a semantic parser directly from ambiguous supervision, specifically NL instructions paired with their complete landmarks plans as context."}
{"sent_id": "f8fc3634684ff37ab3d29cee910443-C001-145", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f8fc3634684ff37ab3d29cee910443_10", "text": "science.mq.edu.au/˜mjohnson/Software.htm which was also used by Börschinger et al. (2011) ."}
{"sent_id": "f8fc3634684ff37ab3d29cee910443-C001-115", "intents": ["@USE@"], "paper_id": "ABC_f8fc3634684ff37ab3d29cee910443_10", "text": "Lexeme rules come from the schemata of Börschinger et al. (2011) , and allow every lexeme MR to generate one or more NL words."}
{"sent_id": "1cd671c60486a137377096cae435ec-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_1cd671c60486a137377096cae435ec_10", "text": "The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries (Hill et al., 2015; Hermann et al., 2015) , which permit fast integration loops between model conception and experimental evaluation."}
{"sent_id": "1cd671c60486a137377096cae435ec-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_1cd671c60486a137377096cae435ec_10", "text": "In a pragmatic approach, recent work (Hill et al., 2015) formed such questions by extracting a sentence from a larger document."}
{"sent_id": "1cd671c60486a137377096cae435ec-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_1cd671c60486a137377096cae435ec_10", "text": "The CBT (Hill et al., 2015) and CNN (Hermann et al., 2015) corpora are two such datasets."}
{"sent_id": "1cd671c60486a137377096cae435ec-C001-26", "intents": ["@DIF@"], "paper_id": "ABC_1cd671c60486a137377096cae435ec_10", "text": "We present a novel iterative, alternating attention mechanism that, unlike existing models (Hill et al., 2015; Kadlec et al., 2016) , does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time."}
{"sent_id": "1cd671c60486a137377096cae435ec-C001-77", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_1cd671c60486a137377096cae435ec_10", "text": "The attention we use here is similar to the formulation used in (Hill et al., 2015; Sukhbaatar et al., 2015) , but with two differences."}
{"sent_id": "1cd671c60486a137377096cae435ec-C001-27", "intents": ["@UNSURE@"], "paper_id": "ABC_1cd671c60486a137377096cae435ec_10", "text": "Our architecture tightly integrates previous ideas related to bidirectional readers (Kadlec et al., 2016) and iterative attention processes (Hill et al., 2015; Sukhbaatar et al., 2015) ."}
{"sent_id": "1cd671c60486a137377096cae435ec-C001-38", "intents": ["@MOT@"], "paper_id": "ABC_1cd671c60486a137377096cae435ec_10", "text": "We will focus our evaluation solely on the first two subsets, i.e. CBT-NE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by (Hill et al., 2015) ."}
{"sent_id": "1cd671c60486a137377096cae435ec-C001-197", "intents": ["@SIM@"], "paper_id": "ABC_1cd671c60486a137377096cae435ec_10", "text": "Our model is closely related to (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015; Kadlec et al., 2016; Hill et al., 2015) , which were also applied to question answering."}
{"sent_id": "1cd671c60486a137377096cae435ec-C001-208", "intents": ["@SIM@"], "paper_id": "ABC_1cd671c60486a137377096cae435ec_10", "text": "Finally, our iterative inference process shares similarities to the iterative hops in Memory Networks (Sukhbaatar et al., 2015; Hill et al., 2015) ."}
{"sent_id": "1cd671c60486a137377096cae435ec-C001-127", "intents": ["@USE@"], "paper_id": "ABC_1cd671c60486a137377096cae435ec_10", "text": "The Humans, LSTMs and Memory Networks (MemNNs) results are taken from (Hill et al., 2015) and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by (Kadlec et al., 2016) ."}
{"sent_id": "02521fd9721c264ee05315dec9b31d-C001-7", "intents": ["@DIF@"], "paper_id": "ABC_02521fd9721c264ee05315dec9b31d_10", "text": "Moreover, compared to previous work that uses two models in tandem (Baevski et al., 2019b) , by using one model for both BERT pre-trainining and fine-tuning, our model provides an average relative WER reduction of 9%."}
{"sent_id": "02521fd9721c264ee05315dec9b31d-C001-19", "intents": ["@DIF@"], "paper_id": "ABC_02521fd9721c264ee05315dec9b31d_10", "text": "Different from (Baevski et al., 2019b) where the a BERT-like model is trained with the masked language model loss, frozen, and then used as a feature extractor in tandem with a final fully supervised convolutional ASR model (Collobert et al., 2016) , in this work, our \"Discrete BERT\" approach achieves an average relative Word Error Rate (WER) reduction of 9% by pre-training and fine-tuning the same BERT model using a Connectionist Temporal Classification (Graves et al.) loss."}
{"sent_id": "02521fd9721c264ee05315dec9b31d-C001-137", "intents": ["@DIF@"], "paper_id": "ABC_02521fd9721c264ee05315dec9b31d_10", "text": "Compared to the two-model tandem system proposed in (Baevski et al., 2019b) , which uses a the discrete BERT features to train another ASR system from scratch, our discrete BERT model provides an average of 13% and 6% of WER reduction on clean and other subsets respectively, by pre-training and fine-tuning the same BERT model on the 10h labeled set."}
{"sent_id": "02521fd9721c264ee05315dec9b31d-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_02521fd9721c264ee05315dec9b31d_10", "text": "Recently impressive results have been reported for representation learning, that generalizes to different downstream tasks, through self-supervised learning for text and speech (Devlin et al., 2018; Baevski et al., 2019a; van den Oord et al., 2018; Baevski et al., 2019b) ."}
{"sent_id": "02521fd9721c264ee05315dec9b31d-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_02521fd9721c264ee05315dec9b31d_10", "text": "vq-wav2vec (Baevski et al., 2019b) learns vector quantized (VQ) representations of audio data using a future time-step prediction task."}
{"sent_id": "02521fd9721c264ee05315dec9b31d-C001-156", "intents": ["@BACK@"], "paper_id": "ABC_02521fd9721c264ee05315dec9b31d_10", "text": "The the success of BERT (Devlin et al., 2018) and Word2Vec (Mikolov et al., 2013) for NLP tasks motivated more research on self-supervised approaches for acoustic word embedding and unsupervised acoustic feature representation (Bengio and Heigold; Levin et al.; Chung et al., b; He et al.; van den Oord et al., 2018; Baevski et al., 2019b) , either by predicting masked discrete or continuous input, or by contrastive prediction of neighboring or similarly sounding segments using distant supervision or proximity in the audio signal as an indication of similarity."}
{"sent_id": "02521fd9721c264ee05315dec9b31d-C001-59", "intents": ["@EXT@"], "paper_id": "ABC_02521fd9721c264ee05315dec9b31d_10", "text": "Our work builds on the recently proposed work in (Baevski et al., 2019b) where audio is quantized using a contrastive loss, then features learned on top by a BERT model (Devlin et al., 2018) ."}
{"sent_id": "02521fd9721c264ee05315dec9b31d-C001-60", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_02521fd9721c264ee05315dec9b31d_10", "text": "For the vq-wav2vec quantization, we use the gumbelsoftmax vq-wav2vec model with the same setup as described in (Baevski et al., 2019b) ."}
{"sent_id": "02521fd9721c264ee05315dec9b31d-C001-64", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_02521fd9721c264ee05315dec9b31d_10", "text": "We train a standard BERT model (Devlin et al., 2018; with only the masked language modeling task on each set of inputs in the same way as described in (Baevski et al., 2019b) , namely by choosing tokens for masking with probability of 0.05, expanding each chosen token to a span of 10 masked tokens (spans may overlap) and then computing a cross-entropy loss which attempts to maximize the likelihood of predicting the true token for each one that was masked ( Figure  1a )."}
{"sent_id": "02521fd9721c264ee05315dec9b31d-C001-92", "intents": ["@USE@"], "paper_id": "ABC_02521fd9721c264ee05315dec9b31d_10", "text": "We first train the vq-wav2vec quantization model following the gumbel-softmax recipe described in (Baevski et al., 2019b) ."}
{"sent_id": "02521fd9721c264ee05315dec9b31d-C001-100", "intents": ["@USE@"], "paper_id": "ABC_02521fd9721c264ee05315dec9b31d_10", "text": "To mask the input sequence, we follow (Baevski et al., 2019b) and randomly sample p = 0.05 of all tokens to be a starting index, without replacement, and mask M = 10 consecutive tokens from every sampled index; spans may overlap."}
{"sent_id": "02521fd9721c264ee05315dec9b31d-C001-134", "intents": ["@USE@"], "paper_id": "ABC_02521fd9721c264ee05315dec9b31d_10", "text": "We expect that increasing the size of the fixed positional embeddings, or switching to relative positional embeddings will improve performance on longer examples, but in this work we wanted to stay consistent with the setup in Baevski et al. (2019b) ."}
{"sent_id": "473cf4603dea14ff89ca12d6e0cb50-C001-120", "intents": ["@BACK@"], "paper_id": "ABC_473cf4603dea14ff89ca12d6e0cb50_10", "text": "According to (Ortega and Vu, 2017) , prior work by (Ji and Bilmes, 2006) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly."}
{"sent_id": "d9567072d2df6c0010b32e1d1eb676-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_d9567072d2df6c0010b32e1d1eb676_10", "text": "This has led to several attempts to use GANs for text generation, with a generator using either a recurrent neural network (RNN) Guo et al., 2017; Press et al., 2017; Rajeswar et al., 2017) , or a Convolutional Neural Network (CNN) (Gulrajani et al., 2017; Rajeswar et al., 2017) ."}
{"sent_id": "d9567072d2df6c0010b32e1d1eb676-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_d9567072d2df6c0010b32e1d1eb676_10", "text": "One solution is to perform a continuous relaxation of the GAN output, which leads to generators that emit a nearly discrete continuous distribution (Press et al., 2017) ."}
{"sent_id": "d9567072d2df6c0010b32e1d1eb676-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_d9567072d2df6c0010b32e1d1eb676_10", "text": "• N-gram overlap: Press et al., 2017) : Inspired by BLEU (Papineni et al., 2002) , this measures whether n-grams generated by the model appear in a held-out corpus."}
{"sent_id": "d9567072d2df6c0010b32e1d1eb676-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_d9567072d2df6c0010b32e1d1eb676_10", "text": "In RNNbased GANs, the previous output token is used at inference time as the input x t Guo et al., 2017; Press et al., 2017; Rajeswar et al., 2017) ."}
{"sent_id": "c182062efc486f83eb27f9a3859a9a-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_c182062efc486f83eb27f9a3859a9a_10", "text": "Since the phenomenon is elusive, researchers often use lists of offensive terms to collect datasets with the aim to increase the likelihood of catching instances of hate speech Waseem and Hovy, 2016) ."}
{"sent_id": "c182062efc486f83eb27f9a3859a9a-C001-29", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_c182062efc486f83eb27f9a3859a9a_10", "text": "We consider a particular hate speech corpus -a Twitter corpus collected by Waseem and Hovy (2016) , which has been gaining traction as a resource for training hate speech detection models (Waseem and Hovy, 2016; Gambäck and Utpal, 2017; Park and Fung, 2017) -and analyse it critically to better understand its usefulness as a hate speech resource."}
{"sent_id": "c182062efc486f83eb27f9a3859a9a-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_c182062efc486f83eb27f9a3859a9a_10", "text": "Waseem and Hovy (2016) state that they use a logistic regression classifier for their hate speech prediction task. What is not mentioned is which implementation of the algorithm is used, how the model was fit to the data, whether the features were scaled, and whether any other additional parameters had been used."}
{"sent_id": "c182062efc486f83eb27f9a3859a9a-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_c182062efc486f83eb27f9a3859a9a_10", "text": "Waseem and Hovy (2016) explore several feature types: they employ n-gram features -specifically, they find that character n-grams of lengths up to 4 perform best -and in addition, they combine them with gender information, geographic location information and tweet length, finding that combining n-gram features with gender features yields slightly better results than just n-gram features do, while mixing in any of the other features results in slightly lower scores."}
{"sent_id": "c182062efc486f83eb27f9a3859a9a-C001-94", "intents": ["@BACK@"], "paper_id": "ABC_c182062efc486f83eb27f9a3859a9a_10", "text": "To date, most research on hate speech within the NLP community has been done in the area of automatic detection using a variety of techniques, from lists of prominent keywords (Warner and Hirschberg, 2012) to regression classifiers as seen in the previous section (Nobata et al., 2016; Waseem and Hovy, 2016) , naive Bayes, decision trees, random forests, and linear SVMs , as well as deep learning models with convolutional neural networks (Gambäck and Utpal, 2017; Park and Fung, 2017) ."}
{"sent_id": "c182062efc486f83eb27f9a3859a9a-C001-170", "intents": ["@BACK@"], "paper_id": "ABC_c182062efc486f83eb27f9a3859a9a_10", "text": "According to the annotation guidelines devised by Waseem and Hovy (2016) for the purpose of annotating this corpus, a tweet is tagged as offensive if it: (1) uses a sexist or racial slur, (2) attacks a minority, (3) seeks to silence a minority, (4) criticizes a minority (without a well founded argument), (5) promotes, but does not directly use, hate speech or violent crime, (6) criticizes a minority and uses a straw man argument, (7) blatantly misrepresents truth or seeks to distort views on a minority with unfounded claims, (8) shows support of problematic hashtags (e.g. #BanIslam, #whori-ental, #whitegenocide), (9) negatively stereotypes a minority, (10) defends xenophobia or sexism, (11) the tweet is ambiguous (at best); and contains a screen name that is offensive as per the previous criteria; and is on a topic that satisfies any of the above criteria."}
{"sent_id": "c182062efc486f83eb27f9a3859a9a-C001-31", "intents": ["@USE@"], "paper_id": "ABC_c182062efc486f83eb27f9a3859a9a_10", "text": "• We report the outcome of a reproduction experiment, where we attempt to replicate the results by Waseem and Hovy (2016) on hate speech detection using their Twitter corpus."}
{"sent_id": "c182062efc486f83eb27f9a3859a9a-C001-37", "intents": ["@USE@"], "paper_id": "ABC_c182062efc486f83eb27f9a3859a9a_10", "text": "We aim to replicate the results on hate speech detection by Waseem and Hovy (2016) using the hate speech Twitter corpus created by the authors."}
{"sent_id": "c182062efc486f83eb27f9a3859a9a-C001-95", "intents": ["@USE@"], "paper_id": "ABC_c182062efc486f83eb27f9a3859a9a_10", "text": "Our intent in this section is to explore hate speech beyond just detection, using the Twitter corpus by Waseem and Hovy (2016) ."}
{"sent_id": "c182062efc486f83eb27f9a3859a9a-C001-127", "intents": ["@UNSURE@"], "paper_id": "ABC_c182062efc486f83eb27f9a3859a9a_10", "text": "We use an large set of features inspired by related work (Waseem and Hovy, 2016; Sutton et al., 2015; Suh et al., 2010; Zaman et al., 2013; Hong et al., 2011; Zhang et al., 2012; Cheng et al., 2014; Ma et al., 2013; Zhao et al., 2015) ."}
{"sent_id": "c182062efc486f83eb27f9a3859a9a-C001-157", "intents": ["@UNSURE@"], "paper_id": "ABC_c182062efc486f83eb27f9a3859a9a_10", "text": "In Section 2., we pointed out the ephemeral nature of the corpus by Waseem and Hovy (2016) , common to all Twitter datasets."}
{"sent_id": "fa00b8bac394b48bf950f154c65216-C001-24", "intents": ["@USE@", "@SIM@", "@DIF@"], "paper_id": "ABC_fa00b8bac394b48bf950f154c65216_10", "text": "As mentioned earlier, we can take advantage of recent pre-trained Transformer encoders for the document encoding part as in Liu and Lapata (2019) ."}
{"sent_id": "fa00b8bac394b48bf950f154c65216-C001-168", "intents": ["@USE@"], "paper_id": "ABC_fa00b8bac394b48bf950f154c65216_10", "text": "Following previous work (See et al., 2017; Zhang et al., 2019; Liu and Lapata, 2019) , we use the non-anonymized version of CNNDM."}
{"sent_id": "fa00b8bac394b48bf950f154c65216-C001-171", "intents": ["@USE@"], "paper_id": "ABC_fa00b8bac394b48bf950f154c65216_10", "text": "We closely follow the preprocessing procedures described in Durrett et al. (2016) and Liu and Lapata (2019) ."}
{"sent_id": "fa00b8bac394b48bf950f154c65216-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_fa00b8bac394b48bf950f154c65216_10", "text": "However, Liu and Lapata (2019) leave the decoder randomly initialized."}
{"sent_id": "fa00b8bac394b48bf950f154c65216-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_fa00b8bac394b48bf950f154c65216_10", "text": "Very recently, the feature learning part was replaced again with pretrained transformers (Zhang et al., 2019; Liu and Lapata, 2019 ) that lead to another huge improvement of summarization performance."}
{"sent_id": "fa00b8bac394b48bf950f154c65216-C001-231", "intents": ["@BACK@"], "paper_id": "ABC_fa00b8bac394b48bf950f154c65216_10", "text": "BERTExt (Liu and Lapata, 2019 ) is an extractive model fine-tuning on BERT (Devlin et al., 2019) that outperforms other extractive systems."}
{"sent_id": "fa00b8bac394b48bf950f154c65216-C001-234", "intents": ["@BACK@"], "paper_id": "ABC_fa00b8bac394b48bf950f154c65216_10", "text": "BERTAbs (Liu and Lapata, 2019) and UniLM (Dong et al., 2019) are both pre-training based SEQ2SEQ summarization models."}
{"sent_id": "fa00b8bac394b48bf950f154c65216-C001-257", "intents": ["@UNSURE@"], "paper_id": "ABC_fa00b8bac394b48bf950f154c65216_10", "text": "We compared the best preforming STEP model (i.e., pre-training on the GIGA-CM dataset using SR task) with human references (denoted as Gold), RoBERTa-S2S, and two pre-training based models, BERTAbs (Liu and Lapata, 2019) and UniLM (Dong et al., 2019) 9 ."}
{"sent_id": "dbe1f1bdf7d94824f6f7cd176a4f6d-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_dbe1f1bdf7d94824f6f7cd176a4f6d_10", "text": "As a result, neural network models that use dense vectors have been shown to have inferior performance against traditional systems that use manually crafted features, unless the dense vectors are combined with the hand-crafted surface features (Ji and Eisenstein, 2015) ."}
{"sent_id": "dbe1f1bdf7d94824f6f7cd176a4f6d-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_dbe1f1bdf7d94824f6f7cd176a4f6d_10", "text": "However, these solutions still suffer from the data sparsity problem and almost always require extensive feature selection to work well (Park and Cardie, 2012; Lin et al., 2009; Ji and Eisenstein, 2015) ."}
{"sent_id": "dbe1f1bdf7d94824f6f7cd176a4f6d-C001-104", "intents": ["@BACK@"], "paper_id": "ABC_dbe1f1bdf7d94824f6f7cd176a4f6d_10", "text": "Previous work in this task has been done over three schemes of evaluation: top-level 4-way classification (Pitler et al., 2009 ), second-level 11-way classification (Lin et al., 2009; Ji and Eisenstein, 2015) , and modified second-level classification introduced in the CoNLL 2015 Shared Task ."}
{"sent_id": "dbe1f1bdf7d94824f6f7cd176a4f6d-C001-125", "intents": ["@BACK@"], "paper_id": "ABC_dbe1f1bdf7d94824f6f7cd176a4f6d_10", "text": "We use the Berkeley parser to parse all of the data (Petrov et al., 2006 too little data, 50-dimensional WSJ-trained word vectors have previously been shown to be the most effective in this task (Ji and Eisenstein, 2015) ."}
{"sent_id": "dbe1f1bdf7d94824f6f7cd176a4f6d-C001-92", "intents": ["@SIM@"], "paper_id": "ABC_dbe1f1bdf7d94824f6f7cd176a4f6d_10", "text": "This model is similar to the recursive neural networks proposed by Ji and Eisenstein (2015) ."}
{"sent_id": "dbe1f1bdf7d94824f6f7cd176a4f6d-C001-123", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_dbe1f1bdf7d94824f6f7cd176a4f6d_10", "text": "The data split and the label set are exactly the same as previous works that use this label set (Lin et al., 2009; Ji and Eisenstein, 2015) ."}
{"sent_id": "dbe1f1bdf7d94824f6f7cd176a4f6d-C001-133", "intents": ["@DIF@"], "paper_id": "ABC_dbe1f1bdf7d94824f6f7cd176a4f6d_10", "text": "It outperforms the recursive neural network with bilinear output layer introduced by Ji and Eisenstein (2015) (p < 0.05; bootstrap test) and performs comparably with the surface feature baseline (Lin et al., 2009) , which uses various lexical and syntactic features and extensive feature selection."}
{"sent_id": "dbe1f1bdf7d94824f6f7cd176a4f6d-C001-169", "intents": ["@DIF@"], "paper_id": "ABC_dbe1f1bdf7d94824f6f7cd176a4f6d_10", "text": "Another point of contrast between our work and Ji and Eisenstein's (2015) is the modeling choice for inter-argument interaction."}
{"sent_id": "dbe1f1bdf7d94824f6f7cd176a4f6d-C001-174", "intents": ["@DIF@"], "paper_id": "ABC_dbe1f1bdf7d94824f6f7cd176a4f6d_10", "text": "The recursive model by Ji and Eisenstein (2015) is limited to 50 units due to the bilinear layer."}
{"sent_id": "385ce03aee1e3d3de193de09fa1278-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_385ce03aee1e3d3de193de09fa1278_10", "text": "Despite the steadily increasing body of research on text-adventure games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018; Ammanabrolu and Riedl, 2019) , and in addition to the ubiquity of deep reinforcement learning applications (Parisotto et al., 2016; Zambaldi et al., 2019) , teaching an agent to play text-adventure games remains a challenging task."}
{"sent_id": "385ce03aee1e3d3de193de09fa1278-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_385ce03aee1e3d3de193de09fa1278_10", "text": "Learning a control policy for a text-adventure game requires a significant amount of exploration, resulting in training runs that take hundreds of thousands of simulations (Narasimhan et al., 2015; Ammanabrolu and Riedl, 2019) ."}
{"sent_id": "385ce03aee1e3d3de193de09fa1278-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_385ce03aee1e3d3de193de09fa1278_10", "text": "Text-adventure games, in which an agent must interact with the world entirely through natural language, provide us with two challenges that have proven difficult for deep reinforcement learning to solve (Narasimhan et al., 2015; Haroush et al., 2018; Ammanabrolu and Riedl, 2019) : (1) The agent must act based only on potentially incomplete textual descriptions of the world around it."}
{"sent_id": "385ce03aee1e3d3de193de09fa1278-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_385ce03aee1e3d3de193de09fa1278_10", "text": "Multiple recent works have explored the challenges associated with these games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Tao et al., 2018; Ammanabrolu and Riedl, 2019) ."}
{"sent_id": "385ce03aee1e3d3de193de09fa1278-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_385ce03aee1e3d3de193de09fa1278_10", "text": "Previous work (Ammanabrolu and Riedl, 2019) introduced the use of knowledge graphs and questionanswering pre-training to aid in the problems of partial observability and a combinatorial action space."}
{"sent_id": "385ce03aee1e3d3de193de09fa1278-C001-64", "intents": ["@EXT@"], "paper_id": "ABC_385ce03aee1e3d3de193de09fa1278_10", "text": "We make minor modifications to the rules used in Ammanabrolu and Riedl (2019) to better achieve such a graph in general interactive fiction environments."}
{"sent_id": "385ce03aee1e3d3de193de09fa1278-C001-71", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_385ce03aee1e3d3de193de09fa1278_10", "text": "As this is too large a space for a RL agent to effectively explore, the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in (Ammanabrolu and Riedl, 2019) The architecture for the deep Q-network consists of two separate neural networks-encoding state and action separately-with the final Q-value for a state-action pair being the result of a pairwise interaction function between the two (Figure 1 )."}
{"sent_id": "385ce03aee1e3d3de193de09fa1278-C001-102", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_385ce03aee1e3d3de193de09fa1278_10", "text": "The portions that are pre-trained are the same parts of the architecture as in Ammanabrolu and Riedl (2019) ."}
{"sent_id": "385ce03aee1e3d3de193de09fa1278-C001-124", "intents": ["@USE@"], "paper_id": "ABC_385ce03aee1e3d3de193de09fa1278_10", "text": "For the metrics tested after convergence, we set = 0.1 following both Narasimhan et al. (2015) and Ammanabrolu and Riedl (2019) ."}
{"sent_id": "385ce03aee1e3d3de193de09fa1278-C001-129", "intents": ["@USE@"], "paper_id": "ABC_385ce03aee1e3d3de193de09fa1278_10", "text": "Following Ammanabrolu and Riedl (2019), we use TextWorld's \"home\" theme to generate the games for the question-answering system."}
{"sent_id": "385ce03aee1e3d3de193de09fa1278-C001-125", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_385ce03aee1e3d3de193de09fa1278_10", "text": "We use similar hyperparameters to those reported in (Ammanabrolu and Riedl, 2019) for training the KG-DQN with action pruning, with the main difference being that we use 100 dimensional word embeddings instead of 50 dimensions for the horror genre."}
{"sent_id": "60cc075e5351a756de8f9919d5a84e-C001-97", "intents": ["@UNSURE@"], "paper_id": "ABC_60cc075e5351a756de8f9919d5a84e_10", "text": "and Clark and Curran (2004) give a detailed description of the dependency structures."}
{"sent_id": "60cc075e5351a756de8f9919d5a84e-C001-90", "intents": ["@BACK@"], "paper_id": "ABC_60cc075e5351a756de8f9919d5a84e_10", "text": "Clark and Curran (2004) evaluate a number of log-linear parsing models for CCG."}
{"sent_id": "60cc075e5351a756de8f9919d5a84e-C001-100", "intents": ["@BACK@"], "paper_id": "ABC_60cc075e5351a756de8f9919d5a84e_10", "text": "In Clark and Curran (2004) we describe a discriminative method for estimating the parameters of a log-linear parsing model."}
{"sent_id": "60cc075e5351a756de8f9919d5a84e-C001-147", "intents": ["@BACK@"], "paper_id": "ABC_60cc075e5351a756de8f9919d5a84e_10", "text": "In Clark and Curran (2004) we show that the parsing model resulting from training data generated in this way produces state-of-the-art CCG dependency recovery: 84.6 F-score over labelled dependencies."}
{"sent_id": "60cc075e5351a756de8f9919d5a84e-C001-188", "intents": ["@BACK@"], "paper_id": "ABC_60cc075e5351a756de8f9919d5a84e_10", "text": "The parser is extremely fast, and in Clark and Curran (2004) we show that the F-score for labelled dependencies is almost 98%."}
{"sent_id": "60cc075e5351a756de8f9919d5a84e-C001-94", "intents": ["@USE@"], "paper_id": "ABC_60cc075e5351a756de8f9919d5a84e_10", "text": "The feature set we use is from the best performing normal-form model in Clark and Curran (2004) ."}
{"sent_id": "60cc075e5351a756de8f9919d5a84e-C001-171", "intents": ["@USE@"], "paper_id": "ABC_60cc075e5351a756de8f9919d5a84e_10", "text": "The results in this section are all using the best performing normal-form model in Clark and Curran (2004) , which corresponds to row 3 in Table 3 ."}
{"sent_id": "60cc075e5351a756de8f9919d5a84e-C001-107", "intents": ["@MOT@"], "paper_id": "ABC_60cc075e5351a756de8f9919d5a84e_10", "text": "In Clark and Curran (2004) we describe efficient methods for performing the calculations using packed charts."}
{"sent_id": "60cc075e5351a756de8f9919d5a84e-C001-150", "intents": ["@DIF@"], "paper_id": "ABC_60cc075e5351a756de8f9919d5a84e_10", "text": "In Clark and Curran (2004) we show that using this more restrictive setting has a small negative impact on the accuracy of the resulting parser (about 0.6 F-score over labelled dependencies)."}
