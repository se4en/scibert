{"sent_id": "8f0aab7fd30ffc56cc477b25e6bb16-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_8f0aab7fd30ffc56cc477b25e6bb16_0", "text": "His chief focus is on semantic relations in FrameNet (Ruppenhofer et al. 2006) , how they can be used for paraphrase (Ellsworth & Janin 2007) , and mapping to other resources (Sche↵czyk & Ellsworth 2006; Ferrández et al. 2010b) ."}
{"sent_id": "8abffa3f807bad5ae2073aa7db215d-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_8abffa3f807bad5ae2073aa7db215d_0", "text": "Bansal et al. [8] and Melamud et al. [11] show the benefits of such modified-context embeddings in dependency parsing task."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-3", "intents": ["@MOT@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "Recently, Ott et al. (2011) have introduced an opinion spam dataset containing gold standard deceptive positive hotel reviews."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-16", "intents": ["@MOT@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "While previous related work (Ott et al., 2011; Ott et al., 2012) has explored characteristics of positive deceptive opinion spam, the complementary problem of negative deceptive opinion spam remains largely unstudied."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-5", "intents": ["@SIM@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "Following an approach similar to Ott et al. (2011) , in this work we create and study the first dataset of deceptive opinion spam with negative sentiment reviews."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-17", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "Following the framework of Ott et al. (2011) , we use Amazon's Mechanical Turk service to produce the first publicly available 1 dataset of negative deceptive opinion spam, containing 400 gold standard deceptive negative reviews of 20 popular Chicago hotels."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-42", "intents": ["@SIM@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "However, because the truthful reviews are on average longer than our deceptive reviews, we sample the truthful reviews according to a log-normal distribution fit to the lengths of our deceptive reviews, similarly to Ott et al. (2011) Table 1 : Deception detection performance, incl."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-20", "intents": ["@USE@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "In conjunction with Ott et al. (2011) 's positive deceptive opinion spam dataset, we then explore the interaction between sentiment and deception with respect to three types of language features: (1) changes in first-person singular use, often attributed to psychological distancing (Newman et al., 2003) , (2) decreased spatial awareness and more narrative form, consistent with theories of reality monitoring (Johnson and Raye, 1981) and imaginative writing (Biber et al., 1999; Rayson et al., 2001) , and (3) increased negative emotion terms, often attributed to leakage cues (Ekman and Friesen, 1969) , but perhaps better explained in our case as an exaggeration of the underlying review sentiment."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-28", "intents": ["@USE@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "Deceptive negative reviews are gathered from Mechanical Turk using the same procedure as Ott et al. (2011) ."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-92", "intents": ["@USE@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "We have additionally explored, albeit briefly, the relationship between sentiment and deception by utilizing Ott et al. (2011) 's positive deceptive opinion spam dataset in conjunction with our own."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-36", "intents": ["@DIF@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "5 The average accepted review length was 178 words, higher than for the positive reviews gathered by Ott et al. (2011) , who report an average review length of 116 words."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "Standard n-gram-based text categorization techniques have been shown to be effective at detecting deception in text (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; Ott et al., 2011; Feng et al., 2012) ."}
{"sent_id": "5f62958d0cdd32b15067c1afe458a5-C001-9", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_5f62958d0cdd32b15067c1afe458a5_0", "text": "e unsupervised learning of low-dimensional, semantic representations of words and entities has recently gained a ention for the entity-oriented tasks of expert nding [9] and product search [8] ."}
{"sent_id": "5f62958d0cdd32b15067c1afe458a5-C001-13", "intents": ["@EXT@"], "paper_id": "ABC_5f62958d0cdd32b15067c1afe458a5_0", "text": "In this paper we describe the Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our previously published entity representation models [8, 9] ."}
{"sent_id": "5f62958d0cdd32b15067c1afe458a5-C001-42", "intents": ["@EXT@", "@SIM@"], "paper_id": "ABC_5f62958d0cdd32b15067c1afe458a5_0", "text": "An alternative option that exists within the toolkit is to resample word sequence/entity pairs such that every entity is associated with the same number of word sequences, as used for product search [8] ."}
{"sent_id": "5f62958d0cdd32b15067c1afe458a5-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_5f62958d0cdd32b15067c1afe458a5_0", "text": "In the case of product search, an associated document is a product description or review [8] ."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-4", "intents": ["@MOT@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "In this paper, we consider the referential games of Lazaridou et al. (2017) , and investigate the representations the agents develop during their evolving interaction."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-22", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "We replicate Lazaridou's games, and we find that, in both, the agents develop successfully aligned representations that, however, are not capturing conceptual properties at all."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-105", "intents": ["@MOT@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "However, the important contribution of Lazaridou et al. (2017) is to play a signaling game with real-life images instead of artificial symbols."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "Since an analysis of vocabulary usage brings inconclusive evidence that the agents are using the symbols to represent natural concepts (such as beaver or bayonet), Lazaridou and colleagues next modify the game, by presenting to the Sender and the Receiver different images for each of the two concepts (e.g., the Sender must now signal that the target is a beaver, while seeing a different beaver from the one shown to the Receiver)."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "See Lazaridou et al. (2017) for details."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-82", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "Lazaridou et al. (2017) designed their second game to encourage more general, concept-like referents."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-27", "intents": ["@USE@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "Architecture We re-implement Lazaridou's Sender and Receiver architectures (using their better-behaved \"informed\" Sender)."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-36", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "Data Following Lazaridou et al. (2017) , for each of the 463 concepts they used, we randomly sample 100 images from ImageNet (Deng et al., 2009 )."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-43", "intents": ["@USE@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "Games We re-implement both Lazaridou's same-image game, where Sender and Receiver are shown the same two images (always of different concepts), and their different-image game, where the Receiver sees different images than the Sender's."}
{"sent_id": "737a452057be3e254b35bd8df492be-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_737a452057be3e254b35bd8df492be_0", "text": "2) Deep Learning models which learn feature representations on their own. [10] released the popular data set of 16k tweets annotated as belonging to sexism, racism or none class 1 , and provided a feature engineered model for detection of abuse in their corpus."}
{"sent_id": "737a452057be3e254b35bd8df492be-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_737a452057be3e254b35bd8df492be_0", "text": "On the data set released by [10] , [5] experiment with a two-step approach of detecting abusive language first and then classifying them into specific types i.e. racist, sexist or none."}
{"sent_id": "737a452057be3e254b35bd8df492be-C001-63", "intents": ["@USE@"], "paper_id": "ABC_737a452057be3e254b35bd8df492be_0", "text": "At the time of the experiment, the [10] data set had a total of 15,844 tweets out of which 1,924 were labelled as belonging to racism, 3,058 as sexism and 10,862 as none."}
{"sent_id": "737a452057be3e254b35bd8df492be-C001-98", "intents": ["@USE@"], "paper_id": "ABC_737a452057be3e254b35bd8df492be_0", "text": "The first tweet is a sexist tweet from [10] where as the second tweet is an example of racist tweet from the same datset ."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-17", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "While incorporating existing knowledge (from curated knowledge bases) for the purpose of question-answering [12, 9, 16] is an important area of research, we consider the simpler setting where all the information is contained within the text itself -which is the approach taken by many recent memory based neural network models [17, 18, 19, 20] ."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-20", "intents": ["@MOT@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "However, this model lacks any module for relational reasoning."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "Note that there can be multiple entites in a sentence hence a sigmoid operation is more suitable, and it is also more scalable [18] ."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-75", "intents": ["@BACK@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "As described previously, the model is closely related to the Recurrent Entity Networks model [18] which describes an end-to-end approach to model entities in text but does not directly model relations."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-98", "intents": ["@DIF@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-33", "intents": ["@SIM@", "@EXT@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "The input encoder and output module implementations are similar to the Entity Network [18] and main novelty lies in the dynamic memory."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-62", "intents": ["@SIM@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "Similar to [18] , we normalize the memories after each update step (that is after reading each sentence)."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-91", "intents": ["@SIM@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "We keep all other details similar to [18] for a fair comparison."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-38", "intents": ["@USE@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "We use a simple encoder with a learned multiplicative mask [18, 17] :"}
{"sent_id": "b5097b3d901d073bfe06bcd88318ac-C001-11", "intents": ["@DIF@"], "paper_id": "ABC_b5097b3d901d073bfe06bcd88318ac_0", "text": "Besides, using a small corpus, we obtain better human-assigned WordSim scores, corresponding Spearman correlation and better downstream (NER & SA) performance compared to Mikolov's model, trained on 100 billion word corpus."}
{"sent_id": "b5097b3d901d073bfe06bcd88318ac-C001-119", "intents": ["@DIF@"], "paper_id": "ABC_b5097b3d901d073bfe06bcd88318ac_0", "text": "Mikolov et al. (2013a) performed second worst of all, despite originating from a very huge corpus."}
{"sent_id": "b5097b3d901d073bfe06bcd88318ac-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_b5097b3d901d073bfe06bcd88318ac_0", "text": "A loglinear classifier is used in both architectures (Mikolov et al. (2013a) )."}
{"sent_id": "b5097b3d901d073bfe06bcd88318ac-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_b5097b3d901d073bfe06bcd88318ac_0", "text": "Combination of both syntactic and semantic analyses is possible and provided (totaling over 19,000 questions) as Google analogy test set by Mikolov et al. (2013a) ."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-16", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "The basic motivation behind syntax-based model is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "Zhang et al. (2008a) made it possible to utilize the non-syntactic rules and even the phrases which are used in phrase based model by advancing a general tree sequence to tree sequence framework based on the tree-to-tree model presented in (Zhang et al., 2007) ."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "The SRR stands for the rules which have at least two non-terminal leaf nodes with inverted order in the source and target side."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-58", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "Then, the syntax rules can also fall into two categories according to whether equipping with generalization capability (Chiang, 2007; Zhang et al., 2008a) : 1) Initial rules (Initial): all leaf nodes of this rule are terminals."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "The Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR) mentioned by (Zhang et al., 2008a) can be regarded as more in-depth classification of the syntax rules."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "Definition 2: The Discontiguous Phrase Rule (DPR) refers to the rule having at least one nonterminal leaf node between two lexicalized leaf nodes."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-49", "intents": ["@MOT@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "Our proposed rule classification is inspired by these works."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-78", "intents": ["@DIF@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "According to Definition 2, it is a DPR."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-107", "intents": ["@FUT@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "In the future works, aiming to analyze the rule contributions and the redundances issues using the presented rule classification based on some real translation systems, we plan to implement some synchronous grammar based syntax translation models such as the one presented in (Liu et al., 2007) or in (Zhang et al., 2008a) ."}
{"sent_id": "e834dadbcf08cf14e476b5f5cbf79e-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_e834dadbcf08cf14e476b5f5cbf79e_0", "text": "In particular, the memory network Chien and Lin, 2018) , neural variational learning (Serban et al., 2017; Chung et al., 2015) , neural discrete representation (Jang et al., 2016; Maddison et al., 2016; van den Oord et al., 2017) , recurrent ladder network (Rasmus et al., 2015; Prémont-Schwarz et al., 2017; Sønderby et al., 2016) , stochastic neural network (Fraccaro et al., 2016; Goyal et al., 2017; Shabanian et al., 2017) , Markov recurrent neural network (Venkatraman et al., 2017; Kuo and Chien, 2018) , sequence GAN (Yu et al., 2017) and reinforcement learning (Tegho et al., 2017) are introduced in various deep models which open a window to more practical tasks, e.g. reading comprehension, sentence generation, dialogue system, question answering and machine translation."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-9", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "Following Gong et al. (2018) , we consider two document collections heterogeneous if their documents differ systematically with respect to vocabulary and / or level of abstraction."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-57", "intents": ["@MOT@", "@UNSURE@", "@EXT@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "The extent to which this information is used by Gong et al. (2018) is not entirely clear, so we experiment with several setups (cf. Section 4)."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-145", "intents": ["@MOT@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "We presented a simple method for semantic matching of documents from heterogeneous collections as a solution to the Concept-Project matching task by Gong et al. (2018) ."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "The approach by Gong et al. (2018) is based on the idea that the longer document in the pair is reduced to a set of topics which capture the essence of the document in a way that eliminates the effect of a potential length difference."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "Accordingly, they use Doc2Vec (Le and Mikolov (2014) ) as one of their baselines, and show that its performance is inferior to their method."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "As a second baseline, they use Word Mover's Distance (Kusner et al. (2015) ), which is based on word-level distances, rather than distance of global document representations, but which also fails to be competitive with their topic-based method."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-15", "intents": ["@USE@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "We demonstrate our method with the Concept-Project matching task (Gong et al. (2018) ), which is described in the next section."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-53", "intents": ["@DIF@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "Gong et al. (2018) do not provide any specification, or annotation guidelines, of the semantics of the 'matches' relation to be annotated."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-92", "intents": ["@DIF@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "Note that Gong et al. (2018) used only 10% of their 537 instances data set as tuning data."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-105", "intents": ["@DIF@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "Since the original data split used by Gong et al. (2018) is unknown, we cannot exactly replicate their settings, but we also perform ten runs using randomly selected 10% of our 408 instances test data set, and report average P, R, F, and standard deviation."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-114", "intents": ["@DIF@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "Thus, while the performance of our proposed TOP n COS SIM AVG method is superior to the approach by Gong et al. (2018) , it is itself outperformed by the 'baseline' AVG COS SIM method with appropriate weighting."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-109", "intents": ["@SIM@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "Note that our Both setting is probably the one most similar to the concept input used by Gong et al. (2018) ."}
{"sent_id": "e75e14ff2812f34ff456eb472a36d2-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_e75e14ff2812f34ff456eb472a36d2_0", "text": "In the third part, a series of deep models including deep unfolding (Chien and Lee, 2018) , Bayesian RNN (Gal and Ghahramani, 2016; Chien and Ku, 2016) , sequence-to-sequence learning (Graves et al., 2006; Gehring et al., 2017) , CNN (Kalchbrenner et al., 2014; Xingjian et al., 2015; , GAN (Tsai and Chien, 2017) and VAE are introduced."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-3", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "The EmbodiedQA task requires an agent to answer a question by intelligently navigating in a simulated environment, gathering necessary visual information only through first-person vision before finally answering."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "The EmbodiedQA task requires an agent to intelligently navigate in a simulated household environment [25] and answer questions through egocentric vision."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-18", "intents": ["@USE@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "To our surprise, blindfold baselines achieve state-of-the-art performance on the EmbodiedQA task, except in the case when the agent is spawned extremely close to the object."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-92", "intents": ["@USE@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "We show that simple question only baselines largely outperform or closely compete with existing methods on the EmbodiedQA task."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-55", "intents": ["@DIF@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "To get rid of peaky answers, an entropy pruning method was applied by [5] where questions with normalized entropy below 0.5 were excluded."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-62", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "We also train the [5] text embedding model (an LSTM) with the optimization settings described in [5] for 200 epochs."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-73", "intents": ["@SIM@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "As noted earlier, this means that models don't need to generalize across unseen combinations of rooms/objects/colors to perform well on this task (b) Despite entropy-pruning, there is a noticeable bias in the answer distribution of EQAv1 questions (see [5, Appendix A])."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-94", "intents": ["@FUT@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "Besides providing a benchmark score for future researchers working on this task, our results suggest considerations for future dataset and task construction in EQA and related tasks."}
{"sent_id": "f2dfc35b67e47c12cba3cd0ec743a5-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_f2dfc35b67e47c12cba3cd0ec743a5_1", "text": "Although such networks have proved useful in some semantical-dependent tasks (see e.g. a topological approach to word sense disambiguation in [17] ), I believe that the creation of novel semantic-based measurements would improve the state of the art."}
{"sent_id": "684a637d08e8dbabddc1f1982f5393-C001-6", "intents": ["@USE@"], "paper_id": "ABC_684a637d08e8dbabddc1f1982f5393_1", "text": "In this talk, we evaluate two recent approaches to information presentation in SDS: (1) the Refiner approach (Polifroni et al., 2003) which generates summaries by clustering the options to maximize coverage of the domain, and (2) the user-model based summarize and refine (UMSR) approach (Demberg and Moore, 2006) which clusters options to maximize utility with respect to a user model, and uses linguistic devices (e.g., discourse cues, adverbials) to highlight the trade-offs among the presented items."}
{"sent_id": "684a637d08e8dbabddc1f1982f5393-C001-8", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_684a637d08e8dbabddc1f1982f5393_1", "text": "Using a Wizard-of-Oz methodology to evaluate the approaches in an interactive setting, we show that in addition to being preferred by users, the UMSR approach is superior to the Refiner approach in terms of both task success and dialogue efficiency, even when the user is performing a demanding secondary task."}
{"sent_id": "53bc4206427e95d600f787e0531df1-C001-15", "intents": ["@USE@"], "paper_id": "ABC_53bc4206427e95d600f787e0531df1_1", "text": "Thus, we want to apply PMI score in the original BTM."}
{"sent_id": "53bc4206427e95d600f787e0531df1-C001-16", "intents": ["@USE@"], "paper_id": "ABC_53bc4206427e95d600f787e0531df1_1", "text": "A suitable way to apply PMI scores is modifying the priors in the BTM."}
{"sent_id": "cf2cc67035107f5bdaab85a760e56e-C001-9", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_cf2cc67035107f5bdaab85a760e56e_1", "text": "Also, such embeddings are usually trained on Indonesian Wikipedia (Al-Rfou et al., 2013; Bojanowski et al., 2017) whose size is relatively small, approximately 60M tokens."}
{"sent_id": "cf2cc67035107f5bdaab85a760e56e-C001-21", "intents": ["@USE@"], "paper_id": "ABC_cf2cc67035107f5bdaab85a760e56e_1", "text": "We used fastText pretrained embeddings introduced in (Bojanowski et al., 2017 ) and (Grave et al., 2018) , which have been trained on Indonesian Wikipedia and Indonesian Wikipedia plus Common Crawl data respectively."}
{"sent_id": "cf2cc67035107f5bdaab85a760e56e-C001-29", "intents": ["@USE@"], "paper_id": "ABC_cf2cc67035107f5bdaab85a760e56e_1", "text": "We used gensim 3 to run word2vec and fastText and the original C implementation for GloVe."}
{"sent_id": "7c5c5f13205c40a27d2629727df840-C001-9", "intents": ["@USE@"], "paper_id": "ABC_7c5c5f13205c40a27d2629727df840_1", "text": "For representing quantification, we actually prefer to use Hilbert's ǫ and τ -terms constructed with two constants ǫ, τ : Λα. (α → t) → α and one for generic elements [8] ."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Conneau et al. (2018) improved this approach with post-mapping refinements, showing impressive results for several language pairs."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-72", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Conneau et al. (2018) show impressive results with adversarial training and refinement with the Procrustes solution."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-159", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Conneau et al. (2018) and Artetxe et al. (2018b) propose fine-tuning methods to refine the initial mappings."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-164", "intents": ["@BACK@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "For finding the nearest neighbors, we use the Cross-domain Similarity Local Scaling (CSLS) which works better in mitigating the hubness problem (Conneau et al., 2018) ."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-192", "intents": ["@MOT@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "1. Does the unsupervised mapping method based on our proposed adversarial autoencoder model improve over the best existing adversarial method of Conneau et al. (2018) in terms of mapping accuracy and convergence (Section 5.1)?"}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-114", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Our discriminators have the same architecture as Conneau et al. (2018) ."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-152", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "We also apply the orthogonalization update to the mappers following Conneau et al. (2018) with β = 0.01."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-160", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Similar to Conneau et al. (2018) ), we finetune our initial mappings (G and F ) by iteratively solving the Procrustes problem and applying a dictionary induction step."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-161", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "This method uses singular value decomposition or SVD of Z T y Z x to find the optimal mappings G (similarly SVD(Z T x Z y ) for F ) given the approximate alignment of words from the previous step."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-184", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "For some of the baselines, results are reported from their papers, while for the rest we report results by running the publicly available codes on our machine."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-196", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "**COMPARISON WITH CONNEAU ET AL. (2018)**"}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-197", "intents": ["@USE@", "@DIF@", "@SIM@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Since our approach follows the same steps as Conneau et al. (2018), we first compare our proposed model with their model on European (Table 1) , non-European and low-resource languages (Table  2 ) on their dataset."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-205", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "We ran their code 10 times for Ms→En but failed every time."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-214", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "In this section, we compare our model with other state-of-the-art methods that do not follow the same procedure as us and Conneau et al. (2018) ."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-218", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Our Adversarial autoencoder + Conneau et al. (2018) Refinement performs better than most of the other methods on this dataset, achieving the highest accuracy for 4 out of 6 translation tasks."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-230", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "In our first experiment, we use their method to induce the initial seed dictionary and then apply iterative Procrustes solution (same refinement procedure of Conneau et al. (2018) ) for refinement."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-243", "intents": ["@USE@", "@DIF@", "@SIM@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "This setup allows us to compare our model directly with the adversarial model of Conneau et al. (2018) , putting the effect of finetuning aside."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-35", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Our results show that our model is more robust and yields significant gains over Conneau et al. (2018) for all translation tasks in all evaluation measures."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-75", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "However, while all these methods learn the mapping in the original embedding space, our approach learns it in the latent code space considering both the mapper and the target encoder as adversary."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-201", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Our method is also superior to theirs for the non-European and low-resource language pairs in Table 2 ."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-207", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "If we compare our method with the method of Conneau et al. (2018) on the more challenging Dinu-Artexe dataset in Table 3 , we see here also our method performs better than their method in all the four translation tasks involving European language pairs."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-208", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "In this dataset, our method shows more robustness compared to their method."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-254", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "As we compare our full model with the model of Conneau et al. (2018) in the without fine-tuning setting, we notice large improvements in all measures across all datasets: 5.1 -7.3% in En→Es, 3 -6% in Es→En, 3.4 -4.3% in En→De, 1 -3% in De→En, 3.4 -4.3% in En→It, and 0.3 -3.7% in It→En."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-255", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "These improvements demonstrate that our model finds a better mapping compared to Conneau et al. (2018) ."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-272", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Through extensive experimentations on six different language pairs comprising European, nonEuropean and low-resource languages from two different data sources, we demonstrate that our method outperforms the method of Conneau et al. (2018) for all translation tasks in all measures (P@{1,5,10}) across all settings (with and without fine-tuning)."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-4", "intents": ["@DIF@", "@MOT@", "@USE@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "With proper averaging in place, we notice that the distillation model described in Hu et al. (2016) , which incorporates explicit logic rules for sentiment classification, is ineffective."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-15", "intents": ["@MOT@", "@USE@", "@EXT@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "Part of our contribution is to identify an important gap in the methodology used in Hu et al. (2016) for performance measurement, which is addressed by averaging the experiments over several executions."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-14", "intents": ["@USE@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "In this work, we carry out an in-depth study of the effectiveness of the techniques in Hu et al. (2016) and Peters et al. (2018a) for sentiment classification of complex sentences."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-16", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "With the averaging in place, we obtain three key findings: (1) the improvements in Hu et al. (2016) can almost entirely be attributed to just one of their two proposed mechanisms and are also less pronounced than previously reported; (2) contextualized word embeddings (Peters et al., 2018a) incorporate the \"A-but-B\" rules more effectively without explicitly programming for them; and (3) an analysis using crowdsourcing reveals a bigger picture where the errors in the automated systems have a striking correlation with the inherent sentiment-ambiguity in the data."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-25", "intents": ["@USE@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "Next, we discuss the two techniques from Hu et al. (2016) for incorporating rules into models: projection, which directly alters a trained model, and distillation, which progressively adjusts the loss function during training."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-53", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "**PERFORMANCE OF HU ET AL. (2016)**"}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-54", "intents": ["@USE@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "We carry out an averaged analysis of the publicly available implementation 4 of Hu et al. (2016) ."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-55", "intents": ["@USE@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "Our analysis reveals that the reported performance of their two mechanisms (projection and distillation) is in fact affected by the high variability across random seeds."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-88", "intents": ["@USE@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "We confirm that ELMo's predictions are much closer to the A-but-B rule's manifold than those of the other models by computing KL(q θ ||p θ ) where p θ and q θ are the original and projected distributions: Averaged across all A-but-B sentences and 100 seeds, this gives 0.27, 0.26 and 0.13 for the Kim (2014) , Hu et al. (2016) with distillation and ELMo systems respectively."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "Recently, Hu et al. (2016) incorporate logical rules into a neural model and show that these rules increase the model's accuracy on sentences containing contrastive conjunctions, while Peters et al. (2018a) demonstrate increased overall accuracy on sentiment analysis by initializing a model with representations from a language model trained on millions of sentences."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "Hu et al. (2016) computes q θ after every gradient update by projecting the current p θ , as described above."}
{"sent_id": "3b0a82129333203eca96a7473095f3-C001-2", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_3b0a82129333203eca96a7473095f3_1", "text": "In a recent paper advocating a corpus-based and probabilistic approach to grammar development, Black, Lafferty, and Roukos (1992) argue that \"the current state of the art is far from being able to produce a robust parser of general English\" and advocate \"steady and quantifiable,\" empirically corpus-driven grammar development and testing."}
{"sent_id": "3b0a82129333203eca96a7473095f3-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_3b0a82129333203eca96a7473095f3_1", "text": "While I am sympathetic to Oostdijk's position and think that the grammar she goes on to present is impressive enough to bias us towards the opposite conclusion, it is a mistake to accept the assumption that the two approaches are incompatible, as much recent work (including that of Black et al. 1992) has demonstrated the usefulness of combining statistical techniques with rule-based systems."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015; Lample et al., 2016; Ma and Hovy, 2016) ."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-22", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Ma and Hovy, 2016) ."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-82", "intents": ["@BACK@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "Using a CNN structure to encode character sequences was firstly proposed by Santos and Zadrozny (2014), and followed by many subsequent investigations (dos Santos et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016) ."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-68", "intents": ["@SIM@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "3) Our findings are more consistent with most previous work on configurations such as usefulness of character information (Lample et al., 2016; Ma and Hovy, 2016) , optimizer (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016) and tag scheme (Ratinov and Roth, 2009; Dai et al., 2015) ."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-190", "intents": ["@SIM@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "Our observation is consistent with most literature (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016) ."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-83", "intents": ["@USE@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "In our experiments, we take the same structure as Ma and Hovy (2016) , using one layer CNN structure with max-pooling to capture character-level representations."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-203", "intents": ["@USE@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "Following Ma and Hovy (2016) , words in the test set are divided into four subsets: in-vocabulary words, out-of-training-vocabulary words (OOTV), out-of-embedding-vocabulary words (OOEV) and out-of-both-vocabulary words (OOBV)."}
{"sent_id": "2b6dd9388c43df4416c738b2d1ed5f-C001-33", "intents": ["@USE@"], "paper_id": "ABC_2b6dd9388c43df4416c738b2d1ed5f_1", "text": "In this work, we use the datasets released by (Davidson et al. 2017 ) and HEOT dataset provided by (Mathur et al. 2018) ."}
{"sent_id": "2b6dd9388c43df4416c738b2d1ed5f-C001-56", "intents": ["@USE@"], "paper_id": "ABC_2b6dd9388c43df4416c738b2d1ed5f_1", "text": "As indicated by the Figure 1 , the model was initially trained on the dataset provided by (Davidson et al. 2017) , and then re-trained on the HEOT dataset so as to benefit from the transfer of learned features in the last stage."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-33", "intents": ["@DIF@", "@USE@", "@SIM@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "The proposed model uses the same encoder/decoder structure as [1] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-36", "intents": ["@DIF@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the baseline."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-80", "intents": ["@DIF@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [1] ."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-129", "intents": ["@DIF@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than baseline."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-135", "intents": ["@DIF@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "Experiments show that the proposed approach outperforms the baseline model with CE loss by relative 22%-54% across a variety of conditions."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "Recently introduced end-to-end trainable DNN approaches [1, 13] further improved accuracy and lowered resource requirements using highly optimizable system design."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "In [1] , the top level loss is derived by integrating frame-level losses, which are computed using frame-level labels from LVCSR."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "In [1] , the encoder model is trained to predict phonemelevel labels provided from LVCSR."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-90", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "We selected E2E 318K architecture in [1] as the baseline and use the same structure for testing all other models."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-125", "intents": ["@USE@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "Fig.3 shows the ROC curves of various models (baseline, Max1-Max4) across different conditions."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-81", "intents": ["@SIM@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "Both the baseline and the proposed model have the same architecture."}
{"sent_id": "0da20f60adaff9637ebdbe2a27f2a4-C001-4", "intents": ["@MOT@"], "paper_id": "ABC_0da20f60adaff9637ebdbe2a27f2a4_2", "text": "Such an issue is particularly prevalent when employing VAE-RNN architectures for text modelling (Bowman et al., 2016) ."}
{"sent_id": "0da20f60adaff9637ebdbe2a27f2a4-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_0da20f60adaff9637ebdbe2a27f2a4_2", "text": "When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder (Bowman et al., 2016; Xu and Durrett, 2018; Dieng et al., 2019) ."}
{"sent_id": "0da20f60adaff9637ebdbe2a27f2a4-C001-57", "intents": ["@DIF@"], "paper_id": "ABC_0da20f60adaff9637ebdbe2a27f2a4_2", "text": "( 3) As can be seen in Eq. 3, our solution to the KL collapse issue does not require any engineering for balancing the weight between the reconstruction term and KL loss as commonly the case in existing works (Bowman et al., 2016; ."}
{"sent_id": "0da20f60adaff9637ebdbe2a27f2a4-C001-92", "intents": ["@USE@"], "paper_id": "ABC_0da20f60adaff9637ebdbe2a27f2a4_2", "text": "We can see that the KL loss of VAE-LSTMbase, which uses Sigmoid annealing (Bowman et al., 2016) , collapses to zero, leading to a poor generative performance as indicated by the high reconstruction loss."}
{"sent_id": "be26538a785f9ec9edc1ea031194cf-C001-13", "intents": ["@MOT@"], "paper_id": "ABC_be26538a785f9ec9edc1ea031194cf_2", "text": "Although there are many previous works which deal with Hindi and English hate speech (the top two languages in India), but very few on the code-switched version (Hinglish) of the two (Mathur et al. 2018) ."}
{"sent_id": "be26538a785f9ec9edc1ea031194cf-C001-26", "intents": ["@DIF@"], "paper_id": "ABC_be26538a785f9ec9edc1ea031194cf_2", "text": "Thus, with this in mind, we build a transfer learning based model for the code-switched language Hinglish, which outperforms the baseline model of (Mathur et al. 2018) ."}
{"sent_id": "12c5d72fad925c8ec025cda87a0fd9-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_12c5d72fad925c8ec025cda87a0fd9_2", "text": "Verb-noun combinations (VNCs), consisting of a verb with a noun in its direct object position, are a common type of semantically-idiomatic MWE in English and cross-lingually (Fazly et al., 2009 )."}
{"sent_id": "12c5d72fad925c8ec025cda87a0fd9-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_12c5d72fad925c8ec025cda87a0fd9_2", "text": "Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012) , treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008) , incorporated topic models (e.g., Li et al., 2010) , and made use of distributed representations of words (Gharbieh et al., 2016) ."}
{"sent_id": "12c5d72fad925c8ec025cda87a0fd9-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_12c5d72fad925c8ec025cda87a0fd9_2", "text": "Fazly et al. (2009) showed that idiomatic usages of a VNC tend to occur in that expression's canonical form, while literal usages do not."}
{"sent_id": "12c5d72fad925c8ec025cda87a0fd9-C001-116", "intents": ["@SIM@"], "paper_id": "ABC_12c5d72fad925c8ec025cda87a0fd9_2", "text": "In line with the findings of Fazly et al. (2009) , CForm achieves higher precision and recall on idiomatic usages than literal ones."}
{"sent_id": "15dd59368074f3473b57d86568807f-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_15dd59368074f3473b57d86568807f_2", "text": "Although traditional AES methods typically rely on handcrafted features (Larkey, 1998; Foltz et al., 1999; Attali and Burstein, 2006; Dikli, 2006; Wang and Brown, 2008; Chen and He, 2013; Somasundaran et al., 2014; Yannakoudakis et al., 2014; Phandi et al., 2015) , recent results indicate that state-of-the-art deep learning methods reach better performance (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016; Song et al., 2017; Tay et al., 2018) , perhaps because these methods are able to capture subtle and complex information that is relevant to the task (Dong and Zhang, 2016) ."}
{"sent_id": "15dd59368074f3473b57d86568807f-C001-20", "intents": ["@DIF@"], "paper_id": "ABC_15dd59368074f3473b57d86568807f_2", "text": "The empirical results indicate that our approach yields a better performance than state-of-the-art approaches (Phandi et al., 2015; Dong and Zhang, 2016; Tay et al., 2018) ."}
{"sent_id": "15dd59368074f3473b57d86568807f-C001-111", "intents": ["@DIF@"], "paper_id": "ABC_15dd59368074f3473b57d86568807f_2", "text": "Our score in this case (0.728) is even higher than both scores of Phandi et al. (2015) and Dong and Zhang (2016) when they use n t = 50."}
{"sent_id": "15dd59368074f3473b57d86568807f-C001-130", "intents": ["@DIF@"], "paper_id": "ABC_15dd59368074f3473b57d86568807f_2", "text": "Using a shallow approach, we report better results compared to recent deep learning approaches (Dong and Zhang, 2016; Tay et al., 2018) ."}
{"sent_id": "15dd59368074f3473b57d86568807f-C001-68", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_15dd59368074f3473b57d86568807f_2", "text": "As Dong and Zhang (2016), we scaled the essay scores into the range 0-1."}
{"sent_id": "15dd59368074f3473b57d86568807f-C001-73", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_15dd59368074f3473b57d86568807f_2", "text": "For the cross-domain experiments, we use the same source→target domain pairs as (Phandi et al., 2015; Dong and Zhang, 2016) , namely, 1→2, 3→4, 5→6 and 7→8."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "The progress in both fields has inspired researchers to build holistic architectures for challenging grounding [14, 15] , natural language generation from image/video [16, 17, 18] , image-to-sentence alignment [19, 20, 21, 22] , and recently presented question-answering problems [23, 24, 25, 26, 27] ."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "DAQUAR [27] is a challenging, large dataset for a question answering task based on real-world images."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-76", "intents": ["@BACK@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "The machine world in DAQUAR is represented as a set of images and questions about their content."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "DAQUAR also contains other parts of speech where only colors and spatial prepositions are grounded in [27] ."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-87", "intents": ["@BACK@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "Common sense knowledge DAQUAR includes questions that can be reliably answered using common sense knowledge."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-73", "intents": ["@MOT@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "In this section we discuss in isolation different challenges reflected in DAQUAR."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-91", "intents": ["@MOT@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "To sum up, we believe that common sense knowledge is an interesting venue to explore with DAQUAR."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-28", "intents": ["@USE@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "We exemplify some of our findings on the DAQUAR dataset [27] with the aim of demonstrating different challenges that are present in the dataset."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-110", "intents": ["@USE@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "We exemplify the aforementioned requirements by illustrating the WUPS scorean automatic metric that quantifies performance of the holistic architectures proposed by [27] ."}
{"sent_id": "4821d5a283c1d4ba162b58e5fac8bc-C001-108", "intents": ["@DIF@"], "paper_id": "ABC_4821d5a283c1d4ba162b58e5fac8bc_2", "text": "We observe that the difference between our best QWK scores and the other approaches are sometimes much higher in the cross-domain setting than in the in-domain setting."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-4", "intents": ["@MOT@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "However, their alternating optimization approach suffers from instabilities during training, requiring additional loss terms and careful hyperparameter tuning."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-23", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "Tu and Gimpel (2018) fine-tuned the cost-augmented network to match the test-time criterion, but found only minimal change from this fine-tuning."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "To stabilize training, Tu and Gimpel (2018) experimented with several additional terms in the training objectives, finding performance to be dependent on their inclusion."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "While SPENs have been used for multiple NLP tasks, including multi-label classification (Belanger and McCallum, 2016) , part-of-speech tagging (Tu and Gimpel, 2018) , and semantic role labeling (Belanger et al., 2017) , they are not widely used in NLP."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "When training the energy function parameters Θ, Tu and Gimpel (2018) replaced the cost-augmented inference step in the structured hinge loss from Belanger and McCallum (2016) with a costaugmented inference network F Φ and trained the energy function and inference network parameters jointly:"}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "As alternating optimization can be difficult in practice (Salimans et al., 2016) , Tu & Gimpel experimented with including several additional terms in the above objective to stabilize training."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-100", "intents": ["@BACK@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "Tu and Gimpel (2018) used the following objective for the cost-augmented inference network (maximizing it with respect to Φ):"}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-246", "intents": ["@BACK@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "Tu and Gimpel (2018) use truncation and CE during training."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-54", "intents": ["@USE@", "@SIM@", "@DIF@", "@EXT@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "We adopt the same learning framework as Tu & Gimpel of jointly learning the energy function and inference network, but we propose a novel objective function that jointly trains a cost-augmented inference network, a test-time inference network, and the energy function."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-210", "intents": ["@USE@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "In the relaxed output space Y R (x), y t,j can be interpreted as the probability of the tth position being labeled with label j. We then use the following energy for sequence labeling (Tu and Gimpel, 2018) :"}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-227", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "Like Tu and Gimpel (2018) , we use a BiLSTM to compute the input feature vector for each position, using hidden dimension of size 100."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-70", "intents": ["@SIM@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "We treat this optimization problem as a minmax game and find a saddle point for the game similar to Tu and Gimpel (2018) and Goodfellow et al. (2014) ."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-213", "intents": ["@SIM@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "Tu and Gimpel (2018) also added a global energy term that they referred to as a \"tag language model\" (TLM)."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-73", "intents": ["@DIF@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "Joint Parameterizations. If we were to train independent inference networks A Ψ and F Φ , this new objective could be much slower than the original approach of (Tu and Gimpel, 2018) ."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-108", "intents": ["@DIF@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "This is likely why Tu and Gimpel (2018) found it important to add several stabilization terms to the l 0 objective."}
{"sent_id": "03b7c2e050957dcff336183823e6f1-C001-57", "intents": ["@USE@"], "paper_id": "ABC_03b7c2e050957dcff336183823e6f1_2", "text": "To search for the most probable parse, we use the heuristic search algorithm described in (Titov and Henderson, 2007b) , which is a form of beam search."}
{"sent_id": "03b7c2e050957dcff336183823e6f1-C001-25", "intents": ["@MOT@"], "paper_id": "ABC_03b7c2e050957dcff336183823e6f1_2", "text": "When conditioning on words, we treated each word feature individually, as this proved to be useful in (Titov and Henderson, 2007b) ."}
{"sent_id": "03b7c2e050957dcff336183823e6f1-C001-82", "intents": ["@SIM@"], "paper_id": "ABC_03b7c2e050957dcff336183823e6f1_2", "text": "As was demonstrated in (Titov and Henderson, 2007b) , even a minimal set of local explicit features achieves results which are non-significantly different from a carefully chosen set of explicit features, given the language independent definition of locality described in section 2."}
{"sent_id": "03b7c2e050957dcff336183823e6f1-C001-54", "intents": ["@DIF@"], "paper_id": "ABC_03b7c2e050957dcff336183823e6f1_2", "text": "Unlike (Titov and Henderson, 2007b ), in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in (Henderson, 2003) ."}
{"sent_id": "9a8b29b10539be9e6c65172a97b16f-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_9a8b29b10539be9e6c65172a97b16f_2", "text": "[9] use a similar handcrafted feature engineered model to identify offensive language and distinguish it from hate speech."}
{"sent_id": "9a8b29b10539be9e6c65172a97b16f-C001-99", "intents": ["@USE@"], "paper_id": "ABC_9a8b29b10539be9e6c65172a97b16f_2", "text": "The third tweet is from [9] data set labelled as offensive language."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-25", "intents": ["@EXT@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "We present a reproduction and extension to the work of Schulder et al. (2017) , which introduced a lexicon of verbal polarity shifters, as well as methods to increase the size of this lexicon through bootstrapping."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-28", "intents": ["@EXT@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "We apply it to German, validating the generality of the approach and creating a new resource, a German lexicon of 677 verbal polarity shifters."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-46", "intents": ["@EXT@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "(i) we introduce a German lexicon of verbal polarity shifters; (ii) we reproduce and adapt the approach of Schulder et al. (2017) to German to extend our lexicon; (iii) we introduce additional methods that take advantage of the existence of the English verbal polarity shifter lexicon and improve upon the current state of the art."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-57", "intents": ["@EXT@", "@USE@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "As we reproduce and extend the work of Schulder et al. (2017) , all further use of and comparison to an English shifter lexicon refers to their bootstrapped lexicon as well."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-102", "intents": ["@EXT@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "In this section we briefly describe how we adapt the features of Schulder et al. (2017) to German language data."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-253", "intents": ["@EXT@", "@BACK@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "In their intrinsic evaluation Schulder et al. (2017) showed that explicit knowledge of a large number of polarity shifters can improve sentiment analysis."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Among verbs alone there are many hundreds (Schulder et al., 2017) ."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "To reduce the cost of creating such polarity shifter lexicons, Schulder et al. (2017) introduced methods to automatically generate a labeled list of words using either a limited amount of labeled training data or no labeled data at all."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "So far the only larger resources for polarity shifters are the English-language verbal shifter lexicons recently introduced by Schulder et al. (2017) and Schulder et al. (2018) ."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Schulder et al. (2017) also make use of NPIs in addition to a number of other features."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-149", "intents": ["@MOT@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "The motivation behind the work of Schulder et al. (2017) was to introduce a large lexicon of verbal polarity shifters."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-166", "intents": ["@FUT@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Schulder et al. (2017) also observe in their error analysis that some verbs act as shifters in only some of their word senses."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-277", "intents": ["@FUT@", "@SIM@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "While we have shown that the same approach for classifying verbal shifters works for German and English, future work will expand the number of languages, especially to verify that these methods can also be applied to non-Indo-European languages, such as Chinese, Japanese or Arabic."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-234", "intents": ["@DIF@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "SVM data+resource represents the previously best classifier (Schulder et al., 2017) ."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-112", "intents": ["@SIM@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "The word embeddings are created using Word2Vec (Mikolov et al., 2013) on the German web corpus DeWaC (Baroni et al., 2009) , using the same hyperparameters as Schulder et al. (2017) and German translations of their negation seeds."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-132", "intents": ["@SIM@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Schulder et al. (2017) used glosses, hypernyms and supersenses taken from the English WordNet (Miller et al., 1990) as features in their work."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-137", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Schulder et al. (2017) use the frame memberships of verbs as a feature, hypothesizing that verbal shifters will be found in the same frames."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-215", "intents": ["@SIM@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "7 As in Schulder et al. (2017) , accuracy proves to be a problematic measure, as it has a strong majority label bias."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-260", "intents": ["@SIM@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Like Schulder et al. (2017) , we see very high performance for the first quartile, matching their observation that manual confirmation is not strictly necessary for high confidence labels."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-71", "intents": ["@USE@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "We create a gold standard for German verbal shifters, following the approach Schulder et al. (2017) used for their English gold standard."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-201", "intents": ["@USE@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "We start our evaluation by reproducing the classifier evaluation of Schulder et al. (2017) ."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "While several techniques for merging both spaces have been proposed [1, 2, 3, 4, 5, 6] , little effort has been made in finding the most appropriate image embeddings to be used in that process."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "The generic pipeline defined by Kiros et al. [1] has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks [4, 18] ."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "In their work, Kiros et al. [1] define a vectorized representation of an input text by using GRU RNNs."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-67", "intents": ["@BACK@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "The elements of the multimodal pipeline that are tuned during the training phase of the model are shown in orange in Figure  1 ."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-17", "intents": ["@EXT@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "We do so by integrating the FNE into the multimodal embedding pipeline defined by Kiros et al. [1] , which is based in the use of a Gated Recurrent Units neural network (GRU) [10] for text encoding and CNN for image encoding."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-62", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "In our approach, we integrate the FNE with the multimodal embedding pipeline of Kiros et al. [1] ."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-79", "intents": ["@EXT@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "Additionally, we define a second baseline by using the original multimodal pipeline with a training configuration closer to the one used for the FNE experiments (i.e., same source CNN, same MME dimensionality, etc.)."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-21", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "We choose to test our contribution on this pipeline for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (e.g., caption generation)."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-91", "intents": ["@USE@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "These splits are the same ones used by Kiros et al. [1] and by Karpathy and Fei-Fei [22] ."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "Results obtained by the pipeline including the FNE are compared with the original pipeline of Kiros et al. [1] using a one-layer embedding, and also with the methods currently obtaining state-of-the-art results on the three datasets."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-78", "intents": ["@DIF@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "To properly measure the relevance of the FNE, we compare the results of the FN-MME with those of the original multimodal pipeline reported by Kiros et al. [1] (CNN-MME)."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-130", "intents": ["@DIF@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "We include these for the MSCOCO dataset, which was not evaluated in the original paper [1] ."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-132", "intents": ["@DIF@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "On all cases, the multimodal pipeline proposed by Kiros et al. [1] obtains equal or better results when using the FNE."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-147", "intents": ["@DIF@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "Since this happens for all models using the same pipeline (CNN-MME, CNN-MME †, CNN-MME*), these results indicate that the original architecture of Kiros et al. [1] is itself outperformed in general by more problem-specific techniques."}
{"sent_id": "95883b369c4b019fa98493a728c1a0-C001-31", "intents": ["@DIF@"], "paper_id": "ABC_95883b369c4b019fa98493a728c1a0_2", "text": "Figure 1 illustrates the results of (Bolukbasi et al. 2016 ) bias detection algorithm for a list of profession names with word embedding vectors of dimension 256 trained using the Skipgram algorithm (Mikolov et al. 2013 ) on a sample of 23k Wikipedia articles with 50k term vocabulary."}
{"sent_id": "95883b369c4b019fa98493a728c1a0-C001-39", "intents": ["@DIF@"], "paper_id": "ABC_95883b369c4b019fa98493a728c1a0_2", "text": "The authors assert that the direct bias measure can be used as a metric to conclude how much an embedding is biased."}
{"sent_id": "95883b369c4b019fa98493a728c1a0-C001-57", "intents": ["@DIF@"], "paper_id": "ABC_95883b369c4b019fa98493a728c1a0_2", "text": "We conclude that while meta analyses of the bias metrics proposed by (Bolukbasi et al. 2016) indicate that the metrics capture and somewhat quantify sociologically meaningful biases present in learned embedding spaces, the metrics are highly sensitive to the hyperparameter configurations of the algorithms used to learn them."}
{"sent_id": "95883b369c4b019fa98493a728c1a0-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_95883b369c4b019fa98493a728c1a0_2", "text": "For instance, using such a bias measure, (Bolukbasi et al. 2016) concluded that \"word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent\"."}
{"sent_id": "95883b369c4b019fa98493a728c1a0-C001-25", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_95883b369c4b019fa98493a728c1a0_2", "text": "In this section, we evaluate the stability of the bias measure developed in (Bolukbasi et al. 2016) which is claimed to measure societal biases in word embeddings."}
{"sent_id": "822b2010b07d3e1103f904fa45388a-C001-54", "intents": ["@MOT@"], "paper_id": "ABC_822b2010b07d3e1103f904fa45388a_2", "text": "• a. We wanted to replicate the basic transfer learning results (Zoph et al., 2016) and hence chose French, German for Hausa and Uzbek."}
{"sent_id": "822b2010b07d3e1103f904fa45388a-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_822b2010b07d3e1103f904fa45388a_2", "text": "In the case of low resource languages like Hausa, vanilla NMT is either worse than or comparable to PBSMT (Zoph et al., 2016) ."}
{"sent_id": "822b2010b07d3e1103f904fa45388a-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_822b2010b07d3e1103f904fa45388a_2", "text": "Transfer learning for NMT (Zoph et al., 2016) is an approach where previously trained NMT models for French and German to English (resource rich pairs) were used to initialize models for Hausa, Uzbek, Spanish to English (resource poor pairs)."}
{"sent_id": "6b147afca676882878e67bc10abd58-C001-18", "intents": ["@SIM@"], "paper_id": "ABC_6b147afca676882878e67bc10abd58_2", "text": "Hence, paralleling Barzilay and Lapata (2008) , our model has the following structure."}
{"sent_id": "6b147afca676882878e67bc10abd58-C001-131", "intents": ["@SIM@"], "paper_id": "ABC_6b147afca676882878e67bc10abd58_2", "text": "When compared to the results obtained by Barzilay and Lapata (2008) and Barzilay and Lee (2004) , it would appear that direct sentenceto-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset."}
{"sent_id": "6b147afca676882878e67bc10abd58-C001-49", "intents": ["@DIF@"], "paper_id": "ABC_6b147afca676882878e67bc10abd58_2", "text": "Yet, in contrast to Barzilay and Lapata (2008) , who employ the syntactic properties of the respective occurrences, we reduce the accounts to whether or not the entities occur in subsequent sentences (similar to Karamanis (2004) 's NOCB metric)."}
{"sent_id": "6b147afca676882878e67bc10abd58-C001-60", "intents": ["@MOT@"], "paper_id": "ABC_6b147afca676882878e67bc10abd58_2", "text": "However, the coherence of a text is clearly not only defined by direct relations, but also requires longer range relations between sentences (e.g., Barzilay and Lapata (2008) )."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-55", "intents": ["@EXT@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "Note that we started with the raw transcripts of dialogue sessions to create our own version of the dataset for the model."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-12", "intents": ["@USE@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "This research makes use of a recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017) , which contains multiple dialogue sessions in the fashion domain."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "The MMD dataset provides an interesting new challenge, combining recent efforts on task-oriented dialogue systems, as well as visually grounded dialogue."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "However, in contrast to previous slot-filling dialogue systems, e.g. (Rieser and Lemon, 2011; Young et al., 2013) , it heavily relies on the extra visual modality to drive the conversation forward (see Figure 1) ."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-42", "intents": ["@DIF@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "Example contexts for a given system utterance; note the difference in our approach from Saha et al. (2017) when extracting the training data from the original chat logs."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-56", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "This is done since the authors originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3 )."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-70", "intents": ["@DIF@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "4 Note that the results reported in their paper are on a different version of the corpus, hence not directly comparable."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-86", "intents": ["@DIF@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "Contrary to their results, our generation outputs improved by adding attention and increasing context size."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-4", "intents": ["@MOT@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "Wang et al. (2014a) rely on Wikipedia anchors, making the applicable scope quite limited."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-7", "intents": ["@DIF@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of Wang et al. (2014a) , which is encouraging as we do not use any anchor information."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-77", "intents": ["@DIF@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "Alignment Model This part is different from Wang et al. (2014a) ."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "An important milestone, the approach of Wang et al. (2014a) solves issue (1) by jointly embedding entities, relations, and words into the same vector space and hence is able to deal with words/phrases beyond entities in KBs."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-36", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "Most knowledge embedding models thereafter including this paper are variants of this model (Wang et al., 2014b; Wang et al., 2014a; Lin et al., 2015) ."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-65", "intents": ["@USE@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "We follow the jointly embedding framework of (Wang et al., 2014a) , i.e., learning optimal embeddings by minimizing the following loss"}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-67", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in (Wang et al., 2014a) ."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-70", "intents": ["@USE@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "where z(h, r, t) = b − 0.5 · h + r − t 2 2 , b = 7 as suggested by Wang et al. (2014a) ."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-117", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "We follow the same protocol in (Wang et al., 2014a) ."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-134", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "Since both Mintz and MIML are probabilistic models, we use the same method in (Wang et al., 2014a) to linearly combine the scores."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-149", "intents": ["@SIM@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "(3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in (Wang et al., 2014a) ."}
{"sent_id": "c60a1131c6b1639b772b0e5c59588e-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_c60a1131c6b1639b772b0e5c59588e_2", "text": "To improve the expressivity of the added paths, instead of the unlexicalized labels, (Gardner et al., 2013) augmented the KB graph with verbs (surface relations) from a corpus containing over 600 million Subject-Verb-Object (SVO) triples."}
{"sent_id": "c60a1131c6b1639b772b0e5c59588e-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_c60a1131c6b1639b772b0e5c59588e_2", "text": "Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 2013) ."}
{"sent_id": "c60a1131c6b1639b772b0e5c59588e-C001-32", "intents": ["@DIF@"], "paper_id": "ABC_c60a1131c6b1639b772b0e5c59588e_2", "text": "Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by (Gardner et al., 2014) , but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature."}
{"sent_id": "c60a1131c6b1639b772b0e5c59588e-C001-45", "intents": ["@DIF@"], "paper_id": "ABC_c60a1131c6b1639b772b0e5c59588e_2", "text": "Thus, the number of paths added in this manner is much lower than the number of surface relations added using the procedure in (Gardner et al., 2013) ."}
{"sent_id": "7f78697390e28cc7798f8cb183cb59-C001-14", "intents": ["@USE@"], "paper_id": "ABC_7f78697390e28cc7798f8cb183cb59_2", "text": "In this paper, following prior work (e.g., Salton et al., 2016 ), we frame token-level identification of VNCs as a supervised binary classification problem, i.e., idiomatic vs. literal."}
{"sent_id": "7f78697390e28cc7798f8cb183cb59-C001-69", "intents": ["@USE@"], "paper_id": "ABC_7f78697390e28cc7798f8cb183cb59_2", "text": "Following Salton et al. (2016) , we use DEV and TEST, and ignore all token instances annotated as \"unknown\"."}
{"sent_id": "7f78697390e28cc7798f8cb183cb59-C001-75", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_7f78697390e28cc7798f8cb183cb59_2", "text": "We retain We then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as Salton et al. (2016) ."}
{"sent_id": "7f78697390e28cc7798f8cb183cb59-C001-81", "intents": ["@USE@"], "paper_id": "ABC_7f78697390e28cc7798f8cb183cb59_2", "text": "We randomly divide both DEV and TEST into training and testing portions ten times, following Salton et al. (2016) ."}
{"sent_id": "7f78697390e28cc7798f8cb183cb59-C001-17", "intents": ["@DIF@"], "paper_id": "ABC_7f78697390e28cc7798f8cb183cb59_2", "text": "Surprisingly, we find that an approach based on representing sentences as the average of their word embeddings performs comparably to, or better than, the skip-thoughts based approach previously proposed by Salton et al. (2016) ."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-142", "intents": ["@DIF@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "This score is higher than that from Kajiwara and Yamamoto (2015) by 0.190."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-52", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "Figure 1 shows a part of the dataset of Kajiwara and Yamamoto (2015) ."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-133", "intents": ["@USE@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "The baseline integration ranking used an average score (Kajiwara and Yamamoto, 2015) ."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-159", "intents": ["@USE@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "Annotated data is collected by our and Kajiwara and Yamamoto (2015)'s work in ranking substitutes task, and which size is 21,700 ((2010 + 2330) 5) rankings."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-181", "intents": ["@USE@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "Finally, to compare two datasets, we used the Pearson product-moment correlation coefficient between our dataset and the dataset of Kajiwara and Yamamoto (2015) against the annotated data."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "Kajiwara and Yamamoto (2015) constructed an evaluation dataset for Japanese lexical simplification 1 in languages other than English."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-19", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "However, there are four drawbacks in the dataset of Kajiwara and Yamamoto (2015) : (1) they extracted sentences only from a newswire corpus; (2) they substituted only a single target word; (3) they did not allow ties; and (4) they did not integrate simplification ranking considering the quality."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "They used crowdsourcing to find five annotators different from those who performed the substitute extraction task."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-41", "intents": ["@MOT@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "During the substitute extraction task, they collected substitutes of each target word in 10 different contexts."}
{"sent_id": "6330c615d4c62b30933dac3057c9d6-C001-31", "intents": ["@USE@"], "paper_id": "ABC_6330c615d4c62b30933dac3057c9d6_3", "text": "The discriminator decoder takes the hidden state at the last time step of a sequence concatenated with both the max-pooled and mean-pooled representation of the hidden states [20] and outputs a number in the range [0, 1]."}
{"sent_id": "6330c615d4c62b30933dac3057c9d6-C001-42", "intents": ["@USE@"], "paper_id": "ABC_6330c615d4c62b30933dac3057c9d6_3", "text": "We first pre-train our generator on the Gutenberg dataset [25] for 20 epochs and then fine-tune [20] them to our target datasets with a language modeling objective."}
{"sent_id": "6330c615d4c62b30933dac3057c9d6-C001-35", "intents": ["@SIM@"], "paper_id": "ABC_6330c615d4c62b30933dac3057c9d6_3", "text": "We use Adam optimizer [23] with β1 = 0.7 and β2 = 0.8 similar to [20] and use a batch size of 50."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-5", "intents": ["@DIF@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "Our proposed supervised and unsupervised approaches perform better than the supervised and unsupervised approaches of Fazly et al. (2009) , respectively."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-30", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "We then propose an unsupervised approach to this task, that combines word embeddings with Fazly et al.'s unsupervised CFORM approach, that improves over CFORM."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-82", "intents": ["@DIF@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "All of these accuracies are higher than those reported by Fazly et al. (2009) for their supervised approach."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-83", "intents": ["@DIF@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "They are also substantially higher than the most-frequent class baseline, and the unsupervised CFORM method of Fazly et al."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-106", "intents": ["@DIF@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "For k = 4 and 5 on TEST, this approach surpasses the unsupervised CFORM method of Fazly et al. (2009) ; however, on DEV this approach does not outperform Fazly et al.'s CFORM approach for any of the val-ues of k considered."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "They are a common and productive type of English idiom, and occur cross-lingually (Fazly et al., 2009) ."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "They define lexico-syntactic patterns for VNIC token instances based on the noun's determiner (e.g., a, the, or possibly no determiner), the number of the noun (singular or plural), and the verb's voice (active or passive)."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "2 They then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "The CFORM method of Fazly et al. (2009 ) is a strong unsupervised benchmark for this task, and relies on the lexico-syntactic pattern in which an MWE token instance occurs."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-76", "intents": ["@USE@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "Following Fazly et al. (2009) , the supervised approach was evaluated using a leave-one-token-out strategy."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "The goal of (Herbelot and Vecchi, 2015) is to analyze whether there exists a transformation from the word embedding of a concept to its model-theoretic vector, the gold standard being the human annotations."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-58", "intents": ["@USE@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "(Herbelot and Vecchi, 2015) 's system."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-80", "intents": ["@USE@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "(Herbelot and Vecchi, 2015) ) in the last row."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-77", "intents": ["@DIF@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "• Random vectors: (Herbelot and Vecchi, 2015) used pre-trained word embeddings as input to the PLSR, we instead simply use random vectors of same dimension (300, continuous uniform distribution between 0 and 1)."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-93", "intents": ["@DIF@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "Furthermore, the mode baseline yields results that are good on the AD dataset (0.554, vs. 0.634 in (Herbelot and Vecchi, 2015) vs. 0.572 in our PLSR + word2vec implementation), and significantly better than all other models on the QMR dataset (0.522, vs. 0.346 in (Herbelot and Vecchi, 2015) , i.e. +51% improvement)."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-84", "intents": ["@SIM@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "For each run, a train/test split was randomly chosen (60 training samples for AD, 400 for QMR, in order to have the same number of training samples as in (Herbelot and Vecchi, 2015) 's Table 2 )."}
{"sent_id": "7e380d496bb253885218465b778cc1-C001-44", "intents": ["@USE@"], "paper_id": "ABC_7e380d496bb253885218465b778cc1_3", "text": "We use Average Normalized Entropy (ANE) (Boito et al., 2019a) computed over these matrices for selecting the most confident one for segmenting each phoneme sequence."}
{"sent_id": "7e380d496bb253885218465b778cc1-C001-63", "intents": ["@USE@"], "paper_id": "ABC_7e380d496bb253885218465b778cc1_3", "text": "Lastly, following the methodology from Boito et al. (2019a) , we extract the most confident alignments (in terms of ANE) discovered by the bilingual models."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-17", "intents": ["@USE@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "Specifically, we evaluate a state-of-the-art coreference resolution system ) that makes use of ELMo's contextual embeddings on WinoBias (Zhao et al., 2018a) , a coreference diagnostic dataset that evaluates whether systems behave differently on decisions involving male and female entities of stereotyped or anti-stereotyped occupations."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-43", "intents": ["@USE@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "We use the set of occupation words defined in the WinoBias corpus and their assignments as prototypically male or female (Zhao et al., 2018a) ."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-54", "intents": ["@USE@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "To visualize the gender subspace, we pick a few sentence pairs from WinoBias (Zhao et al., 2018a) ."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-77", "intents": ["@USE@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "Table 2 : F1 on OntoNotes and WinoBias development sets."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-94", "intents": ["@USE@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "We evaluate the performance of both aspects of this approach."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-103", "intents": ["@USE@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "ELMo Bias Transfers to Coreference Row 3 in Table 2 summarizes performance of the ELMo based coreference system on WinoBias."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-105", "intents": ["@USE@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "It exhibits large differences between pro-and anti-stereotyped sets (|Diff|) on both semantic and syntactic examples in WinoBias."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-5", "intents": ["@EXT@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-27", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-76", "intents": ["@BACK@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "It contains two different subsets, pro-stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti-stereotype, when the opposite relation is true."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-86", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "Previous work (Zhao et al., 2018a) evaluated the systems based on GloVe embeddings but here we evaluate a state-of-the-art system that trained on the OntoNotes corpus with ELMo embeddings ."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-91", "intents": ["@BACK@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "Zhao et al. (2018a) propose a method to reduce gender bias in coreference resolution by augmenting the training corpus for this task."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-108", "intents": ["@MOT@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "Neutralization is less effective than augmentation and cannot fully remove gender bias on the Semantics Only portion of WinoBias, indicating it is effective only for simpler cases."}
{"sent_id": "311b238406da4891c09cb9c3c0334d-C001-44", "intents": ["@DIF@"], "paper_id": "ABC_311b238406da4891c09cb9c3c0334d_3", "text": "We did not identify strong candidates to build a domain specific dictionary as in [3] ."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "Ambati et al. (2013) showed that the performance of Malt (Nivre et al., 2007b) on the free word order language, Hindi, is improved by using lexical categories from Combinatory Categorial Grammar (CCG) (Steedman, 2000) ."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013) , they first created a Hindi CCGbank from a Hindi dependency treebank and built a supertagger."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "In addition to the above mentioned features, Ambati et al. (2013) employed morphological features useful for Hindi."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-119", "intents": ["@EXT@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "This is true both in the case of English (a fixed word order language) and Hindi (free word order and morphologically richer language), extending the result of Ambati et al. (2013) ."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-43", "intents": ["@USE@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "We used the English (Hockenmaier and Steedman, 2007) and Hindi CCGbanks (Ambati et al., 1 http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html 2013) for our experiments."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-61", "intents": ["@USE@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "For Hindi, we also did all our experiments using automatic features Ambati et al. (2013) )."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-106", "intents": ["@SIM@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "Ambati et al. (2013) reported similar improvements for Malt as well."}
{"sent_id": "f1a800c7cd47ac2edf2172cedb5889-C001-37", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_f1a800c7cd47ac2edf2172cedb5889_3", "text": "These results are interesting, since in vintage parsers, predicted case usually harmed accuracy (Tsarfaty et al., 2010) ."}
{"sent_id": "4dbf6e2bfa30e96816b7f6d9c6e069-C001-29", "intents": ["@USE@"], "paper_id": "ABC_4dbf6e2bfa30e96816b7f6d9c6e069_3", "text": "To aggregate the information from multiple utterances, we adapt an existing integer linear programming (ILP) based fusion technique [1] ."}
{"sent_id": "4dbf6e2bfa30e96816b7f6d9c6e069-C001-66", "intents": ["@USE@"], "paper_id": "ABC_4dbf6e2bfa30e96816b7f6d9c6e069_3", "text": "In order to solve the above ILP problem, we impose a number of constraints."}
{"sent_id": "4dbf6e2bfa30e96816b7f6d9c6e069-C001-67", "intents": ["@EXT@"], "paper_id": "ABC_4dbf6e2bfa30e96816b7f6d9c6e069_3", "text": "Some of the constraints have been directly adapted from the original ILP formulation [1] ."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-71", "intents": ["@EXT@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "To achieve such an evaluation, we have first extended the work of (Schnabel et al., 2015) to include sentential context to avoid word sense ambiguity faced by a human tester."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-76", "intents": ["@EXT@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "Our chief idea is to extend the work of (Schnabel et al., 2015) by adding a context sentence for each query term."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "The crowdsourcingbased intrinsic evaluation which tests embeddings for semantic relationship between words focuses on a direct comparison of word embeddings with respect to individual queries."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "The method in (Schnabel et al., 2015) started by creating a query inventory which is a pre-selected set of query terms and semantically related target words."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-46", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "Although the experiments in (Schnabel et al., 2015) incorporated participants with adequate knowledge of English, the ambiguity is inherent in the language."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-80", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "In fact, (Schnabel et al., 2015) have already considered 'I don't know the meaning of one (or several) of the words'; however, when the context is in place, there may be a situation when none of the embeddings make a good match for the query term, and in that case 'None of the above' is more appropriate."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-151", "intents": ["@BACK@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "At the end of Sec. 2.2, we explained how word sense ambiguity is accommodated in (Schnabel et al., 2015) ."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-21", "intents": ["@MOT@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "Although the method is promising for evaluating different word embeddings, it has some shortcomings."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-25", "intents": ["@MOT@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "Thus, if the approach is based only on the word without its context, it will be difficult for humans to understand the meaning of a particular word, and it could result in word sense ambiguity (WSA)."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-167", "intents": ["@DIF@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "The method of (Schnabel et al., 2015) relies on user's subjective and knowledge dependent ability to select 'preferred' meanings whereas our method would deal with this problem selecting explicit contexts for words."}
{"sent_id": "fe539365c7bb4555280fd1a5478aba-C001-22", "intents": ["@USE@"], "paper_id": "ABC_fe539365c7bb4555280fd1a5478aba_3", "text": "We evaluated the proposed features with a Support Vector Machines (SVM) classifier using a corpus of 1600 reviews of hotels (Ott et al., 2011; Ott et al., 2013) ."}
{"sent_id": "fe539365c7bb4555280fd1a5478aba-C001-120", "intents": ["@USE@"], "paper_id": "ABC_fe539365c7bb4555280fd1a5478aba_3", "text": "In Table 4 we can observe the indirect comparison of our results with those of (Banerjee and Chua, 2014) and (Ren et al., 2014) obtained with a 10 fold cross validation experiment, and then, with a 5 fold cross validation in order to make a fair comparison with the results of (Ott et al., 2011) and (Feng and Hirst, 2013) ."}
{"sent_id": "fe539365c7bb4555280fd1a5478aba-C001-121", "intents": ["@USE@"], "paper_id": "ABC_fe539365c7bb4555280fd1a5478aba_3", "text": "Note that the results are expressed in terms of the accuracy as those were published by the authors; the results correspond only to positive reviews of the Opinion Spam corpus because the authors experimented in that corpus alone."}
{"sent_id": "fe539365c7bb4555280fd1a5478aba-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_fe539365c7bb4555280fd1a5478aba_3", "text": "From the 800 positive reviews (Ott et al., 2011) , the 400 truthful where mined from TripAdvisor 5-star reviews about the 20 most popular hotels in Chicago area."}
{"sent_id": "fe539365c7bb4555280fd1a5478aba-C001-132", "intents": ["@SIM@"], "paper_id": "ABC_fe539365c7bb4555280fd1a5478aba_3", "text": "5 fold cross-validation (Ott et al., 2011) 89.8% (Feng and Hirst, 2013) 91.3% Our approach 89.8% Table 4 : Indirect comparison of the performance."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "This shortcoming of aggregate parsing metrics was highlighted in a recent study by Rimell et al. (2009) , introducing a new parser evaluation corpus containing around 700 sentences annotated with unbounded dependencies in seven different grammatical constructions."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "Moreover, Rimell et al. (2009) show that, although individual types of unbounded dependencies may be rare, the unbounded dependency types in the corpus, considered as a class, occur in as many as 10% of sentences in the PTB."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "It is well known that questions are very rare in the WSJ data, and Rimell et al. (2009) found that parsers trained only on WSJ data generally performed badly on the questions included in the evaluation corpus, while the C&C parser equipped with a model trained on a combination of WSJ and question data had much better performance."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-118", "intents": ["@BACK@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "WAvg excludes ObQ sentences, since frequency statistics were not available for this construction in Rimell et al. (2009) ."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-14", "intents": ["@EXT@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "In this paper, we extend the evaluation of Rimell et al. (2009) to two dependency parsers, MSTParser (McDonald, 2006) and MaltParser (Nivre et al., 2006a) , trained on data from the PTB, converted to Stanford typed dependencies (de Marneffe et al., 2006) , and combined with a simple post-processor to extract unbounded dependencies from the basic dependency tree."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-71", "intents": ["@DIF@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "One important difference between MSTParser and MaltParser, on the one hand, and the best performing parsers evaluated in Rimell et al. (2009) , on the other, is that the former were never developed specifically as parsers for English."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-89", "intents": ["@USE@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "All the development and test sets in the corpus of Rimell et al. (2009) were parsed using MSTParser and MaltParser after part-of-speech tagging the input using SVMTool (Giménez and Màrquez, 2004 ) trained on section 2-21 of the WSJ data in Stanford basic dependency format."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-108", "intents": ["@USE@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "The evaluation was performed using the same criteria as in Rimell et al. (2009) ."}
{"sent_id": "9f1d2be80dbfd726a24fb2a05e130b-C001-17", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_9f1d2be80dbfd726a24fb2a05e130b_3", "text": "Based on successful studies on Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014; Hirschberg and Manning, 2015) , in this paper, we proposed a character-based encoderdecoder model which learn to transliterate endto-end."}
{"sent_id": "9f1d2be80dbfd726a24fb2a05e130b-C001-35", "intents": ["@USE@"], "paper_id": "ABC_9f1d2be80dbfd726a24fb2a05e130b_3", "text": "We conducted a set of experiments to show the effectiveness of RNN Encoder-Decoder model (Cho et al., 2014b; Sutskever et al., 2014) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task ."}
{"sent_id": "fca14f99953b9dc30a594525ee92b5-C001-12", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_fca14f99953b9dc30a594525ee92b5_3", "text": "In this paper, we first present a short review and comparison of two representative efforts on this topic [6, 7] , where both efforts involve using an auto-encoder and can be applied to the same task (i.e., voice conversion), but the key disentangling algorithms and underlying ideas are very different."}
{"sent_id": "fca14f99953b9dc30a594525ee92b5-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_fca14f99953b9dc30a594525ee92b5_3", "text": "In [6] , the authors proposed an unsupervised factorized hierarchical variational autoencoder (FHVAE)."}
{"sent_id": "fca14f99953b9dc30a594525ee92b5-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_fca14f99953b9dc30a594525ee92b5_3", "text": "That is, in [6] , z1 and z2 are in fact corresponding to general fast-changing and slow-changing information, i.e., z1 may contain other fast-changing information such as emotion, while z2 may contain slow-changing factors such as background and channel noise."}
{"sent_id": "fca14f99953b9dc30a594525ee92b5-C001-20", "intents": ["@DIF@"], "paper_id": "ABC_fca14f99953b9dc30a594525ee92b5_3", "text": "Different from [6] , in [7] , the authors propose a supervised approach based on adversarial training [11, 12, 13, 14] (illustrated in Figure 2 (left))."}
{"sent_id": "fca14f99953b9dc30a594525ee92b5-C001-41", "intents": ["@DIF@"], "paper_id": "ABC_fca14f99953b9dc30a594525ee92b5_3", "text": "In contrast, a coarse-grained disentangled representation [6, 7] may only support a simple voice speaker conversion task."}
{"sent_id": "e7f972baa73e7ababa28eded3adad9-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_e7f972baa73e7ababa28eded3adad9_3", "text": "To further catalyse the research in word segmentation for Sanskrit, Krishna et al. (2017) has released a dataset for the word segmentation task."}
{"sent_id": "e7f972baa73e7ababa28eded3adad9-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_e7f972baa73e7ababa28eded3adad9_3", "text": "The work emphasises the need for not just predicting the inflected word form but also the prediction of the associated morphological information of the word."}
{"sent_id": "e7f972baa73e7ababa28eded3adad9-C001-195", "intents": ["@BACK@", "@FUT@"], "paper_id": "ABC_e7f972baa73e7ababa28eded3adad9_3", "text": "Given the importance of morphological segmentation in morphologically rich languages such as Hebrew and Arabic (Seeker and Ç etinoglu, 2015) , the same applies to the morphologically rich Sanskrit as well (Krishna et al., 2017) ."}
{"sent_id": "e7f972baa73e7ababa28eded3adad9-C001-69", "intents": ["@FUT@"], "paper_id": "ABC_e7f972baa73e7ababa28eded3adad9_3", "text": "The additional information will be beneficial in further processing of Sanskrit texts, such as Dependency parsing or summarisation (Krishna et al., 2017) .So far, no system successfully predicts the morphological information of the words in addition to the final word form."}
{"sent_id": "e7f972baa73e7ababa28eded3adad9-C001-88", "intents": ["@USE@"], "paper_id": "ABC_e7f972baa73e7ababa28eded3adad9_3", "text": "In our case we use 105,000 parallel strings from the Digital Corpus of Sanskrit as released in Krishna et al. (2017) ."}
{"sent_id": "1540b0b172971ac75771b414765f1d-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_1540b0b172971ac75771b414765f1d_3", "text": "Specifically, Bollmann et al. (2017) showed that multitask learning with German grapheme-to-phoneme translation as an auxiliary task improves a stateof-the-art sequence-to-sequence model for historical text normalization of medieval German manuscripts."}
{"sent_id": "1540b0b172971ac75771b414765f1d-C001-19", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_1540b0b172971ac75771b414765f1d_3", "text": "Specifically, we evaluate a state-ofthe-art approach to historical text normalization (Bollmann et al., 2017) with and without various auxiliary tasks, across 10 historical text normalization datasets."}
{"sent_id": "1540b0b172971ac75771b414765f1d-C001-35", "intents": ["@USE@"], "paper_id": "ABC_1540b0b172971ac75771b414765f1d_3", "text": "Model We use the same encoder-decoder architecture with attention as described in Bollmann et al. (2017) ."}
{"sent_id": "1540b0b172971ac75771b414765f1d-C001-44", "intents": ["@USE@"], "paper_id": "ABC_1540b0b172971ac75771b414765f1d_3", "text": "The hyperparameters were set on a randomly selected subset of 50,000 tokens from each of the following datasets: English, German (Anselm), Hungarian, Icelandic, and Slovene (Gaj)."}
{"sent_id": "81499fd759b958a0c02d9ed9d72a46-C001-23", "intents": ["@EXT@"], "paper_id": "ABC_81499fd759b958a0c02d9ed9d72a46_3", "text": "We also extend Duong et al. (2016) , which used a lexicon to learn bilingual word embeddings."}
{"sent_id": "81499fd759b958a0c02d9ed9d72a46-C001-52", "intents": ["@EXT@", "@USE@"], "paper_id": "ABC_81499fd759b958a0c02d9ed9d72a46_3", "text": "Here we also adopt their approach, and extend it to multilingual embeddings."}
{"sent_id": "91e4fd2556d4a04e477ea97208b218-C001-27", "intents": ["@DIF@"], "paper_id": "ABC_91e4fd2556d4a04e477ea97208b218_4", "text": "Evaluated by 3 human graders, questions generated by our system are significantly better then Serban et al. (2016) on grammaticality and naturalness."}
{"sent_id": "91e4fd2556d4a04e477ea97208b218-C001-64", "intents": ["@DIF@"], "paper_id": "ABC_91e4fd2556d4a04e477ea97208b218_4", "text": "We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than Serban et al. (2016) ."}
{"sent_id": "91e4fd2556d4a04e477ea97208b218-C001-69", "intents": ["@DIF@"], "paper_id": "ABC_91e4fd2556d4a04e477ea97208b218_4", "text": "On the other hand, questions from Serban et al. (2016) Ma et al. (2015) 85.48 Ours 85.65 Table 3 : Precision on the web snippet dataset was someone who was involved in the leukemia ?\" and \"whats the title of a book of the subject of the bible ?\"), unnatural (\"what 's one of the mountain where can you found in argentina in netflix ?\") or confusing (\"who was someone who was involved in the leukemia ?\")."}
{"sent_id": "91e4fd2556d4a04e477ea97208b218-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_91e4fd2556d4a04e477ea97208b218_4", "text": "Treating question generation as a machine translation problem, Serban et al. (2016) train a neural machine translation (NMT) system with 10,000 triple 1 , question pairs."}
{"sent_id": "91e4fd2556d4a04e477ea97208b218-C001-53", "intents": ["@USE@"], "paper_id": "ABC_91e4fd2556d4a04e477ea97208b218_4", "text": "In the first experiment, we compare our end-to-end system with the previous state-of-the-art method (Serban et al., 2016) on Freebase (Bollacker et al., 2008) , a domain-general KB."}
{"sent_id": "91e4fd2556d4a04e477ea97208b218-C001-62", "intents": ["@USE@"], "paper_id": "ABC_91e4fd2556d4a04e477ea97208b218_4", "text": "Using the averaged language model score as index, the top 500 questions are selected to compare with the results from Serban et al. (2016) ."}
{"sent_id": "91e4fd2556d4a04e477ea97208b218-C001-67", "intents": ["@USE@"], "paper_id": "ABC_91e4fd2556d4a04e477ea97208b218_4", "text": "Shown in Table 1 , we compare our questions with Serban et al. (2016) where questions in the same line describe the same entity."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-159", "intents": ["@MOT@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "However, such tuning, often empirically set to a small number such as S = 3 (Wang et al. 2015) , fails to infer varying number of senses of words, especially for words with a higher number of senses."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "STM also used word embeddings (Mikolov et al. 2013) to assign similarity weights during inference (STM+w2v) (Wang et al. 2015) ."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-158", "intents": ["@BACK@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "Recent models such as HC (Chang, Pei, and Chen 2014) and STM (Wang et al. 2015) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-116", "intents": ["@USE@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following (Wang et al. 2015) ."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-139", "intents": ["@USE@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (Wang et al. 2015) ."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-140", "intents": ["@USE@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (Jurgens and Klapaftis 2013) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (Wang et al. 2015) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (Chang et al. 2018) , and g) Multi Context Continuous model MCC as reported in (Komninos and Manandhar 2016) ."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-169", "intents": ["@USE@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "We compare the cluster errors of LDA (Blei, Ng, and Jordan 2003) , STM (Wang et al. 2015) , HC (Chang, Pei, and Chen 2014) , and a nonparametric model HDP (Teh et al. 2004 ), with AutoSense."}
{"sent_id": "55160a7ab2df9a86e677bcc72d9842-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_55160a7ab2df9a86e677bcc72d9842_4", "text": "Neural encoder-decoder models have proved effective in mapping NL to logical forms (Dong and Lapata, 2016) and also for directly producing general purpose programs (Iyer et al., 2017 (Iyer et al., , 2018 ."}
{"sent_id": "55160a7ab2df9a86e677bcc72d9842-C001-74", "intents": ["@USE@"], "paper_id": "ABC_55160a7ab2df9a86e677bcc72d9842_4", "text": "We apply our approach to the context dependent encoder-decoder model of Iyer et al. (2018) on the CONCODE dataset, and compare performance to a better tuned instance of their best model."}
{"sent_id": "55160a7ab2df9a86e677bcc72d9842-C001-92", "intents": ["@USE@"], "paper_id": "ABC_55160a7ab2df9a86e677bcc72d9842_4", "text": "Then, h 1 , . . . , h z , andt i ,v i ,r i ,m i are passed on to the attention mechanism in the decoder, exactly as in Iyer et al. (2018) ."}
{"sent_id": "55160a7ab2df9a86e677bcc72d9842-C001-100", "intents": ["@USE@"], "paper_id": "ABC_55160a7ab2df9a86e677bcc72d9842_4", "text": "We use a BPE vocabulary of 10K tokens for embedding matrix B and get the best validation set results using the original hyperparameters used by Iyer et al. (2018) ."}
{"sent_id": "55160a7ab2df9a86e677bcc72d9842-C001-103", "intents": ["@SIM@"], "paper_id": "ABC_55160a7ab2df9a86e677bcc72d9842_4", "text": "Iyer et al. (2018) ."}
{"sent_id": "55160a7ab2df9a86e677bcc72d9842-C001-115", "intents": ["@DIF@"], "paper_id": "ABC_55160a7ab2df9a86e677bcc72d9842_4", "text": "Compared to the model of Iyer et al. (2018) , our significantly reduced training time enables us to train on their extended training set."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-3", "intents": ["@EXT@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "Our contributions are summarized as follows: (1) we extend a probablistic model used in the previous work which concurrently performs word reordering and dependency parsing; (2) we conducted an evaluation experiment using our semi-automatically constructed evaluation data so that sentences in the data are more likely to be spontaneously written by natives than the automatically constructed evaluation data in the previous work."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-39", "intents": ["@EXT@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "In this paper, we refine the probabilistic model proposed by Yoshida et al. (2014) to improve the accuracy."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-102", "intents": ["@EXT@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "Especially, we extended the probablistic model proposed by Yoshida et al. (2014) to deal with sentences spontaneously written by a native."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "To solve the problem, we previously proposed a method for concurrently performing word reordering and dependency parsing and confirmed the effectiveness of their proposed method using evaluation data created by randomly changing the word order in newspaper article sentences (Yoshida et al., 2014) ."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-65", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "Therefore, our previous work (Yoshida et al., 2014) artificially generated sentences which were not easy to read, by just automatically changing the word order of newspaper article sentences in Kyoto Text Corpus 3 based on the dependency structure."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-45", "intents": ["@USE@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "The structure S is defined as a tuple S = ⟨O, D⟩ where In the probablistic model proposed by Yoshida et al. (2014) , P (S|B) was calculated as follows:"}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-56", "intents": ["@USE@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "Each factor in Formula (2) is estimated by the maximum entropy method in the same approximation procedure as that of Yoshida et al. (2014) ."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-83", "intents": ["@USE@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "All of the methods used the same training features as those described in Yoshida et al. (2014) ."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-94", "intents": ["@DIF@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "On the other hand, the sentence accuracy of our method was highest among all the methods although there were no significant differences in them."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "Although the prediction of Evocation ratings has attracted some attention (Boyd-Graber et al., 2006; Hayashi, 2016) , to the best of our knowledge this is the first work to focus on the prediction of USF or EAT strengths."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "They employed a combination of WordNet structure-based features, word embedding-based features, and lexical features and found that vector offsets, i.e. the mathematical difference between vectors, were a strong indicator of Evocation ratings."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file)."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-56", "intents": ["@EXT@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "Our word association prediction system extends the method in Hayashi (2016) with several modifications to make it better suited to the USF and EAT datasets."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-65", "intents": ["@EXT@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "Third, we extend the notion of dirRel, introduced in Hayashi (2016) to leverage the semantic network structure of WordNet."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-61", "intents": ["@DIF@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, instead using a one-hot encoding we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-68", "intents": ["@DIF@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "This may increase the probability that |nb(S, k)∩nb(T, k)| > 0, a shortcoming of the original dirRel due to WordNet's \"relatively sparse connective structure\" (Hayashi, 2016) ."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-74", "intents": ["@DIF@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in Hayashi (2016) to compute vector offsets are not well suited for our task."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-98", "intents": ["@DIF@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "On Evocation, our system does not perform as well as Hayashi (2016) ."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-82", "intents": ["@USE@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "To act as a baseline, we also reimplemented the system described in Hayashi (2016) and trained it on the same 80/20 split of Evocation as our system."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-96", "intents": ["@SIM@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "The results of our Hayashi (2016) implementation are roughly comparable to those reported in the original paper (r = 0.374, ρ = 0.401 compared to r = 0.439, ρ = 0.400)."}
{"sent_id": "87a87855d67d90c691ab5bedb4d460-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_87a87855d67d90c691ab5bedb4d460_4", "text": "The first attempt to build an end-to-end speech-to-text translation system (which does not use source language) is our own work [2] but it was applied to a synthetic (TTS) speech corpus."}
{"sent_id": "87a87855d67d90c691ab5bedb4d460-C001-18", "intents": ["@EXT@"], "paper_id": "ABC_87a87855d67d90c691ab5bedb4d460_4", "text": "While previous works [2, 3] investigated the extreme case where source language transcription is not available during learning nor decoding (unwritten language scenario defined in [6, 7] ), we also investigate, in this paper, a midway case where a certain amount of source language transcription is available during training."}
{"sent_id": "87a87855d67d90c691ab5bedb4d460-C001-50", "intents": ["@USE@"], "paper_id": "ABC_87a87855d67d90c691ab5bedb4d460_4", "text": "We also mirror our experiments on the BTEC synthetic speech corpus, as a follow-up to [2] ."}
{"sent_id": "87a87855d67d90c691ab5bedb4d460-C001-57", "intents": ["@USE@"], "paper_id": "ABC_87a87855d67d90c691ab5bedb4d460_4", "text": "The speech encoder is a mix between the convolutional encoder presented in [3] and our previously proposed encoder [2] ."}
{"sent_id": "87a87855d67d90c691ab5bedb4d460-C001-115", "intents": ["@DIF@"], "paper_id": "ABC_87a87855d67d90c691ab5bedb4d460_4", "text": "Contrary to [2] , we only present mono-reference results."}
{"sent_id": "87a87855d67d90c691ab5bedb4d460-C001-125", "intents": ["@DIF@"], "paper_id": "ABC_87a87855d67d90c691ab5bedb4d460_4", "text": "The large improvements on MT and AST on the BTEC corpus, compared to [2] are mostly due to our use of a better decoder, which outputs characters instead of words."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-23", "intents": ["@DIF@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "By using 10-fold cross validation (Kohavi and John, 1995) on the training set to automatically determine the best k to use, we have obtained improved disambiguation accuracy on a large sensetagged corpus first used in (Ng and Lee, 1996) ."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-107", "intents": ["@DIF@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of (Ng and Lee, 1996) in the third row, also with k = 1."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "On the other hand, our past work on WSD (Ng and Lee, 1996) used an exemplar-based (or nearest neighbor) learning approach."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-102", "intents": ["@BACK@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "However, the feature value pruning method of (Ng and Lee, 1996) only selects surrounding words and local collocations as feature values if they are indicative of some sense class as measured by conditional probability (See (Ng and Lee, 1996) for details)."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-128", "intents": ["@BACK@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "Our past work (Ng and Lee, 1996) suggests that multiple sources of knowledge are indeed useful for WSD."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-54", "intents": ["@USE@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "We have used the default values for all parameter settings in our previous work on exemplar-based WSD reported in (Ng and Lee, 1996) ."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-81", "intents": ["@SIM@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "Both test sets are identical to the ones reported in (Ng and Lee, 1996) ."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-132", "intents": ["@FUT@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "Also, given the relative importance of the various knowledge sources as reported in (Ng and Lee, 1996) , it may be possible to improve disambignation performance by introducing feature weighting."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "The innovation of BERT (Devlin et al., 2018) comes from the \"masked language model\" with a pre-training objective, inspired by the Cloze task (Taylor, 1953) ."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "BERT is based on the encoder of the transformer model (Vaswani et al., 2017) , which has been proven to obtain state-of-the-art accuracy across a broad range of NLP applications (Devlin et al., 2018) ."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-30", "intents": ["@MOT@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "2 Related work 2.1 BERT Our work focuses on improving the transformer architecture (Vaswani et al., 2017) , which motivated the recent breakthrough in language representation, BERT (Devlin et al., 2018) ."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-65", "intents": ["@USE@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "We used the same input and output representations, i.e., the embedding and positional encoding, and the same loss objective, i.e., masked LM prediction and next sentence prediction, from the BERT paper (Devlin et al., 2018) ."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-89", "intents": ["@USE@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "Following the BERT (Devlin et al., 2018) , we use masked language model loss and next sentence prediction (NSP) loss to train the models."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-111", "intents": ["@USE@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "We use the same large-scale data which has been used for BERT model pre-training, the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2.5B words) (Wikipedia contributors, 2004; Devlin et al., 2018) ."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-132", "intents": ["@USE@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "Following the previous work (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019a; Lan et al., 2019) , we evaluate our models on the General Language Understanding Evaluation (GLUE) benchmark and the Stanford Question Answering Dataset (SQuAD 1.1) (Rajpurkar et al., 2016) ."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-149", "intents": ["@USE@", "@UNSURE@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "EM F1 BERT-base (Devlin et al., 2018)"}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-179", "intents": ["@USE@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "Table 5 shows the results of GLUE datasets for original BERT (Devlin et al., 2018) , ours TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base and large settings respectively."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-122", "intents": ["@SIM@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "Similar to (Devlin et al., 2018) , the training data generator chooses 15% of the token positions at random for making."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-3", "intents": ["@MOT@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "It has also been shown that retraining models on data generated by AddSent has limited effect on their robustness."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-19", "intents": ["@MOT@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "In this paper, we show that their method of adversarial training failed because the specificity of the AddSent algorithm along with the lack of naturally-occurring counterexamples allow models to learn superficial clues regarding what is a 'distractor' and subsequently ignore it; thus significantly limiting their robustness."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-33", "intents": ["@MOT@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "When tested on these adversarial examples, Jia and Liang (2017) showed that even the most 'robust' amongst published models (the Mnemonic Reader (Hu et al., 2017) ) only achieved 46.6% F1 (compared to 79.6% F1 on the regular task)."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-69", "intents": ["@MOT@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "These shortcomings are reflected by our results in Sec. 4.6, where we see that we can't resolve all AddSent- style adversaries by diversifying the training data alone."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-5", "intents": ["@EXT@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "Further, in order to improve robustness to AddSent's semantic perturbations (e.g., antonyms), we jointly improve the model's semantic-relationship learning capabilities in addition to our AddSentDiversebased adversarial training data augmentation."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-39", "intents": ["@EXT@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "We show that using AddSent to generate adversarial training data introduces new superficial trends for a model to exploit; and instead we propose the AddSentDiverse algorithm that generates highly varied data for adversarial training, resulting in more robust models."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-46", "intents": ["@EXT@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "We thus introduce the AddSentDiverse algorithm, which adds two modifications to AddSent that allows for generating higher-variance adversarial examples."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "Many state-ofthe-art models exhibit a nearly 50% reduction in F1 score on AddSent, showing their over-reliance on syntactic similarity and limited semantic understanding."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "The models' failures on AddSent demonstrates their ignorance of this aspect of the task."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "In the field of Q&A, Jia and Liang (2017) introduced the AddSent algorithm, which generates adversaries that punish model failure in the other direction: overstability, or the inability to detect semantic-altering noise."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "For each {context, question, answer} triple, AddSent does the following: (1) Several antonym and named-entity based semantic altering perturbations (swapping) are applied to the question; (2) A fake answer is generated that matches the 'type' of the original answer (e.g., Prague → Chicago, etc.); (3) The fake answer and the altered question are combined into a distractor statement based on a set of manually defined rules; (4) Errors in grammar are fixed by crowd-workers; (5) The finalized distractor is appended to the end of the context."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "Overall, we demonstrate that with our adversarial training method and model improvement, we can increase the performance of a state-of-theart model by 36.46% on the AddSent evaluation set."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-38", "intents": ["@DIF@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "Despite performing well when evaluated on AddSent, the retrained model suffers a more than 30% decrease in F1 performance when tested on a slightly different adversarial dataset generated by AddSentMod (which differs from AddSent in two superficial ways: using a different set of fake answers and prepending instead of appending the distractor sentence to the context)."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-67", "intents": ["@DIF@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "But even if we force the model to learn some deeper methods for identifying/discarding the distractors, it only has limited ability in recognizing semantic differences because its current inputs do not capture crucial aspects of lexical semantics such as antonymy (which were inserted by Jia and Liang (2017) when generating the AddSent adversaries; see Sec. 3)."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-93", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "The retrained models are tested on AddSent and AddSentPrepend, whose only difference is where the distractor is located."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-80", "intents": ["@USE@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "Models are evaluated on the original SQuAD dev set and 4 adversarial datasets: AddSent, the adversarial evaluation set by Jia and Liang (2017) , and 3 variations of AddSent: AddSentPrepend, where the distractor is prepended to the context, AddSentRandom, where the distractor is randomly inserted into the context, 4 and AddSentMod (Jia and Liang, 2017) , where a different set of fake answers is used and the distractor is prepended to the context."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-84", "intents": ["@USE@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "In our main experiment, we compare the BSAE model's performance on different test sets when trained with three different training sets: the original SQuAD data (Original-SQuAD), SQuAD data augmented with AddSent generated adversaries (similar to adversarial training conducted by Jia and Liang (2017)), and SQuAD data augmented with our AddSentDiverse generated adversaries."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-109", "intents": ["@USE@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "Finally, we examined the errors of our final adversarially-trained BSAE+SA model on the AddSent dataset and found that out of the 21.09% remaining errors (Table 4) , 33.3% (46 cases) of these erroneous predictions occurred within the inserted distractor, and 63.7% (88 cases) occurred on questions that the model got wrong in the original SQuAD dev set (without the inserted distractors)."}
{"sent_id": "15c8ca572430c214d9c571fbe0db95-C001-10", "intents": ["@SIM@"], "paper_id": "ABC_15c8ca572430c214d9c571fbe0db95_4", "text": "In this paper, we present a phrase-based unigram system similar to the one in (Tillmann and Xia, 2003) , which is extended by an unigram orientation model."}
{"sent_id": "15c8ca572430c214d9c571fbe0db95-C001-50", "intents": ["@USE@"], "paper_id": "ABC_15c8ca572430c214d9c571fbe0db95_4", "text": "This is the model presented in (Tillmann and Xia, 2003) ."}
{"sent_id": "91c1a4ab0347fb8b11ff213a97e864-C001-5", "intents": ["@USE@"], "paper_id": "ABC_91c1a4ab0347fb8b11ff213a97e864_4", "text": "In this paper, we generalize the violation-fixing perceptron of Huang et al. (2012) to hypergraphs and apply it to the cube-pruning parser of Zhang and McDonald (2012) ."}
{"sent_id": "91c1a4ab0347fb8b11ff213a97e864-C001-55", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_91c1a4ab0347fb8b11ff213a97e864_4", "text": "This is the strategy used by Zhang and McDonald (2012) ."}
{"sent_id": "91c1a4ab0347fb8b11ff213a97e864-C001-58", "intents": ["@USE@"], "paper_id": "ABC_91c1a4ab0347fb8b11ff213a97e864_4", "text": "We ran a number of experiments on the cubepruning dependency parser of Zhang and McDonald (2012) , whose search space can be represented as a hypergraph in which the nodes are the complete and incomplete states and the hyperedges are the instantiations of the two parsing rules in the Eisner algorithm (Eisner, 1996) ."}
{"sent_id": "91c1a4ab0347fb8b11ff213a97e864-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_91c1a4ab0347fb8b11ff213a97e864_4", "text": "This idea was adapted to graph-based dependency parsers by Zhang and McDonald (2012) and shown to outperform left-to-right beam search."}
{"sent_id": "bdd7a4dabf8a8d7c0a2b638eb6eb72-C001-41", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_bdd7a4dabf8a8d7c0a2b638eb6eb72_4", "text": "Instead of hard mapping of surface relations to latent embeddings, (Gardner et al., 2014 ) perform a 'soft' mapping using vector space random walks."}
{"sent_id": "bdd7a4dabf8a8d7c0a2b638eb6eb72-C001-94", "intents": ["@BACK@"], "paper_id": "ABC_bdd7a4dabf8a8d7c0a2b638eb6eb72_4", "text": "PRA-SVO, PRA-VS are the systems proposed in (Gardner et al., 2013; Gardner et al., 2014) ."}
{"sent_id": "bdd7a4dabf8a8d7c0a2b638eb6eb72-C001-118", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_bdd7a4dabf8a8d7c0a2b638eb6eb72_4", "text": "The PRA-VS runtime also includes the time taken for generating embeddings to perform the vector space random walk."}
{"sent_id": "bdd7a4dabf8a8d7c0a2b638eb6eb72-C001-61", "intents": ["@DIF@"], "paper_id": "ABC_bdd7a4dabf8a8d7c0a2b638eb6eb72_4", "text": "**PRA-SVO AND PRA-VS**"}
{"sent_id": "bdd7a4dabf8a8d7c0a2b638eb6eb72-C001-106", "intents": ["@USE@"], "paper_id": "ABC_bdd7a4dabf8a8d7c0a2b638eb6eb72_4", "text": "For our experiments, we used the same 10 NELL relation data as used in (Gardner et al., 2014) ."}
{"sent_id": "bdd7a4dabf8a8d7c0a2b638eb6eb72-C001-109", "intents": ["@USE@"], "paper_id": "ABC_bdd7a4dabf8a8d7c0a2b638eb6eb72_4", "text": "Values for d max , and K, the most frequent paths, were obtained by tuning on a development set for 4 relations (athleteplaysforsport,actorstarredinmovie,citylocatedincountry (Gardner et al., 2013; Gardner et al., 2014) ."}
{"sent_id": "6092234b23f2620c356c2e417c2ce8-C001-10", "intents": ["@EXT@"], "paper_id": "ABC_6092234b23f2620c356c2e417c2ce8_4", "text": "We now extend this work to show that using only self-attention also works by substituting BERT (Devlin et al., 2018) ."}
{"sent_id": "6092234b23f2620c356c2e417c2ce8-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_6092234b23f2620c356c2e417c2ce8_4", "text": "In our earlier work (Kitaev and Klein, 2018) , we showed that such representations are helpful for constituency parsing."}
{"sent_id": "6092234b23f2620c356c2e417c2ce8-C001-29", "intents": ["@SIM@"], "paper_id": "ABC_6092234b23f2620c356c2e417c2ce8_4", "text": "All other hyperparameters are unchanged from Kitaev and Klein (2018) and Devlin et al. (2018) ."}
{"sent_id": "a127218cca5653f1700c0de6c8318a-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_a127218cca5653f1700c0de6c8318a_4", "text": "Recently, contextualized word embeddings such as (Devlin et al., 2019) were proposed in NLP to capture the context of each word usage in vectors and to model the semantic distances between the usages using contexts as a clue."}
{"sent_id": "a127218cca5653f1700c0de6c8318a-C001-36", "intents": ["@USE@"], "paper_id": "ABC_a127218cca5653f1700c0de6c8318a_4", "text": "Users can 1 https://github.com/huggingface/ pytorch-pretrained-BERT 2 Fig. 2 and Fig. 3 shows use cases on a 10, 000-sentence experpt of the BNC corpus to avoid having too many hits hinder the reading of the paper."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "Transformer [8] , which relies solely on self-attention to capture the temporal correlations in sequential signals, is a new type of neural network structure for sequence modeling, which has achieved excellent results in machine translation [8] , language modeling [9] , as well as end-to-end speech recognition [10, 11] ."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "In [10] , the authors compared RNNs with transformers for various speech recognition and synthesis tasks, and obtained competitive or even better results with transformers."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "While whole sequence-level self-attention has been applied in sequence-to-sequence models [10] , hybrid model is different in the sense that it is required to maintain strict frame-level alignments before performing predictions, which may be challenging for a transformer with multiple layers of self-attention as it may reorder the sequence."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "The attention mechanism in transformer is technically the same as in the original RNN-based attention model [19] ."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "where Q, K, V are referred to the query, key and value according to [8] ."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-100", "intents": ["@BACK@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "As for dropout training, it was pointed out in [8] that transformer model for sequence to-sequence ASR may suffer from overfitting easily, and regularization such as dropout is important to address such kind of issue."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-3", "intents": ["@MOT@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "Recently, there have been a few studies on transformer for end-to-end speech recognition, while its application for hybrid acoustic model is still very limited."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-23", "intents": ["@MOT@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "However, the key challenge for transformer-based sequence-to-sequence model is to perform online streaming speech recognition, as there is no clear boundary for chunk-wise self-attention."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-152", "intents": ["@MOT@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "While transformer has been very successful in the area of nature language processing, its application to speech recognition is mostly within the end-to-end architecture."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-18", "intents": ["@EXT@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "In this paper, we study the more standard transformer for speech recognition within the hybrid framework, and provide further insight to this model through experiments on the Librispeech public dataset."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-37", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "In this section, we review each component in the standard transformer model, and discuss a model structure that is mainly investigated for speech recognition in this work."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-64", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "In this paper, we propose a transformer model with interleaved 1D convolution and self-attention, with the motivation that the convolution layer can maintain the sequential information of the input sequence, while at the same time, it can learn the local correlations."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-71", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "Table 1 shows the number of parameters in each component of the transformer model studied in this paper."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-87", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "Our transformer acoustic models were trained using the PyKaldi2 toolbox [22] , which is built on top of Kaldi and PyTorch through the PyKaldi [23] wrapper."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-98", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "We performed a sanity check by removing the positional encoding when evaluating a transformer model trained with positional encoding, and obtained results which are only around 0.1% worse absolute."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-107", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "We then evaluated the impact of the 1D convolution layers in our transformer model by removing all the convolution layers."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-140", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "In Table 5 , we show the sequence training results of the transformer model trained with the maximum mutual information (MMI) criterion."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-154", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "In this paper, we have presented a transformer model with interleaved self-attention and convolution for hybrid acoustic modeling, although this structure may be also applicable to end-to-end models."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-44", "intents": ["@DIF@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "In transformer, both Q and K are from the source sequence, while in the conventional RNN-based attention model [19] , Q is from the decoder hidden state, and K is from the encoder hidden state."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-80", "intents": ["@DIF@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "We did not train deeper transformer models due to the memory constraint."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-88", "intents": ["@SIM@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "We used the Adam optimizer [24] cross entropy (CE) training, and the same learning rate scheduler as in [8] ."}
{"sent_id": "1dd3adcb79c8bc4b5187b85d836ceb-C001-12", "intents": ["@USE@"], "paper_id": "ABC_1dd3adcb79c8bc4b5187b85d836ceb_4", "text": "This approach has been shown to be accurate, relatively efficient, and robust using both generative and discriminative models (Roark, 2001; Roark, 2004; Collins and Roark, 2004) ."}
{"sent_id": "3251c6cd1afccf6ad8d5391a4360b0-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_3251c6cd1afccf6ad8d5391a4360b0_4", "text": "Translation from speech utterances is a challenging problem that has been studied both under statistical, symbolic approaches (Ney, 1999; Casacuberta et al., 2004; Kumar et al., 2015) and more recently using neural models (Sperber et al., 2017) ."}
{"sent_id": "3251c6cd1afccf6ad8d5391a4360b0-C001-43", "intents": ["@SIM@"], "paper_id": "ABC_3251c6cd1afccf6ad8d5391a4360b0_4", "text": "1 This procedure is also done in Sperber et al. (2017) ."}
{"sent_id": "3251c6cd1afccf6ad8d5391a4360b0-C001-62", "intents": ["@USE@"], "paper_id": "ABC_3251c6cd1afccf6ad8d5391a4360b0_4", "text": "For the word-based models, we remove any tokens with frequency lower than 2 (as in Sperber et al. (2017) ), while for subword models we do not perform any threshold pruning."}
{"sent_id": "3251c6cd1afccf6ad8d5391a4360b0-C001-101", "intents": ["@USE@", "@FUT@"], "paper_id": "ABC_3251c6cd1afccf6ad8d5391a4360b0_4", "text": "The approaches used by Sperber et al. (2017) can provide a starting point in this direction."}
{"sent_id": "3251c6cd1afccf6ad8d5391a4360b0-C001-80", "intents": ["@DIF@"], "paper_id": "ABC_3251c6cd1afccf6ad8d5391a4360b0_4", "text": "We also slightly outperform Sperber et al. (2017) in the setting where they ignore lattice scores, as in our approach."}
{"sent_id": "3251c6cd1afccf6ad8d5391a4360b0-C001-83", "intents": ["@DIF@"], "paper_id": "ABC_3251c6cd1afccf6ad8d5391a4360b0_4", "text": "Given those differences in training time, it is worth mentioning that the best model in Sperber et al. (2017) is surpassed by our best ensemble using lattices only."}
{"sent_id": "3251c6cd1afccf6ad8d5391a4360b0-C001-92", "intents": ["@DIF@"], "paper_id": "ABC_3251c6cd1afccf6ad8d5391a4360b0_4", "text": "It is worth noticing that Sperber et al. (2017) has a more principled approach to incorporate scores: by modifying the attention module."}
{"sent_id": "d2c95c3198f21e793549d7b16bdaf8-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_d2c95c3198f21e793549d7b16bdaf8_4", "text": "The goal of an EL approach is as follows: Given a piece of text, a reference knowledge base K and a set of entity mentions in that text, map each entity mention to the corresponding resource in K [4] ."}
{"sent_id": "d2c95c3198f21e793549d7b16bdaf8-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_d2c95c3198f21e793549d7b16bdaf8_4", "text": "-Search by Context -This boolean parameter provides a search of candidates using a context index [4] ."}
{"sent_id": "d2c95c3198f21e793549d7b16bdaf8-C001-21", "intents": ["@MOT@"], "paper_id": "ABC_d2c95c3198f21e793549d7b16bdaf8_4", "text": "However, MAG (Multilingual AGDISTIS) [4] showed that the underlying models being trained on English corpora make them prone to failure when migrated to a different language."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "IR-based approaches try to identify the best possible match between the knowledge base and the question (Bordes et al., 2014a; Bordes et al., 2014b; Yao and Van Durme, 2014; Dong et al., 2015) ."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "Something even more interesting (Bordes et al., 2014b) is that the system can have a good performance even without using a paraphrase corpus."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "In their approach, KB triples and NL questions are represented as sums of embeddings of KB symbols and words respectively."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-44", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "Interestingly, Bordes' (2014b) system performs relatively well (MAP score 0.34) on the Wikianswers dataset even without using the paraphrase corpus."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-91", "intents": ["@BACK@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "Originally in (Bordes et al., 2014b) , given a question to be answered, training is performed by imposing a margin-constraint between the correct answer and negative ones."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-29", "intents": ["@USE@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "Like (Bordes et al., 2014b) , we use al-most no linguistic features such as POS tagging, parsing, etc."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-137", "intents": ["@USE@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "We compare the model (Bordes et al., 2014b) with the model where we enforce E and R (and also \"E\" and \"R\") to be orthogonal."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-147", "intents": ["@USE@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "The Embedding scores are taken from (Bordes et al., 2014b) Table 3 shows that our technique improves the performance also on the larger, non-synthetic, dataset provided by Fader (2013) over the Bordes (2014b)'s method."}
{"sent_id": "9272b3f7e628a156caed328d475d0c-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_9272b3f7e628a156caed328d475d0c_5", "text": "They observed that English readability formulas did not work well on Bengali texts [11] , [21] ."}
{"sent_id": "9272b3f7e628a156caed328d475d0c-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_9272b3f7e628a156caed328d475d0c_5", "text": "Sinha et al. alleviated these problems by considering six parameters instead of just two [21] ."}
{"sent_id": "55f67c918001335974608200a87cfc-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_55f67c918001335974608200a87cfc_5", "text": "Fusing structured knowledge from knowledge graphs into deep models using Graph Neural Networks (GNN) [23, 30, 29] is shown to improve their performance on tasks such as visual question answering [12] , object detection [9] , natural language inference [2] , neural machine translation [11] , and opendomain question answering [17] ."}
{"sent_id": "55f67c918001335974608200a87cfc-C001-34", "intents": ["@USE@"], "paper_id": "ABC_55f67c918001335974608200a87cfc_5", "text": "Following [17] , we first extract a subgraph G q ⊂ G which contains v aq with high probability."}
{"sent_id": "55f67c918001335974608200a87cfc-C001-58", "intents": ["@DIF@"], "paper_id": "ABC_55f67c918001335974608200a87cfc_5", "text": "This is distinct from previous approaches which only process incoming nodes [17] ."}
{"sent_id": "bdcd8b0f3a56606427ee298d454b52-C001-13", "intents": ["@DIF@"], "paper_id": "ABC_bdcd8b0f3a56606427ee298d454b52_5", "text": "Because SLMs can be trained from only unlabeled text, they can be applied for ADS even when the relations of interest are not specified in advance (Downey et al., 2007) ."}
{"sent_id": "bdcd8b0f3a56606427ee298d454b52-C001-83", "intents": ["@DIF@"], "paper_id": "ABC_bdcd8b0f3a56606427ee298d454b52_5", "text": "As shown in Table 2, HMM 1-100 reduces error over the HMM-T model in (Downey et al., 2007) by 26%, on average."}
{"sent_id": "bdcd8b0f3a56606427ee298d454b52-C001-23", "intents": ["@USE@"], "paper_id": "ABC_bdcd8b0f3a56606427ee298d454b52_5", "text": "For relations of arity greater than one, we consider the typechecking task, an important sub-task of extraction (Downey et al., 2007) ."}
{"sent_id": "bdcd8b0f3a56606427ee298d454b52-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_bdcd8b0f3a56606427ee298d454b52_5", "text": "Unsupervised Hidden Markov Models (HMMs) are an alternative SLM approach previously shown to offer accuracy and scalability advantages over ngram models in ADS (Downey et al., 2007) ."}
{"sent_id": "bdcd8b0f3a56606427ee298d454b52-C001-40", "intents": ["@EXT@"], "paper_id": "ABC_bdcd8b0f3a56606427ee298d454b52_5", "text": "In our experiments, we utilize an ADS approach previously proposed for HMMs (Downey et al., 2007) and adapt it to also apply to n-gram models, as detailed below."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-92", "intents": ["@USE@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "This embedding method has shown outstanding performance on the Visual Madlibs task [32] ."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-129", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "The dataset consists of about 360k descriptions, spanning 12 different categories specified by different types of templates, of about 10k images."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-149", "intents": ["@USE@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "Guided by the results of the previous experiments, we compare nCCA that uses Edge Boxes object proposals (nCCA (ours)) with the state-ofthe-arts on Visual Madlibs (nCCA [32] )."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-162", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "To see the limits, we compare nCCA (ours) against nCCA (bbox) [32] that crops over ground truth bounding boxes from MS COCO segmentations and next averages over theirs representations (Table 3 in [32] shows that ground truth bounding boxes outperforms automatically detected bounding boxes, and hence they can be seen as an upper bound for a detection method trained to detect objects on MS COCO)."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-168", "intents": ["@USE@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "Since the accuracies of CNN+LSTM [32] are unavailable for two categories, we report average over 10 categories in this case."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-5", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on Visual Madlibs."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-105", "intents": ["@DIF@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "On the other side, for each completion candidate s we compute its representation by averaging over word2vec [18] representations of the words contributing to s. However, in contrast to the prior work [32] , instead of comparing the discrete output of the network with the representation of s, we directly optimize an objective in the embedding space."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-151", "intents": ["@DIF@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "The models are trained per category (a model trained over all the categories performs inferior on the hard task [32] )."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-166", "intents": ["@DIF@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "On one hand nCCA tops the leaderboard on the Visual Madlibs task [32] ."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-171", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "This contrasts to a post-hoc process used in [32] where an image description architecture (CNN+LSTM) first generates a completion that is next compared against the candidates in the word2vec space (see section 3 for more details)."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "While such ambiguities can be handled using appropriate metrics [14, 15, 17, 26] , Visual Madlibs [32] has taken another direction, and handles them directly within the task."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "While a majority of the most recent work on visual question answering combine LSTM [7] with CNN [11, 23, 24] by concatenation or summation or piece-wise multiplication, Canonical Correlation Analysis (CCA and nCCA) [6] have also been shown to be a very effective multimodal embedding technique [32] ."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-29", "intents": ["@EXT@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "Text-Embedding Loss: Motivated by the popularity of deep architectures for visual question answering, that combine a global CNN image representation with an LSTM [7] question representation [4, 13, 17, 20, 29, 30, 31] , as well as the leading performance of nCCA on the multi-choice Visual Madlibs task [32] , we propose a novel extension of the CNN+LSTM architecture that chooses a prompt completion out of four candidates (see Figure 4 ) by measuring similarities directly in the embedding space."}
{"sent_id": "740db031e3fc086dfdb2477caeac66-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_740db031e3fc086dfdb2477caeac66_5", "text": "Xu and Durrett (2018) addressed the problem by replacing the standard normal distribution for the prior with the von Mises-Fisher (vMF) distribution."}
{"sent_id": "247bbc4eb671895222065ed425f968-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_247bbc4eb671895222065ed425f968_5", "text": "Several sites have made significant progress to lower the WER to within the 5%-10% range on the Switchboard-CallHome subsets of the Hub5 2000 evaluation [2, 3, 4, 5] ."}
{"sent_id": "247bbc4eb671895222065ed425f968-C001-36", "intents": ["@SIM@"], "paper_id": "ABC_247bbc4eb671895222065ed425f968_5", "text": "In addition to automatic speech recognition results, similar to [2] , we also present human performance on the same BN test sets."}
{"sent_id": "247bbc4eb671895222065ed425f968-C001-81", "intents": ["@SIM@"], "paper_id": "ABC_247bbc4eb671895222065ed425f968_5", "text": "In [2] , two kinds of acoustic models, a convolutional and a non-convolutional acoustic model with comparable performance, are used since they produce good complementary outputs which can be further combined for improved performance."}
{"sent_id": "247bbc4eb671895222065ed425f968-C001-140", "intents": ["@SIM@"], "paper_id": "ABC_247bbc4eb671895222065ed425f968_5", "text": "We observe significant WER gains after using the LSTM LMs similar to those reported in [2] ."}
{"sent_id": "247bbc4eb671895222065ed425f968-C001-58", "intents": ["@USE@"], "paper_id": "ABC_247bbc4eb671895222065ed425f968_5", "text": "The transcriptions were also filtered to remove non-speech markers, partial words, punctuation marks etc as described in [2] ."}
{"sent_id": "247bbc4eb671895222065ed425f968-C001-160", "intents": ["@DIF@"], "paper_id": "ABC_247bbc4eb671895222065ed425f968_5", "text": "4. Compared to the telephone conversation confusions recorded in [2] -one symbol that is clearly missing is the back-channel response -this is probably from the very nature of the BN domain."}
{"sent_id": "1056d36c5ed22c7a34f6fe82b4962f-C001-11", "intents": ["@USE@"], "paper_id": "ABC_1056d36c5ed22c7a34f6fe82b4962f_5", "text": "While there is a substantial amount of work on statistical (Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2014; Yannakoudakis et al., 2017) and neural (Ji et al., 2017; Xie et al., 2016; Yuan and Briscoe, 2016; Chollampatt et al., 2016; Chollampatt and Ng, 2017; Chollampatt and Ng, 2018) machine translation methods for GEC, we follow the approach of Bryant and Briscoe (2018) and explore how such models would fare in this task when treated as simple language models."}
{"sent_id": "1056d36c5ed22c7a34f6fe82b4962f-C001-48", "intents": ["@USE@"], "paper_id": "ABC_1056d36c5ed22c7a34f6fe82b4962f_5", "text": "Concretely, let P (s c ) be the probability of the candidate sentence and P (s o ) the probability of the Table 2 : Results of our Transformer-Language Model approach against similar approaches (Bryant and Briscoe, 2018) and state-of-the-art on Grammatical Error Correction."}
{"sent_id": "1056d36c5ed22c7a34f6fe82b4962f-C001-86", "intents": ["@USE@"], "paper_id": "ABC_1056d36c5ed22c7a34f6fe82b4962f_5", "text": "Table 2 presents the results of our method comparing them against recent state-of-the-art supervised models and the simple n-gram language model used by Bryant and Briscoe (2018) ."}
{"sent_id": "1056d36c5ed22c7a34f6fe82b4962f-C001-12", "intents": ["@EXT@", "@MOT@"], "paper_id": "ABC_1056d36c5ed22c7a34f6fe82b4962f_5", "text": "More specifically, Bryant and Briscoe (2018) train a 5-gram language model on the One Billion Word Benchmark (Chelba et al., 2013) dataset and find that it produces competitive baseline results without any supervised training."}
{"sent_id": "1056d36c5ed22c7a34f6fe82b4962f-C001-27", "intents": ["@EXT@"], "paper_id": "ABC_1056d36c5ed22c7a34f6fe82b4962f_5", "text": "In this work, we follow the setup from Bryant and Briscoe (2018) substituting the 5-gram language model for different language models based on the Transformer architecture."}
{"sent_id": "1056d36c5ed22c7a34f6fe82b4962f-C001-106", "intents": ["@EXT@", "@MOT@"], "paper_id": "ABC_1056d36c5ed22c7a34f6fe82b4962f_5", "text": "Our key motivation was to corroborate and extend the results of Bryant and Briscoe (2018) to current state-of-the-art language models which have been trained in several languages and show that these models are tough baselines to beat for novel GEC systems."}
{"sent_id": "1056d36c5ed22c7a34f6fe82b4962f-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_1056d36c5ed22c7a34f6fe82b4962f_5", "text": "However, Bryant and Briscoe (2018) recently revived the idea, achieving competitive performance with the state-ofthe-art, demonstrating the effectiveness of the approaches to the task without using any annotated data for training."}
{"sent_id": "1056d36c5ed22c7a34f6fe82b4962f-C001-71", "intents": ["@SIM@"], "paper_id": "ABC_1056d36c5ed22c7a34f6fe82b4962f_5", "text": "Similar to Bryant and Briscoe (2018) , we report results on three metrics."}
{"sent_id": "f6694f359ae948b6e4563b927a672c-C001-82", "intents": ["@BACK@"], "paper_id": "ABC_f6694f359ae948b6e4563b927a672c_5", "text": "As it can be seen, the method by Xing et al. (2015) performs better than that of Mikolov et al. (2013b) in the translation induction task, which is in line with what they report in their paper."}
{"sent_id": "f6694f359ae948b6e4563b927a672c-C001-79", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f6694f359ae948b6e4563b927a672c_5", "text": "As discussed before, (Mikolov et al., 2013b) and (Xing et al., 2015) were implemented as part of our framework, so they correspond to our uncostrained mapping with no preprocessing and orthogonal mapping with length normalization, respectively."}
{"sent_id": "f6694f359ae948b6e4563b927a672c-C001-41", "intents": ["@DIF@"], "paper_id": "ABC_f6694f359ae948b6e4563b927a672c_5", "text": "In fact, our experiments show that orthogonality is more relevant than length normalization, in contrast to Xing et al. (2015) , who introduce orthogonality only to ensure that unit length is preserved after mapping."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-12", "intents": ["@MOT@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; ."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-25", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "We take a different approach to representation learning, addressing two potential limitations of Miwa and Bansal (2016) ."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "Miwa and Bansal (2016) were among the first to use neural networks for end-to-end relation extraction, showing highly promising results."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-123", "intents": ["@BACK@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "Previous work (Miwa and Bansal, 2016; trains model parameters by modeling each step for labeling one input sentence separately."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-229", "intents": ["@BACK@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "LSTM features have been extensively exploited for NLP tasks, including tagging Lample et al., 2016) , parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) , relation classification Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis ."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-31", "intents": ["@DIF@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "In contrast, the method of Miwa and Bansal (2016) must consider tree LSTM formulations that are specific to grammar formalisms, which can be structurally different (Tai et al., 2015) ."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-35", "intents": ["@DIF@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "Evaluation on two benchmark datasets shows that our method outperforms previous methods of Miwa and Bansal (2016) , Li and Ji (2014) and Miwa and Sasaki (2014) , giving the best reported results on both benchmarks."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-88", "intents": ["@DIF@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "Different from Miwa and Bansal (2016) , who use the output hidden vectors {h i } of LSTMs to represent words, we exploit segment representations as well."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-204", "intents": ["@DIF@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "Miwa and Bansal (2016) , who exploit end-to-end LSTM neural networks with local optimization, and L&J (2014) and M&S (2014) refer to Li and Ji (2014) and Miwa and Sasaki (2014) , respectively, which are both globally optimized models using discrete features, giving the top F-scores among statistical models."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-60", "intents": ["@USE@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "Following Miwa and Bansal (2016) , we use a neural network to learn the vector representation of T i−1 , and then use Equation 1 to rank candidate next labels."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-158", "intents": ["@USE@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "For the ACE05 dataset, we follow Li and Ji (2014) and Miwa and Bansal (2016) , splitting and preprocessing the dataset into training, development and test sets."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-171", "intents": ["@USE@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "For the local model, we follow Miwa and Bansal (2016) , training parameters only for entity detection during the first 20 iterations."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-198", "intents": ["@USE@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "We also compare our feature integration method with the traditional methods based on syntactic outputs which Miwa and Bansal (2016) and all previous methods use."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-228", "intents": ["@USE@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; , and we follow this line of work in the study."}
{"sent_id": "3fd7a249a8fa7a71a4c6aa2e79fecf-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_3fd7a249a8fa7a71a4c6aa2e79fecf_5", "text": "Ling et al. (2015) give results only on POS dataset, while some papers (Chiu and Nichols, 2016; Lample et al., 2016; Strubell et al., 2017) report results on the NER dataset only."}
{"sent_id": "3fd7a249a8fa7a71a4c6aa2e79fecf-C001-139", "intents": ["@SIM@"], "paper_id": "ABC_3fd7a249a8fa7a71a4c6aa2e79fecf_5", "text": "The results of Lample et al. (2016) can be reproduced by our CLSTM+WLSTM+CRF."}
{"sent_id": "13091dd4d06e11957a5cb7785b92d4-C001-10", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_13091dd4d06e11957a5cb7785b92d4_5", "text": "Systems that are trained on clean data generally perform poorly when faced with such errors at test time (Heigold et al., 2017; Belinkov and Bisk, 2018) ."}
{"sent_id": "13091dd4d06e11957a5cb7785b92d4-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_13091dd4d06e11957a5cb7785b92d4_5", "text": "Belinkov and Bisk (2018) report significant degradations in performance after applying noise to only a small fraction of input tokens."}
{"sent_id": "13091dd4d06e11957a5cb7785b92d4-C001-72", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_13091dd4d06e11957a5cb7785b92d4_5", "text": "As observed in prior art (Heigold et al., 2017; Belinkov and Bisk, 2018) , when there are significant amounts of natural noise, the model's performance drops significantly."}
{"sent_id": "13091dd4d06e11957a5cb7785b92d4-C001-105", "intents": ["@BACK@", "@DIF@", "@EXT@"], "paper_id": "ABC_13091dd4d06e11957a5cb7785b92d4_5", "text": "They find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text."}
{"sent_id": "13091dd4d06e11957a5cb7785b92d4-C001-109", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_13091dd4d06e11957a5cb7785b92d4_5", "text": "Belinkov and Bisk (2018) experiment with a bag of characters, while Sakaguchi et al. (2017) use character RNNs combined with special representations for the first and last characters of each token."}
{"sent_id": "13091dd4d06e11957a5cb7785b92d4-C001-12", "intents": ["@MOT@"], "paper_id": "ABC_13091dd4d06e11957a5cb7785b92d4_5", "text": "So far, using synthetic noise at training time has been found to only improve performance on test data with exactly the same kind of synthetic noise, while at the same time impairing performance on clean test data (Heigold et al., 2017; Belinkov and Bisk, 2018) ."}
{"sent_id": "13091dd4d06e11957a5cb7785b92d4-C001-107", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_13091dd4d06e11957a5cb7785b92d4_5", "text": "Our ablation results suggest that deletion and insertion noise -which were not included by Belinkov and Bisk -are essential to achieving robustness to natural noise."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "As mentioned earlier, there has been a lot of useful, previous work on using word embeddings for NLP tasks such as similarity, tagging, NER, sentiment analysis, and parsing (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Huang et al., 2012; Al-Rfou' et al., 2013; Hisamoto et al., 2013; Andreas and Klein, 2014; Bansal et al., 2014; Guo et al., 2014; Pennington et al., 2014; Wang et al., 2015) , inter alia."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-16", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "At the same time, these link embedding features maintain dependency parsing improvements similar to the complex, template-based features on word clusters and embeddings by previous work (Koo et al., 2008; Bansal et al., 2014 ) (up to 9% relative error reduction), and also stack statistically significantly over them (up to an additional 5% relative error reduction)."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-51", "intents": ["@DIF@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "Our dense, unary, link-embedding based Bucket and Bit-string features are substantially fewer than the sparse, n-ary, template-based features used in the MSTParser baseline, in BROWN, and in the word embedding SKIP DEP result of Bansal et al. (2014) ."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-66", "intents": ["@DIF@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "8 Moreover, unlike Bansal et al. (2014) , our Bucket features achieve statistically significant improvements, most likely because they fired D pairwise, conjoined features, one per dimension d, consisting of the two bucket values from the head and argument word vectors."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-87", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "In related work, Bansal et al. (2014) also use dependency context to tailor word embeddings to dependency parsing."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-89", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "Our structured link embeddings achieve similar improvements as theirs (and better in the case of direct, per-dimension bucket features) with a substantially smaller and simpler (unary) set of features that are aimed to directly capture hidden relationships between the substructures that dependency parsing factors on."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-63", "intents": ["@SIM@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "Moreover, the Bit-string result (92.6) is the same, i.e., has no statistically significant difference from the BROWN result (92.7), and also from the Bansal et al. (2014) SKIP DEP result (92.7)."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-38", "intents": ["@USE@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "The BROWN cluster features are based on Bansal et al. (2014) , who follow Koo et al. (2008) Koo et al. (2008) )."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "Their study is based on a unique network representation of the corpus called a distributional thesauri (DT) network built using Google books syntactic n-grams."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-88", "intents": ["@BACK@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "The authors then compare the sense clusters extracted across two different time points to obtain the suitable signals of sense change."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-30", "intents": ["@EXT@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "While Mitra et al. [19] reported a precision close to 0.6 over a random sample of 49 words, we take another random sample of 100 words separately and repeat manual evaluation."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-139", "intents": ["@EXT@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "Secondly, we also take 100 random samples for each time point pair Word Word for computing precision of our approach independently of Mitra et al. [19] , i.e., the proposed approach is not informed of the 'birth' cluster reported by Mitra et al. [19] , instead all the clusters in old and new time point are shown."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-34", "intents": ["@USE@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "Using a set intersecting with the 100 random samples for Mitra et al. [19] , we obtain the precision values of 0.21 and 0.28, respectively."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-154", "intents": ["@USE@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "Note that for these 100 random samples (that are all marked 'true' by Mitra et al. [19] ), it is possible to find an upper bound on the recall of Lau et al. [16] 's approach automatically."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-157", "intents": ["@USE@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "We see that the filtering using SVM classification improves the precision for both the time point pairs (T 1 and T 2 ) significantly, boosting it from the range of 0.23-0.32 to 0.74-0.86. Note that, as per our calculations, indeed the recall of Mitra et al. [19] would be 100% (as we are taking random samples for annotation from the set of reported 'birth' cases by Mitra et al. [19] only)."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-163", "intents": ["@USE@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "Further, we check if we can meaningfully combine the results reported by both the methods of Mitra et al. [19] and Lau et al. [16] for more accurate sense detection; and how does this compare with Table 8 ; both the senses look quite similar."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-80", "intents": ["@MOT@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "Datasets and graph construction: The authors used the Google books corpus, consisting of texts from over 3.4 million digitized English books published between 1520 and 2008."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-182", "intents": ["@DIF@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "We did another experiment in order to estimate the performance of our model for detecting novel sense, independent of the method of Mitra et al. [19] ."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-221", "intents": ["@DIF@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "The 'birth' sense clusters for 'guy', 'bush' in the new time period, as detected by split-join algorithm contain general terms like \"someone, anyone, men, woman, mother, son\" and \"cloud, air, sky, sunlight\" respectively."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-155", "intents": ["@SIM@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "While the low recall might be justified because this is a different approach, even the precision is found to be in the same range as that of Mitra et al. [19] ."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-10", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "One state-of-the-art model for semantic parsing is our recently introduced relaxed hybrid tree model (Lu, 2014) , which performs integrated lexicon acquisition and semantic parsing within a single framework utilizing efficient algorithms for training and inference."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-14", "intents": ["@MOT@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Unfortunately, the relaxed hybrid tree model has an important limitation: it essentially does not allow certain sentence-semantics pairs to be jointly encoded using the proposed relaxed hybrid tree representations."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-56", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "In relaxed hybrid tree, H(n, m) was implemented as a packed forest representation for exponentially many possible relaxed hybrid trees where pattern X was excluded."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-59", "intents": ["@MOT@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "When pattern X is allowed, both m a and m b are not directly associated with any natural language word, so we are able to further insert arbitrarily many (compatible) semantic units between the two units m a and m b while the resulting relaxed hybrid tree remains valid."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-62", "intents": ["@MOT@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "To eliminate relaxed hybrid trees consisting of an infinite number of nodes, pattern X is disallowed in the relaxed hybrid trees model (Lu, 2014) ."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-81", "intents": ["@DIF@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Such a constrained semantic forest is a packed forest representation of exponentially many possible unique semantic trees, where we set the height of the forest to c. By contrast, it was not possible in our previous relaxed hybrid tree model to introduce such a compact representation over all possible semantic trees."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-104", "intents": ["@DIF@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Results showed that our system consistently yielded higher results than all the previous systems, including our state-of-the-art relaxed hybrid tree system (the full model, when all the features are used), in terms of both accuracy score and F 1 -measure."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-106", "intents": ["@DIF@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "First, our model is able to handle certain sentence-semantics pairs which could not be handled by RHT during both training and evaluation as discussed in Section 3.1."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-111", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Here in this work, we only used simple unigram features (concatenation of a semantic unit and an individual word that appears directly below that unit in the joint representation), pattern features (concatenation of a semantic unit and the pattern below that unit) as well as transition features (concatenation of two semantic units that form a parent-child relationship) described in (Lu, 2014) ."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-114", "intents": ["@DIF@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "It can be seen that our system only required about 2-3% of the Table 3 : Number of features involved for both the RHT system and our new system using constrained semantic forests, across four different languages."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "The model allows natural language phrases to be recursively mapped to semantic units, where certain long-distance dependencies can be captured."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "The model is essentially discriminative, and allows rich features to be incorporated."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "The relaxed hybrid tree model has achieved the state-of-the-art results on standard benchmark datasets across different languages."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "The model is a discriminative semantic parsing model which extends the generative hybrid tree model (Lu et al., 2008) ."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Typically, to limit the space of latent structures, certain assumptions have to be made to h. In our work, we assume that h must be from a space consisting of relaxed hybrid tree structures (Lu, 2014) ."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "One major distinction between these two types of representations is that the relaxed hybrid tree representations are able to capture unbounded long-distance dependencies in a principled way."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "In the relaxed hybrid tree, however, each word is not only directly associated with exactly one semantic unit m, but also indirectly associated with all other semantic units that are predecessors of m. For example, the word w 3 now is directly associated with m b , but is also indirectly associated with m a ."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-102", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "RHT (Lu, 2014) is the discriminative semantic parsing system based on relaxed hybrid trees."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-96", "intents": ["@USE@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Following previous works, we regarded an output semantic representation as correct if and only if it returned the same answers as the gold standard (Jones et al., 2012; Lu, 2014) ."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-108", "intents": ["@USE@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "We note that in our experiments we used a small subset of the features used by our relaxed hybrid tree work."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-84", "intents": ["@SIM@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "This allows the computation of the denominator to be performed efficiently using similar dynamic programming algorithms described in (Lu, 2014) ."}
{"sent_id": "4b0aab00a99547791bff0597aabc06-C001-59", "intents": ["@USE@"], "paper_id": "ABC_4b0aab00a99547791bff0597aabc06_5", "text": "We also follow Cotterell et al. (2017) and augment the above with the signed number of tokens separating w i and w j , e.g., recording that w i appeared two to the left of w j ; these counts form a 3-tensor."}
{"sent_id": "4b0aab00a99547791bff0597aabc06-C001-93", "intents": ["@USE@"], "paper_id": "ABC_4b0aab00a99547791bff0597aabc06_5", "text": "Both of these baselines are windowed: they are restricted to a local context and cannot take advantage of frames or any lexical signal that can be derived from frames."}
{"sent_id": "4b0aab00a99547791bff0597aabc06-C001-122", "intents": ["@USE@"], "paper_id": "ABC_4b0aab00a99547791bff0597aabc06_5", "text": "The implementation is available at https: //github.com/fmof/tensor-factorization."}
{"sent_id": "4b0aab00a99547791bff0597aabc06-C001-121", "intents": ["@EXT@"], "paper_id": "ABC_4b0aab00a99547791bff0597aabc06_5", "text": "As demonstrated by our experiments, our extension of Cotterell et al. (2017) 's tensor factorization enriches word embeddings by including syntacticsemantic information not often captured, resulting in consistently higher SPR-based correlations."}
{"sent_id": "4b0aab00a99547791bff0597aabc06-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_4b0aab00a99547791bff0597aabc06_5", "text": "With this interpretation, they generalize SG from matrix to tensor factorization, and provide a theoretical basis for modeling higher-order SG (or additional context, such as morphological features of words) within a word embeddings framework."}
{"sent_id": "4b0aab00a99547791bff0597aabc06-C001-96", "intents": ["@DIF@"], "paper_id": "ABC_4b0aab00a99547791bff0597aabc06_5", "text": "Examining Fig. 2a , we see that both model types outperform both the word2vec and Cotterell et al. (2017) baselines in nearly all model configurations and ablations."}
{"sent_id": "d5144370a9361ff870dd3cb2e064ff-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_d5144370a9361ff870dd3cb2e064ff_5", "text": "In particular, in (Linzen et al., 2016) we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences."}
{"sent_id": "d5144370a9361ff870dd3cb2e064ff-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_d5144370a9361ff870dd3cb2e064ff_5", "text": "They feed each sentence word by word into an LSTM, stop right before the focus verb, and ask the model to predict a binary plural/singular decision (supervised setup) or compare the probability assigned by a pre-trained language model (LM) to the plural vs singular forms of the verb (LM setup)."}
{"sent_id": "d5144370a9361ff870dd3cb2e064ff-C001-14", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_d5144370a9361ff870dd3cb2e064ff_5", "text": "Indeed, Tran et al. (2018) finds that transformerbased models perform worse than LSTM models on the Linzen et al. (2016) agreement prediction dataset."}
{"sent_id": "d5144370a9361ff870dd3cb2e064ff-C001-60", "intents": ["@EXT@"], "paper_id": "ABC_d5144370a9361ff870dd3cb2e064ff_5", "text": "I similarly discard 680 sentences from (Linzen et al., 2016) where the focus verb or its inflection were one of 108 out-ofvocabulary tokens, 6 and 28 sentence-pairs (8 tokens 7 ) from (Gulordava et al., 2018) ."}
{"sent_id": "e5886e138ce8d84a48e44db3f3d6a1-C001-18", "intents": ["@MOT@"], "paper_id": "ABC_e5886e138ce8d84a48e44db3f3d6a1_5", "text": "In another attempt, Jana and Goyal (2018b) proposed various complex network measures which can be used as features to build a supervised classifier model for co-hyponymy detection, and showed improvements over other baseline approaches."}
{"sent_id": "e5886e138ce8d84a48e44db3f3d6a1-C001-85", "intents": ["@USE@"], "paper_id": "ABC_e5886e138ce8d84a48e44db3f3d6a1_5", "text": "Table 4 represents the performance comparison of our models with the best stateof-the-art models reported in (Santus et al., 2016) and (Jana and Goyal, 2018b) ."}
{"sent_id": "e5886e138ce8d84a48e44db3f3d6a1-C001-95", "intents": ["@USE@"], "paper_id": "ABC_e5886e138ce8d84a48e44db3f3d6a1_5", "text": "In the third experiment we use the dataset specifically build for co-hyponymy detection in one of the recent works by Jana and Goyal (2018b) ."}
{"sent_id": "e5886e138ce8d84a48e44db3f3d6a1-C001-98", "intents": ["@USE@"], "paper_id": "ABC_e5886e138ce8d84a48e44db3f3d6a1_5", "text": "Following the same setup, we report accuracy scores for ten-fold cross validation for each of these three datasets of our models along with the best models (svmSS, rfALL) reported by Jana and Goyal (2018b) in Table 5 ."}
{"sent_id": "e5886e138ce8d84a48e44db3f3d6a1-C001-99", "intents": ["@BACK@"], "paper_id": "ABC_e5886e138ce8d84a48e44db3f3d6a1_5", "text": "Jana and Goyal (2018b) use SVM classifier with structural similarity between words in a word pair as feature to obtain svmSS and use Random Forest classifier with five complex network measures computed from distributional thesaurus network as features to obtain rfALL."}
{"sent_id": "bebcad79900e9a4a25020ed0d886b5-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_bebcad79900e9a4a25020ed0d886b5_5", "text": "However, in order to control for the possibillity of the model learning to rely on \"semantic\" selectional-preferences cues rather than syntactic ones, they replace each content word with random words from the same part-ofspeech and inflection."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Recent advances in the area of deep reinforcement learning (DRL) have inspired reinforcement learning (RL) based solutions for the KG completion problem [21, 3, 30, 13, 19, 22, 14, 29] ."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-36", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Existing RL-based methods for KG completion do not capture the entity's neighborhood information."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "As a result, more recent methods proposed using RL to solve the multi-hop reasoning problem in knowledge graphs by framing it as a sequential decision-making process [3, 23, 30, 22, 12, 13] ."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-65", "intents": ["@BACK@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Because these RL models treat the KG completion problem as a path reasoning problem instead of a link prediction problem, they are able to overcome both drawbacks of embedding methods that are outlined above."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-67", "intents": ["@BACK@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Many of these RL methods have tried to combine the representational power of embeddings and reasoning power of RL by training an agent to navigate an embedding space."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-253", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Again, other RL baselines struggle with finding the next best step after entity New York."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-115", "intents": ["@MOT@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Inspired by [13] , we use pre-trained KG embeddings based on existing KG embedding methods to design a soft reward function for the terminal state s T based on [17] :"}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-69", "intents": ["@SIM@", "@EXT@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "While we take a similar modular approach, our solution enriches the embedding space with additional information about entity types and local neighborhood information."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-236", "intents": ["@SIM@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "We take a similar approach as [13] to extract to-many and to-one relations."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-116", "intents": ["@USE@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Where f (e s , r, e T ) is a similarity measure calculated based on pre-trained KG embedding approach [13] ."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-125", "intents": ["@USE@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "To reduce the potential impact of argmax leading to the overuse of incorrect paths, we utilize random action dropout as described in [13] ."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-180", "intents": ["@USE@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Because none of these models generalize to unknown entities, followed by previous work [3, 13] , we measure Hits@k and MRR only for queries for which both e s and e d have already been seen at least once by the model during training."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-185", "intents": ["@USE@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "For NELL-995, utilize the same hyperparameters described in [13] when training ConvE, ComplEx, Distmult, and Lin et al [13] baselines."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-210", "intents": ["@USE@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "We believe due to the sparsity of these two knowledge graphs, type information was more effective for action space pruning than entity page ranks, as done in [13] ."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-235", "intents": ["@USE@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "We evaluate our proposed model on different relation types and compare our results with the best performing RL baseline."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-198", "intents": ["@DIF@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Embedding-based methods show an overall higher performance compared to the RL-based methods."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-204", "intents": ["@DIF@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "On the NELL-995 dataset, our method results in 2.8% improvement in MRR and 4% improvement in Hits@1 over the best performing baseline."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-267", "intents": ["@FUT@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "In the future, we plan to explore more efficient strategies for action-space pruning to improve the scalability of existing RL solutions."}
{"sent_id": "0c233d68fb2ccdf033fc6a08c8f4bf-C001-2", "intents": ["@BACK@"], "paper_id": "ABC_0c233d68fb2ccdf033fc6a08c8f4bf_6", "text": "The goal of the Penn Discourse Treebank (PDTB) project is to develop a large-scale corpus, annotated with coherence relations marked by discourse connectives."}
{"sent_id": "0c233d68fb2ccdf033fc6a08c8f4bf-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_0c233d68fb2ccdf033fc6a08c8f4bf_6", "text": "The Penn Discourse Treebank (PDTB) (http://www.seas.upenn.edu/~pdtb) (Prasad et al. 2008a) annotates the argument structure, semantics, and attribution of discourse connectives and their arguments."}
{"sent_id": "0c233d68fb2ccdf033fc6a08c8f4bf-C001-5", "intents": ["@USE@"], "paper_id": "ABC_0c233d68fb2ccdf033fc6a08c8f4bf_6", "text": "We annotated discourse connectives and their arguments in one 4,937-token full-text biomedical article."}
{"sent_id": "0c233d68fb2ccdf033fc6a08c8f4bf-C001-32", "intents": ["@EXT@"], "paper_id": "ABC_0c233d68fb2ccdf033fc6a08c8f4bf_6", "text": "We discussed the annotation results and made suggestions to adapt the PDTB guidelines to biomedical text."}
{"sent_id": "0c233d68fb2ccdf033fc6a08c8f4bf-C001-52", "intents": ["@EXT@"], "paper_id": "ABC_0c233d68fb2ccdf033fc6a08c8f4bf_6", "text": "We will annotate a wider variety of nominalizations as arguments than allowed by the PDTB guidelines."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-16", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "Compared with previous recursive neural networks (Socher et al., 2013; 2012) , S-LSTM has the potentials of avoiding gradient vanishing and hence may model long-distance interaction over trees."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-18", "intents": ["@MOT@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "S-LSTM can be considered as bringing the merits of a recursive neural network and a recurrent neural network togetherStanford Sentiment Tree Bank (Socher et al., 2013) to determine the sentiment for different granularities of phrases in a tree."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-34", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "We compare our model with the RvNN models presented in (Socher et al., 2013) , as we directly replaced the tensor-enhanced composition layer at each tree node with a S-LSTM memory block."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-85", "intents": ["@DIF@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "The major difference from that of (Socher et al., 2013 ) is we use LSTM-like backpropagation, where unlike a regular LSTM, pass of error needs to discriminate between the left and right children, or in a topology with more than two children, needs to discriminate between children."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-135", "intents": ["@DIF@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "Table 1 showed that S-LSTM achieved the best predictive performance, when compared to all the models reported in (Socher et al., 2013) ."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "These networks are defined over recursive tree structures-a tree node is a vector computed from its children."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-68", "intents": ["@USE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "In this paper, we assume there are two children at each nodes, same as in (Socher et al., 2013) and therefore we use their data in our experiments."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-97", "intents": ["@USE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "Following (Socher et al., 2013) , the overall objective function we used to learn S-LSTM in this paper is simply minimizing the overall cross-entropy errors and a sum of that at all nodes."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-102", "intents": ["@USE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "We specifically attempt to determine the sentiment of different granularities of phrases in a tree, within the Stanford Sentiment Tree Bank benchmark data (Socher et al., 2013) ."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-113", "intents": ["@USE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "We use the same split of the training and test data as in (Socher et al., 2013) to predict the sentiment categories of the roots (sentences) and all phrases (including sentences)."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-119", "intents": ["@USE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "As mentioned before, we follow (Socher et al., 2013) to minimize the cross-entropy error for all nodes or for roots only, depending on specific experiment settings."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-129", "intents": ["@USE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "In the default setting, we conducted experiments as in (Socher et al., 2013) ."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-148", "intents": ["@FUT@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "Detailed annotations in the tree bank enable much interesting work to be possible, e.g., the study of the effect of negation in changing sentiment (Zhu et al., 2014) ."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "Empirical arguments in favor of these methods have been provided recently by Choi et al. (2001) in a study using Latent Semantic Analysis (Latent Semantic Indexing, Deerwester et al. 1990 ) to extract a semantic space from a corpus allowing determination of the similarity of meanings of words, sentences, or paragraphs."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "However, implications of Choi et al.'s study for text segmentation and for the use of LSA in natural language processing are unclear due to the methodology employed."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-15", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "First, Choi et al.'s segmentation procedure does not rely on supervised learning in which a system learns how to efficiently segment a text from training data."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "Choi et al. (2001) claimed that it was possible to improve the inter-sentence similarities index by taking into account the semantic proximities between words estimated on the basis of Latent Semantic Analysis (LSA)."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-58", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "First of all, paragraphs were used as documents for building the lexical tables because Choi et al. observed that such middle-sized units were more effective than shorter units (i.e., sentences)."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-66", "intents": ["@USE@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "The segmentation accuracy was evaluated by means of the index reported by Choi et al. (2001) : the Pk measure of segmentation inaccuracy (Beeferman, Berger, and Lafferty 1999) , which gives the proportion of sentences that are wrongly predicted to belong to the same segment or wrongly predicted to belong to different segments."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-82", "intents": ["@USE@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "Experiment 1 was conducted on the Choi et al. (2001) LSA corpus, a 1,000,000-word collection of texts from very different genres and with varied themes."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-44", "intents": ["@MOT@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "The aim of this experiment is to determine the impact of the presence of the test materials in the LSA corpus on the results obtained by Choi et al. (2001) ."}
{"sent_id": "9567cb276162a6e9d445f13f06f5a2-C001-19", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9567cb276162a6e9d445f13f06f5a2_6", "text": "In recent work, we showed (Zapirain et al., 2009 ) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system."}
{"sent_id": "9567cb276162a6e9d445f13f06f5a2-C001-22", "intents": ["@EXT@"], "paper_id": "ABC_9567cb276162a6e9d445f13f06f5a2_6", "text": "In this paper we advance (Zapirain et al., 2009) in two directions: (1) We learn separate SPs for prepositions and verbs, showing improvement over using SPs for verbs alone."}
{"sent_id": "9567cb276162a6e9d445f13f06f5a2-C001-37", "intents": ["@USE@"], "paper_id": "ABC_9567cb276162a6e9d445f13f06f5a2_6", "text": "Distributional similarity: Following (Zapirain et al., 2009) we considered both first order and second order similarity."}
{"sent_id": "c856f5ce5d2cdcfc71027d6fa4c6b3-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_c856f5ce5d2cdcfc71027d6fa4c6b3_6", "text": "Before reporting these experiments, Choi's algorithm and the use of LSA within this framework are described."}
{"sent_id": "c856f5ce5d2cdcfc71027d6fa4c6b3-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_c856f5ce5d2cdcfc71027d6fa4c6b3_6", "text": "The procedure initially proposed by Choi (2000) , C99, rests exclusively on the information contained in the text to be segmented."}
{"sent_id": "c856f5ce5d2cdcfc71027d6fa4c6b3-C001-52", "intents": ["@USE@"], "paper_id": "ABC_c856f5ce5d2cdcfc71027d6fa4c6b3_6", "text": "For the present experiment, I used the most general test materials built by Choi (2000) , in which the size of the segments within each sample varies randomly from 3 to 11 sentences."}
{"sent_id": "c856f5ce5d2cdcfc71027d6fa4c6b3-C001-63", "intents": ["@USE@"], "paper_id": "ABC_c856f5ce5d2cdcfc71027d6fa4c6b3_6", "text": "An 11 × 11 rank mask was used for the ordinal transformation, as recommended by Choi (2000) ."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-129", "intents": ["@DIF@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "The additional table rows are described in Sections 4-6, and cumulatively yield substantial improvements over the Boulis and Ostendorf (2005) with the work reported by Boulis and Ostendorf (2005) ), all of the above models can be easily extended to per-speaker evaluation by pooling in the predictions from multiple conversations of the same speaker."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-153", "intents": ["@DIF@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "To give maximal consistency/benefit to the Boulis and Ostendorf (2005) n-gram-based model, we did not filter the self-reporting n-grams such as \"im forty\" and \"im thirty\", putting our sociolinguisticliterature-based and discourse-style-based features at a relative disadvantage."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-17", "intents": ["@EXT@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "While small-scale sociolinguistic studies on monologues have shed some light on important features, we focus on modeling attributes from spoken conversations, building upon the work of Boulis and Ostendorf (2005) and show how gender and other attributes can be accurately predicted based on the following original contributions:"}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-25", "intents": ["@EXT@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "Application to new attributes: We show how the lexical model of Boulis and Ostendorf (2005) can be extended to Age and Native vs. Non-native prediction, with further improvements gained from our partner-sensitive models and novel sociolinguistic features."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-35", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "While prosodic features have been shown to be useful in gender/age classification (e.g. Shafran et al., 2003) , their work makes use of speech transcripts along the lines of Boulis and Ostendorf (2005) in order to build a general model that can be applied to electronic conversations as well."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-39", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "Boulis and Ostendorf (2005) have also constrained themselves to lexical n-gram features, while we show improvements via the incorporation of non-lexical features such as the percentage domination of the conversation, degree of passive usage, usage of subordinate clauses, speaker rate, usage profiles for filler words (e.g. \"umm\"), mean-utterance length, and other such properties."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-46", "intents": ["@USE@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "The primary task we employed was identical to Boulis and Ostendorf (2005) , namely the classification of gender, etc."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-49", "intents": ["@USE@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "We followed the preprocessing steps and experimental setup of Boulis and Ostendorf (2005) as closely as possible given the details presented in their paper, although some details such as the exact training/test partition were not currently obtainable from either the paper or personal communication."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-60", "intents": ["@USE@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "Also, only the ngrams with frequency greater than 5 were retained in the feature set following Boulis and Ostendorf (2005) ."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-68", "intents": ["@USE@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "For compatibility with Boulis and Ostendorf (2005) , no special pre- processing for names is performed, and they are treated as just any other unigrams or bigrams 1 ."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-127", "intents": ["@USE@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "As noted before, the standard reference algorithm is Boulis and Ostendorf (2005) , and all cited relative error reductions are based on this established standard, as implemented in this paper."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-2", "intents": ["@BACK@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016) ."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-88", "intents": ["@BACK@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Waseem and Hovy (2016) obtain an F1-score of 73.91 on their data set, using character n-grams and gender information."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-23", "intents": ["@USE@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Our data set is obtained by sampling tweets from the 130k tweets extracted by Waseem and Hovy (2016) ."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-38", "intents": ["@USE@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "If a tweet fails any of the tests, the annotators are instructed to label it as the relevant form of hate speech."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-18", "intents": ["@EXT@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Our data set extends the Waseem and Hovy (2016) data set by 4, 033 tweets."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-26", "intents": ["@SIM@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Racism Sexism Neither Count 1 95 2780 Given the distribution of the labels in Waseem and Hovy (2016) and our annotated data set (see Table  2 ), it is to be expected the largest overlap occurs with tweets annotated as negative for hate speech."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-47", "intents": ["@SIM@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Overlap Considering the overlap with the Waseem and Hovy (2016), we see that the agreement is extremely low (mean pairwise κ = 0.14 between all annotator groups and Waseem and Hovy (2016) )."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-87", "intents": ["@SIM@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Our work closely resembles Waseem and Hovy (2016) , as they also run classification experiments on a hate speech data set."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-94", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Our best model is on par with previous work on the Waseem and Hovy (2016) data set for the binary classification task but under-performs for the multi-class classification task."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-27", "intents": ["@DIF@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Observing Table 2 , we see that the label distribution in our data set generally differs from the distribution in Waseem and Hovy (2016) ."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-77", "intents": ["@DIF@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Waseem and Hovy (2016) may suffer from personal bias, as the only the authors annotated, and only the annotations positive for hate speech were reviewed by one other person."}
{"sent_id": "04b525b91b48e31258287a015d0401-C001-18", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_04b525b91b48e31258287a015d0401_6", "text": "When analyzing the behavior of the LSA representation of (Gliozzo et al., 2005) we noticed that it captures two types of similarities between the category name and document terms."}
{"sent_id": "04b525b91b48e31258287a015d0401-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_04b525b91b48e31258287a015d0401_6", "text": "This was accompanied by representing the category names and documents (step 2) in LSA space, obtained through cooccurrence-based dimensionality reduction."}
{"sent_id": "04b525b91b48e31258287a015d0401-C001-21", "intents": ["@MOT@"], "paper_id": "ABC_04b525b91b48e31258287a015d0401_6", "text": "This limits the method's precision, due to false-positive classifications of contextually-related documents that do not discuss the specific category topic (such as other sports documents wrongly classified to Baseball)."}
{"sent_id": "04b525b91b48e31258287a015d0401-C001-45", "intents": ["@USE@"], "paper_id": "ABC_04b525b91b48e31258287a015d0401_6", "text": "We thus extend the scheme in Figure 1 by creating two vectors per category (in steps 1 and 2): a reference vector c ref in term space, consisting of referring terms for the category name; and a context vector c con , representing the category name in LSA space, as in (Gliozzo et al., 2005) ."}
{"sent_id": "04b525b91b48e31258287a015d0401-C001-77", "intents": ["@USE@"], "paper_id": "ABC_04b525b91b48e31258287a015d0401_6", "text": "We tested our method on the two corpora used in (Gliozzo et al., 2005) : 20-NewsGroups, classified by a single-class scheme (single category per document), and Reuters-10 3 , of a multi-class scheme."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-10", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance learning models."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-25", "intents": ["@BACK@", "@MOT@", "@EXT@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "We further note that iterative bootstrapping over a single distant supervision system is difficult, because state-of-the-art systems (Surdeanu et al., 2012; Hoffmann et al., 2011; Riedel et al., 2010; Mintz et al., 2009) , detect only few false negatives in the training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009) ."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-42", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "While relation extraction systems exploit rich and complex features that are necessary to extract the exact relation (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011) , passage retrieval components use coarse features in order to provide different and complementary feedback to information extraction models."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "MULTIR is based on multi-instance learning, which assumes that at least one sentence of those matching a given entity-pair contains the relation of interest (Riedel et al., 2010) in the given knowledge base to tolerate false positive noise in the training data and superior than previous models (Riedel et al., 2010; Mintz et al., 2009 ) by allowing overlapping relations."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-26", "intents": ["@EXT@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "We present a reliable and novel way to address these issues and achieve significant improvement over the MULTIR system (Hoffmann et al., 2011) , increasing recall from 47.7% to 61.2% at comparable precision."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-64", "intents": ["@USE@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "For evaluating extraction accuracy, we follow the experimental setup of Hoffmann et al. (2011) , and use their implementation of MULTIR 4 with 50 training iterations as our baseline."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-66", "intents": ["@USE@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "We use the same datasets as in Hoffmann et al. (2011) and Riedel et al. (2010) , which include 3-years of New York Times articles aligned with Freebase."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-67", "intents": ["@USE@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "The sentential extraction evaluation is performed on a small amount of manually annotated sentences, sampled from the union of matched sentences and Table 1 : Overall sentential extraction performance evaluated on the original test set of Hoffmann et al. (2011) and our corrected test set: Our proposed relevance feedback technique yields a substantial increase in recall."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-73", "intents": ["@USE@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "Note that the sentences are sampled from the union of Freebase matches and sentences from which some systems in Hoffmann et al. (2011) extracted a relation."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-27", "intents": ["@USE@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "The results are also discussed in relation to the state-of-the-art DSMs, as reported in Hill et al. (2015) ."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-112", "intents": ["@USE@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "All of them include the pos-tagged target words used in the three datasets (i.e. MEN, WordSim-353 and SimLex-999) and the pos-tagged contexts having frequency above 100 in the two corpora."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-121", "intents": ["@USE@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets Simlex-999, WordSim-353 and MEN."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-124", "intents": ["@USE@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "In particular, Table 1 describes the performances on SimLex-999, WordSim-353 and MEN for the measures applied on RCV Vol."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-139", "intents": ["@USE@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in SimLex-999 (i.e. genuine similarity)."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-147", "intents": ["@USE@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "Finally, few words need to be spent with regard to the ability of calculating genuine similarity, as distinguished from word relatedness (Turney, 2001; Agirre et al., 2009; Hill et al., 2015) ."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (Turney, 2001; Agirre et al., 2009; Hill et al., 2015) , we test the ability of the models to quantify genuine semantic similarity."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-85", "intents": ["@BACK@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "Even though such a classification made a clear distinction between the two types of relations (i.e. similarity and association), Hill et al. (2015) argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-90", "intents": ["@BACK@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "SimLex-999 is the dataset introduced by Hill et al. (2015) to address the above mentioned criticisms of confusion between similarity and association."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-166", "intents": ["@SIM@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "Interestingly, our best models achieve results that are comparable to -or even better than -those reported by Hill et al. (2015) for the stateof-the-art word embeddings models."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-140", "intents": ["@DIF@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "On top of it, despite Hill et al. (2015) 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al., 2009; Kiela and Clark, 2014) , we need to mention that window 5 was abandoned because of its low performance."}
{"sent_id": "14529822630fb469f5fc8f37aaf473-C001-79", "intents": ["@USE@"], "paper_id": "ABC_14529822630fb469f5fc8f37aaf473_6", "text": "Following Reddy et al. (2011) , when using the UNI operation, we experiment with weighting the contributions of each constituent to the composed APT representation using the parameter, h. For example, if A 2 is the APT associated with the head of the phrase and A δ 1 is the properly aligned APT associated with the modifier where δ is the dependency path from the head to the modifier (e.g. NMOD or AMOD), the composition operations can be defined as:"}
{"sent_id": "14529822630fb469f5fc8f37aaf473-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_14529822630fb469f5fc8f37aaf473_6", "text": "Compositionality detection (Reddy et al., 2011) involves deciding whether a given multiword expression is compositional or not i.e., whether the meaning can be understood from the literal meaning of its parts."}
{"sent_id": "14529822630fb469f5fc8f37aaf473-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_14529822630fb469f5fc8f37aaf473_6", "text": "Accordingly, as observed elsewhere (Reddy et al., 2011; Salehi et al., 2015; Yazdani et al., 2015) , compositional methods can be evaluated by correlating the similarity of composed and observed phrase representations with the human judgments of compositionality."}
{"sent_id": "14529822630fb469f5fc8f37aaf473-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_14529822630fb469f5fc8f37aaf473_6", "text": "Used 3-fold cross-validation, they found that using weighted addition outperformed multiplication as a compositionality function."}
{"sent_id": "14529822630fb469f5fc8f37aaf473-C001-72", "intents": ["@BACK@"], "paper_id": "ABC_14529822630fb469f5fc8f37aaf473_6", "text": "Using interpolation to mitigate the sparse data problem, their model beat the baseline of weighted addition on the Reddy et al. (2011) evaluation task when trained on the BNC."}
{"sent_id": "14529822630fb469f5fc8f37aaf473-C001-95", "intents": ["@DIF@"], "paper_id": "ABC_14529822630fb469f5fc8f37aaf473_6", "text": "Using the cbow model with 100 dimensions and a subsampling threshold of t = 10 −3 gives a performance of 0.74 which is significantly higher than the previous state-ofthe-art reported in Reddy et al. (2011) ."}
{"sent_id": "4176674f83dec5389a23d9d45654c7-C001-41", "intents": ["@USE@"], "paper_id": "ABC_4176674f83dec5389a23d9d45654c7_6", "text": "We used both the MaltParser (Nivre, 2008) and the Easy-First Parser (Goldberg and Elhadad, 2010) ."}
{"sent_id": "4176674f83dec5389a23d9d45654c7-C001-62", "intents": ["@USE@"], "paper_id": "ABC_4176674f83dec5389a23d9d45654c7_6", "text": "The best performing set of features on non-gold input, obtained in Marton et al. (2013) , are shown in Table 1 ."}
{"sent_id": "4176674f83dec5389a23d9d45654c7-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_4176674f83dec5389a23d9d45654c7_6", "text": "In this section, we summarize Marton et al. (2013) ."}
{"sent_id": "4176674f83dec5389a23d9d45654c7-C001-36", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_4176674f83dec5389a23d9d45654c7_6", "text": "In contrast to these negative results, Marton et al. (2013) showed positive results for using agreement morphology for Arabic."}
{"sent_id": "4176674f83dec5389a23d9d45654c7-C001-72", "intents": ["@DIF@"], "paper_id": "ABC_4176674f83dec5389a23d9d45654c7_6", "text": "The data split used in the shared task is different from the data split we used in (Marton et al., 2013) , so we retrained our models on the new splits (Diab et al., 2013) ."}
{"sent_id": "a6564c4b215e6c5ad4f53eeb5dd69c-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_a6564c4b215e6c5ad4f53eeb5dd69c_6", "text": "The main objective of continuous turn-taking prediction as proposed in [13] is to predict the future speech activity annotations of one of the speakers in a dyadic conversation using input speech features from both speakers (x t )."}
{"sent_id": "a6564c4b215e6c5ad4f53eeb5dd69c-C001-24", "intents": ["@MOT@"], "paper_id": "ABC_a6564c4b215e6c5ad4f53eeb5dd69c_6", "text": "In these models LSTMs are used to make continuous predictions of a person's speech activity at each individual time step of 50ms."}
{"sent_id": "a6564c4b215e6c5ad4f53eeb5dd69c-C001-26", "intents": ["@MOT@"], "paper_id": "ABC_a6564c4b215e6c5ad4f53eeb5dd69c_6", "text": "Rather than designing classifiers to make specific decisions, these continuous models are able to capture general information about turn-taking in the data that they are trained on."}
{"sent_id": "a6564c4b215e6c5ad4f53eeb5dd69c-C001-28", "intents": ["@MOT@"], "paper_id": "ABC_a6564c4b215e6c5ad4f53eeb5dd69c_6", "text": "A downside to the approach in [13] is that, since a single LSTM is being used, all input features must be processed at the same rate."}
{"sent_id": "a6564c4b215e6c5ad4f53eeb5dd69c-C001-38", "intents": ["@EXT@"], "paper_id": "ABC_a6564c4b215e6c5ad4f53eeb5dd69c_6", "text": "In this paper we present significant extensions to the original work of Skantze in [13] ."}
{"sent_id": "a6564c4b215e6c5ad4f53eeb5dd69c-C001-93", "intents": ["@USE@"], "paper_id": "ABC_a6564c4b215e6c5ad4f53eeb5dd69c_6", "text": "In our discussion and results below, \"Ling 50ms\" refers to using word features that have been sampled at regular 50ms intervals, as was proposed in [13] ."}
{"sent_id": "a6564c4b215e6c5ad4f53eeb5dd69c-C001-156", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_a6564c4b215e6c5ad4f53eeb5dd69c_6", "text": "Comparing our results with previously published baselines reported on the same dataset by Skantze in [13] , our best result on the PAUSE 500 task of 0.8553 is a large improvement over his reported score of 0.762. Looking at the results from the fusion of visual and acoustic modalities shown in in Table 2 , we were able to achieve our best BCE loss using our multiscale approach to fuse acoustic features at a 10ms timescale and visual features at a 58Hz timescale."}
{"sent_id": "780b96afa8d417aa241e01ad594ce9-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_780b96afa8d417aa241e01ad594ce9_6", "text": "In most of the previous studies (Mihalcea and Strapparava, 2005; Purandare and Litman, 2006; Yang et al., 2015) , humor recognition was modeled as a binary classification task."}
{"sent_id": "780b96afa8d417aa241e01ad594ce9-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_780b96afa8d417aa241e01ad594ce9_6", "text": "Yang et al. (2015) explained and computed stylistic features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style."}
{"sent_id": "780b96afa8d417aa241e01ad594ce9-C001-33", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_780b96afa8d417aa241e01ad594ce9_6", "text": "1 CNNbased text categorization methods have been applied to humor recognition (e.g., in (Bertero and Fung, 2016b) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in Yang et al. (2015) is missing; (b) CNN's performance in the previous research is not quite clear; and (c) some important techniques that can improve CNN performance (e.g., using varied-sized filters and dropout regularization (Hinton et al., 2012)) were not applied."}
{"sent_id": "780b96afa8d417aa241e01ad594ce9-C001-55", "intents": ["@USE@"], "paper_id": "ABC_780b96afa8d417aa241e01ad594ce9_6", "text": "Following Yang et al. (2015) , we applied Random Forest (Breiman, 2001 ) to perform humor recognition by using the following two groups of features."}
{"sent_id": "780b96afa8d417aa241e01ad594ce9-C001-87", "intents": ["@SIM@"], "paper_id": "ABC_780b96afa8d417aa241e01ad594ce9_6", "text": "On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in Yang et al. (2015) ."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-26", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "In the results section, we compare the performance of BERT under different settings and share our submission results for the shared task."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-40", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "We make use of BERT-Base pretrained model provided by Devlin et al. (2018) in order to avoid pretraining from scratch."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-45", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "Then, we give the details of our experiments and results with BERT under pretraining and finetuning settings."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-64", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "We used BERT-Base which consists of 12 transformer blocks on top of each other applying 12 headed attention mechanism, hidden size of 768 and a total of 110 million parameters."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-81", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "For Masked LM task, we follow the same approach with Devlin et al. (2018) ."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-84", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "Starting from the pretrained model of BERT-Base instead of a cold start, we trained the model with a learning rate of 3e-5 and 256 as the maximum sequence length for 290k iterations."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "Among those, BERT attracts researchers most because of (i) its transformer based architecture enabling faster training and (ii) state of the art results in many different tasks."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "BERT restricts the input length to a maximum of 512 tokens."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-24", "intents": ["@MOT@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "In this study, on the other hand, we address (1) the performance of BERT by comparing its domain specific pre-trained and fine-tuned performances, and (2) in the setting where the target domain has extensively more data."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-82", "intents": ["@DIF@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "At the end of pretraining data generation process, we accumulated near 3.5 million samples, only running the process once on our train split, so without any duplication unlike Devlin et al. (2018) because of time restrictions."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-89", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "Table 3 demonstrates that pretraining BERT with domain specific data using unsupervised tasks improves the performance of the model on the supervised classificiation task."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-103", "intents": ["@DIF@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "Although our experiments ( Table 3) show us that pretraining BERT further with data from news domain has a positive effect on overall accuracy, we are not able to observe the similar effect on \"by-article-test-set\"."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-97", "intents": ["@EXT@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "In our last attempt, we pretrained BERT with our portal-wise train split, and then fine-tune it as described before."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Local context score Golden Figure 1 : One error case on AIDA-CoNLL development set of the full model of Ganea and Hofmann (2017) ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-25", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Such state-of-the-art entity linking models (Ganea and Hofmann 2017; Le and Titov 2018) employ attention-based bag-of-words context model and pre-trained entity embeddings bootstrapped from word embeddings to assess topic level context compatibility."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "On the other hand, the pre-trained entity embedding of Ganea and Hofmann (2017) is not very sensitive to entity types."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-71", "intents": ["@BACK@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Besides, Ganea and Hofmann (2017) combined this context score with the priorp(e|m) (computed by mixing mention-entity hyperlink count statistics from Wikipedia, a large Web corpus and YAGO."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-92", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Previous work (Yamada et al. 2016 ; Ganea and Hofmann 2017) on learning entity representation are mostly extensions of the embedding methods proposed by (Mikolov et al. 2013 )."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-179", "intents": ["@BACK@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "The reason is that BERT-based context representation space and Ganea and Hofmanns entity embeddings space are heterogeneous."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-29", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "As shown in Fig. 1 , the full model of Ganea and Hofmann (2017) incorrectly links the mention \"Milwaukee\" to the entity MILWAUKEE BREWERS."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-53", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "• We integrate a BERT-based entity similarity into the local model of a SOTA model (Ganea and Hofmann 2017) ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-62", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Next, we introduce the general formulation of entity linking problem with a focus on the well known DeepED model (Ganea and Hofmann 2017) ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-126", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Finally, as for the local disambiguation model, we integrate the BERT-based entity similarity Ψ BERT (e, c) with the local context score Ψ long (e, c) (defined in Equation 2) and the priorp(e|m i ) with two fully connected layers of 100 hidden units and ReLU non-linearities following the same feature composition methods as Ganea and Hofmann (2017) ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-140", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Following previous work (Ganea and Hofmann 2017), we only consider in-KB mentions."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-145", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "So we evaluate the performance when integrating the BERT-based entity similarity into the local context model of Ganea and Hofmann (2017) ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-146", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "We also evaluate our model with or without global modeling method of Ganea and Hofmann (2017) ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-152", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "The resources (word and entity embeddings) used to train the local context model of Ganea and Hofmann (2017) are obtained from DeepED 7 ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-205", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "This indicates that Ganea and Hofmann (2017) produces many type errors due to its inability to consider the entity type information in mention context."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-244", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "We follow Papernot and Mc-Daniel (2018) Ganea and Hofmann (2017) and BERT based entity representation space neighbour in the context representation space."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-254", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "We also retrieve nearest entities in the embedding space of Ganea and Hofmann (2017) and ours."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-49", "intents": ["@DIF@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "In addition, we conduct detailed experiment analysis on AIDA-CoNLL development set which shows our proposed model can reduce 67.03% type errors of the state-of-the-art model (Ganea and Hofmann 2017) and more than 90% of the remaining type error cases are due to over estimation of prior and global modeling problem which we leave as the further work."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-173", "intents": ["@DIF@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Our local model achieves a 1.31 improvement in terms of F1 over its corresponding baseline (Ganea and Hofmann 2017) , yielding a very competitive local model with an average 90.06 F1 score even surpassing the performance of four local & global models."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-185", "intents": ["@DIF@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "On average, our proposed model (BERT-Entity-Sim) outperforms the local & global version of Ganea and Hofmann; Le and Titov (2017; by an average 0.80 and 0.51 on F1."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-199", "intents": ["@DIF@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "As shown in Table 3 , our proposed entity embedding from BERT significantly outperforms the entity embedding proposed by Ganea and Hofmann (2017) on three typing sys-tems FIGER, BBN and OntoNotes fine ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-234", "intents": ["@DIF@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "DCA is a global entity linking model featuring better efficiency and effectiveness than that of Ganea and Hofmann (2017) by breaking the \"all-mention coherence\" assumption."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-164", "intents": ["@SIM@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Similar to Ganea and Hofmann (2017) , all the entity embeddings are fixed during fine-tuning."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-217", "intents": ["@EXT@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "It is nature to conjecture that we can also correct type errors by incorporating explicit type information into Ganea and Hofmann (2017) ."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-3", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "The model relies heavily on an adversarial, unsupervised alignment of word embedding spaces for bilingual dictionary induction (Conneau et al., 2018), which we examine here."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-25", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "In §4.7, we correlate our similarity metric with performance on unsupervised BDI."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-28", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "If we take the top k most frequent words in English, and the top k most frequent words in German, and build nearest neighbor graphs for English and German using the monolingual word embeddings used in Conneau et al. (2018) , the graphs are of course very different."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-52", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "The work of Conneau et al. (2018) , which we focus on here, also makes several implicit assumptions that may or may not be necessary to achieve such isomorphism, and which may or may not scale to low-resource languages."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-54", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "We focus on the work of Conneau et al. (2018) , who present a fully unsupervised approach to aligning monolingual word embeddings, induced using fastText (Bojanowski et al., 2017) ."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-87", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Using this seed dictionary, we then run the refinement step using Procrustes analysis of Conneau et al. (2018) ."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-90", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "In the following experiments, we investigate the robustness of unsupervised cross-lingual word embedding learning, varying the language pairs, monolingual corpora, hyper-parameters, etc., to obtain a better understanding of when and why unsupervised BDI works."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-96", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Following a standard evaluation practice (Vulić and Moens, 2013; Mikolov et al., 2013b; Conneau et al., 2018) , we report Precision at 1 scores (P@1): how many times one of the correct translations of a source word w is retrieved as the nearest neighbor of w in the target language."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-103", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "The third columns lists eigenvector similarities between 10 randomly sampled source language nearest neighbor subgraphs of 10 nodes and the subgraphs of their translations, all from the benchmark dictionaries in Conneau et al. (2018) ."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-140", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "In order to assess the sensitivity of unsupervised BDI to the comparability and domain similarity of the monolingual corpora, we replicate the experiments in Conneau et al. (2018) using combinations of word embeddings extracted from three different domains: 1) parliamentary proceedings from EuroParl.v7 (Koehn, 2005) , 2) Wikipedia (Al- Rfou et al., 2013) , and 3) the EMEA corpus in the medical domain (Tiedemann, 2009) ."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-192", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Finally, in order to get a better understanding of the limitations of unsupervised BDI, we correlate the graph similarity metric described in §2 (right column of Table 2 ) with performance across languages (left column)."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Some researchers have even presented unsupervised methods that do not rely on any form of cross-lingual supervision at all (Barone, 2016; Conneau et al., 2018; Zhang et al., 2017) ."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Unsupervised neural machine translation relies on BDI using cross-lingual embeddings (Lample et al., 2018a; Artetxe et al., 2018) , which in turn relies on the assumption that word embedding graphs are approximately isomorphic."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-71", "intents": ["@BACK@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "4 The approach builds on existing work on learning a mapping between monolingual word embeddings (Mikolov et al., 2013b; Xing et al., 2015) and consists of the following steps: 1) Monolingual word embeddings: An off-the-shelf word embedding algorithm (Bojanowski et al., 2017 ) is used to learn source and target language spaces X and Y ."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-156", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Conneau et al. (2018) use the same hyperparameters for inducing embeddings for all languages."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-200", "intents": ["@BACK@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Cross-lingual word embeddings Cross-lingual word embedding models typically, unlike Conneau et al. (2018) , require aligned words, sentences, or documents (Levy et al., 2017) ."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-208", "intents": ["@BACK@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "All unsupervised NMT methods critically rely on accurate unsupervised BDI and back-translation."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-35", "intents": ["@MOT@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Eigenvector similarity Since the nearest neighbor graphs are not isomorphic, even for frequent translation pairs in neighboring languages, we want to quantify the potential for unsupervised BDI using a metric that captures varying degrees of graph similarity."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-116", "intents": ["@MOT@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Agglutinative languages with mixed or double marking show more morphological variance with content words, and we speculate whether unsupervised BDI is challenged by this kind of morphological complexity."}
{"sent_id": "c3f71bea55f85633568c7ba57f6fd5-C001-74", "intents": ["@EXT@"], "paper_id": "ABC_c3f71bea55f85633568c7ba57f6fd5_6", "text": "We adapted the original work of Johnson et al. [11] to our acoustical context by updating the function catalog and the relationships between the objects of the scene."}
{"sent_id": "c3f71bea55f85633568c7ba57f6fd5-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_c3f71bea55f85633568c7ba57f6fd5_6", "text": "In the visual domain, several strategies have been proposed to make this kind of data available to the community [11, 2, 25, 7] ."}
{"sent_id": "c3f71bea55f85633568c7ba57f6fd5-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_c3f71bea55f85633568c7ba57f6fd5_6", "text": "Johnson et al. [11] made their way around this constraint by using synthetic data."}
{"sent_id": "c3f71bea55f85633568c7ba57f6fd5-C001-34", "intents": ["@MOT@"], "paper_id": "ABC_c3f71bea55f85633568c7ba57f6fd5_6", "text": "Inspired by the work on CLEVR [11] , we propose an acoustical question answering (AQA) task by defining a synthetic dataset that comprises audio scenes composed by sequences of elementary sounds and questions relating properties of the sounds in each scene."}
{"sent_id": "c3f71bea55f85633568c7ba57f6fd5-C001-42", "intents": ["@USE@"], "paper_id": "ABC_c3f71bea55f85633568c7ba57f6fd5_6", "text": "To represent questions, we use the same semantic representation through functional programs that is proposed in [11, 12] ."}
{"sent_id": "c3f71bea55f85633568c7ba57f6fd5-C001-86", "intents": ["@USE@"], "paper_id": "ABC_c3f71bea55f85633568c7ba57f6fd5_6", "text": "A validation process [11] is responsible for rejecting both ill-posed and degenerate questions during the generation phase."}
{"sent_id": "b7278824bdae498021b899fbc6c638-C001-20", "intents": ["@DIF@"], "paper_id": "ABC_b7278824bdae498021b899fbc6c638_6", "text": "(3) For entity recognition, we integrate the gazetteer with a simple, but effective machine learning classifier, and experimentally show that the extended gazetteers improve the F 1 score between 7% and 12% over our baseline approach and outperform (Zhang and Iria, 2009 ) on all learned concepts (subject, location, temporal)."}
{"sent_id": "b7278824bdae498021b899fbc6c638-C001-29", "intents": ["@SIM@"], "paper_id": "ABC_b7278824bdae498021b899fbc6c638_6", "text": "Most closely related to our own work, the authors of (Zhang and Iria, 2009 ) build an approach solely on WIKIPEDIA which does not only exploit the article text but also analyzes the structural elements of WIKIPEDIA:"}
{"sent_id": "b7278824bdae498021b899fbc6c638-C001-54", "intents": ["@SIM@", "@BACK@"], "paper_id": "ABC_b7278824bdae498021b899fbc6c638_6", "text": "It has been previously observed, (Zhang and Iria, 2009 ) and (Strube and Ponzetto, 2006) , that the category graph of poor quality."}
{"sent_id": "b7278824bdae498021b899fbc6c638-C001-79", "intents": ["@USE@"], "paper_id": "ABC_b7278824bdae498021b899fbc6c638_6", "text": "In this evaluation, we use the same setup as in (Zhang and Iria, 2009 ): A corpus of 30 full length UK archaeological reports archived by the Arts and Humanities Data Service (AHDS)."}
{"sent_id": "6ed955baf28ad1c7fd6d590e660c20-C001-64", "intents": ["@USE@"], "paper_id": "ABC_6ed955baf28ad1c7fd6d590e660c20_7", "text": "As such, we use Tian et al. (2014) 's inference engine to pin down statements that are actually needed for proving H (usually just 2 or 3 statements), and try to prove H by Prover9 again, using only necessary statements."}
{"sent_id": "6ed955baf28ad1c7fd6d590e660c20-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_6ed955baf28ad1c7fd6d590e660c20_7", "text": "3 Alignment with logical clues Tian et al. (2014) proposed a way to generate onthe-fly knowledge to fill knowledge gaps: if H is not proven, compare DCS trees of T and H to generate path alignments (e.g. blamed for death ∼ cause loss of life, as underscored in Figure 1) ; evaluate the path alignments by a similarity score function; and path alignments with a score greater than a threshold (0.4) are accepted and converted to inference rules."}
{"sent_id": "6ed955baf28ad1c7fd6d590e660c20-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_6ed955baf28ad1c7fd6d590e660c20_7", "text": "Tian et al. (2014) used some logical clues to filter out irrelevant path alignments, which helps to keep a high precision."}
{"sent_id": "6ed955baf28ad1c7fd6d590e660c20-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_6ed955baf28ad1c7fd6d590e660c20_7", "text": "However, despite the expectation that all entailment matters can be explained logically, our observation is that currently logical inference only fills very limited short gaps from T to H. The logical phenomena easily addressed by Tian et al. (2014) Table 2 : Proportion (%) of exit status of Prover9"}
{"sent_id": "926e7df3c367ae29da574ba465504f-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_926e7df3c367ae29da574ba465504f_7", "text": "In recent works, short phrases (Lau et al., 2011) or images (Aletras and Stevenson, 2013) have been used as alternatives."}
{"sent_id": "926e7df3c367ae29da574ba465504f-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_926e7df3c367ae29da574ba465504f_7", "text": "It is an iterative method that has a runtime complexity of O(n 2 ) which makes it infeasible to run over large number of images."}
{"sent_id": "926e7df3c367ae29da574ba465504f-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_926e7df3c367ae29da574ba465504f_7", "text": "Furthermore, its accuracy is limited by the recall of the information retrieval engine."}
{"sent_id": "926e7df3c367ae29da574ba465504f-C001-77", "intents": ["@USE@"], "paper_id": "ABC_926e7df3c367ae29da574ba465504f_7", "text": "Our evaluation follows prior work (Lau et al., 2011; Aletras and Stevenson, 2013) using two metrics."}
{"sent_id": "926e7df3c367ae29da574ba465504f-C001-91", "intents": ["@EXT@"], "paper_id": "ABC_926e7df3c367ae29da574ba465504f_7", "text": "We adapt the original method of Aletras and Stevenson (2013) to compute the PageRank scores of all the available images in the test set of each fold for each topic (Global PPR)."}
{"sent_id": "926e7df3c367ae29da574ba465504f-C001-100", "intents": ["@DIF@"], "paper_id": "ABC_926e7df3c367ae29da574ba465504f_7", "text": "Top-1 aver. rating nDCG-1 nDCG-3 nDCG-5 Global PPR (Aletras and Stevenson, 2013) 1 (Aletras and Stevenson, 2013) 2.24 --- The DNN (Topic+Caption) model that uses only textual information, obtains a Top-1 Average performance of 1.94."}
{"sent_id": "2eeffe385539b28c7d31eeb176e926-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_2eeffe385539b28c7d31eeb176e926_7", "text": "In the past year, the field of Natural Language Processing (NLP) has seen the rise of pretrained language models such as as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) and BERT (Devlin et al., 2019) ."}
{"sent_id": "2eeffe385539b28c7d31eeb176e926-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_2eeffe385539b28c7d31eeb176e926_7", "text": "Applying these models to the General Language Understanding Evaluation (GLUE) benchmark introduced by Wang et al. (2018) has achieved the best performance to date on tasks ranging from sentiment classification to question answering (Devlin et al., 2019) ."}
{"sent_id": "2eeffe385539b28c7d31eeb176e926-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_2eeffe385539b28c7d31eeb176e926_7", "text": "While large pretrained models offer significantly increased performance, they come with their own constraints, as the number of parameters in the classic BERT-base model exceeds 100 million."}
{"sent_id": "2eeffe385539b28c7d31eeb176e926-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_2eeffe385539b28c7d31eeb176e926_7", "text": "More recent work has addressed this challenge by 'distilling' the models, training smaller versions of BERT which reduce the number of parameters to train by 40% while retaining more than 95% of the full model performance and even outperforming it on two out of eleven GLUE tasks ."}
{"sent_id": "2eeffe385539b28c7d31eeb176e926-C001-32", "intents": ["@EXT@"], "paper_id": "ABC_2eeffe385539b28c7d31eeb176e926_7", "text": "Language Models: We constructed two models, EduBERT and EduDistilBERT, which respectively refine BERT-base and DistilBERT , both of which were trained on general domain text from books and Wikipedia (Devlin et al., 2019) ."}
{"sent_id": "2eeffe385539b28c7d31eeb176e926-C001-42", "intents": ["@USE@"], "paper_id": "ABC_2eeffe385539b28c7d31eeb176e926_7", "text": "We compare between the four classifiers BERT-base, DistilBERT, EduBERT and EduDistilBERT."}
{"sent_id": "2eeffe385539b28c7d31eeb176e926-C001-45", "intents": ["@USE@"], "paper_id": "ABC_2eeffe385539b28c7d31eeb176e926_7", "text": "Table 1 compares EduBERT, EduDistilBERT to their base versions, as well as the state-of-the-art (SoA) for urgency detection (Guo et al. 2019) ."}
{"sent_id": "c7c9266b5063ec85494fde45d1dce1-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_c7c9266b5063ec85494fde45d1dce1_7", "text": "Most of the earlier work revolves either around manual feature extraction [6] or use representation learning methods followed by a linear classifier [1, 4] of complex problems in speech, vision and text applications."}
{"sent_id": "c7c9266b5063ec85494fde45d1dce1-C001-50", "intents": ["@USE@"], "paper_id": "ABC_c7c9266b5063ec85494fde45d1dce1_7", "text": "We experimented with a dataset of 16K annotated tweets made available by the authors of [6] ."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "Seminal work from [5] presents a comparative style analysis of hyperpartisan news, evaluating features such as characters n-grams, stop words, part-of-speech, readability scores, and ratios of quoted words and external links."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-51", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "We used the BuzzedFeed-Webis Fake News Corpus 2016 collected by [5] whose articles were labeled with respect to three political orientations: mainstream, left-wing, and right-wing (see Table 2 )."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-57", "intents": ["@USE@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "4 Following the settings of [5] , we balance the training set using random duplicate oversampling."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-70", "intents": ["@USE@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "Evaluation: We performed 3-fold cross-validation with the same configuration used in [5] ."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-76", "intents": ["@USE@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "We compare with [5] against their topic and style-based methods."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-54", "intents": ["@EXT@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "During initial data analysis and prototyping we identified a variety of issues with the original dataset: we cleaned the data excluding articles with empty or bogus texts, e.g. 'The document has moved here' (23 and 14 articles respectively)."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-82", "intents": ["@SIM@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "Similar to [5] , the topic-based model achieves better results than the style-related model."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-83", "intents": ["@DIF@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "However, the differences between the results of the two evaluated approaches are much higher (0.66 vs. 0.57 according to Macro F 1 ) than those shown in [5] ."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-90", "intents": ["@DIF@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "In fact, comparing the results of [5] against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results."}
{"sent_id": "ec0ae4e56c069e3efb4a2dc12199cd-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_ec0ae4e56c069e3efb4a2dc12199cd_7", "text": "In Tomanek et al. (2007a) we introduced the selection agreement (SA) curve -the average agreement amongst the selected examples plotted over time."}
{"sent_id": "ec0ae4e56c069e3efb4a2dc12199cd-C001-100", "intents": ["@USE@"], "paper_id": "ABC_ec0ae4e56c069e3efb4a2dc12199cd_7", "text": "We employed the committee-based AL approach described in Tomanek et al. (2007a) ."}
{"sent_id": "74e7b114ae968e196ea87f529f5eff-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_74e7b114ae968e196ea87f529f5eff_7", "text": "Cohyponymy (or coordination), on the other hand, is the relation held by words sharing a close hypernym, which are therefore attributionally similar (Weeds et al., 2014) ."}
{"sent_id": "74e7b114ae968e196ea87f529f5eff-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_74e7b114ae968e196ea87f529f5eff_7", "text": "For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers' ability in such discrimination, generally achieving promising results (Weeds et al., 2014; Rimmel, 2014; Geffet and Dagan, 2005) ."}
{"sent_id": "74e7b114ae968e196ea87f529f5eff-C001-19", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_74e7b114ae968e196ea87f529f5eff_7", "text": "Such results are competitive with the state-of-the-art (Weeds et al., 2014) ."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "Recently, Wang et al. (2019) introduced an embedding alignment approach to enable continual learning for relation extraction models."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "While they obtained promising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-79", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "We conduct experiments on Lifelong FewRel and Lifelong SimpleQuestions datasets, both introduced in Wang et al. (2019) ."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "We report two measures, ACC whole and ACC avg , both introduced in Wang et al. (2019) ."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-88", "intents": ["@BACK@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "We also report ACC avg , which measures the average accuracy on the test set of only observed (seen) tasks."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-20", "intents": ["@EXT@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "Unlike the use of a separate alignment model as proposed in Wang et al. (2019) , the proposed approach does not introduce additional parameters."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-64", "intents": ["@USE@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "In order to use the same number of parameters and ensure fair comparison to Wang et al. (2019) , we adopt as the relation extraction model f θ the Hier- arachical Residual BiLSTM (HR-BiLSTM) model of Yu et al. (2017) , which is the same model used by Wang et al. (2019) for their experiments."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-91", "intents": ["@USE@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "Full Supervision Results Table 1 gives both the ACC whole and ACC avg results of our approach compared to other approaches including Episodic Memory Replay (EMR) and its various embedding-aligned variants EA-EMR as proposed in Wang et al. (2019) ."}
{"sent_id": "6fbfc9f887e736472510bce30c9228-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_6fbfc9f887e736472510bce30c9228_7", "text": "Ivanovic (2005b) describes the manual process of segmenting the messages into utterances and labelling the utterances with dialogue act tags to produce a gold standard version of the data."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-49", "intents": ["@USE@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "We train our classifiers on the exact training set defined by Zitouni et al. (2006) , a subpart of the third segment of the Penn Arabic Treebank (Maamouri et al., 2004 ) (\"ATB3-Train\", 288,000 words)."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-82", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "To our knowledge, their system is the best performing currently, and we have set up our experiments to allow us to compare our results directly to their results."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-86", "intents": ["@USE@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "In order to assure maximal comparability with the work of Zitouni et al. (2006) , we adopt their metric."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-79", "intents": ["@BACK@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "They then use segment n-grams, segment position of the character being diacritized, the POS of the current segment, along with lexical features, including letter and word n-grams."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-105", "intents": ["@DIF@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "We can see the effect of our different approaches to diacritization in the numbers: while for WER we reduce the Zitouni et al error by 17.2%, the DER error reduction is only 10.9%."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-113", "intents": ["@DIF@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "We also note the issue of unknown words, which will affect our system much more than that of (Zitouni et al., 2006) ."}
{"sent_id": "fd5a6307b398f37d8729c21cfce6c1-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_fd5a6307b398f37d8729c21cfce6c1_7", "text": "Strubell et al. (2017) and Chiu and Nichols (2016) apply word spelling features and further integrate context features."}
{"sent_id": "fd5a6307b398f37d8729c21cfce6c1-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_fd5a6307b398f37d8729c21cfce6c1_7", "text": "Some literature reports results using mean and standard deviation under different random seeds (Chiu and Nichols, 2016; Peters et al., 2017; Liu et al., 2018) ."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "Their zeroshot model generalised from seen relations to unseen relations by translating all relations into natural language question templates, such as Where was XXX born? for the relation place of birth."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "These examples were derived from a pre-existing relation extraction resource, as their intention was to show the utility of the QA model."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "Levy et al. (2017) provide a number of train/dev/test splits, to allow them to evaluate a variety of modes of generalisation."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-41", "intents": ["@EXT@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "Random samples of 10 3 , 10 4 , 10 5 and 10 6 UWRE instances are added to our SQuAD training set, while leaving the SQuAD dev dataset untouched."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-57", "intents": ["@EXT@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "In this second experiment, we want to test the ability of the models decribed above to generalise to data beyond the UWRE test set."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-42", "intents": ["@USE@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "Models We employ the same modified BiDAF (Seo et al., 2016 ) model as Levy et al. (2017) , which uses an additional bias term to allow the model to signal when no answer is predicted within the text."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-43", "intents": ["@USE@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "Evaluation Following the approach of Levy et al. (2017) , we report F1 scores on the answers returned by the model."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-49", "intents": ["@USE@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "We compare training purely on UWRE instances to those same instances combined with the whole SQuAD dataset."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-69", "intents": ["@USE@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "Although the original UWRE model achieved an F1 of around 90% on the unmodified entity split test set, here it only manages to get 2% of its predictions correct."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-37", "intents": ["@USE@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "The parser output is evaluated using the gold-standard Faroese test treebank developed by Tyers et al. (2018) ."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-69", "intents": ["@USE@", "@SIM@", "@DIF@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "We use the same raw corpus, alignments and tokenized and segmented versions of the source translations 4 as Tyers et al. (2018) who release all of their data."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-83", "intents": ["@USE@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "We use word alignments between the Faroese text and the source translations generated by Tyers et al. (2018) using fast align (Dyer et al., 2013) , a word alignment tool based on IBM Model 2."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-163", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "Comparing the results in Tables 4 and 5, we see that LAS scores tend to be slightly lower than on the version which included all target sen-WORK RESULT Rosa and Mareček (2018) 49.4 Tyers et al. (2018) 64.4 Our implementation 68.0 of Tyers et al. (2018) Our Best Model 71.5 tences, indicating that we did lose some information by filtering out a large number of sentences."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-14", "intents": ["@EXT@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "We build on recent work by Tyers et al. (2018) who show that in the absence of annotated training data for the target language, a lexicalized treebank can be created by translating a target language corpus into a number of related source languages and parsing the translations using models trained on the source language treebanks."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-15", "intents": ["@SIM@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "1 These annotations are then projected to the target language using separate word alignments for each source language, combined into a single graph for each sentence and decoded (Sagae and Lavie, 2006) , resulting in a treebank for the target language, Faroese in the case of Tyers et al.'s and our experiments."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-30", "intents": ["@DIF@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "Our best result of 71.5 -an absolute improvement of 7.2 points over the result reported by Tyers et al. (2018) -was achieved with multi-treebank target learning over the monolingual projections."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-70", "intents": ["@DIF@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "5 In this way, the experimental pipeline is the same as theirs but we predict POS tags and dependency annotations using our own models."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-74", "intents": ["@BACK@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "Machine Translation As noted by Tyers et al. (2018) , popular repositories for developing machine translation systems such as OPUS (Tiedemann, 2016) contain an inadequate amount of sentences to train a data-driven machine translation system for Faroese."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "For a more thorough description of the machine translation process and for resource creation in general, see the work of Tyers et al. (2018) ."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-113", "intents": ["@BACK@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "Annotation Projection Once the synthetic source treebanks are compiled, i. e. the translations are parsed, the annotations are then projected from the source translations to the target language using the word alignments and Tyers et al.'s projection tool, resulting in a Faroese treebank."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-116", "intents": ["@BACK@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "Tyers et al.'s projection setup removes unsuitable projected trees containing e. g. more than one root token, a token that is its own head or a token with a head outside the range of the sentence."}
{"sent_id": "05fe3e9c1598f5b36b6efa79216309-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_05fe3e9c1598f5b36b6efa79216309_7", "text": "presented an efficient method to generate the next word in a sequence when it is added an attention mechanism, improving the performance for long textual sequences [1] ."}
{"sent_id": "05fe3e9c1598f5b36b6efa79216309-C001-31", "intents": ["@USE@"], "paper_id": "ABC_05fe3e9c1598f5b36b6efa79216309_7", "text": "The model is trained using two real-world datasets: BeerAdvocate [5] and Amazon book reviews [1] ."}
{"sent_id": "05fe3e9c1598f5b36b6efa79216309-C001-46", "intents": ["@USE@"], "paper_id": "ABC_05fe3e9c1598f5b36b6efa79216309_7", "text": "The characters are given by maximizing the softmax conditional probability p, based on the new character dependencies H attention t [1] , as presented in Eq. 2 p = softmax(H attention t W + b), char = arg max p (2)"}
{"sent_id": "008d5261ee7385a2b7e39772938f51-C001-67", "intents": ["@BACK@"], "paper_id": "ABC_008d5261ee7385a2b7e39772938f51_7", "text": "While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000) , the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011) ."}
{"sent_id": "008d5261ee7385a2b7e39772938f51-C001-57", "intents": ["@SIM@"], "paper_id": "ABC_008d5261ee7385a2b7e39772938f51_7", "text": "The baseline score, shown in bold, is obtained with no context window and is comparable to the results reported by Athar (2011) ."}
{"sent_id": "008d5261ee7385a2b7e39772938f51-C001-64", "intents": ["@DIF@"], "paper_id": "ABC_008d5261ee7385a2b7e39772938f51_7", "text": "Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data (Athar, 2011) ."}
{"sent_id": "d06a49ad232f73328874282d91cde0-C001-32", "intents": ["@EXT@"], "paper_id": "ABC_d06a49ad232f73328874282d91cde0_7", "text": "Along the lines of the analysis presented in (Zhang et al., 2007a) , in this paper we propose a more elaborated par-tial parsing model, in order to further simplify the training procedure, so that full parse disambiguation models can be reused in partial parsing."}
{"sent_id": "d06a49ad232f73328874282d91cde0-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_d06a49ad232f73328874282d91cde0_7", "text": "Based on this fact, (Zhang et al., 2007a) have proposed to use partial parsing models to recover the most useful fragment analyses from the intermediate parsing results in cases of unsuccessful parses."}
{"sent_id": "d06a49ad232f73328874282d91cde0-C001-82", "intents": ["@BACK@"], "paper_id": "ABC_d06a49ad232f73328874282d91cde0_7", "text": "Based on a similar definition of partial parse, Zhang et al. (2007a) formulated the following statistical model:"}
{"sent_id": "d06a49ad232f73328874282d91cde0-C001-87", "intents": ["@BACK@"], "paper_id": "ABC_d06a49ad232f73328874282d91cde0_7", "text": "Furthermore, Zhang et al. (2007a) evaluated the fragment semantic outputs based on a practical estimation of RMRS similarities described by Dridan and Bond (2006) ."}
{"sent_id": "d06a49ad232f73328874282d91cde0-C001-157", "intents": ["@BACK@"], "paper_id": "ABC_d06a49ad232f73328874282d91cde0_7", "text": "As Zhang et al. (2007a) have also pointed out, the evaluation of a partial parser is a very difficult task as such, due to the lack of gold-standard annotation for sentences that are not fully analysed by the grammar."}
{"sent_id": "9731b4cea1405b7cbf3792aed5b1e4-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_9731b4cea1405b7cbf3792aed5b1e4_7", "text": "Conditional Random Fields (CRFs) (Lafferty et al., 2001 ) have been widely used named entity recognition (Ratinov and Roth, 2009; Finkel et al., 2005) , a task similar to our own."}
{"sent_id": "9731b4cea1405b7cbf3792aed5b1e4-C001-28", "intents": ["@USE@"], "paper_id": "ABC_9731b4cea1405b7cbf3792aed5b1e4_7", "text": "We treat sense recognition in text as a sequence labeling task where each sentence is a sequence of tokens labeled using the BIO tagging scheme (Ratinov and Roth, 2009 )."}
{"sent_id": "a2b945e18ab6b73b4021a2db8bda4f-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_a2b945e18ab6b73b4021a2db8bda4f_7", "text": "The advantage of having a simple annotation scheme is two-fold: it allows for more reliable human annotations and it enables better performance for argumentation mining systems designed to automatically identify the argumentative structure (Stab and Gurevych, 2014b) ."}
{"sent_id": "a2b945e18ab6b73b4021a2db8bda4f-C001-23", "intents": ["@USE@"], "paper_id": "ABC_a2b945e18ab6b73b4021a2db8bda4f_7", "text": "Second, we show that the argumentation features extracted based on argumentative structures automatically predicted by a state-of-the-art argumentation mining system (Stab and Gurevych, 2014b) are also good predictors of essays scores (Section 4)."}
{"sent_id": "a2b945e18ab6b73b4021a2db8bda4f-C001-72", "intents": ["@USE@"], "paper_id": "ABC_a2b945e18ab6b73b4021a2db8bda4f_7", "text": "In the first setting, we used the dataset of 90 high quality persuasive essays from (Stab and Gurevych, 2014b ) (S&G) as training and use T OEF L arg for testing (out-of-domain setting)."}
{"sent_id": "a2b945e18ab6b73b4021a2db8bda4f-C001-75", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_a2b945e18ab6b73b4021a2db8bda4f_7", "text": "We ran experiments for all different features groups and observe that with the exception of the P class, the F1 scores for all the other classes is comparable to the results reported by Stab and Gurevych (2014b) ."}
{"sent_id": "a2b945e18ab6b73b4021a2db8bda4f-C001-80", "intents": ["@DIF@"], "paper_id": "ABC_a2b945e18ab6b73b4021a2db8bda4f_7", "text": "The F1 score of identifying support relations is 84.3% (or 89% using top100), much higher than reported by Stab and Gurevych (2014b) ."}
{"sent_id": "a2b945e18ab6b73b4021a2db8bda4f-C001-83", "intents": ["@DIF@"], "paper_id": "ABC_a2b945e18ab6b73b4021a2db8bda4f_7", "text": "There can be two explanations for this: 1) essays in T OEF L arg have multiple short paragraphs where the position features such as position of the arguments in the essay and paragraph (Structural group) are strong indicators for argument relations; and 2) due to short paragraphs, the percentage of N S instances are less than in the S&G dataset, hence the Lexical features (i.e., word-pairs between Arg 1 and Arg 2 ) perform very well."}
{"sent_id": "78afdf391c70d7992200b4071e4ac2-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_78afdf391c70d7992200b4071e4ac2_7", "text": "Recently, Bonial et al. (2014) have introduced an approach to improve the handling of MWEs in PB while keeping annotation costs low."}
{"sent_id": "78afdf391c70d7992200b4071e4ac2-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_78afdf391c70d7992200b4071e4ac2_7", "text": "Bonial et al. (2014) conducted a pilot study re-annotating 138 CPs involving the verb take."}
{"sent_id": "78afdf391c70d7992200b4071e4ac2-C001-169", "intents": ["@EXT@"], "paper_id": "ABC_78afdf391c70d7992200b4071e4ac2_7", "text": "We have presented an approach to handle CPs in SRL that extends on work from Bonial et al. (2014) ."}
{"sent_id": "78afdf391c70d7992200b4071e4ac2-C001-110", "intents": ["@SIM@"], "paper_id": "ABC_78afdf391c70d7992200b4071e4ac2_7", "text": "In line with the results from Bonial et al. (2014) who aliased 100 out of 138 uncompositional take MWEs, we were also able to alias most of the CPs in our annotation set."}
{"sent_id": "78afdf391c70d7992200b4071e4ac2-C001-116", "intents": ["@USE@"], "paper_id": "ABC_78afdf391c70d7992200b4071e4ac2_7", "text": "In addition, we evaluated our system on the dataset from Bonial et al. (2014) , restricted to the type of CP our system handles (LVCs and VPCs) and verb aliases (as opposed to aliases being a noun or adjective roleset)."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-40", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "Wish Detection: Goldberg et al. (2009) performed wish detection on datasets obtained from political discussion forums and product reviews."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "Following two datasets are made available: a. Political Discussions: 6379 sentences, out of which 34% are annotated wishes."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "Table 1 presents some examples from these datasets."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-106", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "5 Subjunctive Feature Evaluation Goldberg et al. (2009) evaluated their approach using a 10 fold cross validation on their datasets."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-109", "intents": ["@BACK@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "AUC was also used by Goldberg et al. (2009) ."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-41", "intents": ["@MOT@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "They automatically extracted sentence templates from a corpus of new year wishes, and used them as features with a statistical classifier."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-59", "intents": ["@EXT@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "• Suggestion Detection Product reviews (new): We re-annotated the product review dataset from Goldberg et al. (2009) , for suggestions."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-107", "intents": ["@USE@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "In order to compare subjunctive features against their wish template features, we also perform 10 fold cross validation on their wish datasets (politics and products)."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-117", "intents": ["@DIF@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "Wish Detection: Unigrams vs Subjunctive: One probable reason for the better performance of subjunctive features over unigrams in the case of product dataset, could be the small size of the dataset."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-119", "intents": ["@DIF@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "Goldberg et al. (2009) perform better than our subjunctive features for the politics data."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-66", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "Unlabeled Argument Detection (UA) Inspired by the method presented in (Fitzgerald et al., 2018) , arguments are matched using a span matching criterion of intersection over union ≥ 0.5 ."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-112", "intents": ["@BACK@", "@MOT@", "@USE@", "@DIF@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "To illustrate the effectiveness of our new goldstandard, we use its Wikinews development set to evaluate the currently available parser from (Fitzgerald et al., 2018) ."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-116", "intents": ["@BACK@", "@USE@", "@DIF@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "As expected, the parser's recall against our gold is substantially lower than the 84.2 recall reported in (Fitzgerald et al., 2018) against Dense, due to the limited recall of Dense relative to our gold set."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-40", "intents": ["@MOT@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "Also noteworthy, is that while traditional SRL annotations contain a single authoritative and nonredundant annotation, the 2018 dataset provides the raw annotations of all annotators."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-121", "intents": ["@MOT@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "While the parser correctly predicts 82% of non-implied roles, it skips half of the implied ones."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-48", "intents": ["@USE@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "Annotation We adopt the annotation machinery of (Fitzgerald et al., 2018) implemented using Amazon's Mechanical Turk, 2 and annotate each predicate by 2 trained workers independently, while a third consolidates their annotations into a final set of roles and arguments."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-60", "intents": ["@USE@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "QA generating annotators are paid the same as in Fitzgerald et al. (2018) , while the consolidator is rewarded 5¢ per verb and 3¢ per question."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-78", "intents": ["@USE@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "As we will see, our evaluation heuristics, adapted from those in Fitzgerald et al. (2018) , significantly underestimate agreement between annotations, hence reflecting performance lower bounds."}
{"sent_id": "d72f0608fddd1bf1cdef7ca6a20bdf-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_d72f0608fddd1bf1cdef7ca6a20bdf_8", "text": "Recent trends in speech recognition [7, 8, 9] have demonstrated impressive performance on Switchboard and Fisher data."}
{"sent_id": "d72f0608fddd1bf1cdef7ca6a20bdf-C001-82", "intents": ["@BACK@", "@USE@", "@SIM@"], "paper_id": "ABC_d72f0608fddd1bf1cdef7ca6a20bdf_8", "text": "Also, the CNN models always gave better results, confirming similar observations from studies reported earlier [8] ."}
{"sent_id": "d72f0608fddd1bf1cdef7ca6a20bdf-C001-119", "intents": ["@FUT@"], "paper_id": "ABC_d72f0608fddd1bf1cdef7ca6a20bdf_8", "text": "Also, advanced acoustic modeling, through the use of timedelayed neural nets (TDNNs), long short-term memory neural nets (LSTMs), and the VGG nets, should also be explored as their performance has been mostly reported using MFB features, and the use of multi-view features can help further improve their performance."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "Olabiyi et al. [7] tackle this problem by training a modified HRED generator alongside an adversarial discriminator in order to provide a stronger guarantee to the generator's output."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "In the case of hredGAN [7] , it is a bidirectional RNN that discriminates at the word level to capture both the syntactic and semantic difference between the ground truth and the generator output."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "where λ G and λ M are hyperparameters and L cGAN (G, D) and L MLE (G) are defined in Eqs. (5) and (7) of [7] respectively."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-28", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "To overcome these limitations, we propose phredGAN , a multi-modal hredGAN dialogue system which additionally conditions the adversarial framework proposed by Olabiyi et al. [7] on speaker and/or utterance attributes in order to maintain response quality of hredGAN and still capture speaker and other modalities within a conversation."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-33", "intents": ["@USE@", "@EXT@", "@SIM@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "We train and sample the proposed phredGAN similar to the procedure for hredGAN [7] ."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-92", "intents": ["@USE@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "The parameter update is conditioned on the discriminator accuracy performance as in [7] with acc D th = 0.99 and acc G th = 0.75."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-120", "intents": ["@USE@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "We follow the same training, development, and test split as the UDC dataset in [7] , with 90%, 5%, and 5% proportions, respectively."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-127", "intents": ["@USE@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "We use similar evaluation metrics as in [7] including perplexity, BLEU [15] , ROUGE [16] , and distinct n-gram [17] scores."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-108", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "In this section, we explore phredGAN 's results on two conversational datasets and compare its performance to the persona system in Li et al. [8] and hredGAN [7] in terms of quantitative and qualitative measures."}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-21", "intents": ["@USE@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "Following (Hoshino et al., 2013) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order."}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-39", "intents": ["@USE@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "Since this method is the one we re-implemented in this paper, we will describe their method in detail below."}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-48", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "3 Extension to (Hoshino et al., 2013) Our proposed preordering model is based on (Hoshino et al., 2013) with three extensions to better handle academic writing in scientific papers."}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-104", "intents": ["@USE@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "All the preordering models using (Hoshino et al., 2013) are our re-implementation of their paper."}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-24", "intents": ["@EXT@", "@MOT@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "• We propose an extension to (Hoshino et al., 2013) in order to deal with abbreviation and passivization frequently found in scientific papers."}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-95", "intents": ["@DIF@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "In terms of BLEU, our re-implementation of (Hoshino et al., 2013) is below the baseline method while our proposed methods better than the baseline."}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "Third, Hoshino et al. (2013) proposed predicate-argument structure-based preordering rules in two-level for the Japanese-English patent translation task."}
{"sent_id": "e9e733d38affa8a39a633ffb4d9d71-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_e9e733d38affa8a39a633ffb4d9d71_8", "text": "Krishna et al. (2016) is currently the state of the art in Sanskrit word segmentation."}
{"sent_id": "e9e733d38affa8a39a633ffb4d9d71-C001-70", "intents": ["@MOT@"], "paper_id": "ABC_e9e733d38affa8a39a633ffb4d9d71_8", "text": "Though Krishna et al. (2016) has designed their system with this requirement in mind and outlined the possible extension of their system for the purpose, the system currently only predicts the final word-form."}
{"sent_id": "fdfb8fbdb8544dca17b1aeba768124-C001-12", "intents": ["@USE@"], "paper_id": "ABC_fdfb8fbdb8544dca17b1aeba768124_8", "text": "In this paper, we compare CFG filtering techniques for LTAG (Harbusch, 1990; Poller and Becker, 1998) and HPSG (Torisawa et al., 2000; Kiefer and Krieger, 2000) , following an approach to parsing comparison among different grammar formalisms )."}
{"sent_id": "fdfb8fbdb8544dca17b1aeba768124-C001-27", "intents": ["@USE@"], "paper_id": "ABC_fdfb8fbdb8544dca17b1aeba768124_8", "text": "**CFG FILTERING TECHNIQUES**"}
{"sent_id": "fdfb8fbdb8544dca17b1aeba768124-C001-28", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_fdfb8fbdb8544dca17b1aeba768124_8", "text": "An initial offline step of CFG filtering is performed to approximate a given grammar with a CFG."}
{"sent_id": "fdfb8fbdb8544dca17b1aeba768124-C001-55", "intents": ["@USE@"], "paper_id": "ABC_fdfb8fbdb8544dca17b1aeba768124_8", "text": "In this section, we compare a pair of CFG filtering techniques for LTAG (Poller and Becker, 1998) and HPSG (Torisawa et al., 2000) described in Section 2.2.1 and 2.2.2."}
{"sent_id": "fdfb8fbdb8544dca17b1aeba768124-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_fdfb8fbdb8544dca17b1aeba768124_8", "text": "The CFG filtering generally consists of two steps."}
{"sent_id": "fdfb8fbdb8544dca17b1aeba768124-C001-89", "intents": ["@DIF@"], "paper_id": "ABC_fdfb8fbdb8544dca17b1aeba768124_8", "text": "Experimental results showed that the existing CF approximation of HPSG (Torisawa et al., 2000) produced a more effective filter than that of LTAG (Poller and Becker, 1998) ."}
{"sent_id": "3e2fb3d4c1e224c084117c22a5db78-C001-20", "intents": ["@USE@"], "paper_id": "ABC_3e2fb3d4c1e224c084117c22a5db78_8", "text": "OGTD uses a working definition of offensive language inspired by the OLID dataset for English (Zampieri et al., 2019a) used in the recent OffensEval (SemEval-2019 Task 6) (Zampieri et al., 2019b) ."}
{"sent_id": "3e2fb3d4c1e224c084117c22a5db78-C001-26", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_3e2fb3d4c1e224c084117c22a5db78_8", "text": "OGTD considers a more general definition of offensiveness inspired by the first layer of the hierarchical annotation model described in (Zampieri et al., 2019a) ."}
{"sent_id": "3e2fb3d4c1e224c084117c22a5db78-C001-59", "intents": ["@USE@"], "paper_id": "ABC_3e2fb3d4c1e224c084117c22a5db78_8", "text": "URLs, Emojis and Emoticons were removed, while usernames and user mentions were filtered as @USER following the same methodology described in OLID (Zampieri et al., 2019a) ."}
{"sent_id": "3e2fb3d4c1e224c084117c22a5db78-C001-115", "intents": ["@USE@"], "paper_id": "ABC_3e2fb3d4c1e224c084117c22a5db78_8", "text": "We used the same guidelines used in the annotation of the English OLID dataset (Zampieri et al., 2019a) ."}
{"sent_id": "3e2fb3d4c1e224c084117c22a5db78-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_3e2fb3d4c1e224c084117c22a5db78_8", "text": "The most recent project expanded on existing ideas for defining offensive language and presented the OLID (Offensive Language Identification Dataset), a corpus of Twitter posts hierarchically annotated on three levels, whether they contain offensive language or not, whether the offense is targeted and finally, the target of the offense (Zampieri et al., 2019a) ."}
{"sent_id": "91c82c4a49815fb2de300d99312754-C001-59", "intents": ["@USE@"], "paper_id": "ABC_91c82c4a49815fb2de300d99312754_8", "text": "We also incorporate additional temporal information and speaker information into dialog utterances as (Madotto et al., 2018) and adopt a (subject, relation, object) representation of KB information as (Eric and Manning, 2017b) ."}
{"sent_id": "91c82c4a49815fb2de300d99312754-C001-82", "intents": ["@USE@"], "paper_id": "ABC_91c82c4a49815fb2de300d99312754_8", "text": "We use Per-response/dialog Accuracy (Bordes et al., 2017) , BLEU (Papineni et al., 2002) and Entity F1 (Madotto et al., 2018) to compare the performance of different models."}
{"sent_id": "91c82c4a49815fb2de300d99312754-C001-106", "intents": ["@USE@"], "paper_id": "ABC_91c82c4a49815fb2de300d99312754_8", "text": "First we randomly select 100 samples from the DSTC2 test set, then generate the corresponding responses using WMM2Seq and Mem2Seq, and finally ask two human subjects to judge the quality of the generated responses according to the appropriateness and humanlikeness on a scale from 1 to 5."}
{"sent_id": "91c82c4a49815fb2de300d99312754-C001-95", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_91c82c4a49815fb2de300d99312754_8", "text": "We can safely conclude that the separation of context memory and KB memory benefits the performance, as WMM2Seq performs well with less parameters than Mem2Seq on task 5."}
{"sent_id": "91c82c4a49815fb2de300d99312754-C001-94", "intents": ["@DIF@"], "paper_id": "ABC_91c82c4a49815fb2de300d99312754_8", "text": "Second, we find that WMM2Seq outperforms Mem2Seq, which uses a unified memory to store dialog history and KB information."}
{"sent_id": "91c82c4a49815fb2de300d99312754-C001-107", "intents": ["@DIF@"], "paper_id": "ABC_91c82c4a49815fb2de300d99312754_8", "text": "As shown in Table 4 , WMM2Seq outperforms Mem2Seq in both measures, which is coherent to the automatic evaluation."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-31", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "For each disorder, Cohan et al. (2018) analyze the differences in language use between diagnosed users and their respective control groups."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-32", "intents": ["@USE@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "They also provide benchmark results for the binary classification task of predicting whether the user belongs to the diagnosed or the control group."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-33", "intents": ["@USE@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "We reproduce their baseline models for each disorder and compare to our deep learning-based model, explained in Section 2.3."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-63", "intents": ["@USE@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "We implement the baselines as in Cohan et al. (2018) ."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "The SMHD dataset (Cohan et al., 2018) is a largescale dataset of Reddit posts from users with one or multiple mental health conditions."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-95", "intents": ["@BACK@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "In the categories Affective processes, Social processes, and Biological processes, Cohan et al. (2018) report significant differences between depressed and control group, similar to some other disorders."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-40", "intents": ["@MOT@", "@BACK@", "@USE@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "The approach in (Barzilay and McKeown, 2001) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-115", "intents": ["@MOT@", "@USE@", "@EXT@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "As described in Sec. 2, BL utilizes a threshold to control the number of patterns mined."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "They can also be obtained from parallel corpora if such data is available (Barzilay and McKeown, 2001; Ibrahim et al., 2003) ."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-42", "intents": ["@USE@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "The following provides a summary of their technique."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-93", "intents": ["@USE@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "This set of chunk pairs are later fed to the method in (Barzilay and McKeown, 2001 ) to produce a set of patterns with affixed scores."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-121", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "In our approach, we first form chunk pairs from the 5,935 pairs of parallel sentences and then use the baseline approach at a low threshold to ob-tain patterns."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-67", "intents": ["@EXT@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "Global Context-Based Scoring Our contextbased paraphrase scoring method is an extension of (Barzilay and McKeown, 2001 ) described in Sec. 2."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-132", "intents": ["@DIF@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "From the figure we can see that our holistic approach using global-context score to rank and co-occurrence score to filter (i.e., Rk-S g +Ft-S c ) has higher precision than the baseline approach (i.e., BL) in all ks."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-134", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "Interestingly, the graph shows that using only one of the scores alone (i.e., Rk-S g and Rk-S c ) does not result in a significantly higher precision than the baseline approach."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-23", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "Table 3 in (Dreyer and Marcu, 2012) ) have shown that permutations have only very little impact while significantly increasing the computational complexity of HyTER computation."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-10", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "The HyTER metric (Dreyer and Marcu, 2012 ) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-12", "intents": ["@MOT@", "@USE@", "@SIM@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-15", "intents": ["@USE@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "Furthermore, we generate lattices for standard datasets from a recent WMT Metrics Shared Task and perform the first evaluation of HyTER on large and noisier datasets."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-51", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "We first use the setting of Dreyer and Marcu (2012) , in Section § 5.1, to compare the score estimated by HyTER and HyTERA to hTER scores."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-72", "intents": ["@USE@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "Note that the tested systems were selected by NIST to cover a variety of system architectures (statistical, rule-based, hybrid) and performances (Dreyer and Marcu, 2012) , which makes distinction between them an easy task for all metrics."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-66", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "9 In all cases, there is a high correlation between HyTER, HyTERA and hTER, significantly higher than the correlation between BLEU and hTER."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "The HyTER metric (Dreyer and Marcu, 2012) computes the similarity between a translation hypothesis and a reference lattice that compactly encodes millions of meaning-equivalent translations."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-25", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "The HyTER metric has already been successfully used in MT evaluation but only with handcrafted lattices."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "Dreyer and Marcu (2012) show that it can be closely approximated by HyTER scores."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-71", "intents": ["@SIM@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "All metrics rank the systems in the same order, except from HyTER with allParsFiltered that only inverts two systems."}
{"sent_id": "c6bae8dbdb66092865945e776148e6-C001-18", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_c6bae8dbdb66092865945e776148e6_8", "text": "The overall supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors, and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural (similar to the classifier setup of Bowman et al. (2015) and Conneau et al. (2017) )."}
{"sent_id": "c6bae8dbdb66092865945e776148e6-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_c6bae8dbdb66092865945e776148e6_8", "text": "Figure 1 shows the overview of our encoding model (the standard classifier setup is not shown here; see Bowman et al. (2015) and Conneau et al. (2017) for that)."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-31", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "LR-Hiero with CP was introduced in (Siahbani et al., 2013) ."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-89", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "Pop limit for Hiero and LRHiero+CP is 500 and beam size LR-Hiero is 500."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-17", "intents": ["@USE@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "Although, LR-Hiero performs much faster than Hiero in decoding and obtains BLEU scores comparable to phrase-based translation system on some language pairs, there is still a notable gap between CKY-Hiero and LR-Hiero (Siahbani et al., 2013) ."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-91", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "To make the results comparable we use the same feature set for all baselines, Hiero as well (including new features proposed by (Siahbani et al., 2013) )."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-108", "intents": ["@USE@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "As we can see in this figure, adding new modified rules slightly increases the number of language model queries on Cs-En and De-En so that LR-Hiero+CP still works 2 to 3 times faster than Hiero."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-101", "intents": ["@DIF@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "We can see that for all language pairs (ab) constantly improves performance of LRHiero, significantly better than LR-Hiero+CP and LR-Hiero (p-value<0.05) on Cs-En and Zh-En, evaluated by MultEval (Clark et al., 2011) ."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-103", "intents": ["@DIF@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "Row 4 is the same translation system as row 3 (LR-Hiero+CP)."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-109", "intents": ["@DIF@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "On Zh-En, LR-Hiero+CP applies queue diversity (QD=15) which reduces search errors and improves translation quality but increases the number of hypothesis generation as well."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by Naseem et al. (2012) for multisource cross-lingual transfer."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "To account for this issue, Naseem et al. (2012) recently introduced a novel generative model of dependency parsing, in which the generative process is factored into separate steps for the selection of dependents and their ordering."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-228", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "Naseem et al. observed an increase from 59.3% to 60.4% on average by adding unlabeled target language data and the gains were not consistent across languages."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-20", "intents": ["@DIF@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "The resulting parser outperforms the method of Naseem et al. (2012) on 12 out of 16 evaluated languages."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-77", "intents": ["@DIF@", "@USE@", "@EXT@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "The first baseline, NBG, is the generative model with selective parameter sharing from Naseem et al. (2012) ."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-85", "intents": ["@DIF@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "that diverge from the Indo-European majority family, the selective sharing model, NBG, achieves substantially higher accuracies."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-137", "intents": ["@DIF@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "Furthermore, Family achieves a 7% relative error reduction over the NBG baseline and outperforms it on 12 of the 16 languages."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-231", "intents": ["@DIF@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "The relative error reduction with respect to the base Family model is 9% on average, while the average reduction with respect to NBG+EM is 13%."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-246", "intents": ["@DIF@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "On average, our best model provides a relative error reduction of 13% over the state-ofthe-art model of Naseem et al. (2012) , outperforming it on 15 out of 16 evaluated languages."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-62", "intents": ["@USE@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "To facilitate comparison with the state of the art, we use the same treebanks and experimental setup as Naseem et al. (2012) ."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-73", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "In line with Naseem et al. (2012), we use gold part-of-speech tags and evaluate only on sentences of length 50 or less excluding punctuation."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-212", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "As in Naseem et al. (2012) , we use the CoNLL training sets, stripped of all dependency information, as the unlabeled target language data in our experiments."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-218", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "First, NBG+EM is the generative model of Naseem et al. (2012) trained with expectation-maximization on additional unlabeled target language text."}
{"sent_id": "87af486eb2e968d2055eeab094b3f9-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_87af486eb2e968d2055eeab094b3f9_8", "text": "Very recently, [8] proposed a multi-modal encoder-decoder framework that, given an image caption, jointly predicts another caption and the features of associated image."}
{"sent_id": "87af486eb2e968d2055eeab094b3f9-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_87af486eb2e968d2055eeab094b3f9_8", "text": "There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings [8, 19] , language models [20] through multi-modal learning of vision and language."}
{"sent_id": "87af486eb2e968d2055eeab094b3f9-C001-43", "intents": ["@USE@"], "paper_id": "ABC_87af486eb2e968d2055eeab094b3f9_8", "text": "We base our model on the encoder-decoder framework introduced in [8] ."}
{"sent_id": "87af486eb2e968d2055eeab094b3f9-C001-68", "intents": ["@USE@"], "paper_id": "ABC_87af486eb2e968d2055eeab094b3f9_8", "text": "Following the experimental design of [8] , we conduct experiments on three different learning objectives: CAP2ALL, CAP2CAP, CAP2IMG."}
{"sent_id": "87af486eb2e968d2055eeab094b3f9-C001-87", "intents": ["@USE@"], "paper_id": "ABC_87af486eb2e968d2055eeab094b3f9_8", "text": "Adhering to the experimental settings of [8] , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) [33] ."}
{"sent_id": "87af486eb2e968d2055eeab094b3f9-C001-103", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_87af486eb2e968d2055eeab094b3f9_8", "text": "These findings show that visually grounding self-attended sentence representations helps to expose word-level visual features onto sentence representations [8] ."}
{"sent_id": "dcf84cf05e3e7950cabbdd8d8f304c-C001-17", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_dcf84cf05e3e7950cabbdd8d8f304c_8", "text": "One issue faced by such approaches is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them."}
{"sent_id": "dcf84cf05e3e7950cabbdd8d8f304c-C001-114", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_dcf84cf05e3e7950cabbdd8d8f304c_8", "text": "Word embedding models -such as that used in the approach to predicting compositionality of Salehi et al. (2015) -typically do not learn representations for low frequency items."}
{"sent_id": "dcf84cf05e3e7950cabbdd8d8f304c-C001-68", "intents": ["@USE@"], "paper_id": "ABC_dcf84cf05e3e7950cabbdd8d8f304c_8", "text": "The proposed model is evaluated over the same three datasets as Salehi et al. (2015) , which cover two languages (English and German) and two kinds of MWEs (noun compounds and verb-particle constructions)."}
{"sent_id": "dcf84cf05e3e7950cabbdd8d8f304c-C001-76", "intents": ["@USE@"], "paper_id": "ABC_dcf84cf05e3e7950cabbdd8d8f304c_8", "text": "We evaluate our proposed approach following Salehi et al. (2015) by computing Pearson's correlation between the predicted compositionality (i.e., from either comp 1 or comp 2 ) and human ratings for overall compositionality."}
{"sent_id": "dcf84cf05e3e7950cabbdd8d8f304c-C001-83", "intents": ["@USE@"], "paper_id": "ABC_dcf84cf05e3e7950cabbdd8d8f304c_8", "text": "For comp 1 , we set α to 0.7 for ENC and GNC following Salehi et al. (2015) ; for EVPC we set α to 0.5."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "Our previous work (Guo and Diab, 2012b ) models the sentences in the weighted matrix factorization framework ( Figure 1 )."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-32", "intents": ["@BACK@", "@MOT@", "@USE@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "Similar words pairs can be seamlessly modeled in WTMF, since in the matrix factorization framework a latent vector profile is explicitly created for each word, while in LDA all the data structures are designed for documents/sentences."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-116", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "With lexical semantics explicitly modeled, WTMF+PK yields better results than WTMF (see Table 1 )."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-123", "intents": ["@BACK@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "In (Guo and Diab, 2012b; Guo and Diab, 2012c) , we show the superiority of the latent space approach in WTMF."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-77", "intents": ["@USE@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "We build the model WTMF+PK on the same corpora as used in our previous work (Guo and Diab, 2012b) , comprising the following: Brown corpus (each sentence is treated as a document), sense definitions from Wiktionary and Wordnet (only definitions without target words and usage examples)."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-89", "intents": ["@USE@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "For WTMF, we run 20 iterations and fix the missing words weight at w m = 0.01 with a regularization coefficient set at λ = 20, which is the best condition found in (Guo and Diab, 2012b) ."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-28", "intents": ["@EXT@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "We also integrate knowledge-based semantics in the WTMF framework."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-124", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "In this paper, we improve the WTMF model and achieve state-of-the-art Pearson correlation on two standard SS datasets."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-95", "intents": ["@SIM@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "Same as in (Guo and Diab, 2012b) , LSA performance degrades dramatically when trained on a corpus of sentence sized documents, yielding results worse than the surface words baseline 31% (Agirre et al., 2012) ."}
{"sent_id": "0f5c87e5434785a612c6578244543d-C001-71", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_0f5c87e5434785a612c6578244543d_8", "text": "As for the method by Faruqui and Dyer (2014) , we used their original implementation in Python and MAT-LAB 6 , which we extended to cover cases where the dictionary contains more than one entry for the same word."}
{"sent_id": "0f5c87e5434785a612c6578244543d-C001-47", "intents": ["@DIF@"], "paper_id": "ABC_0f5c87e5434785a612c6578244543d_8", "text": "More concretely, Faruqui and Dyer (2014) use Canonical Correlation Analysis (CCA) to project the word embeddings in both languages to a shared vector space."}
{"sent_id": "0f5c87e5434785a612c6578244543d-C001-49", "intents": ["@DIF@"], "paper_id": "ABC_0f5c87e5434785a612c6578244543d_8", "text": "Therefore, the only fundamental difference between both methods is that, while our model enforces monolingual invariance, Faruqui and Dyer (2014) do change the monolingual embeddings to meet this restriction."}
{"sent_id": "0f5c87e5434785a612c6578244543d-C001-90", "intents": ["@SIM@"], "paper_id": "ABC_0f5c87e5434785a612c6578244543d_8", "text": "Following the discussion in Section 2.3, this means that our best performing configuration is conceptually very close to the method by Faruqui and Dyer (2014) , as they both coincide on maximizing the average dimension-wise covariance and length-normalize the embeddings in both languages first, the only difference being that our model enforces monolingual invariance after the normalization while theirs does change the monolingual embeddings to make different dimensions have the same variance and be uncorrelated among themselves."}
{"sent_id": "520437f612e678dcd4ec9c043cf701-C001-5", "intents": ["@DIF@"], "paper_id": "ABC_520437f612e678dcd4ec9c043cf701_8", "text": "Our results improve the best published result on the hotel review data (Ott et al., 2011) reaching 91.2% accuracy with 14% error reduction."}
{"sent_id": "520437f612e678dcd4ec9c043cf701-C001-54", "intents": ["@DIF@", "@BACK@", "@SIM@"], "paper_id": "ABC_520437f612e678dcd4ec9c043cf701_8", "text": "As reported in Ott et al. (2011) , bag-of-words features achieve surprisingly high performance, reaching upto 89.6% accuracy."}
{"sent_id": "520437f612e678dcd4ec9c043cf701-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_520437f612e678dcd4ec9c043cf701_8", "text": "Most are based on dictionarybased word counting using LIWC (Pennebaker et al., 2007 ) (e.g., Hancock et al. (2007) , Vrij et al. (2007) ), while some recent ones explored the use of machine learning techniques using simple lexico-syntactic patterns, such as n-grams and part-of-speech (POS) tags (Mihalcea and Strapparava (2009) , Ott et al. (2011) )."}
{"sent_id": "520437f612e678dcd4ec9c043cf701-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_520437f612e678dcd4ec9c043cf701_8", "text": "Words Previous work has shown that bag-ofwords are effective in detecting domain-specific deception (Ott et al., 2011; Mihalcea and Strapparava, 2009 )."}
{"sent_id": "520437f612e678dcd4ec9c043cf701-C001-88", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_520437f612e678dcd4ec9c043cf701_8", "text": "Two previous work obtained more precise gold standard labels by hiring Amazon turkers to write deceptive articles (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011) ), both of which have been examined in this study with respect to their syntactic characteristics."}
{"sent_id": "425148e63eb84bba50326e362cc5b8-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_425148e63eb84bba50326e362cc5b8_9", "text": "Latent Dirichlet Allocation (LDA) is a topic modeling technique for textual data [5] that is widely applied in software engineering [1-4, 6, 10, 11, 14-16, 19, 24, 25] for different tasks such as requirements engineering [15] , software architecture [10] , source code analysis [9] , defect reports [16] , testing [14] and to bibliometric analysis of software engineering literature [11, 22] ."}
{"sent_id": "425148e63eb84bba50326e362cc5b8-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_425148e63eb84bba50326e362cc5b8_9", "text": "Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs [1] ."}
{"sent_id": "425148e63eb84bba50326e362cc5b8-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_425148e63eb84bba50326e362cc5b8_9", "text": "They proposed using a differential evolution search algorithm to find the input parameters which maximize the topic model stability measured as the similarity of topics between multiple runs."}
{"sent_id": "425148e63eb84bba50326e362cc5b8-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_425148e63eb84bba50326e362cc5b8_9", "text": "As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability [1] , or coherence [23] ."}
{"sent_id": "425148e63eb84bba50326e362cc5b8-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_425148e63eb84bba50326e362cc5b8_9", "text": "Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model [1, 8, 12] ."}
{"sent_id": "425148e63eb84bba50326e362cc5b8-C001-142", "intents": ["@BACK@"], "paper_id": "ABC_425148e63eb84bba50326e362cc5b8_9", "text": "Past work in software engineering [1] and machine learning [12] point out that LDA instability may lead to incorrect conclusions and proposes input parameter optimization to alleviate the problem."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-3", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "At the core of the Transformer is the attention mechanism, which concurrently processes all inputs in the streams."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Instead of performing recurrence (e.g., RNN) or convolution (e.g., TCN) over the sequences, Transformer is a feed-forward model that concurrently processes the entire sequence."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "There are up to three types of attention within the full Transformer model as exemplified with neural machine translation application (Vaswani et al., 2017) : 1) Encoder self-attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Unlike recurrent computation (Sutskever et al., 2014 ) (i.e., RNNs) and temporal convolutional computation (Bai et al., 2018 ) (i.e., TCNs), Transformer's attention is an order-agnostic operation given the order in the inputs (Vaswani et al., 2017) ."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Note that f i can be the word representation (in neural machine translation (Vaswani et al., 2017) ), a pixel in a frame (in video activity recognition (Wang et al., 2018) ), or a music unit (in music generation (Huang et al., 2018b) )."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-70", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "The filtering function M (⋅, ⋅) plays as the role of the mask in decoder self-attention (Vaswani et al., 2017) ."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-81", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "In addition to modeling sequences like word sentences (Vaswani et al., 2017) or music signals (Huang et al., 2018b) , the Transformer can also be applied to images (Parmar et al., 2018) , sets , and multimodal sequences (Tsai et al., 2019a) ."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-113", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "The current Transformers consider two different value function construction: (Vaswani et al., 2017) and Sparse Transformer (Child et al., 2019) :"}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-122", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "(ii) Encoder-Decoder Attention in original Transformer (Vaswani et al., 2017) : For each query x q in decoded sequence, M (x q , S x k ) = S x k contains the keys being all the tokens in the encoded sequence."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-126", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Since the decoded sequence is the output for previous timestep, the query at position i can only observe the keys being the tokens that are decoded with position < i. For convenience, let us define S 1 as the set returned by original Transformer (Vaswani et al., 2017 ) from M (x q , S x k ), which we will use it later."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-241", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Other than relating Transformer's attention mechanism with kernel methods, the prior work (Wang et al., 2018; Shaw et al., 2018; Tsai et al., 2019b ) related the attention mechanism with graph-structured learning."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-51", "intents": ["@USE@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Followed the definition by Vaswani et al. (2017) , we use queries(q)/keys(k)/values(v) to represent the inputs for the attention."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-141", "intents": ["@USE@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Note that t i here is chosen as the mixture of sine and cosine functions as in the prior work (Vaswani et al., 2017; Ott et al., 2019) ."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-248", "intents": ["@DIF@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "In this paper, we presented a kernel formulation for the attention mechanism in Transformer, which allows us to define a larger space for designing attention."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur-ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; , which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999) ."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "They filter out unlikely lexical entries just to help parsing (Clark and Curran, 2004a) , or the probabilistic models for phrase structures were trained independently of the supertagger's probabilistic models (Wang and Harper, 2004; Ninomiya et al., 2006) ."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "The probabilities of their model are defined as the product of probabilities of supertagging and probabilities of the probabilistic model for phrase structures, but their model was trained independently of supertagging probabilities, i.e., the supertagging probabilities are not used for reference distributions."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-146", "intents": ["@BACK@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "On the other hand, Ninomiya et al. (2006)'s model 3 uses the supertagger as an external module."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-74", "intents": ["@SIM@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "The probabilistic model of lexical entry selection and its feature templates are the same as defined in Ninomiya et al. (2006) 's model 1."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-75", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "The formula of our model is the same as Ninomiya et al. (2006)'s model 3. But, their model is not a probabilistic model with a reference distribution."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-160", "intents": ["@DIF@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "Though our model was not as fast as Ninomiya et al. (2006) 's models, it achieved the highest accuracy among them."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-162", "intents": ["@DIF@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "When we sacrifice parsing speed, our model achieved around 2.9 points higher F-score than 's model and around 0.8 points higher F-score than Ninomiya et al. (2006) 's model 3."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "Among these strategies, the confidence measure used to combine the results of CT and DS is a straight-forward one, which is introduced in (Zhang et al., 2006a) ."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-71", "intents": ["@BACK@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "In (Zhang et al., 2006a) , the above CT method is developed as subword-based tagging."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-129", "intents": ["@BACK@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "Table 5 illustrates that, after combining the two results, most original errors on IV words are corrected because DS can achieve higher IV recall as described in Zhang\"s paper. But on OOV part, more new errors are introduced by CM and these new errors decrease the precision of the IV words."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-66", "intents": ["@USE@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "Here, the term \"dictionary-based\" is exactly the method implemented in (Zhang et al., 2006a) , it does not mean the generative language model in general."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-166", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "Table 6 Results of different approach used in our experiments (White background lines are the results we repeat Zhang\"s methods and they have some trivial difference with Table 1. ) Therefore, the most important thing worth to pay attention in future study is how to integrate linguistic information into the statistical model effectively, no matter character or word information."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-93", "intents": ["@DIF@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "Based on IV and OOV recall as we show in Table 1 , Zhang argues that the DS performs better on IV word identification while CT performs better on OOV words. But we can see from the results in Table 6 (the lines about DS and CT), the IV precision of DS approach is much lower than that of CT on all the four corpora, which also causes a lower F measure of IV."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-148", "intents": ["@SIM@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "We can see from the Table 5 columns about EIV, there are more errors eliminated than the new errors introduced after EIV condition added into CM and most CT tags of subwords contained in OOV words maintained unchanged as we supposed. And then, our results (in Table  6 lines about EIV) are comparable with that in Zhang\"s paper."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014) , image captioning (Vinyals et al., 2015) , video description (Venugopalan et al., 2015) , and headline generation (Rush et al., 2015) ."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "Figure 1 illustrates the model structure of ABS."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "For ABS+AMR, we used the two-step training scheme to accelerate the training speed."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-108", "intents": ["@BACK@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline ABS were fixed and unchanged to prevent overfitting."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-141", "intents": ["@BACK@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of Rush et al. (2015) : the combination of the feed-forward neural network language model and attention-based sentence encoder. also adapted the RNN encoder-decoder with attention for headline generation tasks."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-15", "intents": ["@EXT@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "The method is essentially an extension of attention-based summarization (ABS) (Rush et al., 2015) ."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-87", "intents": ["@USE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "To demonstrate the effectiveness of our proposed method, we conducted experiments on benchmark data of the abstractive headline generation task described in Rush et al. (2015) ."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-93", "intents": ["@USE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated Gigaword corpus as well as training data 5 ."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-96", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "For evaluation on Gigaword, we forced the system outputs to be at most 8 words as in Rush et al. (2015) since the average length of headline in Gigaword is 8.3 words."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-125", "intents": ["@USE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "Gigaword test data provided by Rush et al. (2015) is already pre-processed."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-111", "intents": ["@DIF@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "We can see that the proposed method, ABS+AMR, outperforms the baseline ABS on all datasets."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-112", "intents": ["@DIF@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "In particular, ABS+AMR achieved statistically significant gain from ABS (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004."}
{"sent_id": "0a93feafef3ba2d4bb5360ff215171-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_0a93feafef3ba2d4bb5360ff215171_9", "text": "In recent years, a number of VQA datasets have been proposed: VQA 1.0 [4] , VQA-abstract [1] , VQA 2.0 [47, 14] , FM-IQA [13] , DAQUAR [24] , COCO-QA [30] , Visual Madlibs [46] , Visual Genome [20] , VizWiz [16] , Visual7W [48] , TDIUC [18] , CLEVR [17] , SHAPES [3] , Visual Reasoning [34] , Embodied QA [7] . What all these resources have in common is the task for which they were designed: Given an image (either real or abstract) and a question in natural language, models are asked to correctly answer the question."}
{"sent_id": "0a93feafef3ba2d4bb5360ff215171-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_0a93feafef3ba2d4bb5360ff215171_9", "text": "Moreover, it only works with rigid semantic concepts, making it not suitable for phrasal or sentence answers that can be found in [4, 1, 16, 47, 14] ."}
{"sent_id": "0a93feafef3ba2d4bb5360ff215171-C001-160", "intents": ["@USE@"], "paper_id": "ABC_0a93feafef3ba2d4bb5360ff215171_9", "text": "We tested the validity of our metric by experimenting with four VQA datasets: VQA 1.0 [4] , VQA 2.0 [14] , VQA-abstract [1] , and VizWiz [16] ."}
{"sent_id": "0a93feafef3ba2d4bb5360ff215171-C001-161", "intents": ["@USE@"], "paper_id": "ABC_0a93feafef3ba2d4bb5360ff215171_9", "text": "To enable a fair comparison across the datasets, for each dataset we followed the same pipeline: The standard VQA model used in [1] was trained on the training split and tested on the validation split."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Of the two state-of-the-art approaches on dialog act recognition, one uses a deep stack of Recurrent Neural Networks (RNNs) (Schmidhuber, 1990) to capture long distance relations between tokens (Khanpour et al., 2016) , while the other uses multiple parallel temporal Convolutional Neural Networks (CNNs) (Fukushima, 1980) to capture relevant functional patterns with different length (Liu et al., 2017) ."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Concerning information from the surrounding segments, its influence, especially that of preceding segments, has been thoroughly explored in at least two studies (Ribeiro et al., 2015; Liu et al., 2017) ."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-99", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Liu et al. (2017) used 200-dimensional Word2Vec embeddings trained on Facebook data."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-102", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Additionally, Liu et al. (2017) explored the use of context information concerning speaker changes and from the surrounding segments."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-111", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Using the setup with gold standard labels from three preceding segments, Liu et al. (2017) achieved 79.6% and 81.8% on the two sets used to evaluate the approach."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-181", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "As described in Section 3, the convolutional approach by Liu et al. (2017) uses a set of parallel temporal CNNs with different window size, each followed by a max pooling operation."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-248", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Liu et al. (2017) used a different dimensionality value, 200, in their study."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-378", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "for dialog act recognition is the dialog history, with influence decaying with distance (Ribeiro et al., 2015; Lee & Dernoncourt, 2016; Liu et al., 2017) ."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-389", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Liu et al. (2017) further showed that using a single label per segment is better than using the probability of each class."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-132", "intents": ["@SIM@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "This is a dense layer which maps the segment representations into a 100-dimensional space, as in the study by Liu et al. (2017) ."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-393", "intents": ["@SIM@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Liu et al. (2017) stopped at three preceding segments, but noticed a similar pattern."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-490", "intents": ["@SIM@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "However, the result differences between overlapping steps in our experiments are consistent with those described in their paper."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-465", "intents": ["@EXT@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "We used adaptations of the approaches with top performance in previous studies, namely the RNN-based approach by Khanpour et al. (2016) and the CNN-based approach by Liu et al. (2017) ."}
{"sent_id": "e803782890224294066ce447671981-C001-22", "intents": ["@DIF@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "The evaluation of our algorithm on data automatically derived from the Penn Treebank shows an increase in both precision and recall in recovery of non-local dependencies by approximately 10% over the results reported in (Johnson, 2002) ."}
{"sent_id": "e803782890224294066ce447671981-C001-101", "intents": ["@DIF@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "The two algorithms are designed for slightly different purposes: while Johnson's approach allows one to recover free empty nodes (without antecedents), we look for nonlocal dependencies, which corresponds to identification of co-indexed empty nodes (note, however, the modifications we describe in Section 2, when we actually transform free empty nodes into co-indexed empty nodes)."}
{"sent_id": "e803782890224294066ce447671981-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "More specifically, Johnson (2002) describes a pattern-matching algorithm for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recovering non-local dependencies."}
{"sent_id": "e803782890224294066ce447671981-C001-121", "intents": ["@BACK@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "Obviously, because of parsing errors the performance drops significantly: e.g., in the experiments reported in (Johnson, 2002 ) the overall fscore decreases from 0.75 to 0.68 when evaluating on parser output (see Table 4 )."}
{"sent_id": "e803782890224294066ce447671981-C001-44", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "As in (Johnson, 2002) , our patterns are minimal connected fragments containing both nodes involved in a non-local dependency."}
{"sent_id": "e803782890224294066ce447671981-C001-89", "intents": ["@SIM@", "@UNSURE@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "In order to compare our results to the results presented in (Johnson, 2002) , we measured the overall performance of the algorithm across patterns and non-local dependency labels."}
{"sent_id": "e803782890224294066ce447671981-C001-92", "intents": ["@SIM@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "This corresponds to rows 2, 3 and 4 of Table 4 in (Johnson, 2002) ."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "In the meanwhile, two large-scale movie description datasets have been proposed, namely MPII Movie Description (MPII-MD) [28] and Montreal Video Annotation Dataset (M-VAD) [31] ."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "Recently two large-scale movie description datasets have been proposed, MPII Movie Description (MPII-MD) [28] and Montreal Video Annotation Dataset (M-VAD) [31] ."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-22", "intents": ["@DIF@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "This outperforms related work on the MPII-MD dataset, both using automatic and human evaluation; c) we perform a detailed analysis of prior work and our approach to understand the challenges of the movie description task."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-173", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank three systems (the best SMT of [28] , S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to [28] ."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-184", "intents": ["@DIF@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "An interesting characteristic is the output vocabulary size, which is 94 for [28] , 86 for [33] and 605 for our method, while the test set contains 6422 unique words."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-82", "intents": ["@USE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "In order to find the verbs among the labels we rely on the semantic parser of [28] ."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-110", "intents": ["@USE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "In this section we first analyze our approach on the MPII-MD [28] dataset and explore different design choices."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-118", "intents": ["@USE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "We use the visual features (DT, LSDA, PLACES) provided with the MPII-MD dataset [28] ."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-171", "intents": ["@USE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "We compare the best method of [28] , the recently proposed method S2VT [33] and our proposed \"Visual Labels\"-LSTM on the test set of the MPII-MD dataset (6,578 clips)."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-241", "intents": ["@USE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "We obtain the sense information from the semantic parser of [28] , thus senses might be noisy."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-11", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "In this work we adopt the step-by-step framework of Moryossef et al. (2019) and propose four independent extensions that improve aspects of our original system: we suggest a new plan generation mechanism, based on a trainable-yetverifiable neural decoder, that is orders of magnitude faster than the original one ( §3); we use knowledge of the plan structure to add typing information to plan elements."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-39", "intents": ["@USE@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "Each truncated DFS traversal corresponds to a sentence plan, following the DFS-to-plan procedure of Moryossef et al. (2019) : the linearized plan is generated incrementally at each step of the traversal."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-132", "intents": ["@USE@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "We repeat the coverage experiment in (Moryossef et al., 2019) , counting the number of output texts that contain all the entities in the input graph, and, of these text, counting the ones in which the entities appear in the exact same order as the plan."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-3", "intents": ["@EXT@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "We suggest four extensions to that framework: (1) we introduce a trainable neural planning component that can generate effective plans several orders of magnitude faster than the original planner; (2) we incorporate typing hints that improve the model's ability to deal with unseen relations and entities; (3) we introduce a verification-by-reranking stage that substantially improves the faithfulness of the resulting texts; (4) we incorporate a simple but effective referring expression generation module."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "The data-to-plan component in Moryossef et al. (2019) exhaustively generates all possible plans, scores them using a heuristic, and chooses the highest scoring one for realization."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-70", "intents": ["@BACK@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "While the clear mapping between plans and text helps to reduce these issues greatly, the system in Moryossef et al. (2019) still has 2% errors of these kinds."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-57", "intents": ["@DIF@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "Speed On a 7 edges graph, the planner of Moryossef et al. (2019) takes an average of 250 seconds to generate a plan, while our planner takes 0.0025 seconds, 5 orders of magnitude faster."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-33", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "In Module 1, we encode all of the questions, also by Skip-Thought Vectors, from the training and validation sets of VQA (Antol et al. 2015) dataset as a 4800 by 186027 dimension basic question (BQ) matrix, and then solve the LASSO optimization problem (Huang, Alfadly, and Ghanem 2017) , with MQ, to find the top 3 similar BQ of MQ."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-101", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "At the beginning, we take all of the training and validation questions from the VQA dataset (Antol et al. 2015) to be our basic question candidates."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-128", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "According to the above subsections, Question Encoding and Problem Formulation, we can encode all basic question candidates from the training and validation question sets of VQA dataset (Antol et al. 2015) by Skip-Thought Vectors, and then we have a matrix of basic question candidates."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-136", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "We propose a novel large scale dataset, called Basic (Antol et al. 2015) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and VQA dataset and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and VQA dataset."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-140", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "Furthermore, we exploit the BQD to do robustness analysis of the 6 available pretrained state-of-the-art VQA models (Antol et al. 2015; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) in the next subsection."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-152", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "First, we measure the accuracy of the model on the clean VQA dataset (Antol et al. 2015) and we call it Acc vqa ."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-164", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "We conduct our experiments on BQD and VQA (Antol et al. 2015) dataset."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-165", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "VQA dataset is based on the MS COCO Table 4 : MUTAN with Attention model evaluation results on BQD and VQA dataset (Antol et al. 2015) ."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-180", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "VQA dataset provides multiple-choice and open-ended task for evaluation."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-215", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "We want to do more advanced analysis on this model, so we claim that if the quality of Figure 3 : The accuracy decrement of state-of-the-art VQA models evaluated on BQD and VQA dataset (Antol et al. 2015) ."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-250", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "Furthermore, we can use the proposed BQD, R score and VQA dataset (Antol et al. 2015) to measure the robustness of VQA models."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "Recently, there are many papers (Antol et al. 2015; Shih, Singh, and Hoiem 2016; Chen et al. 2016; Kafle and Kanan 2016; Ma, Lu, and Li 2016; Ren, Kiros, and Zemel 2015; Zhu et al. 2016; Wu et al. 2016; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) have proposed methods to solve the challenging VQA task."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-170", "intents": ["@BACK@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "In the VQA dataset, each question is associated with 10 answers annotated by different people from Amazon Mechanical Turk (AMT)."}
{"sent_id": "40d73d5fc22686c13a14946946dd18-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_40d73d5fc22686c13a14946946dd18_9", "text": "Beyond the existing state-of-the-art models (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018 ), we exploit character-level modeling, beneficial when considering multiple languages."}
{"sent_id": "40d73d5fc22686c13a14946946dd18-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_40d73d5fc22686c13a14946946dd18_9", "text": "Phase II: As core sequence representation component, users can choose between a self-attention encoding (Tan et al., 2018) , a regular Bi-LSTM (Hochreiter and Schmidhuber, 1997) or a highway Bi-LSTM (Zhang et al., 2016; He et al., 2017) ."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "Prabhumoye et al. (2018) introduces the technique of back-translation to perform style transfer."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "They also use feedback from a pre-trained classifier to guide the generators to generate the desired style."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-101", "intents": ["@BACK@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "As reported by them, the BST model performs better in preservation of meaning for the tasks of gender and political slant transfer."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-75", "intents": ["@USE@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "We use three tasks described in (Prabhumoye et al., 2018) to evaluate our models."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-97", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "We reuse the instructions provided by (Prabhumoye et al., 2018) for the three tasks. But unlike (Prabhumoye et al., 2018), we perform our evaluation on Amazon Mechanical Turk."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-121", "intents": ["@DIF@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "The over-all averaged scores for the two models MBST and MBST+F is the same 3.08, whereas it is much lower 2.79 for BST and 2.57 for CAE."}
{"sent_id": "b71da01fb46900d81162b3a3c3cd41-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_b71da01fb46900d81162b3a3c3cd41_9", "text": "This idea is implemented in the transition system proposed by Nivre (2009) ."}
{"sent_id": "b71da01fb46900d81162b3a3c3cd41-C001-59", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_b71da01fb46900d81162b3a3c3cd41_9", "text": "Our experiments are based on the same five data sets as Nivre (2009) ."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "The word order between source and target languages significantly influences the translation quality in statistical machine translation (SMT) (Tillmann, 2004; Hayashi et al., 2013; Nakagawa, 2015) ."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "In particular, preordering effectively improves the translation quality because it solves long-distance reordering and computational complexity issues (Jehl et al., 2014; Nakagawa, 2015) ."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "Neubig et al. (2012) and Nakagawa (2015) proposed methods that construct a binary tree and reordering simultaneously from a source sentence."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-28", "intents": ["@SIM@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "The results confirm that the proposed method achieves comparable translation quality to the state-of-the-art preordering method (Nakagawa, 2015) that requires a manual feature design."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-130", "intents": ["@SIM@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "The experiments confirmed that the proposed method achieved a translation quality comparable to the state-of-the-art preordering method that requires a manual feature design."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-65", "intents": ["@USE@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "5 Source-totarget and target-to-source word alignments were calculated using IBM model 1 and hidden Markov model, and they were combined with the intersection heuristic following (Nakagawa, 2015) ."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-107", "intents": ["@USE@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "Compared to the plain PBSMT without preordering, both BLEU and RIBES increased significantly with preordering by RvNN and BTG."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-116", "intents": ["@DIF@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "Furthermore, the ratio of high Kendall's τ by RvNN is more than that of BTG, implying that preordering by RvNN is better than that by BTG."}
{"sent_id": "9b203bfa690c4a79c1324360a4b8dc-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_9b203bfa690c4a79c1324360a4b8dc_11", "text": "Recent attempts to solve this task deal with proposing similarity measures based on distributional semantic models (Roller et al., 2014; Weeds et al., 2014; Santus et al., 2016; Shwartz et al., 2017; Roller and Erk, 2016) ."}
{"sent_id": "9b203bfa690c4a79c1324360a4b8dc-C001-84", "intents": ["@USE@"], "paper_id": "ABC_9b203bfa690c4a79c1324360a4b8dc_11", "text": "Following the same experimental setup as (Santus et al., 2016) , we report percentage F1 scores on a ten-fold cross validation for binary classification of co-hyponyms vs random pairs, as well as co-hyponyms vs. hypernyms using both SVM and Random Forest classifiers."}
{"sent_id": "3128481fa4e5d2c4af7deba2c28950-C001-23", "intents": ["@USE@"], "paper_id": "ABC_3128481fa4e5d2c4af7deba2c28950_11", "text": "In this study we apply the methods of Foltz et al. (1998) , Hearst (1994 Hearst ( , 1997 , and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue."}
{"sent_id": "3128481fa4e5d2c4af7deba2c28950-C001-142", "intents": ["@USE@"], "paper_id": "ABC_3128481fa4e5d2c4af7deba2c28950_11", "text": "This combination matches Hearst (1994) 's heuristic of choosing the window size to be the average paragraph length."}
{"sent_id": "3128481fa4e5d2c4af7deba2c28950-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_3128481fa4e5d2c4af7deba2c28950_11", "text": "Both Hearst (1994 Hearst ( , 1997 and Foltz et al. (1998) use vector space methods discussed below to represent and compare units of text."}
{"sent_id": "3128481fa4e5d2c4af7deba2c28950-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_3128481fa4e5d2c4af7deba2c28950_11", "text": "The text unit's definition in Hearst (1994 Hearst ( , 1997 and Foltz et al. (1998) is generally task dependent, depending on what size gives the best results."}
{"sent_id": "1c1b524d2bfe00c62a5a2e1a05ffc7-C001-3", "intents": ["@BACK@"], "paper_id": "ABC_1c1b524d2bfe00c62a5a2e1a05ffc7_11", "text": "Vaswani et al. (2017) propose a new architecture that avoids recurrence and convolution completely."}
{"sent_id": "1c1b524d2bfe00c62a5a2e1a05ffc7-C001-35", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_1c1b524d2bfe00c62a5a2e1a05ffc7_11", "text": "The Transformer network (Vaswani et al., 2017) avoids the recurrence completely and uses only self-attention."}
{"sent_id": "1c1b524d2bfe00c62a5a2e1a05ffc7-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_1c1b524d2bfe00c62a5a2e1a05ffc7_11", "text": "For the sake of brevity, we refer the reader to Vaswani et al. (2017) for additional details regarding the architecture."}
{"sent_id": "1c1b524d2bfe00c62a5a2e1a05ffc7-C001-150", "intents": ["@USE@"], "paper_id": "ABC_1c1b524d2bfe00c62a5a2e1a05ffc7_11", "text": "Transformer (small) (Vaswani et al., 2017) 27.3 38.1 Weighted Transformer (small) 28.4 38.9"}
{"sent_id": "1c1b524d2bfe00c62a5a2e1a05ffc7-C001-153", "intents": ["@DIF@"], "paper_id": "ABC_1c1b524d2bfe00c62a5a2e1a05ffc7_11", "text": "Our proposed model outperforms the state-of-the-art models including the Transformer (Vaswani et al., 2017) ."}
{"sent_id": "79e96060492c3978dc5a7a0d5f293f-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_79e96060492c3978dc5a7a0d5f293f_11", "text": "One of the earlier successful approaches (Blitzer et al. 2006 (Blitzer et al. , 2007 involved Structural Correspondence Learning (SCL)."}
{"sent_id": "79e96060492c3978dc5a7a0d5f293f-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_79e96060492c3978dc5a7a0d5f293f_11", "text": "The feature groups are Chen et al. (2011) use a specific co-training algorithm for domain adaptation on the Blitzer et al. (2007) data set."}
{"sent_id": "79e96060492c3978dc5a7a0d5f293f-C001-115", "intents": ["@BACK@"], "paper_id": "ABC_79e96060492c3978dc5a7a0d5f293f_11", "text": "Blitzer et al. (2007) employ the Structural Correspondence Learning (SCL) algorithm for sentiment domain adaptation."}
{"sent_id": "79e96060492c3978dc5a7a0d5f293f-C001-78", "intents": ["@USE@"], "paper_id": "ABC_79e96060492c3978dc5a7a0d5f293f_11", "text": "4. We also compared the results of approaches 1 and 2 to published results on Structural Correspondence Learning (SCL) by using the same datasets as in Blitzer et al. (2007) ."}
{"sent_id": "79e96060492c3978dc5a7a0d5f293f-C001-160", "intents": ["@USE@"], "paper_id": "ABC_79e96060492c3978dc5a7a0d5f293f_11", "text": "We employed the four domains datasets used in Blitzer et al. (2007) to train and test the all-in one and the ensemble classifiers."}
{"sent_id": "79e96060492c3978dc5a7a0d5f293f-C001-121", "intents": ["@SIM@"], "paper_id": "ABC_79e96060492c3978dc5a7a0d5f293f_11", "text": "The baseline in Blitzer et al. (2007) is a linear classifier trained without adaptation, while their ceiling reference is the same as ours, which is the in-domain classifier trained and tested on the same domain."}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-3", "intents": ["@EXT@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "We introduce a very simple change to the loss function used in the original formulation by Kiros et al. (2014) , which leads to drastic improvements in the retrieval performance."}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-47", "intents": ["@EXT@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "Our work builds on Visual-Semantic Embeddings Kiros et al. (2014) ."}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-10", "intents": ["@DIF@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "On Flickr30K, we more than double R@1 as reported by Kiros et al. (2014) in both image and caption retrieval, and achieve near state-of-the-art performance."}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-137", "intents": ["@DIF@", "@BACK@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "We further note that in Kiros et al. (2014) , the caption embedding is normalized, while the image embedding is not."}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-227", "intents": ["@DIF@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "We have shown that a new loss function that uses only violation incurred by hard negatives drastically improves performance over the typical loss that sums the violations across the negatives, typically used in previous work (Kiros et al. (2014) ; Vendrov et al. (2015) )."}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "Works such as Kiros et al. (2014) , Karpathy & Fei-Fei (2015) , Zhu et al. (2015) , Socher et al. (2014) use a rank loss to learn the joint visual-semantic embedding."}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-18", "intents": ["@USE@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "This paper investigates the visual-semantic embeddings (VSE) of Kiros et al. (2014) for imagecaption retrieval."}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-123", "intents": ["@USE@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "We perform experiments with our VSE++ and compare it to the original formulation of Kiros et al. (2014) (referred to as VSE), as well as state-of-the-art approaches."}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-134", "intents": ["@SIM@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "For the caption encoder, we use a GRU similar to the one used in Kiros et al. (2014) ."}
{"sent_id": "4a90cd18be0df0c41a94febe2f68ef-C001-48", "intents": ["@USE@"], "paper_id": "ABC_4a90cd18be0df0c41a94febe2f68ef_11", "text": "Following Dua et al. (2019) , we first predict the answer type of a given passage-question pair, and then adopt individual prediction strategies."}
{"sent_id": "4a90cd18be0df0c41a94febe2f68ef-C001-73", "intents": ["@USE@"], "paper_id": "ABC_4a90cd18be0df0c41a94febe2f68ef_11", "text": "Answer type prediction Inspired by the Augmented QANet model (Dua et al., 2019) , we use the contextualized token representations from the last four blocks (H L−3 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively."}
{"sent_id": "4a90cd18be0df0c41a94febe2f68ef-C001-144", "intents": ["@USE@"], "paper_id": "ABC_4a90cd18be0df0c41a94febe2f68ef_11", "text": "Dataset We consider the reading comprehension benchmark that requires Discrete Reasoning Over Paragraphs (DROP) (Dua et al., 2019) prehensive understanding of the context as well as the ability of numerical reasoning are required."}
{"sent_id": "4a90cd18be0df0c41a94febe2f68ef-C001-225", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_4a90cd18be0df0c41a94febe2f68ef_11", "text": "Recently, Dua et al. (2019) released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers."}
{"sent_id": "4a90cd18be0df0c41a94febe2f68ef-C001-18", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_4a90cd18be0df0c41a94febe2f68ef_11", "text": "First, to produce various answer types, Dua et al. (2019) extend previous one-type answer prediction (Seo et al., 2017) to multi-type prediction that supports span extraction, counting, and addition/subtraction."}
{"sent_id": "4a90cd18be0df0c41a94febe2f68ef-C001-156", "intents": ["@SIM@"], "paper_id": "ABC_4a90cd18be0df0c41a94febe2f68ef_11", "text": "Baselines Following the implementation of Augmented QANet (NAQANet) (Dua et al., 2019) , we introduce a similar baseline called Augmented BERT (NABERT)."}
{"sent_id": "fa3d20d5975ec59454abfca68f8935-C001-96", "intents": ["@BACK@"], "paper_id": "ABC_fa3d20d5975ec59454abfca68f8935_11", "text": "Chen and Bansal (2018) introduced a stop criterion in their reinforcement learning process."}
{"sent_id": "fa3d20d5975ec59454abfca68f8935-C001-29", "intents": ["@USE@"], "paper_id": "ABC_fa3d20d5975ec59454abfca68f8935_11", "text": "We see abstractive summarization in same light as several other authors (Chen and Bansal, 2018; Hsu et al., 2018; Liu et al., 2018 ) -extract salient sentences and then abstract; thus sharing similar advantages as the popular divide-and-conquer algorithm."}
{"sent_id": "fa3d20d5975ec59454abfca68f8935-C001-148", "intents": ["@USE@", "@UNSURE@"], "paper_id": "ABC_fa3d20d5975ec59454abfca68f8935_11", "text": "Abstractive Model R-1 R-2 R-L RL+Intra-Att (Paulus et al., 2017) 41.16 15.75 39.08 KIGN+Pred (Li et al., 2018) 38.95 17.12 35.68 FAST (Chen and Bansal, 2018) 40.88 17.80 38.54 Bottom-Up (Gehrmann et al., 2018) Grusky et al. (2018) corpus contains over 1.3M news articles together with various metadata information such as the title, summary, coverage and compression ratio."}
{"sent_id": "fa3d20d5975ec59454abfca68f8935-C001-194", "intents": ["@SIM@"], "paper_id": "ABC_fa3d20d5975ec59454abfca68f8935_11", "text": "Similar to Rush et al. (2015) ; Chen and Bansal (2018) we abstract by simplifying our extracted sentences."}
{"sent_id": "fa3d20d5975ec59454abfca68f8935-C001-188", "intents": ["@DIF@"], "paper_id": "ABC_fa3d20d5975ec59454abfca68f8935_11", "text": "Some authors have employed integer linear programming (Martins and Smith, 2009; Gillick and Favre, 2009; Boudin et al., 2015) , graph concepts (Erkan and Radev, 2004; , ranking with reinforcement learning (Narayan et al., 2018) and mostly related to our work -binary classification (Shen et al., 2007; Nallapati et al., 2017; Chen and Bansal, 2018) Our binary classification architecture differs significantly from existing models because it uses a transformer as the building block instead of a bidirectional GRU-RNN (Nallapati et al., 2017) , or bidirectional LSTM-RNN (Chen and Bansal, 2018) ."}
{"sent_id": "81bdddc7d6b04c88407537f57c0580-C001-12", "intents": ["@USE@"], "paper_id": "ABC_81bdddc7d6b04c88407537f57c0580_11", "text": "In this paper, we study the critical yet under-addressed Answer Triggering (Yang et al., 2015) problem: Given a question and a set of answer candidates, determine whether the candidate set contains any correct answer, and if so, select a correct answer as system output."}
{"sent_id": "81bdddc7d6b04c88407537f57c0580-C001-90", "intents": ["@USE@"], "paper_id": "ABC_81bdddc7d6b04c88407537f57c0580_11", "text": "We can see that GAT combined with Cnt features improves the F 1 score from Yang et al. (2015) and Jurczyk et al. (2016) by around 11.1% and 6.6% (from 32.17 and 36.65 to 43.27), which shows the effectiveness of our framework."}
{"sent_id": "81bdddc7d6b04c88407537f57c0580-C001-98", "intents": ["@USE@"], "paper_id": "ABC_81bdddc7d6b04c88407537f57c0580_11", "text": "We first test a variant of our full framework by replacing the Encoder and QA Matching component with the CNN based model from (Yang et al., 2015) 2 , denoted as GAT w/ CNN, and train it with our objective."}
{"sent_id": "81bdddc7d6b04c88407537f57c0580-C001-106", "intents": ["@USE@"], "paper_id": "ABC_81bdddc7d6b04c88407537f57c0580_11", "text": "To obtain semantic vectors of questions and candidate answers as input to the subsequent QA Matching component, we leverage Yang et al.(2015) 's released code to train the Encoder component (with CNN) through their well-tuned individual-level optimization, and use their learnt semantic vectors."}
{"sent_id": "81bdddc7d6b04c88407537f57c0580-C001-100", "intents": ["@DIF@"], "paper_id": "ABC_81bdddc7d6b04c88407537f57c0580_11", "text": "However, we leave more advanced encoder and QA matching design for future work, and anticipate that more complex CNN based models can achieve similar or better results than our current design, as in many other QA-related work (Hu et al., 2014; . (2) Compared with the best result from (Yang et al., 2015) in Table 1 , training the CNN based model end-to-end using our objective improves from 32.17% to 35.03%."}
{"sent_id": "69857bcd5ba67cb7ca0b4344a3a85f-C001-92", "intents": ["@BACK@"], "paper_id": "ABC_69857bcd5ba67cb7ca0b4344a3a85f_11", "text": "One of the most frequently used metrics for the automatic evaluation of document coherence is Kendall's (Lapata, 2003; Barzilay and Lee, 2004) ."}
{"sent_id": "69857bcd5ba67cb7ca0b4344a3a85f-C001-110", "intents": ["@USE@"], "paper_id": "ABC_69857bcd5ba67cb7ca0b4344a3a85f_11", "text": "The task on which we conduct our evaluation is information ordering (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005) ."}
{"sent_id": "69857bcd5ba67cb7ca0b4344a3a85f-C001-150", "intents": ["@USE@"], "paper_id": "ABC_69857bcd5ba67cb7ca0b4344a3a85f_11", "text": "We are able to provide this comparison based on the TAU figures reported in (Barzilay and Lee, 2004) ."}
{"sent_id": "69857bcd5ba67cb7ca0b4344a3a85f-C001-153", "intents": ["@DIF@"], "paper_id": "ABC_69857bcd5ba67cb7ca0b4344a3a85f_11", "text": "We first note that, unfortunately, we failed to accurately reproduce the model of Barzilay and Lee (2004) ."}
{"sent_id": "69857bcd5ba67cb7ca0b4344a3a85f-C001-156", "intents": ["@DIF@"], "paper_id": "ABC_69857bcd5ba67cb7ca0b4344a3a85f_11", "text": "The large difference on the EARTHQUAKES corpus between the performance of Barzilay and Lee (2004) and our reproduction of their model is responsible for the overall lower performance (0.47) of our log-linear © model and IDL-CH-HB £ V r V search algorithm, which is nevertheless higher than that of its component model CM (0.39)."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-11", "intents": ["@USE@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "The central aspects of our discussion are (a) three dependency formats: two 'classic' representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012) ; (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007) , MST (McDonald et al., 2005) and the parser of Bohnet and Nivre (2012) ; (c) two approaches to wordcategory disambiguation, e.g. exploiting common PTB tags and using supertags (i.e. specialized ERG lexical types)."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-53", "intents": ["@USE@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "In this section we give a detailed analysis of parsing into SB, CD and DT dependencies with Malt, MST and the Bohnet and Nivre (2012) parser."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-74", "intents": ["@USE@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "For the Bohnet and Nivre (2012) parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies ( Table 2 , Predicted supertags)."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-110", "intents": ["@USE@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "The error rate of Malt, MST and the Bohnet and Nivre (2012) parser for the coordination is not so high for SB and CD ( 1% and 2% correspondingly with MaltParser, PTB tags) whereas for DT the error rate on the CPOSTAGS is especially high (26% with MaltParser, PTB tags)."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-124", "intents": ["@USE@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "In this survey we gave a comparative experimental overview of (i) parsing three dependency schemes, viz., Stanford Basic (SB), CoNLL Syntactic Dependencies (CD) and DELPH-IN Syntactic Derivation Tree (DT), (ii) with three leading dependency parsers, viz., Malt, MST and the Bohnet and Nivre (2012) parser (iii) exploiting two different tagsets, viz., PTB tags and supertags."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-68", "intents": ["@DIF@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "The Bohnet and Nivre (2012) parser outperforms Malt on CD and DT and MST on SB, CD and DT with PTB tags even though it does not receive gold PTB tags during test phase but predicts them (Table 2 , Predicted PTB tags)."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-78", "intents": ["@DIF@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "The parsers have different running times: it takes minutes to run an experiment with Malt, about 2 hours with MST and up to a day with the Bohnet and Nivre (2012) parser."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-77", "intents": ["@EXT@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "For the Bohnet and Nivre (2012) parser we also observe small rise of accuracy when gold supertags are provided as a feature for prediction of PTB tags (compare Predicted PTB tags and Predicted PTB tags + gold supertags sections of Table 2 )."}
{"sent_id": "547551e556d8aa919f731da99424c9-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_547551e556d8aa919f731da99424c9_11", "text": "Some recent work has addressed this by learning general-purpose sentence representations Wieting et al., 2015; Hill et al., 2016; Conneau et al., 2017; McCann et al., 2017; Jernite et al., 2017; Nie et al., 2017; Pagliardini et al., 2017) ."}
{"sent_id": "547551e556d8aa919f731da99424c9-C001-123", "intents": ["@USE@"], "paper_id": "ABC_547551e556d8aa919f731da99424c9_11", "text": "The choice of transfer tasks and evaluation framework 3 are borrowed largely from Conneau et al. (2017) ."}
{"sent_id": "547551e556d8aa919f731da99424c9-C001-186", "intents": ["@USE@"], "paper_id": "ABC_547551e556d8aa919f731da99424c9_11", "text": "For natural language inference, the same encoder is used to encode both the premise and hypothesis and a concatenation of their representations along with the absolute difference and hadamard product (as described in Conneau et al. (2017) ) are given to a single layer MLP with a dropout (Srivastava et al., 2014 ) rate of 0.3."}
{"sent_id": "547551e556d8aa919f731da99424c9-C001-262", "intents": ["@USE@"], "paper_id": "ABC_547551e556d8aa919f731da99424c9_11", "text": "Other numbers were obtained from the evaluation suite provided by Conneau et al. (2017)"}
{"sent_id": "547551e556d8aa919f731da99424c9-C001-143", "intents": ["@DIF@"], "paper_id": "ABC_547551e556d8aa919f731da99424c9_11", "text": "Unlike Conneau et al. (2017) , who use pretrained GloVe word embeddings, we learn our word embeddings from scratch."}
{"sent_id": "28038a4fa4182ccdc6134f2138c0da-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_28038a4fa4182ccdc6134f2138c0da_11", "text": "The task of definition modeling, introduced by Noraset et al. (2017) , consists in generating the dictionary definition of a specific word: for instance, given the word \"monotreme\" as input, the system would need to produce a definition such as \"any of an order (Monotremata) of egg-laying mammals comprising the platypuses and echidnas\"."}
{"sent_id": "28038a4fa4182ccdc6134f2138c0da-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_28038a4fa4182ccdc6134f2138c0da_11", "text": "In their seminal work on definition modeling, Noraset et al. (2017) likened systems generating definitions to language models, which can naturally be used to generate arbitrary text."}
{"sent_id": "28038a4fa4182ccdc6134f2138c0da-C001-52", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_28038a4fa4182ccdc6134f2138c0da_11", "text": "This reformulation can appear contrary to the original proposal by Noraset et al. (2017) , which conceived definition modeling as a \"word-tosequence task\"."}
{"sent_id": "28038a4fa4182ccdc6134f2138c0da-C001-204", "intents": ["@DIF@"], "paper_id": "ABC_28038a4fa4182ccdc6134f2138c0da_11", "text": "As for POS-mismatches, we do note that the work of Noraset et al. (2017) had a much lower rate of 4.29%: we suggest that this may be due to the fact that they employ a learned character-level convolutional network, which arguably would be able to capture orthography and rudiments of morphology."}
{"sent_id": "28038a4fa4182ccdc6134f2138c0da-C001-92", "intents": ["@SIM@"], "paper_id": "ABC_28038a4fa4182ccdc6134f2138c0da_11", "text": "Should we mark the definiendum before encoding, then only the definiendum embedding is passed into the encoder: the resulting system provides out-of-context definitions, like in Noraset et al. (2017) where the definition is not linked to the context of a word but to its definiendum only."}
{"sent_id": "fa7475b6025d010dd6814dfb3905ef-C001-25", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_fa7475b6025d010dd6814dfb3905ef_11", "text": "Recent studies on KD [33, 15] even leverage more sophisticated model-specific distillation loss functions for better performance."}
{"sent_id": "fa7475b6025d010dd6814dfb3905ef-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_fa7475b6025d010dd6814dfb3905ef_11", "text": "Also, selecting various loss functions and balancing the weights of each loss for different tasks and datasets are always laborious [33, 28] ."}
{"sent_id": "fa7475b6025d010dd6814dfb3905ef-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_fa7475b6025d010dd6814dfb3905ef_11", "text": "However, the performance greatly relies on the design of the loss function [14, 33, 15] ."}
{"sent_id": "fa7475b6025d010dd6814dfb3905ef-C001-148", "intents": ["@USE@"], "paper_id": "ABC_fa7475b6025d010dd6814dfb3905ef_11", "text": "Formally, we define the task of compression as trying to retain as much performance as possible when compressing the officially released BERT-base (uncased) 5 to a 6-layer compact model with the same hidden size, following the settings in [28, 33, 37] ."}
{"sent_id": "fa7475b6025d010dd6814dfb3905ef-C001-154", "intents": ["@USE@"], "paper_id": "ABC_fa7475b6025d010dd6814dfb3905ef_11", "text": "Afterward, for training successor models, following [28, 33] , we use the first 6 layers of BERT-base to initialize the successor model since the over-parameterized nature of Transformer [38] could cause the model unable to converge while training on small datasets."}
{"sent_id": "fa7475b6025d010dd6814dfb3905ef-C001-153", "intents": ["@SIM@"], "paper_id": "ABC_fa7475b6025d010dd6814dfb3905ef_11", "text": "As a result, we are able to obtain a predecessor model with comparable performance with that reported in previous studies [28, 33, 15] ."}
{"sent_id": "d51bf6d22d21dcd91e080f6f0b5dcb-C001-51", "intents": ["@USE@"], "paper_id": "ABC_d51bf6d22d21dcd91e080f6f0b5dcb_11", "text": "We use a window size of 4 words based on the experiments in (Razmara et al., 2013) ."}
{"sent_id": "d51bf6d22d21dcd91e080f6f0b5dcb-C001-61", "intents": ["@USE@"], "paper_id": "ABC_d51bf6d22d21dcd91e080f6f0b5dcb_11", "text": "Thus, we use the heuristic applied in previous works (Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014) to reduce the search space."}
{"sent_id": "d51bf6d22d21dcd91e080f6f0b5dcb-C001-111", "intents": ["@USE@"], "paper_id": "ABC_d51bf6d22d21dcd91e080f6f0b5dcb_11", "text": "The MAD graph propagation generalizes the approach used in (Razmara et al., 2013) ."}
{"sent_id": "d51bf6d22d21dcd91e080f6f0b5dcb-C001-202", "intents": ["@USE@"], "paper_id": "ABC_d51bf6d22d21dcd91e080f6f0b5dcb_11", "text": "But as we wish to fairly compare our approach with Razmara et al. (2013) on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning."}
{"sent_id": "b124e65938672691a5589fb5cdb21e-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_b124e65938672691a5589fb5cdb21e_11", "text": "Some of the recent studies on this topic report performance evaluation results of different classifiers using different feature sets (Mohammad et al., 2016b ) while others present publicly-available stance-annotated data sets (Mohammad et al., 2016a; Sobhani et al., 2017; Küçük, 2017) ."}
{"sent_id": "b124e65938672691a5589fb5cdb21e-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_b124e65938672691a5589fb5cdb21e_11", "text": "Also presented in (Küçük, 2017) are the results of the following experiments on this data set:"}
{"sent_id": "b124e65938672691a5589fb5cdb21e-C001-23", "intents": ["@USE@"], "paper_id": "ABC_b124e65938672691a5589fb5cdb21e_11", "text": "We have used the publicly-available tweet data set in Turkish annotated with stance information, together with the results of the corresponding SVM classifiers using unigrams as features in (Küçük, 2017) as the baselines."}
{"sent_id": "b124e65938672691a5589fb5cdb21e-C001-56", "intents": ["@USE@"], "paper_id": "ABC_b124e65938672691a5589fb5cdb21e_11", "text": "In the current study, we have used the stance-annotated tweet data set described in (Küçük, 2017) ."}
{"sent_id": "b124e65938672691a5589fb5cdb21e-C001-71", "intents": ["@SIM@"], "paper_id": "ABC_b124e65938672691a5589fb5cdb21e_11", "text": "Similar to the settings in (Küçük, 2017) , we have used the SVM classifier based on the SMO algorithm (Platt, 1999) , available in the Weka tool (Hall et al., 2009 ), during our stance detection experiments."}
{"sent_id": "76476d80e1d3f65818592ec4caab0e-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_76476d80e1d3f65818592ec4caab0e_11", "text": "Natural language to code generation, a subtask of semantic parsing, is the problem of converting natural language (NL) descriptions to code (Ling et al., 2016; Yin and Neubig, 2017; Rabinovich et al., 2017) ."}
{"sent_id": "76476d80e1d3f65818592ec4caab0e-C001-103", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_76476d80e1d3f65818592ec4caab0e_11", "text": "However, when the code consists of complex logic, partial implementation errors occur, leading to low exact match accuracy (Yin and Neubig, 2017) ."}
{"sent_id": "76476d80e1d3f65818592ec4caab0e-C001-28", "intents": ["@USE@"], "paper_id": "ABC_76476d80e1d3f65818592ec4caab0e_11", "text": "Given an NL description q, our purpose is to generate code (e.g. Python) represented as an AST a. In this work, we start with the syntactic code gen-eration model by Yin and Neubig (2017) , which uses sequences of actions to generate the AST before converting it to surface code."}
{"sent_id": "76476d80e1d3f65818592ec4caab0e-C001-73", "intents": ["@USE@"], "paper_id": "ABC_76476d80e1d3f65818592ec4caab0e_11", "text": "N -gram subtrees from all retrieved sentences are assigned a score, based on the best similarity score Yin and Neubig (2017) of all instances where they appeared."}
{"sent_id": "76476d80e1d3f65818592ec4caab0e-C001-85", "intents": ["@USE@"], "paper_id": "ABC_76476d80e1d3f65818592ec4caab0e_11", "text": "For the neural code generation model, we use the settings explained in Yin and Neubig (2017) ."}
{"sent_id": "76476d80e1d3f65818592ec4caab0e-C001-88", "intents": ["@USE@"], "paper_id": "ABC_76476d80e1d3f65818592ec4caab0e_11", "text": "We compare our model with Yin and Neubig (2017)'s model that we call YN17 for brevity, and a sequence-to-sequence (SEQ2SEQ) model that we implemented."}
{"sent_id": "76476d80e1d3f65818592ec4caab0e-C001-109", "intents": ["@SIM@"], "paper_id": "ABC_76476d80e1d3f65818592ec4caab0e_11", "text": "Another common type of error that we found RECODE's generated outputs is incorrect variable copying, similarly to what is discussed in Yin and Neubig (2017) and Rabinovich et al. (2017) ."}
{"sent_id": "66392c3b6fa3744de79f056f615a75-C001-2", "intents": ["@EXT@"], "paper_id": "ABC_66392c3b6fa3744de79f056f615a75_11", "text": "This paper investigates the task of noun compound interpretation, building on the sense collocation approach proposed by Moldovan et al. (2004) ."}
{"sent_id": "66392c3b6fa3744de79f056f615a75-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_66392c3b6fa3744de79f056f615a75_11", "text": "It has been shown that NCs with semantically similar compo-nents share the same SR ; this is encapsulated by the phrase coined as sense collocation in Moldovan et al. (2004) ."}
{"sent_id": "66392c3b6fa3744de79f056f615a75-C001-36", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_66392c3b6fa3744de79f056f615a75_11", "text": "A majority of research undertaken in interpreting NCs have been based on two statistical methods: SEMANTIC SIMILARITY (Barker and Szpakowicz, 1998; Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase, 2006; Girju, 2007; and SEMANTIC INTER-PRETABILITY (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006) ."}
{"sent_id": "66392c3b6fa3744de79f056f615a75-C001-38", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_66392c3b6fa3744de79f056f615a75_11", "text": "A significant contribution to this area is by Moldovan et al. (2004) , who used the sense collocation (i.e. pair-of-word-senses) as their primary feature in disambiguating NCs."}
{"sent_id": "66392c3b6fa3744de79f056f615a75-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_66392c3b6fa3744de79f056f615a75_11", "text": "The basic idea behind sense collocation method in Moldovan et al. (2004) was based on the \"pair-ofword-senses\" from the component nouns in noun compounds as features of the classifier."}
{"sent_id": "66392c3b6fa3744de79f056f615a75-C001-112", "intents": ["@USE@"], "paper_id": "ABC_66392c3b6fa3744de79f056f615a75_11", "text": "2 The performance of the original method proposed in Moldovan et al. (2004) is considered as a benchmark."}
{"sent_id": "74623c8d812e3c84e7bc6b46e982f5-C001-58", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_74623c8d812e3c84e7bc6b46e982f5_11", "text": "Models: Elsner and Charniak (2008) explored various message-pair feature sets and linear classifiers, combined with local and global inference methods."}
{"sent_id": "74623c8d812e3c84e7bc6b46e982f5-C001-71", "intents": ["@USE@"], "paper_id": "ABC_74623c8d812e3c84e7bc6b46e982f5_11", "text": "4 Annotating the #Linux data enables comparison with Elsner and Charniak (2008) , while the #Ubuntu channel has over 34 million messages, making it an interesting largescale resource for dialogue research."}
{"sent_id": "74623c8d812e3c84e7bc6b46e982f5-C001-179", "intents": ["@USE@"], "paper_id": "ABC_74623c8d812e3c84e7bc6b46e982f5_11", "text": "For channel Two, we consider two annotations of the same underlying text: ours and Elsner and Charniak (2008)'s."}
{"sent_id": "74623c8d812e3c84e7bc6b46e982f5-C001-116", "intents": ["@SIM@"], "paper_id": "ABC_74623c8d812e3c84e7bc6b46e982f5_11", "text": "In Channel Two, we also see mistakes and ambiguous cases, including a particularly long discussion about a user's financial difficulties that could be divided in multiple ways (also noted by Elsner and Charniak (2008) )."}
{"sent_id": "74623c8d812e3c84e7bc6b46e982f5-C001-120", "intents": ["@DIF@"], "paper_id": "ABC_74623c8d812e3c84e7bc6b46e982f5_11", "text": "Results are not shown for Elsner and Charniak (2008) because they did not annotate graphs."}
{"sent_id": "e0df566d073649431c3454a52813e9-C001-18", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_e0df566d073649431c3454a52813e9_11", "text": "1 The CGT transport federation have risen against \"the lack of consultation\" and consider that employees have \"nothing positive to expect from this restructuring.\" 2 While studies have shown that discourse usage of discourse connectives can be accurately identified for English [13, 20] , only a few studies have focused on the disambiguation of discourse connectives in other languages."}
{"sent_id": "e0df566d073649431c3454a52813e9-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_e0df566d073649431c3454a52813e9_11", "text": "Most of previous work on the disambiguation of discourse connectives have focused on English discourse connectives [13, 14, 20] ."}
{"sent_id": "e0df566d073649431c3454a52813e9-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_e0df566d073649431c3454a52813e9_11", "text": "Later, Lin et al. [13] used the context of the connective (i.e. the previous and the following word of the connective) and added seven lexico-syntactic features to the feature set proposed by Pitler and Nenkova [20] ."}
{"sent_id": "e0df566d073649431c3454a52813e9-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_e0df566d073649431c3454a52813e9_11", "text": "Alsaif and Markert have shown that the features proposed by Pitler and Nenkova [20] work well for Arabic with an accuracy of 91.2%."}
{"sent_id": "e0df566d073649431c3454a52813e9-C001-157", "intents": ["@EXT@"], "paper_id": "ABC_e0df566d073649431c3454a52813e9_11", "text": "In this paper, we have investigated the applicability of the syntactic and lexical features proposed by Pitler and Nenkova [20] for the disambiguation of English discourse connectives for French."}
{"sent_id": "eee36102d3feac0f673cd33562d40f-C001-92", "intents": ["@BACK@"], "paper_id": "ABC_eee36102d3feac0f673cd33562d40f_11", "text": "Primadhanty et al. (2015) use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines."}
{"sent_id": "eee36102d3feac0f673cd33562d40f-C001-46", "intents": ["@USE@"], "paper_id": "ABC_eee36102d3feac0f673cd33562d40f_11", "text": "We compared factorization machines with a log-linear model, a polynomial-kernel SVM, and a state-ofthe-art log-bilinear model using nuclear norm for regularization (Primadhanty et al., 2015) ."}
{"sent_id": "eee36102d3feac0f673cd33562d40f-C001-50", "intents": ["@USE@"], "paper_id": "ABC_eee36102d3feac0f673cd33562d40f_11", "text": "We used the dataset provided by Primadhanty et al. (2015) ; this dataset was created for evaluating unknown named entity classification and is context features: Right and left contexts of the candidate in a sentence (do not take the order into account)."}
{"sent_id": "eee36102d3feac0f673cd33562d40f-C001-74", "intents": ["@DIF@"], "paper_id": "ABC_eee36102d3feac0f673cd33562d40f_11", "text": "Note that Primadhanty et al. (2015) used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use."}
{"sent_id": "eee36102d3feac0f673cd33562d40f-C001-91", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_eee36102d3feac0f673cd33562d40f_11", "text": "Both our approach and the methods of Primadhanty et al. (2015) address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized."}
{"sent_id": "eee36102d3feac0f673cd33562d40f-C001-76", "intents": ["@SIM@"], "paper_id": "ABC_eee36102d3feac0f673cd33562d40f_11", "text": "We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by Primadhanty et al. (2015) with fewer features."}
{"sent_id": "950cc4a7fa2db3aa6786cc0ae802b5-C001-12", "intents": ["@USE@"], "paper_id": "ABC_950cc4a7fa2db3aa6786cc0ae802b5_12", "text": "We test our approach on the Yahoo! Answers dataset of manner or How questions introduced by Jansen et al. (2014) , who describe answer reranking experiments on this dataset using a diverse range of features incorporating syntax, lexical semantics and discourse."}
{"sent_id": "950cc4a7fa2db3aa6786cc0ae802b5-C001-44", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_950cc4a7fa2db3aa6786cc0ae802b5_12", "text": "For comparison with recent work in answer reranking (Jansen et al., 2014; Sharp et al., 2015) , we also evaluate the averaged word embedding vectors obtained with the skip-gram model (Mikolov et al., 2013 ) (henceforth referred to as the SkipAvg model)."}
{"sent_id": "950cc4a7fa2db3aa6786cc0ae802b5-C001-63", "intents": ["@USE@"], "paper_id": "ABC_950cc4a7fa2db3aa6786cc0ae802b5_12", "text": "Following Jansen et al. (2014) and Fried et al. (2015) , we implement two baselines: the baseline that selects an answer randomly and the candidate retrieval (CR) baseline."}
{"sent_id": "950cc4a7fa2db3aa6786cc0ae802b5-C001-64", "intents": ["@USE@"], "paper_id": "ABC_950cc4a7fa2db3aa6786cc0ae802b5_12", "text": "The CR baseline uses the same scoring as in Jansen et al. (2014) : the questions and the candidate answers are represented using tf-idf (Salton, 1991) over lemmas; the candidate answers are ranked according to their cosine similarity to the respective question."}
{"sent_id": "950cc4a7fa2db3aa6786cc0ae802b5-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_950cc4a7fa2db3aa6786cc0ae802b5_12", "text": "Further information about the dataset can be found in Jansen et al. (2014) ."}
{"sent_id": "d68bb5264d157cc4c2d9fa9c8f82b6-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_d68bb5264d157cc4c2d9fa9c8f82b6_12", "text": "In English, the Zipf's law in the n-gram data [41] exhibits two regimes: one among words with frequencies above about 0.01% (Zipf's exponent γ ≈ 1) and another (γ ≈ 1.4) among words with frequency below 0.0001% [42] ."}
{"sent_id": "d68bb5264d157cc4c2d9fa9c8f82b6-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_d68bb5264d157cc4c2d9fa9c8f82b6_12", "text": "If the n-gram corpus is truncated by a minimum word count, then as that minimum is raised the Heaps scaling exponent increases from β < 0.5, approaching β < 1 [42] ."}
{"sent_id": "d68bb5264d157cc4c2d9fa9c8f82b6-C001-80", "intents": ["@USE@"], "paper_id": "ABC_d68bb5264d157cc4c2d9fa9c8f82b6_12", "text": "The difference could be resolved, however, with larger The Heaps law exponents, β, for the data series on the left, as well as additional data series, using Table 1 in [42] : all English 1-grams: 0.54 ± 0.01; English fiction: 0.49 ± 0.01; English GB: 0.44 ± 0.01; English US: 0.51 ± 0.01."}
{"sent_id": "d68bb5264d157cc4c2d9fa9c8f82b6-C001-121", "intents": ["@DIF@"], "paper_id": "ABC_d68bb5264d157cc4c2d9fa9c8f82b6_12", "text": "Our canonical model of the PNM differs somewhat from the explanation by [42] , in which a \"decreasing marginal need for additional words\" as the corpus grows is underlain by the \"dependency network between the common words ... and their more esoteric counterparts."}
{"sent_id": "392cbe849c1b8a69aae9923ade41aa-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_392cbe849c1b8a69aae9923ade41aa_12", "text": "Another approach has been proposed by Zhang et al. (2017) who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input."}
{"sent_id": "392cbe849c1b8a69aae9923ade41aa-C001-35", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_392cbe849c1b8a69aae9923ade41aa_12", "text": "Although the labelling approach in Zhang et al. (2017) is simple and efficient, looking at head and dependent only when assigning the labels comes with some disadvantages."}
{"sent_id": "392cbe849c1b8a69aae9923ade41aa-C001-66", "intents": ["@USE@"], "paper_id": "ABC_392cbe849c1b8a69aae9923ade41aa_12", "text": "Our interest is focussed on German, but to put our work in context, we follow Zhang et al. (2017) and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German."}
{"sent_id": "392cbe849c1b8a69aae9923ade41aa-C001-69", "intents": ["@USE@"], "paper_id": "ABC_392cbe849c1b8a69aae9923ade41aa_12", "text": "The German and Czech data come from the CoNLL-X shared task (Buchholz and Marsi, 2006) and our data split follows Zhang et al. (2017) ."}
{"sent_id": "392cbe849c1b8a69aae9923ade41aa-C001-91", "intents": ["@DIF@"], "paper_id": "ABC_392cbe849c1b8a69aae9923ade41aa_12", "text": "3 When applied to unlabelled gold trees, the distance between our models and the baseline becomes larger and the best of our history-based models (BILSTM(B), 97.38%) outperforms the original labeller of Zhang et al. (2017) (96.15%) by more than 1%."}
{"sent_id": "2c3a2999390b82f4e29b00d59f90f2-C001-83", "intents": ["@BACK@"], "paper_id": "ABC_2c3a2999390b82f4e29b00d59f90f2_12", "text": "Two more systems used them in combination with other techniques (Florian et al., 2003; Klein et al., 2003) ."}
{"sent_id": "2c3a2999390b82f4e29b00d59f90f2-C001-90", "intents": ["@BACK@"], "paper_id": "ABC_2c3a2999390b82f4e29b00d59f90f2_12", "text": "Florian et al. (2003) employed the same technique in a combination of learners."}
{"sent_id": "2c3a2999390b82f4e29b00d59f90f2-C001-98", "intents": ["@BACK@"], "paper_id": "ABC_2c3a2999390b82f4e29b00d59f90f2_12", "text": "Florian et al. (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best."}
{"sent_id": "2c3a2999390b82f4e29b00d59f90f2-C001-120", "intents": ["@BACK@"], "paper_id": "ABC_2c3a2999390b82f4e29b00d59f90f2_12", "text": "The inclusion of extra named entity recognition systems seems to have worked well (Florian et al., 2003) ."}
{"sent_id": "2c3a2999390b82f4e29b00d59f90f2-C001-137", "intents": ["@BACK@"], "paper_id": "ABC_2c3a2999390b82f4e29b00d59f90f2_12", "text": "Florian et al. (2003) have also obtained the highest F β=1 rate for the German data."}
{"sent_id": "2c3a2999390b82f4e29b00d59f90f2-C001-149", "intents": ["@BACK@"], "paper_id": "ABC_2c3a2999390b82f4e29b00d59f90f2_12", "text": "The best performance for both languages has been obtained by a combined learning system that used Maximum Entropy Models, transformation-based learning, Hidden Markov Models as well as robust risk minimization (Florian et al., 2003) ."}
{"sent_id": "e3ee86bbaca6ae00906e7ec64f0ac0-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12", "text": "• Knowledge Base (KB): The knowledge Base [2] mainly used to maintain the previous knowledge."}
{"sent_id": "e3ee86bbaca6ae00906e7ec64f0ac0-C001-46", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12", "text": "Previous classical paper [2] chose the sentiment classification as the learning target because it is could be regarded as a task as well as a group of subtasks in different domain."}
{"sent_id": "e3ee86bbaca6ae00906e7ec64f0ac0-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12", "text": "\"Lifelong Sentiment Classification\" (\"LSC\" for simple below) [2] records that which domain does a word have the sentiment orientation."}
{"sent_id": "e3ee86bbaca6ae00906e7ec64f0ac0-C001-67", "intents": ["@USE@"], "paper_id": "ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12", "text": "Use use the same formula as LSC [2] used below."}
{"sent_id": "e3ee86bbaca6ae00906e7ec64f0ac0-C001-117", "intents": ["@USE@"], "paper_id": "ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12", "text": "In the experiment, we use the same datasets as LSC [2] used."}
{"sent_id": "c705c0533600b9b93d2c89bcbc292b-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_c705c0533600b9b93d2c89bcbc292b_12", "text": "Recent state-of-the-art models (Wang et al., 2018; Fried et al., 2018b; Ma et al., 2019) have demonstrated large gains in accuracy on the VLN task."}
{"sent_id": "35233406ffd78d87743478454432d5-C001-17", "intents": ["@SIM@", "@BACK@"], "paper_id": "ABC_35233406ffd78d87743478454432d5_12", "text": "Similar to our previous work in product reviews (Xu et al., 2016) , we call the mouse target entity and those 4 products complementary entities of the target entity."}
{"sent_id": "35233406ffd78d87743478454432d5-C001-30", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_35233406ffd78d87743478454432d5_12", "text": "For the first stage, we employ a similar approach as in (Xu et al., 2016) ; for the second stage, it is reduced to a yes/no answer classification problem (McAuley and Yang, 2016) ."}
{"sent_id": "35233406ffd78d87743478454432d5-C001-50", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_35233406ffd78d87743478454432d5_12", "text": "al. (Xu et al., 2016) ."}
{"sent_id": "35233406ffd78d87743478454432d5-C001-29", "intents": ["@USE@"], "paper_id": "ABC_35233406ffd78d87743478454432d5_12", "text": "Given the structure of a QA pair, our method naturally has a twostage framework: Complementary Entity Recognition (CER) (Xu et al., 2016) and yes/no answer classification."}
{"sent_id": "35233406ffd78d87743478454432d5-C001-85", "intents": ["@USE@"], "paper_id": "ABC_35233406ffd78d87743478454432d5_12", "text": "Since complementary entities are mentioned in yes/no questions and their polarities of compatibility information are in answers, the proposed method naturally has a two-stage framework: Complementary Entity Recognition: we extract complementary entities from questions using dependency paths almost the same as in (Xu et al., 2016) ."}
{"sent_id": "35233406ffd78d87743478454432d5-C001-151", "intents": ["@USE@"], "paper_id": "ABC_35233406ffd78d87743478454432d5_12", "text": "To obtain knowledge about domainspecific verbs, we use 6000 reviews for each product similar as in (Xu et al., 2016) ."}
{"sent_id": "808e0a94b877182dc06447c8682a63-C001-37", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_808e0a94b877182dc06447c8682a63_12", "text": "3 Surface n-gram Features Bansal and Klein (2011) demonstrate that features generated from bucketing simple surface n-gram counts and collecting the top paraphrase-based contextual words over Web1T are useful for almost all attachment decisions, boosting dependency parsing accuracy by up to 0.6%."}
{"sent_id": "808e0a94b877182dc06447c8682a63-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_808e0a94b877182dc06447c8682a63_12", "text": "Affinity features rely on the intuition that frequently co-occurring words in large unlabeled text collections are likely to be in a syntactic relationship (Nakov and Hearst, 2005; Bansal and Klein, 2011) ."}
{"sent_id": "808e0a94b877182dc06447c8682a63-C001-128", "intents": ["@BACK@"], "paper_id": "ABC_808e0a94b877182dc06447c8682a63_12", "text": "Surface n-gram counts from large web corpora have been used to address NP and PP attachment errors (Volk, 2001; Nakov and Hearst, 2005) Aside from Bansal and Klein (2011) , other feature-based approaches to improving dependency parsing include Pitler (2012) , who exploits Brown clusters and point-wise mutual information of surface n-gram counts to specifically address PP and coordination errors."}
{"sent_id": "808e0a94b877182dc06447c8682a63-C001-42", "intents": ["@USE@"], "paper_id": "ABC_808e0a94b877182dc06447c8682a63_12", "text": "We also extend Bansal and Klein's affinity and paraphrase features to second-order."}
{"sent_id": "808e0a94b877182dc06447c8682a63-C001-85", "intents": ["@USE@"], "paper_id": "ABC_808e0a94b877182dc06447c8682a63_12", "text": "As with Bansal and Klein (2011) and Pitler (2012) , we convert the Penn Treebank to dependencies using pennconverter 3 (Johansson and Nugues, 2007) (henceforth LTH) and generate POS tags with MX-POST (Ratnaparkhi, 1996) ."}
{"sent_id": "808e0a94b877182dc06447c8682a63-C001-40", "intents": ["@EXT@"], "paper_id": "ABC_808e0a94b877182dc06447c8682a63_12", "text": "We extract Bansal and Klein (2011) 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts."}
{"sent_id": "da2429450c8d1f1f3e72383c86ec73-C001-120", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_da2429450c8d1f1f3e72383c86ec73_12", "text": "We used the same 3 existing sentiment lexicons as in (Mohammad et al., 2013) ."}
{"sent_id": "da2429450c8d1f1f3e72383c86ec73-C001-124", "intents": ["@USE@"], "paper_id": "ABC_da2429450c8d1f1f3e72383c86ec73_12", "text": "The NRC hashtag sentiment lexicon was generated automatically from a set of 775k tweets containing a hashtag of a small predefined list of positive and negative hashtags (Mohammad et al., 2013) ."}
{"sent_id": "da2429450c8d1f1f3e72383c86ec73-C001-29", "intents": ["@DIF@"], "paper_id": "ABC_da2429450c8d1f1f3e72383c86ec73_12", "text": "Unfortunately our replica system of Mohammad et al. (2013) only achieved an F1-score of 63.25 on the Twitter-2013 test set, while their score in the 2013 competition on the same test set was 69.02, nearly 6 points higher in F1."}
{"sent_id": "da2429450c8d1f1f3e72383c86ec73-C001-107", "intents": ["@DIF@"], "paper_id": "ABC_da2429450c8d1f1f3e72383c86ec73_12", "text": "In comparison, Mohammad et al. (2013) used noncontiguous n-grams (unigram-unigram, unigrambigram, and bigram-bigram pairs) ."}
{"sent_id": "da2429450c8d1f1f3e72383c86ec73-C001-57", "intents": ["@SIM@"], "paper_id": "ABC_da2429450c8d1f1f3e72383c86ec73_12", "text": "For each lexicon, the 4 scores were the same as in (Mohammad et al., 2013) , i.e. per tweet, we use the number of tokens appearing in the lexicon, the sum and the max of the scores, and the last non-zero score."}
{"sent_id": "06db17253d76150772c0926e11131d-C001-61", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_06db17253d76150772c0926e11131d_12", "text": "VQA-CP v2 and VQA-CP v1 [10] were recently introduced as diagnostic datasets containing different answer distributions for each questiontype between train and test splits."}
{"sent_id": "06db17253d76150772c0926e11131d-C001-99", "intents": ["@BACK@"], "paper_id": "ABC_06db17253d76150772c0926e11131d_12", "text": "VQA models are inclined to learn unimodal biases from the datasets [10] ."}
{"sent_id": "06db17253d76150772c0926e11131d-C001-156", "intents": ["@USE@"], "paper_id": "ABC_06db17253d76150772c0926e11131d_12", "text": "Experimental setup We train and evaluate our models on VQA-CP v2 [10] ."}
{"sent_id": "06db17253d76150772c0926e11131d-C001-192", "intents": ["@DIF@"], "paper_id": "ABC_06db17253d76150772c0926e11131d_12", "text": "We report a drop of 1.94 percentage points with respect to our baseline, while [10] report a drop of 3.78 between GVQA and their SAN baseline."}
{"sent_id": "7b9fc52e4479dc5ff9b8796a558981-C001-33", "intents": ["@BACK@", "@EXT@", "@USE@"], "paper_id": "ABC_7b9fc52e4479dc5ff9b8796a558981_12", "text": "To reduce the need for a large seed dictionary, Artetxe et al. (2017) propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence."}
{"sent_id": "7b9fc52e4479dc5ff9b8796a558981-C001-18", "intents": ["@EXT@"], "paper_id": "ABC_7b9fc52e4479dc5ff9b8796a558981_12", "text": "In this work, we extend the modern embeddingbased approach of Artetxe et al. (2017) with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction."}
{"sent_id": "7b9fc52e4479dc5ff9b8796a558981-C001-38", "intents": ["@USE@"], "paper_id": "ABC_7b9fc52e4479dc5ff9b8796a558981_12", "text": "This method augments the embeddings for all words in both languages before using them in the self-learning framework of Artetxe et al. (2017) ."}
{"sent_id": "7b9fc52e4479dc5ff9b8796a558981-C001-73", "intents": ["@USE@"], "paper_id": "ABC_7b9fc52e4479dc5ff9b8796a558981_12", "text": "We use the datasets used by Artetxe et al. (2017) , consisting of three language pairs: EnglishItalian, English-German, and English-Finnish."}
{"sent_id": "c34bbed419bddb6d63b3e3bccf595d-C001-17", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_c34bbed419bddb6d63b3e3bccf595d_12", "text": "Bridging recognition is a difficult task, so that we had to report very low results on this IS class in previous work (Markert et al., 2012) ."}
{"sent_id": "c34bbed419bddb6d63b3e3bccf595d-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_c34bbed419bddb6d63b3e3bccf595d_12", "text": "Similarly, Clark (1975) distinguishes between bridging via necessary, probable and inducible parts/roles and argues that only in the first and maybe the second case the antecedent triggers the 3 See also the high results for our specific category for comparative anaphora (Markert et al., 2012) ."}
{"sent_id": "c34bbed419bddb6d63b3e3bccf595d-C001-60", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_c34bbed419bddb6d63b3e3bccf595d_12", "text": "In Markert et al. (2012) we classify eight finegrained IS categories for NPs in written text: old, new and 6 mediated categories (syntactic, worldKnowledge, bridging, comparative, aggregate and function)."}
{"sent_id": "bcf19914bb67ded47785d298969a7a-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_bcf19914bb67ded47785d298969a7a_12", "text": "FOIL (Shekhar et al., 2017b ) is one such dataset."}
{"sent_id": "bcf19914bb67ded47785d298969a7a-C001-70", "intents": ["@USE@"], "paper_id": "ABC_bcf19914bb67ded47785d298969a7a_12", "text": "Data: We use the dataset for nouns from Shekhar et al. (2017b) 1 and the datasets for other parts of speech from Shekhar et al. (2017a) 2 ."}
{"sent_id": "bcf19914bb67ded47785d298969a7a-C001-82", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_bcf19914bb67ded47785d298969a7a_12", "text": "We hypothesize the following reasons for this: (a) human responses were crowd-sourced, which could have resulted in some noisy annotations; (b) our gold object-based features closely resembles the information used for data-generation as described in Shekhar et al. (2017b) for the foil noun dataset."}
{"sent_id": "bcf19914bb67ded47785d298969a7a-C001-88", "intents": ["@DIF@"], "paper_id": "ABC_bcf19914bb67ded47785d298969a7a_12", "text": "Our overall accuracy is substantially higher than that reported in Shekhar et al. (2017b) ."}
{"sent_id": "bcf19914bb67ded47785d298969a7a-C001-113", "intents": ["@DIF@"], "paper_id": "ABC_bcf19914bb67ded47785d298969a7a_12", "text": "The accuracy of our models is substantially higher than that reported in Shekhar et al. (2017b) , even for equivalent models."}
{"sent_id": "710ec6f6d6d4c7c8c148833c0adfef-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_710ec6f6d6d4c7c8c148833c0adfef_12", "text": "Recently, statistical methods have been exploited to learn sentiment lexicons automatically (Esuli and Sebastiani, 2006; Baccianella et al., 2010; Mohammad et al., 2013) ."}
{"sent_id": "710ec6f6d6d4c7c8c148833c0adfef-C001-114", "intents": ["@DIF@"], "paper_id": "ABC_710ec6f6d6d4c7c8c148833c0adfef_12", "text": "Second, we find many cases where our lexicon gives the correct polarity (e.g. suitable, lazy) but the lexicon of Mohammad et al. (2013) does not."}
{"sent_id": "710ec6f6d6d4c7c8c148833c0adfef-C001-44", "intents": ["@USE@"], "paper_id": "ABC_710ec6f6d6d4c7c8c148833c0adfef_12", "text": "We use the same data source as Mohammad et al. (2013) to train lexicons."}
{"sent_id": "710ec6f6d6d4c7c8c148833c0adfef-C001-103", "intents": ["@USE@"], "paper_id": "ABC_710ec6f6d6d4c7c8c148833c0adfef_12", "text": "We compare our lexicon with the lexicons of NRC 4 (Mohammad et al., 2013) , HIT 5 (Tang et al., 2014a) and WEKA 6 (Bravo-Marquez et al., 2015) ."}
{"sent_id": "fc5de471ba4cc82a2156ed25d2c78b-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_fc5de471ba4cc82a2156ed25d2c78b_12", "text": "For example, Bansal et al. [5] showed that pre-training on either English or French ASR improved their Spanish-English AST system (trained on 20 hours of parallel data) and Tian [10] got improvements on an 8-hour Swahili-English AST dataset using English ASR pretraining."}
{"sent_id": "fc5de471ba4cc82a2156ed25d2c78b-C001-65", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_fc5de471ba4cc82a2156ed25d2c78b_12", "text": "However, as noted by [5] , the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages."}
{"sent_id": "fc5de471ba4cc82a2156ed25d2c78b-C001-63", "intents": ["@USE@"], "paper_id": "ABC_fc5de471ba4cc82a2156ed25d2c78b_12", "text": "Finally, to reproduce one of the experiments from [5] , we pretrained one model using 300 hours of Switchboard English [18] ."}
{"sent_id": "fc5de471ba4cc82a2156ed25d2c78b-C001-83", "intents": ["@USE@"], "paper_id": "ABC_fc5de471ba4cc82a2156ed25d2c78b_12", "text": "We use code and hyperparameter settings from [5] 4 : the Adam optimizer [25] with an initial learning rate of 0.001 and decay it by a factor of 0.5 based on the dev set BLEU score."}
{"sent_id": "24ee9b2bd8c97cbe923bc747b09806-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_24ee9b2bd8c97cbe923bc747b09806_12", "text": "The work by [12, 13, 14, 15] and the results presented here are a step towards more cognitively plausible models of language learning as it is more natural to learn language without prior assumptions about the lexical level."}
{"sent_id": "24ee9b2bd8c97cbe923bc747b09806-C001-71", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_24ee9b2bd8c97cbe923bc747b09806_12", "text": "The approach is based on our own text-based model described in [8] and on the speech-based models described in [13, 15] and we refer to those studies for more details."}
{"sent_id": "24ee9b2bd8c97cbe923bc747b09806-C001-107", "intents": ["@BACK@"], "paper_id": "ABC_24ee9b2bd8c97cbe923bc747b09806_12", "text": "[15] use a binary decision task: given a word and a sentence embedding, decide if the word occurs in the sentence."}
{"sent_id": "24ee9b2bd8c97cbe923bc747b09806-C001-128", "intents": ["@USE@"], "paper_id": "ABC_24ee9b2bd8c97cbe923bc747b09806_12", "text": "We compare our models to [12] and [15] , and include our own character-based model for comparison."}
{"sent_id": "24ee9b2bd8c97cbe923bc747b09806-C001-93", "intents": ["@DIF@"], "paper_id": "ABC_24ee9b2bd8c97cbe923bc747b09806_12", "text": "We use importance sampling to select the mismatched pairs; rather than using all the other samples in the mini-batch as mismatched pairs (as done in [8, 15] ), we calculate the loss using only the hardest examples (i.e. mismatched pairs with high cosine similarity)."}
{"sent_id": "24ee9b2bd8c97cbe923bc747b09806-C001-103", "intents": ["@DIF@"], "paper_id": "ABC_24ee9b2bd8c97cbe923bc747b09806_12", "text": "The main differences with the approaches described in [13, 15] are the use of multi-layered GRUs, importance sampling, the cyclic learning rate, snapshot ensembling and the use of vectorial rather than scalar attention."}
{"sent_id": "26fbf9f4ae740513d8889160ad9f63-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_26fbf9f4ae740513d8889160ad9f63_12", "text": "Several work have shown that discourse relations can improve the results of summarization in the case of factual texts or news articles (e.g. (Otterbacher et al., 2002) )."}
{"sent_id": "26fbf9f4ae740513d8889160ad9f63-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_26fbf9f4ae740513d8889160ad9f63_12", "text": "For example: \"Allied Capital is a closed-end management investment company that will operate as a business development concern.\" As shown in Figure 1 , illustration relations can be sub-divided into sub-categories: joint, list, disjoint, and elaboration relations according to the RST Discourse Treebank (Carlson and Marcu, 2001 ) and the Penn Discourse TreeBank (Prasad et al., 2008) ."}
{"sent_id": "26fbf9f4ae740513d8889160ad9f63-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_26fbf9f4ae740513d8889160ad9f63_12", "text": "The comparison relation subsumes the contrast relation according to the Penn Discourse TreeBank (Prasad et al., 2008) and the analogy and preference relations according to the RST Discourse Treebank (Carlson and Marcu, 2001) ."}
{"sent_id": "26fbf9f4ae740513d8889160ad9f63-C001-128", "intents": ["@BACK@"], "paper_id": "ABC_26fbf9f4ae740513d8889160ad9f63_12", "text": "Finally the most appropriate schema is selected based on a given question type; and candidate sentences fill particular slots in the selected schema based on which discourse relations they contain in order to create the final summary (details of BlogSum can be found in (Mithun, 2012) )."}
{"sent_id": "7d5c01ec5d744747413e42dcbc1a3c-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_7d5c01ec5d744747413e42dcbc1a3c_12", "text": "Commonly used heuristics include (1) using a stop word list to remove non-keywords (e.g., Liu et al. (2009b) ) and (2) allowing words with certain partof-speech tags (e.g., nouns, adjectives, verbs) to be considered candidate keywords (Mihalcea and Tarau (2004) , Liu et al. (2009a) , Wan and Xiao (2008) )."}
{"sent_id": "7d5c01ec5d744747413e42dcbc1a3c-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_7d5c01ec5d744747413e42dcbc1a3c_12", "text": "In the TextRank algorithm (Mihalcea and Tarau, 2004) , a text is represented by a graph."}
{"sent_id": "7d5c01ec5d744747413e42dcbc1a3c-C001-124", "intents": ["@USE@"], "paper_id": "ABC_7d5c01ec5d744747413e42dcbc1a3c_12", "text": "TextRank and SingleRank setup Following Mihalcea and Tarau (2004) and Wan and Xiao (2008) , we set the co-occurrence window size for TextRank and SingleRank to 2 and 10, respectively, as these parameter values have yielded the best results for their evaluation datasets."}
{"sent_id": "7d5c01ec5d744747413e42dcbc1a3c-C001-187", "intents": ["@SIM@"], "paper_id": "ABC_7d5c01ec5d744747413e42dcbc1a3c_12", "text": "While Mihalcea and Tarau (2004) and our reimplementations use all of these gold-standard keyphrases in our evaluation, Hulth (2003) and Liu et al. address Table 3 : Original vs. re-implementation scores of TextRank 3 , and are confident that our implementation is correct."}
{"sent_id": "40742bac72bbbaed4755ff0b74d599-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_40742bac72bbbaed4755ff0b74d599_12", "text": "Techniques for generating lower-rank representations have also been employed, such as PPMI-SVD (Levy et al., 2015) and GloVe (Pennington et al., 2014) , both achieving state-of-the-art performance on a variety of tasks."}
{"sent_id": "40742bac72bbbaed4755ff0b74d599-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_40742bac72bbbaed4755ff0b74d599_12", "text": "As word embeddings with lower dimensionality may improve efficiency and generalization (Levy et al., 2015) , the improved PPMI * matrix can be factorized as a product of two lower rank matrices."}
{"sent_id": "40742bac72bbbaed4755ff0b74d599-C001-30", "intents": ["@USE@"], "paper_id": "ABC_40742bac72bbbaed4755ff0b74d599_12", "text": "Although the optimal value of p is highly task-dependent (Österlund et al., 2015) , we set p = 0.5 as it has been shown to perform well on the word similarity and analogy tasks we use in our experiments (Levy et al., 2015) ."}
{"sent_id": "40742bac72bbbaed4755ff0b74d599-C001-70", "intents": ["@USE@"], "paper_id": "ABC_40742bac72bbbaed4755ff0b74d599_12", "text": "All methods generate both word and context matrices (W andW ): W is used for SGNS, PPMI-SVD and W +W for GloVe (following Levy et al. (2015) , and W and W +W for LexVec."}
{"sent_id": "40742bac72bbbaed4755ff0b74d599-C001-71", "intents": ["@USE@"], "paper_id": "ABC_40742bac72bbbaed4755ff0b74d599_12", "text": "For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015 factorization of logM ) and Skip-gram (implicit factorization of the shifted PMI matrix), and compare the stochastic and mini-batch approaches."}
{"sent_id": "40742bac72bbbaed4755ff0b74d599-C001-88", "intents": ["@SIM@"], "paper_id": "ABC_40742bac72bbbaed4755ff0b74d599_12", "text": "This is inline with results for PPMI-SVD and SGNS models (Levy et al., 2015) ."}
{"sent_id": "0732eaa37366d7ae092f4de0ed72cb-C001-64", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_0732eaa37366d7ae092f4de0ed72cb_12", "text": "As described in Vinyals et al. (2015) , not all the outputs (predicted linearized parse trees) obtained from the Seq2seq parser are valid (well-formed) as a parse tree."}
{"sent_id": "0732eaa37366d7ae092f4de0ed72cb-C001-96", "intents": ["@USE@"], "paper_id": "ABC_0732eaa37366d7ae092f4de0ed72cb_12", "text": "We used the standard split of training (Sec.02-21), development (Sec.22), and test data (Sec.23) and strictly followed the instructions for the evaluation settings explained in Vinyals et al. (2015) ."}
{"sent_id": "0732eaa37366d7ae092f4de0ed72cb-C001-102", "intents": ["@USE@"], "paper_id": "ABC_0732eaa37366d7ae092f4de0ed72cb_12", "text": "We postprocessed such malformed parse trees by simple rules introduced in (Vinyals et al., 2015) ."}
{"sent_id": "4d2488844c1f6f39f1f4b8f3487288-C001-42", "intents": ["@USE@"], "paper_id": "ABC_4d2488844c1f6f39f1f4b8f3487288_13", "text": "The DPMM used in Kawahara et al. (2014) is shown in Figure 1 ."}
{"sent_id": "4d2488844c1f6f39f1f4b8f3487288-C001-93", "intents": ["@USE@"], "paper_id": "ABC_4d2488844c1f6f39f1f4b8f3487288_13", "text": "When incorporating supervision, we flatten VerbNet, using only the top-level categories, simplifying the selection process for y. In Kawahara et al. (2014) , slot features were most effective features at producing a VerbNet-like structure; we follow suit."}
{"sent_id": "4d2488844c1f6f39f1f4b8f3487288-C001-110", "intents": ["@USE@"], "paper_id": "ABC_4d2488844c1f6f39f1f4b8f3487288_13", "text": "We report each metric, and the F1 score combining them, to compare the clustering accuracy with respect to the gold standard G. We use the clustering from Kawahara et al. (2014) as a baseline for comparison."}
{"sent_id": "4d2488844c1f6f39f1f4b8f3487288-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_4d2488844c1f6f39f1f4b8f3487288_13", "text": "Parisien and Stevenson (2011) and Kawahara et al. (2014) showed distinct ways of applying the Hierarchical Dirichlet Process (Teh et al., 2006) to uncover the latent clusters from cluster examples."}
{"sent_id": "4d2488844c1f6f39f1f4b8f3487288-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_4d2488844c1f6f39f1f4b8f3487288_13", "text": "According to (Kawahara et al., 2014) , the best features for inducing verb classes are joint slot:token pairs."}
{"sent_id": "4d2488844c1f6f39f1f4b8f3487288-C001-134", "intents": ["@EXT@"], "paper_id": "ABC_4d2488844c1f6f39f1f4b8f3487288_13", "text": "We have expanded the work in Kawahara et al. (2014) by explicitly modeling a VerbNet class for each verb sense, drawn from a product of experts based on the cluster and verb."}
{"sent_id": "9655fb9abfb1c30b39f3261680fafc-C001-100", "intents": ["@DIF@"], "paper_id": "ABC_9655fb9abfb1c30b39f3261680fafc_13", "text": "The differences between our method and (Wiseman et al., 2017 ) is that we adopt a LSTM for the encoder, while (Wiseman et al., 2017 ) uses a table encoder similar to (Yang et al., 2017) ."}
{"sent_id": "9655fb9abfb1c30b39f3261680fafc-C001-78", "intents": ["@USE@"], "paper_id": "ABC_9655fb9abfb1c30b39f3261680fafc_13", "text": "Data: We use ROTOWIRE dataset (Wiseman et al., 2017) , which is a collection of articles summarizing NBA basketball games, paired with their corresponding box-and line-score tables."}
{"sent_id": "9655fb9abfb1c30b39f3261680fafc-C001-85", "intents": ["@USE@"], "paper_id": "ABC_9655fb9abfb1c30b39f3261680fafc_13", "text": "For relation extractor model, we use an ensemble of CNNs and LSTMs relation classification models (Wiseman et al., 2017) , which achieves the precision of 94.7% and recall of 75.3% given the reference."}
{"sent_id": "9655fb9abfb1c30b39f3261680fafc-C001-107", "intents": ["@USE@"], "paper_id": "ABC_9655fb9abfb1c30b39f3261680fafc_13", "text": "3 Hand crafted templates from (Wiseman et al., 2017 ), e.g.  scored  points (- FG, - 3PT, - FT) 4 The whole game input data and the summaries are relatively lengthy, we presents the first three sentences in the summary and its corresponding game statistics for brevity."}
{"sent_id": "e5bff4a27468139762496abdff3436-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_e5bff4a27468139762496abdff3436_13", "text": "Twitter conversations (Danescu-Niculescu-Mizil, Gamon, and Dumais 2011; Purohit et al. 2013 ) and popular memes (Myers and Leskovec 2012; Coscia 2013) prove this similarity in social media."}
{"sent_id": "e5bff4a27468139762496abdff3436-C001-38", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_e5bff4a27468139762496abdff3436_13", "text": "To measure commonsense for a particular situation is hard, however, adaptations can be easily captured in Twitter conversations (Danescu-NiculescuMizil, Gamon, and Dumais 2011; Purohit et al. 2013) , in memes (Myers and Leskovec 2012; Coscia 2013) , and faceto-face discussions (Danescu-Niculescu-Mizil et al. 2012) ."}
{"sent_id": "e5bff4a27468139762496abdff3436-C001-43", "intents": ["@USE@"], "paper_id": "ABC_e5bff4a27468139762496abdff3436_13", "text": "To this end, we evaluate the open access data of the United States Supreme Court (Hawes, Lin, and Resnik 2009; Hawes 2009; Danescu-Niculescu-Mizil et al. 2012 ), prepare conversation groups with different adaptation levels, implement a suitable algorithm to extract linguistic relations in these group conversations, and finally provide a comparison between the groups and the discovered linguistic relations."}
{"sent_id": "e5bff4a27468139762496abdff3436-C001-67", "intents": ["@USE@"], "paper_id": "ABC_e5bff4a27468139762496abdff3436_13", "text": "Referring exchange theory (Willer 1999; Thye, Willer, and Markovsky 2006) and the measured coordination (Danescu-Niculescu-Mizil et al. 2012) , one can order the relative power of each Justice and lawyer pair"}
{"sent_id": "8c202e3610599c9eee23724ef213de-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_8c202e3610599c9eee23724ef213de_13", "text": "The first of these studies (Park and Cardie, 2014) compiled online user comments from a discussion website and developed a framework for automatically classifying each proposition as either \"unverifiable\", \"verifiable non-experiential\", or \"verifiable experiential\", where the appropriate types of support are reason, evidence, and optional evidence, respectively."}
{"sent_id": "8c202e3610599c9eee23724ef213de-C001-19", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_8c202e3610599c9eee23724ef213de_13", "text": "In classifying propositions, Park and Cardie (2014) followed previous work such as Reed et al. (2008) and Palau and Moens (2009) , employing supervised learning methods."}
{"sent_id": "8c202e3610599c9eee23724ef213de-C001-56", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_8c202e3610599c9eee23724ef213de_13", "text": "(Merely using clause tags without capturing dependencies for important clauses may not help much in distinguishing objective verifiable claims from unverifiable subjective ones.) Park and Cardie (2014) also used tense and person counts for distinguishing verifiable claims from unverifiable claims."}
{"sent_id": "8ef47e16cd41aa3a606cf21c41adb7-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_8ef47e16cd41aa3a606cf21c41adb7_13", "text": "Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (Mehdad et al., 2013; Oya et al., 2014) ."}
{"sent_id": "8ef47e16cd41aa3a606cf21c41adb7-C001-38", "intents": ["@USE@"], "paper_id": "ABC_8ef47e16cd41aa3a606cf21c41adb7_13", "text": "Template Generation follows the approach of (Oya et al., 2014) and, starting from human-authored summaries, produces abstract templates applying slot labeling, summary clustering and template fusion steps."}
{"sent_id": "8ef47e16cd41aa3a606cf21c41adb7-C001-102", "intents": ["@USE@"], "paper_id": "ABC_8ef47e16cd41aa3a606cf21c41adb7_13", "text": "For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in (Oya et al., 2014) ."}
{"sent_id": "8ef47e16cd41aa3a606cf21c41adb7-C001-104", "intents": ["@USE@"], "paper_id": "ABC_8ef47e16cd41aa3a606cf21c41adb7_13", "text": "In the table we also report the performances of the previously published summarization systems that make use of the manual communities - (Oya et al., 2014) and (Mehdad et al., 2013) ; and our run of the system of (Oya et al., 2014) ."}
{"sent_id": "8ef47e16cd41aa3a606cf21c41adb7-C001-75", "intents": ["@DIF@"], "paper_id": "ABC_8ef47e16cd41aa3a606cf21c41adb7_13", "text": "In this paper, different from (Oya et al., 2014) , the sentence ranking is based solely on the n-gram language models trained on the tokens and part-ofspeech tags from the human-authored summaries."}
{"sent_id": "c1eefe276c0ed46d7cd50f3f3bc3f3-C001-138", "intents": ["@BACK@"], "paper_id": "ABC_c1eefe276c0ed46d7cd50f3f3bc3f3_13", "text": "Supervised word embedding models which score (conversation history, response) pairs have been shown to be a strong baseline for both open-ended and goal-oriented dialog (Dodge et al., 2016; Bordes et al., 2017) ."}
{"sent_id": "c1eefe276c0ed46d7cd50f3f3bc3f3-C001-128", "intents": ["@USE@"], "paper_id": "ABC_c1eefe276c0ed46d7cd50f3f3bc3f3_13", "text": "Following Bordes et al. (2017) , we provide baselines on the modified dataset by evaluating several learning methods: rule-based systems, supervised embeddings, and end-to-end Memory networks."}
{"sent_id": "c1eefe276c0ed46d7cd50f3f3bc3f3-C001-23", "intents": ["@EXT@"], "paper_id": "ABC_c1eefe276c0ed46d7cd50f3f3bc3f3_13", "text": "With the ultimate aim of creating such a dataset, this paper aims to be an extension of the bAbI dialog dataset introduced by Bordes et al. (2017) ."}
{"sent_id": "c1eefe276c0ed46d7cd50f3f3bc3f3-C001-185", "intents": ["@EXT@"], "paper_id": "ABC_c1eefe276c0ed46d7cd50f3f3bc3f3_13", "text": "As this work builds on top of the bAbI dialog dataset proposed by Bordes et al. (2017) , crucial aspects of goal-oriented conversation have been split into various synthetically generated tasks to evaluate the strengths and weaknesses of models in a systematic way before applying them on real data."}
{"sent_id": "c1eefe276c0ed46d7cd50f3f3bc3f3-C001-158", "intents": ["@DIF@"], "paper_id": "ABC_c1eefe276c0ed46d7cd50f3f3bc3f3_13", "text": "Compared to results reported on the bAbI dialog tasks in Bordes et al. (2017) , supervised embeddings performed significantly worse on the modified tasks."}
{"sent_id": "1f77b780c98093cd85966243471a1d-C001-65", "intents": ["@DIF@"], "paper_id": "ABC_1f77b780c98093cd85966243471a1d_13", "text": "Even though performing classification at state or county granularity tends to be robust and accurate (Fried et al., 2014) , characteristics that are specific to individuals are more meaningful and practical."}
{"sent_id": "1f77b780c98093cd85966243471a1d-C001-86", "intents": ["@USE@"], "paper_id": "ABC_1f77b780c98093cd85966243471a1d_13", "text": "To this end, we started with the same settings as Fried et al. (2014) : we used the 887,310 tweets they collected which were localizable to a specific state and contained at least one relevant hashtag, such as #breakfast or #dinner."}
{"sent_id": "1f77b780c98093cd85966243471a1d-C001-89", "intents": ["@USE@"], "paper_id": "ABC_1f77b780c98093cd85966243471a1d_13", "text": "To mitigate sparsity, we also included topics generated using Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and all tweets collected by Fried et al. For example, one of the generated topics contains words that approximate the standard American diet (e.g., chicken, potatoes, cheese, baked, beans, fried, mac), which has already been shown to correlate with higher overweight and T2DM rates (Fried et al., 2014 Figure 2 : A decision tree from the random forest classifier trained using state-level Twitter data."}
{"sent_id": "1f77b780c98093cd85966243471a1d-C001-122", "intents": ["@USE@"], "paper_id": "ABC_1f77b780c98093cd85966243471a1d_13", "text": "identical experimental settings as (Fried et al., 2014) , i.e., leave-one-out-cross-validation on the 50 states plus the District of Columbia."}
{"sent_id": "45238fe9b493ccdf5921c8f5284097-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_45238fe9b493ccdf5921c8f5284097_13", "text": "Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets (Sandhaus, 2008; Hermann et al., 2015; Narayan et al., 2018b) which has enabled the development of novel methods, many of them employing recent advances in neural networks (See et al., 2017; Narayan et al., 2018c; , inter alia)."}
{"sent_id": "45238fe9b493ccdf5921c8f5284097-C001-23", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_45238fe9b493ccdf5921c8f5284097_13", "text": "Those that conduct manual assessment of the content, typically use a single reference summary, either directly (Celikyilmaz et al., 2018; Tan et al., 2017) or through questions (Narayan et al., 2018b,c) and thus are also likely to exhibit reference bias."}
{"sent_id": "45238fe9b493ccdf5921c8f5284097-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_45238fe9b493ccdf5921c8f5284097_13", "text": "Clarke and Lapata (2010) proposed a question-answering based approach to improve the agreement among human evaluations for the quality of summary content, which was recently employed by Narayan et al. (2018b) and Narayan et al. (2018c) (QA in Table 1 )."}
{"sent_id": "45238fe9b493ccdf5921c8f5284097-C001-59", "intents": ["@USE@"], "paper_id": "ABC_45238fe9b493ccdf5921c8f5284097_13", "text": "Absolute assessment was also employed in combination with the question answering approach for content evaluation (Narayan et al., 2018b; Mendes et al., 2019) ."}
{"sent_id": "45238fe9b493ccdf5921c8f5284097-C001-129", "intents": ["@USE@"], "paper_id": "ABC_45238fe9b493ccdf5921c8f5284097_13", "text": "We use the extreme summarization dataset (XSUM, Narayan et al., 2018b) 2 which comprises BBC articles paired with their singlesentence summaries, provided by the journalists writing the articles."}
{"sent_id": "45238fe9b493ccdf5921c8f5284097-C001-165", "intents": ["@SIM@"], "paper_id": "ABC_45238fe9b493ccdf5921c8f5284097_13", "text": "The superiority of TCONVS2S is expected; TCONVS2S is better than PTGEN for recognizing pertinent content and generating informative summaries due to its ability to represent high-level document knowledge in terms of topics and long-range dependencies (Narayan et al., 2018b) ."}
{"sent_id": "71a72cfca17b0b15938ed590f9c868-C001-20", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_71a72cfca17b0b15938ed590f9c868_13", "text": "Previous work on learning IS (Nissim, 2006; Rahman and Ng, 2011 ) is restricted in several ways."}
{"sent_id": "71a72cfca17b0b15938ed590f9c868-C001-168", "intents": ["@BACK@"], "paper_id": "ABC_71a72cfca17b0b15938ed590f9c868_13", "text": "One problem when using the SVM Tree kernel as relational classifier is that it allows only for binary classification so that we need to train several binary networks in a one-vs-all paradigm (see also (Rahman and Ng, 2011) ), which will not be able to use the multiclass dependencies of the relational features to optimum effect."}
{"sent_id": "71a72cfca17b0b15938ed590f9c868-C001-129", "intents": ["@DIF@"], "paper_id": "ABC_71a72cfca17b0b15938ed590f9c868_13", "text": "Using such a relational feature catches two birds with one stone: firstly, it integrates the internal structure of a mention into the algorithm, which Rahman and Ng (2011) ignore; secondly, it captures dependencies between parent and child classification, which would not be possible if we integrated the internal structure via flat features or additional tree kernels."}
{"sent_id": "71a72cfca17b0b15938ed590f9c868-C001-106", "intents": ["@USE@"], "paper_id": "ABC_71a72cfca17b0b15938ed590f9c868_13", "text": "We use the following local features, including the features in Nissim (2006) and Rahman and Ng (2011) to be able to gauge how their systems fare on our corpus and as a comparison point for our novel collective classification approach."}
{"sent_id": "71a72cfca17b0b15938ed590f9c868-C001-146", "intents": ["@USE@"], "paper_id": "ABC_71a72cfca17b0b15938ed590f9c868_13", "text": "Following Nissim (2006) and Rahman and Ng (2011) , we perform all experiments on gold standard mentions and use the human WSJ syntactic annotation for feature extraction, when necessary."}
{"sent_id": "71a72cfca17b0b15938ed590f9c868-C001-196", "intents": ["@USE@"], "paper_id": "ABC_71a72cfca17b0b15938ed590f9c868_13", "text": "Since the work reported in this paper relied -following Nissim (2006) and Rahman and Ng (2011) -on gold standard mentions and syntactic annotations, we plan to perform experiments with predicted mentions as well."}
{"sent_id": "03c57679549ff600a024d436d5a107-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_03c57679549ff600a024d436d5a107_13", "text": "Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in Ratinov and Roth (2012) )."}
{"sent_id": "03c57679549ff600a024d436d5a107-C001-82", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_03c57679549ff600a024d436d5a107_13", "text": "Linking is often noisy, so only selecting the high-precision links as in Ratinov and Roth (2012) results in too few matches, while picking an aggregation of all links results in more noise due to lower precision (Rahman and Ng, 2011) ."}
{"sent_id": "03c57679549ff600a024d436d5a107-C001-205", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_03c57679549ff600a024d436d5a107_13", "text": "Ratinov and Roth (2012) extend the multi-sieve coreference model (Raghunathan et al., 2010) by identifying at most a single candidate for each mention, and incorporating high-precision attributes extracted from Wikipedia."}
{"sent_id": "03c57679549ff600a024d436d5a107-C001-177", "intents": ["@USE@"], "paper_id": "ABC_03c57679549ff600a024d436d5a107_13", "text": "To identify the transcripts in the test set, we use the approximation from Ratinov and Roth (2012) that considers a document to be non-transcribed if it contains proper noun mentions and at least a third of those start with a capital letter."}
{"sent_id": "03c57679549ff600a024d436d5a107-C001-175", "intents": ["@DIF@"], "paper_id": "ABC_03c57679549ff600a024d436d5a107_13", "text": "Since these transcripts provide an additional challenge for alignment and coreference, Ratinov and Roth (2012) only use the set of non-transcripts for their evaluation."}
{"sent_id": "03c57679549ff600a024d436d5a107-C001-147", "intents": ["@SIM@"], "paper_id": "ABC_03c57679549ff600a024d436d5a107_13", "text": "This approach is comparable to the fixed alignment model, as in the approaches of Ponzetto and Strube (2006) and Ratinov and Roth (2012) ."}
{"sent_id": "46050691971ea46ce7e18fef5f6d2d-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_46050691971ea46ce7e18fef5f6d2d_13", "text": "Goldberg (2019) adapted the experimental setup of Linzen et al. (2016) , Gulordava et al. (2018) and Marvin and Linzen (2018) to use the cloze test to assess BERT's sensitivity to number agreement in English subject-verb agreement relations."}
{"sent_id": "46050691971ea46ce7e18fef5f6d2d-C001-52", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_46050691971ea46ce7e18fef5f6d2d_13", "text": "' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number (Goldberg, 2019; Gulordava et al., 2018; Linzen et al., 2016) ."}
{"sent_id": "46050691971ea46ce7e18fef5f6d2d-C001-54", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_46050691971ea46ce7e18fef5f6d2d_13", "text": "Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model (Hewitt and Manning, 2019; Goldberg, 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; Clark et al., 2019) ."}
{"sent_id": "46050691971ea46ce7e18fef5f6d2d-C001-95", "intents": ["@EXT@"], "paper_id": "ABC_46050691971ea46ce7e18fef5f6d2d_13", "text": "Our experimental set up is an adaptation of that of Goldberg (2019) ."}
{"sent_id": "46050691971ea46ce7e18fef5f6d2d-C001-148", "intents": ["@DIF@"], "paper_id": "ABC_46050691971ea46ce7e18fef5f6d2d_13", "text": "Second, we used a different evaluation scheme than previous work (Goldberg, 2019) by averaging BERT's predictions over many word types and plan to compare both schemes in future work."}
{"sent_id": "e264c45391853fb008c838aa7ccca8-C001-47", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_e264c45391853fb008c838aa7ccca8_13", "text": "Our method of acquiring hyponymy relations is an extension of the supervised method proposed by Sumida and Torisawa (2008) , but differs in the way of enumerating hyponymy relation candidates (hereafter, HRCs) from the hierarchical layouts, and in the features of machine learning."}
{"sent_id": "e264c45391853fb008c838aa7ccca8-C001-3", "intents": ["@DIF@"], "paper_id": "ABC_e264c45391853fb008c838aa7ccca8_13", "text": "We extract hyponymy relation candidates (HRCs) from the hierachical layouts in Wikipedia by regarding all subordinate items of an item x in the hierachical layouts as x's hyponym candidates, while Sumida and Torisawa (2008) extracted only direct subordinate items of an item x as x's hyponym candidates."}
{"sent_id": "e264c45391853fb008c838aa7ccca8-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_e264c45391853fb008c838aa7ccca8_13", "text": "On the other hand, Sumida and Torisawa (2008) have shown that you could easily obtain numerous hyponymy relations from Wikipedia; in particular, they have acquired more than 0.63 million hyponymy relations only from hierarchical layouts in the 2.2GB Japanese version of Wikipedia (e.g., Figure 1 shows a hierarchical structure of a Wikipedia article shown in Figure 2) ."}
{"sent_id": "e264c45391853fb008c838aa7ccca8-C001-65", "intents": ["@USE@"], "paper_id": "ABC_e264c45391853fb008c838aa7ccca8_13", "text": "In what follows, we briefly review the features proposed by Sumida and Torisawa (2008) , and then explain the novel features introduced in this study."}
{"sent_id": "e264c45391853fb008c838aa7ccca8-C001-79", "intents": ["@USE@"], "paper_id": "ABC_e264c45391853fb008c838aa7ccca8_13", "text": "ATTR Using the attribute set created by Sumida and Torisawa (2008) , when a hypernym/hyponym is included as an element of the attribute set, we set a feature corresponding to the element to 1."}
{"sent_id": "e264c45391853fb008c838aa7ccca8-C001-86", "intents": ["@SIM@"], "paper_id": "ABC_e264c45391853fb008c838aa7ccca8_13", "text": "This reflects Sumida and Torisawa's observation that HRCs whose hypernym matches the patterns are likely to be correct (Sumida and Torisawa, 2008) ."}
{"sent_id": "c3c09df34cf9f81c1cc4fc63a18bf0-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_c3c09df34cf9f81c1cc4fc63a18bf0_13", "text": "It was observed by [Zeng et al., 2015] that 50% of the sentences in the Riedel2010 Distant Supervision dataset [Riedel et al., 2010] , a popular DS benchmark dataset, had 40 or more words in them."}
{"sent_id": "c3c09df34cf9f81c1cc4fc63a18bf0-C001-212", "intents": ["@BACK@"], "paper_id": "ABC_c3c09df34cf9f81c1cc4fc63a18bf0_13", "text": "[ Zheng et al., 2016] aimed to leverage inter-sentence information for relation extraction in a ranking model."}
{"sent_id": "c3c09df34cf9f81c1cc4fc63a18bf0-C001-88", "intents": ["@USE@"], "paper_id": "ABC_c3c09df34cf9f81c1cc4fc63a18bf0_13", "text": "The PCNN layer is applied on the words in the sentence [Zeng et al., 2015] ."}
{"sent_id": "c3c09df34cf9f81c1cc4fc63a18bf0-C001-166", "intents": ["@USE@"], "paper_id": "ABC_c3c09df34cf9f81c1cc4fc63a18bf0_13", "text": "Baselines: We compare proposed models with (a) Piecewise Convolution Neural Network (PCNN) [Zeng et al., 2015] and (b) Neural Relation Extraction with Selective Attention over Instances (NRE) [Lin et al., 2016] ."}
{"sent_id": "c3c09df34cf9f81c1cc4fc63a18bf0-C001-172", "intents": ["@USE@"], "paper_id": "ABC_c3c09df34cf9f81c1cc4fc63a18bf0_13", "text": "We implemented PCNN model baseline following [Zeng et al., 2015] and used author provided results and implementation for NRE baseline."}
{"sent_id": "8ae44e74146d3f40845741fac4dff9-C001-19", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_8ae44e74146d3f40845741fac4dff9_13", "text": "The recent metaphor paraphrasing approach of Shutova (2010) was designed with this requirement in mind and used statistical methods, but still relied on the WordNet (Fellbaum, 1998) database to generate the initial set of paraphrases."}
{"sent_id": "8ae44e74146d3f40845741fac4dff9-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_8ae44e74146d3f40845741fac4dff9_13", "text": "Shutova (2010) used a selectional preference-based model for this purpose, obtaining encouraging results in a supervised setting."}
{"sent_id": "8ae44e74146d3f40845741fac4dff9-C001-89", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_8ae44e74146d3f40845741fac4dff9_13", "text": "To our knowledge, the only metaphor paraphrasing dataset and gold standard available to date is that of Shutova (2010) ."}
{"sent_id": "8ae44e74146d3f40845741fac4dff9-C001-65", "intents": ["@USE@"], "paper_id": "ABC_8ae44e74146d3f40845741fac4dff9_13", "text": "Following Shutova (2010) , we use a selectional preference model to discriminate between literally and metaphorically used substitutes."}
{"sent_id": "8ae44e74146d3f40845741fac4dff9-C001-86", "intents": ["@USE@"], "paper_id": "ABC_8ae44e74146d3f40845741fac4dff9_13", "text": "We thus evaluated the ability of VS on its own to detect literal paraphrases, as well as the effectiveness of the SP model of Shutova (2010) in an unsupervised setting and in combination with VS."}
{"sent_id": "8ae44e74146d3f40845741fac4dff9-C001-132", "intents": ["@USE@"], "paper_id": "ABC_8ae44e74146d3f40845741fac4dff9_13", "text": "At first sight, these improvements of our unsupervised system may not seem very high, in particular when compared to the results of the supervised system of Shutova (2010) ."}
{"sent_id": "2504d707a8123774791d98b755551a-C001-30", "intents": ["@DIF@"], "paper_id": "ABC_2504d707a8123774791d98b755551a_13", "text": "This also enables us to do inference efficiently since our inference time is merely the inference time of two sequential CRF's; in contrast Finkel et al. (2005) reported an increase in running time by a factor of 30 over the sequential CRF, with their Gibbs sampling approximate inference."}
{"sent_id": "2504d707a8123774791d98b755551a-C001-108", "intents": ["@DIF@"], "paper_id": "ABC_2504d707a8123774791d98b755551a_13", "text": "At the same time, the simplicity of our two-stage approach keeps inference time down to just the inference time of two sequential CRFs, when compared to approaches such as those of Finkel et al. (2005) who report that their inference time with Gibbs sampling goes up by a factor of about 30, compared to the Viterbi algorithm for the sequential CRF."}
{"sent_id": "2504d707a8123774791d98b755551a-C001-149", "intents": ["@DIF@"], "paper_id": "ABC_2504d707a8123774791d98b755551a_13", "text": "Additionally, our approach makes it possible to do inference in just about twice the inference time with a single sequential CRF; in contrast, approaches like Gibbs Sampling that model the dependencies directly can increase inference time by a factor of 30 (Finkel et al., 2005 )."}
{"sent_id": "2504d707a8123774791d98b755551a-C001-119", "intents": ["@BACK@"], "paper_id": "ABC_2504d707a8123774791d98b755551a_13", "text": "• Most work has looked to model non-local dependencies only within a document (Finkel et al., 2005; Chieu and Ng, 2002; Sutton and McCallum, 2004; Bunescu and Mooney, 2004) ."}
{"sent_id": "2504d707a8123774791d98b755551a-C001-167", "intents": ["@BACK@"], "paper_id": "ABC_2504d707a8123774791d98b755551a_13", "text": "The approach of Finkel et al. (2005) makes it possible a to model a broader class of longdistance dependencies than Sutton and McCallum (2004) , because they do not need to make any initial assumptions about which nodes should be connected and they too model dependencies between whole token sequences representing entities and between entity token sequences and their token supersequences that are entities."}
{"sent_id": "a3ad95d75b7750b8a879fa183e30f6-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_a3ad95d75b7750b8a879fa183e30f6_13", "text": "Furthermore, with the recent progress on domain translation [13, 14, 15] , conditional synthesis of indomain data without parallel data has become achievable, which makes data augmentation-based adaptation a more promising direction to investigate."}
{"sent_id": "a3ad95d75b7750b8a879fa183e30f6-C001-87", "intents": ["@SIM@"], "paper_id": "ABC_a3ad95d75b7750b8a879fa183e30f6_13", "text": "We now derive two data augmentation methods similar to those proposed in [13] , named nuisance factor replacement and nuisance factor perturbation."}
{"sent_id": "a3ad95d75b7750b8a879fa183e30f6-C001-151", "intents": ["@SIM@"], "paper_id": "ABC_a3ad95d75b7750b8a879fa183e30f6_13", "text": "To verify the superiority of the proposed method of drawing random perturbation vectors, we compare two alternative sampling methods: rev-p and uni-p, similar to [13] , with the same expected squared Euclidean norm as the proposed method."}
{"sent_id": "a3ad95d75b7750b8a879fa183e30f6-C001-146", "intents": ["@USE@"], "paper_id": "ABC_a3ad95d75b7750b8a879fa183e30f6_13", "text": "VAE-DA [13] results with nuisance factor replacement (repl) and latent nuisance perturbation (p) are shown in the last three rows."}
{"sent_id": "4bc12aca138835b5ed80b0cf69febf-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_4bc12aca138835b5ed80b0cf69febf_13", "text": "BERT (Devlin et al., 2018) , for example, trains on the BooksCorpus (Zhu et al., 2015) and English Wikipedia, for a combined 3,200M words."}
{"sent_id": "4bc12aca138835b5ed80b0cf69febf-C001-72", "intents": ["@SIM@"], "paper_id": "ABC_4bc12aca138835b5ed80b0cf69febf_13", "text": "We do this process after the cloze task masking, similar to Devlin et al. (2018) ."}
{"sent_id": "4bc12aca138835b5ed80b0cf69febf-C001-132", "intents": ["@DIF@"], "paper_id": "ABC_4bc12aca138835b5ed80b0cf69febf_13", "text": "Contrary to Devlin et al. (2018) , we do language model fine-tuning in addition to classification finetuning."}
{"sent_id": "4bc12aca138835b5ed80b0cf69febf-C001-161", "intents": ["@USE@"], "paper_id": "ABC_4bc12aca138835b5ed80b0cf69febf_13", "text": "For BERT baselines, we use the process in Devlin et al. (2018) , and use the [CLS] token, without attention, for classification."}
{"sent_id": "518b281a4bc04b3504d3b385a5dc62-C001-4", "intents": ["@EXT@"], "paper_id": "ABC_518b281a4bc04b3504d3b385a5dc62_13", "text": "We extend the MorphoChains system (Narasimhan et al., 2015) to provide morphological analyses that can abstract over spelling differences in functionally similar morphs."}
{"sent_id": "518b281a4bc04b3504d3b385a5dc62-C001-53", "intents": ["@USE@"], "paper_id": "ABC_518b281a4bc04b3504d3b385a5dc62_13", "text": "We predict child-parent pairs using a log-linear model, following Narasimhan et al. (2015) ."}
{"sent_id": "518b281a4bc04b3504d3b385a5dc62-C001-62", "intents": ["@USE@"], "paper_id": "ABC_518b281a4bc04b3504d3b385a5dc62_13", "text": "We use the same neighbourhood functions as Narasimhan et al. (2015) ."}
{"sent_id": "518b281a4bc04b3504d3b385a5dc62-C001-140", "intents": ["@USE@"], "paper_id": "ABC_518b281a4bc04b3504d3b385a5dc62_13", "text": "Hyperparameters In addition to threshold values described above, we use the same λ = 1 (Equation 3) as Narasimhan et al. (2015) ."}
{"sent_id": "518b281a4bc04b3504d3b385a5dc62-C001-147", "intents": ["@USE@"], "paper_id": "ABC_518b281a4bc04b3504d3b385a5dc62_13", "text": "Baselines We compare our model to three other systems: Morfessor 2.0 (Virpioja et al., 2013) , MORSEL (Lignos et al., 2009; Lignos, 2010) and MorphoChains (Narasimhan et al., 2015) ."}
{"sent_id": "518b281a4bc04b3504d3b385a5dc62-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_518b281a4bc04b3504d3b385a5dc62_13", "text": "To identify possible affixes to use as features, Narasimhan et al. (2015) counted the number of words that end (or start) with each substring Table 3 : Examples illustrating which of the binary features in the model are active for various potential child-parent pairs."}
{"sent_id": "c4e2f43e223f61d81d81ac2c9aaa3f-C001-71", "intents": ["@BACK@"], "paper_id": "ABC_c4e2f43e223f61d81d81ac2c9aaa3f_13", "text": "Subspaces (Virtanen, Klami, and Kaski 2011; Bousmalis et al. 2016 ) should allow the model to focus on task-specific and shared features in different parts of its parameter space."}
{"sent_id": "c4e2f43e223f61d81d81ac2c9aaa3f-C001-166", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_c4e2f43e223f61d81d81ac2c9aaa3f_13", "text": "Joint model Most work on MTL for NLP uses a single auxiliary task (Bingel and Søgaard 2017; Martínez Alonso and Plank 2017) ."}
{"sent_id": "c4e2f43e223f61d81d81ac2c9aaa3f-C001-174", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_c4e2f43e223f61d81d81ac2c9aaa3f_13", "text": "Task Properties and Performance Bingel and Søgaard (2017) correlate meta-characteristics of task pairs and gains compared to hard parameter sharing across a large set of NLP task pairs."}
{"sent_id": "c4e2f43e223f61d81d81ac2c9aaa3f-C001-80", "intents": ["@USE@"], "paper_id": "ABC_c4e2f43e223f61d81d81ac2c9aaa3f_13", "text": "We introduce an orthogonality constraint (Bousmalis et al. 2016 ) between the layer-wise subspaces of each model:"}
{"sent_id": "412c2daf6d060f520850d187c6eb36-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_412c2daf6d060f520850d187c6eb36_13", "text": "Yarowsky (1992) introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data."}
{"sent_id": "412c2daf6d060f520850d187c6eb36-C001-54", "intents": ["@SIM@"], "paper_id": "ABC_412c2daf6d060f520850d187c6eb36_13", "text": "Like the thesaurusbased approach of Yarowsky (1992) , our approach relies on the dilution of this noise by their distribution through all the 1792 defining concepts."}
{"sent_id": "412c2daf6d060f520850d187c6eb36-C001-105", "intents": ["@USE@"], "paper_id": "ABC_412c2daf6d060f520850d187c6eb36_13", "text": "Our system is tested on the twelve words discussed in Yarowsky (1992) and previous publications on sense disambiguation."}
{"sent_id": "412c2daf6d060f520850d187c6eb36-C001-146", "intents": ["@USE@"], "paper_id": "ABC_412c2daf6d060f520850d187c6eb36_13", "text": "(thesaurus) marks the column with the results of Yarowsky (1992) tested on the Grolier's Encyclopedia."}
{"sent_id": "412c2daf6d060f520850d187c6eb36-C001-108", "intents": ["@DIF@"], "paper_id": "ABC_412c2daf6d060f520850d187c6eb36_13", "text": "Numerically, the result is not as good as the 92% as reported in Yarowsky (1992) ."}
{"sent_id": "412c2daf6d060f520850d187c6eb36-C001-167", "intents": ["@DIF@"], "paper_id": "ABC_412c2daf6d060f520850d187c6eb36_13", "text": "On the other hand, the thesaurus-based method of Yarowsky (1992) may suffer from loss of information (since it is semi-class-based) as well as data sparseness (since H Classes used in Resnik (1992) are based on the WordNet taxonomy while classes of Brown et al. (1992) and Pereira et al. (1993) are derived from statistical data collected from corpora."}
{"sent_id": "1c89c8f4849d1c8214a3e5f6b9ff1a-C001-27", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14", "text": "Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field (Sproat and Jaitly, 2016) ."}
{"sent_id": "1c89c8f4849d1c8214a3e5f6b9ff1a-C001-52", "intents": ["@USE@"], "paper_id": "ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14", "text": "Following Sproat and Jaitly (2016), we implement a seq2seq model trained on window-based data."}
{"sent_id": "1c89c8f4849d1c8214a3e5f6b9ff1a-C001-96", "intents": ["@USE@"], "paper_id": "ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14", "text": "The data for the window-based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written/speech formatted text from Sproat and Jaitly (2016) ."}
{"sent_id": "1c89c8f4849d1c8214a3e5f6b9ff1a-C001-121", "intents": ["@USE@"], "paper_id": "ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14", "text": "Our first approach replicates the window-based seq2seq model of Sproat and Jaitly (2016) ."}
{"sent_id": "1c89c8f4849d1c8214a3e5f6b9ff1a-C001-181", "intents": ["@USE@"], "paper_id": "ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14", "text": "Our labels are generated directly from the Google FST (Sproat and Jaitly, 2016) ."}
{"sent_id": "1c89c8f4849d1c8214a3e5f6b9ff1a-C001-143", "intents": ["@DIF@"], "paper_id": "ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14", "text": "Data with TELEPHONE labels were not included in the initial analysis of Sproat and Jaitly (2016) , but were made available in the dataset release."}
{"sent_id": "22da24997f66a6dafa911f83f061e5-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_22da24997f66a6dafa911f83f061e5_14", "text": "UKP-Athene (Hanselowski et al., 2018) , the highest document retrieval scoring team, uses MediaWiki API 1 to search the Wikipedia database for the claims noun phrases."}
{"sent_id": "22da24997f66a6dafa911f83f061e5-C001-70", "intents": ["@BACK@"], "paper_id": "ABC_22da24997f66a6dafa911f83f061e5_14", "text": "The UKP-Athene team (Hanselowski et al., 2018) achieved the highest sentence retrieval recall using ESIM and pairwise training."}
{"sent_id": "22da24997f66a6dafa911f83f061e5-C001-122", "intents": ["@USE@"], "paper_id": "ABC_22da24997f66a6dafa911f83f061e5_14", "text": "In addition, we experiment with the modified Hinge loss functions like (Hanselowski et al., 2018) :"}
{"sent_id": "22da24997f66a6dafa911f83f061e5-C001-161", "intents": ["@DIF@"], "paper_id": "ABC_22da24997f66a6dafa911f83f061e5_14", "text": "x shows the UNC, UCL, UPK-Athene, DREAM XLNet, and DREAM RoBERTa scores (Nie et al., 2019; Yoneda et al., 2018; Hanselowski et al., 2018; Zhong et al., 2019) methods surpass the pairwise methods in terms of recall-precision performance."}
{"sent_id": "260489da0fb3f7a201a6a1cce8f03b-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_260489da0fb3f7a201a6a1cce8f03b_14", "text": "End-to-end neural machine translation (NMT) is a newly proposed paradigm for machine translation [Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015] ."}
{"sent_id": "260489da0fb3f7a201a6a1cce8f03b-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_260489da0fb3f7a201a6a1cce8f03b_14", "text": "Such an attentional mechanism has proven to be an effective technique in text generation tasks such as machine translation [Bahdanau et al., 2015; Luong et al., 2015] and image caption generation [Xu et al., 2015] ."}
{"sent_id": "260489da0fb3f7a201a6a1cce8f03b-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_260489da0fb3f7a201a6a1cce8f03b_14", "text": "For example, Bahdanau et al. [2015] use a bidirectional RNN and concatenate the forward and backward states as the hidden state of a source word to capture both forward and backward contexts."}
{"sent_id": "260489da0fb3f7a201a6a1cce8f03b-C001-80", "intents": ["@BACK@", "@USE@", "@EXT@"], "paper_id": "ABC_260489da0fb3f7a201a6a1cce8f03b_14", "text": "GroundHog is an attention-based neural machine translation system [Bahdanau et al., 2015] ."}
{"sent_id": "260489da0fb3f7a201a6a1cce8f03b-C001-117", "intents": ["@BACK@"], "paper_id": "ABC_260489da0fb3f7a201a6a1cce8f03b_14", "text": "Bahdanau et al. [2015] first introduce the attentional mechanism into neural machine translation to enable the decoder to focus on relevant parts of the source sentence during decoding."}
{"sent_id": "8853d810b364ae47a2da71c2502b3e-C001-55", "intents": ["@USE@"], "paper_id": "ABC_8853d810b364ae47a2da71c2502b3e_14", "text": "We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by Plank and Agić (2018) showing that DSDS provides a viable alternative."}
{"sent_id": "8853d810b364ae47a2da71c2502b3e-C001-58", "intents": ["@USE@"], "paper_id": "ABC_8853d810b364ae47a2da71c2502b3e_14", "text": "The wide-coverage Watchtower corpus (WTC) by Agić et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following Plank and Agić (2018) ."}
{"sent_id": "8853d810b364ae47a2da71c2502b3e-C001-110", "intents": ["@USE@"], "paper_id": "ABC_8853d810b364ae47a2da71c2502b3e_14", "text": "The lexicons we use so far are of different sizes (shown in Table 1 of Plank and Agić (2018) ), spanning from 1,000 entries to considerable dictionaries of several hundred thousands entries."}
{"sent_id": "8853d810b364ae47a2da71c2502b3e-C001-74", "intents": ["@SIM@"], "paper_id": "ABC_8853d810b364ae47a2da71c2502b3e_14", "text": "Combining the best of two worlds results in the overall best tagging accuracy, confirming Plank and Agić (2018) : Embedding lexical information into a neural tagger improves tagging accuracy from 83.4 to 84.1 (means over 21 languages)."}
{"sent_id": "bdd0ebe147e277f8f7f04fc351464a-C001-26", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_bdd0ebe147e277f8f7f04fc351464a_14", "text": "The creation of this graph can be considered as completing-as we have previously argued [5] -the Document Planning phase of a typical architecture of a Natural Language Generation (NLG) system [20] ."}
{"sent_id": "bdd0ebe147e277f8f7f04fc351464a-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_bdd0ebe147e277f8f7f04fc351464a_14", "text": "Although this holds true for the general case of Multi-document Summarization, for the case of summarizing evolving events the identification of the similarities and differences should be distinguished, as we have previously argued [1, 2, 4, 5, 6] between two axes: the synchronic and the diachronic axes."}
{"sent_id": "bdd0ebe147e277f8f7f04fc351464a-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_bdd0ebe147e277f8f7f04fc351464a_14", "text": "The name of the relation carries semantic information which, along with the messages that are connected with the relation, are later being exploited by the NLG component (see [5] ) in order to produce the final summary."}
{"sent_id": "bdd0ebe147e277f8f7f04fc351464a-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_bdd0ebe147e277f8f7f04fc351464a_14", "text": "The interested reader is encouraged to consult [1, 2, 4, 5, 6 ] for more information."}
{"sent_id": "bdd0ebe147e277f8f7f04fc351464a-C001-79", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_bdd0ebe147e277f8f7f04fc351464a_14", "text": "The creation of the grid can be considered as completing-as we have previously argued [5] -the Document Planning phase of a typical architecture of an NLG system [20] ."}
{"sent_id": "bdd0ebe147e277f8f7f04fc351464a-C001-136", "intents": ["@USE@"], "paper_id": "ABC_bdd0ebe147e277f8f7f04fc351464a_14", "text": "Nevertheless, in Section 9 of [5] we present specific propositions of how this problem can be alleviated."}
{"sent_id": "cc66b46b34a0d716414e8b845707f9-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_cc66b46b34a0d716414e8b845707f9_14", "text": "Previous work has made significant progress on this task (Chen and Mooney, 2008; Angeli et al., 2010; Konstas and Lapata, 2012) ."}
{"sent_id": "cc66b46b34a0d716414e8b845707f9-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_cc66b46b34a0d716414e8b845707f9_14", "text": "Angeli et al. (2010) propose a unified conceptto-text model that treats joint content selection and surface realization as a sequence of local decisions represented by a log-linear model."}
{"sent_id": "cc66b46b34a0d716414e8b845707f9-C001-147", "intents": ["@USE@"], "paper_id": "ABC_cc66b46b34a0d716414e8b845707f9_14", "text": "We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of Angeli et al. (2010)), respectively (Sec. 5)."}
{"sent_id": "cc66b46b34a0d716414e8b845707f9-C001-152", "intents": ["@DIF@"], "paper_id": "ABC_cc66b46b34a0d716414e8b845707f9_14", "text": "We considered beam search as an alternative to greedy search in our primary setup (Eqn. 1), but this performs worse, similar to what previous work found on this dataset (Angeli et al., 2010) ."}
{"sent_id": "7f234ecfb4cf880502faa8b89cd07b-C001-103", "intents": ["@USE@"], "paper_id": "ABC_7f234ecfb4cf880502faa8b89cd07b_14", "text": "As main tasks, we use chunking (CHUNK), named entity recognition (NER), and a simplified version of semantic role labeling (SRL) where we only identify headwords, and pair them with part-of-speech tagging (POS) as an auxiliary task, following [25] ."}
{"sent_id": "7f234ecfb4cf880502faa8b89cd07b-C001-118", "intents": ["@USE@"], "paper_id": "ABC_7f234ecfb4cf880502faa8b89cd07b_14", "text": "Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model by [25] , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing [6] ; and iv) cross-stitch networks [21] ."}
{"sent_id": "7f234ecfb4cf880502faa8b89cd07b-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_7f234ecfb4cf880502faa8b89cd07b_14", "text": "With loosely related tasks, one task may be better modeled with one hidden layer; another one with two [25] ."}
{"sent_id": "7f234ecfb4cf880502faa8b89cd07b-C001-166", "intents": ["@SIM@"], "paper_id": "ABC_7f234ecfb4cf880502faa8b89cd07b_14", "text": "We see that a) for the low-level simplified SRL, there is more sharing at inner layers, which is in line with [25] , while Chunking and NER also rely on the outer layer, and b) more information is shared from the more complex target tasks than vice versa."}
{"sent_id": "a0af9cf22996a245af9d66cf1d358f-C001-220", "intents": ["@MOT@", "@BACK@", "@FUT@"], "paper_id": "ABC_a0af9cf22996a245af9d66cf1d358f_14", "text": "In addition, we are unable to rule out the possibility that another event attracted attention to the locations under discussion before the crises began (e.g. a political news story relevant to the event's region) Lastly, the study focuses exclusively on location names because of their geographic relevance to events, but other types of named entities (people, organizations) are also likely to undergo changes in descriptor use in response to increased attention (Staliūnaitė et al. 2018) ."}
{"sent_id": "a0af9cf22996a245af9d66cf1d358f-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_a0af9cf22996a245af9d66cf1d358f_14", "text": "Such descriptor phrases provide additional contextual information for named entities (people, organizations and locations) (Staliūnaitė et al. 2018) , helping to locate unfamiliar entities and disambiguate names that could have multiple referents."}
{"sent_id": "a0af9cf22996a245af9d66cf1d358f-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_a0af9cf22996a245af9d66cf1d358f_14", "text": "By looking at public posts written on Twitter concerning natural disasters, we found that the aggregate rate of descriptor phrases decreased following the peaks in these locations' collective attention, supporting prior findings in the change in named entity use (Staliūnaitė et al. 2018) ."}
{"sent_id": "a0af9cf22996a245af9d66cf1d358f-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_a0af9cf22996a245af9d66cf1d358f_14", "text": "The dependent clause may describe attributes of the entity that are relevant to a specific topic, such as \"San Juan, epicenter of Hurricane Maria relief effort,\" or attributes that are generally relevant, such as \"San Juan, Puerto Rico.\" From a collective perspective, prior work that examined the use of descriptor phrases in news media found that writers tend to drop such phrases as the entities gradually become more and more familiar (i.e., shared knowledge) among discussion participants over time (Staliūnaitė et al. 2018) ."}
{"sent_id": "a0af9cf22996a245af9d66cf1d358f-C001-163", "intents": ["@SIM@"], "paper_id": "ABC_a0af9cf22996a245af9d66cf1d358f_14", "text": "To summarize, we found consistently less descriptor use over the course of crisis events even after controlling for other explanatory factors, supporting prior work in long-term descriptor phrase change (Staliūnaitė et al. 2018 )."}
{"sent_id": "8ff1560ac0241a763b4b0d93718b40-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_8ff1560ac0241a763b4b0d93718b40_14", "text": "The production of knowledge bases and the need to answer questions over such resources received researchers attentions to propose different models to find the answer of questions from the knowledge bases, known as KBQA 1 . Answering factoid questions with one relation, also known as simple question answering, has been widely studied in recent years (Dai et al., 2016; Yin et al., 2016; He and Golub, 2016; Yu et al., 2017) ."}
{"sent_id": "8ff1560ac0241a763b4b0d93718b40-C001-94", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_8ff1560ac0241a763b4b0d93718b40_14", "text": "Yin et al. (2016) proposed a new benchmark for evaluating relation extraction task on SimpleQuestion."}
{"sent_id": "8ff1560ac0241a763b4b0d93718b40-C001-128", "intents": ["@BACK@"], "paper_id": "ABC_8ff1560ac0241a763b4b0d93718b40_14", "text": "In fact, the authors of AMPCNN (Yin et al., 2016) , conducted the corresponding experiments on a one-way-attention adaptation of these two models to compare them with the available methods in this task."}
{"sent_id": "8ff1560ac0241a763b4b0d93718b40-C001-58", "intents": ["@USE@"], "paper_id": "ABC_8ff1560ac0241a763b4b0d93718b40_14", "text": "In this regard, following Yin et al. (2016) , we first extract the entity mentions out of question words and put a symbol (e.g. < e >) in its place, so that we will have a question pool in which each question is labeled with its relation that can be considered as a paraphrase of that question."}
{"sent_id": "8ff1560ac0241a763b4b0d93718b40-C001-120", "intents": ["@DIF@"], "paper_id": "ABC_8ff1560ac0241a763b4b0d93718b40_14", "text": "In this table, AMPCNN (Yin et al., 2016) is an attentive max-pooling CNN for matching a question with all relations."}
{"sent_id": "8ff1560ac0241a763b4b0d93718b40-C001-122", "intents": ["@DIF@"], "paper_id": "ABC_8ff1560ac0241a763b4b0d93718b40_14", "text": "These two models Model Accuracy (%) AMPCNN (Yin et al., 2016) 91.3"}
{"sent_id": "8ff1560ac0241a763b4b0d93718b40-C001-124", "intents": ["@DIF@"], "paper_id": "ABC_8ff1560ac0241a763b4b0d93718b40_14", "text": "OWA-ABCNN (Yin et al., 2016) 90.2"}
{"sent_id": "de9eb9b7dff69743252b3ff0ef8894-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_de9eb9b7dff69743252b3ff0ef8894_14", "text": "The Word Embedding Association Test (WEAT) (Caliskan et al., 2017) is an adaptation of the Implicit Association Test (IAT) (Nosek et al., 2002) ."}
{"sent_id": "de9eb9b7dff69743252b3ff0ef8894-C001-59", "intents": ["@USE@"], "paper_id": "ABC_de9eb9b7dff69743252b3ff0ef8894_14", "text": "We first describe the WEAT framework (Caliskan et al., 2017) ."}
{"sent_id": "e68d09937d522dc5acac9637eb2a8b-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_e68d09937d522dc5acac9637eb2a8b_14", "text": "Two baseNP data sets have been put forward by (Ramshaw and Marcus, 1995) ."}
{"sent_id": "e68d09937d522dc5acac9637eb2a8b-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_e68d09937d522dc5acac9637eb2a8b_14", "text": "And third, 1This (Ramshaw and Marcus, 1995) baseNP data set is available via ftp://ftp.cis.upenn.edu/pub/chunker/ 2Software for generating the data is available from http://lcg-www.uia.ac.be/conl199/npb/ with the FZ=I rate which is equal to (2*precision*recall)/(precision+recall)."}
{"sent_id": "e68d09937d522dc5acac9637eb2a8b-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_e68d09937d522dc5acac9637eb2a8b_14", "text": "They have used the (Ramshaw and Marcus, 1995) representation as well (IOB1)."}
{"sent_id": "e68d09937d522dc5acac9637eb2a8b-C001-94", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_e68d09937d522dc5acac9637eb2a8b_14", "text": "Like the data used by (Ramshaw and Marcus, 1995) , this data was retagged by the Brill tagger in order to obtain realistic part-of-speech (POS) tags 3."}
{"sent_id": "26b00c6e5b499eea30e9cef0bbaf9f-C001-38", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_26b00c6e5b499eea30e9cef0bbaf9f_14", "text": "As suggested by Levy et al. (2015) and Salle et al. (2016) , positional contexts (introduced in Levy et al. (2014) ) are a potential solution to poor performance on syntactic analogy tasks."}
{"sent_id": "26b00c6e5b499eea30e9cef0bbaf9f-C001-9", "intents": ["@USE@"], "paper_id": "ABC_26b00c6e5b499eea30e9cef0bbaf9f_14", "text": "In this paper, we focus on improving a state-ofthe-art counting model, LexVec (Salle et al., 2016) , which performs factorization of the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling (WSNS)."}
{"sent_id": "26b00c6e5b499eea30e9cef0bbaf9f-C001-68", "intents": ["@USE@"], "paper_id": "ABC_26b00c6e5b499eea30e9cef0bbaf9f_14", "text": "As recommended in Levy et al. (2015) and used in Salle et al. (2016) , the PPMI matrix used in all LexVec models and in PPMI-SVD is transformed using context distribution smoothing exponentiating context frequencies to the power 0.75."}
{"sent_id": "9e8af6ca401cd74adc9a4137ae05ec-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_9e8af6ca401cd74adc9a4137ae05ec_14", "text": "Toutanova and Moore (2002) extend Brill and Moore (2000) to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling."}
{"sent_id": "9e8af6ca401cd74adc9a4137ae05ec-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_9e8af6ca401cd74adc9a4137ae05ec_14", "text": "Toutanova and Moore (2002) describe an extension to Brill and Moore (2000) where the same noisy channel error model is used to model phone sequences instead of letter sequences."}
{"sent_id": "9e8af6ca401cd74adc9a4137ae05ec-C001-98", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_9e8af6ca401cd74adc9a4137ae05ec_14", "text": "The pronunciationbased spelling correction approach developed in Toutanova and Moore (2002) requires a list of possible pronunciations in order to compare the pronunciation of the misspelling to the pronunciation of correct words."}
{"sent_id": "9e8af6ca401cd74adc9a4137ae05ec-C001-69", "intents": ["@USE@"], "paper_id": "ABC_9e8af6ca401cd74adc9a4137ae05ec_14", "text": "Like Toutanova and Moore (2002) , we use the n-gram LTP model from Fisher (1999) to predict these pronunciations."}
{"sent_id": "9e8af6ca401cd74adc9a4137ae05ec-C001-138", "intents": ["@USE@"], "paper_id": "ABC_9e8af6ca401cd74adc9a4137ae05ec_14", "text": "In order to evaluate the effect of pronunciation variation in Toutanova and Moore (2002) 's spelling correction approach, we compare the performance of the pronunciation model and the combined model with and without pronunciation variation."}
{"sent_id": "967c78ccf905c69d732a4c1ef00289-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_967c78ccf905c69d732a4c1ef00289_14", "text": "The supervised neural model on a single dataset was introduced by Zeng et al. (2014) and followed by many others (Nguyen and Grishman, 2015; Zhou et al., 2016; Miwa and Bansal, 2016; Nguyen and Grishman, 2016; Fu et al., 2017) ."}
{"sent_id": "967c78ccf905c69d732a4c1ef00289-C001-128", "intents": ["@USE@"], "paper_id": "ABC_967c78ccf905c69d732a4c1ef00289_14", "text": "We follow Nguyen and Grishman (2016) to set the position and entity type embedding size to be 50."}
{"sent_id": "17252628fa9c03c2fe0b44763fc7a2-C001-17", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_17252628fa9c03c2fe0b44763fc7a2_14", "text": "The pre-ordering rules can be made manually (Collins et al., 2005; Wang et al., 2007; Han et al., 2012) or extracted automatically from a parallel corpus (Xia and McCord, 2004; Habash, 2007; Zhang et al., 2007; Wu et al., 2011) ."}
{"sent_id": "17252628fa9c03c2fe0b44763fc7a2-C001-20", "intents": ["@DIF@"], "paper_id": "ABC_17252628fa9c03c2fe0b44763fc7a2_14", "text": "Moreover, this rule set substantially decreased the total times of rule application about 60%, compared with a constituent-based approach (Wang et al., 2007) ."}
{"sent_id": "17252628fa9c03c2fe0b44763fc7a2-C001-109", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_17252628fa9c03c2fe0b44763fc7a2_14", "text": "Similar to Wang et al. (2007) , we carried out human evaluations to assess the accuracy of our dependency-based pre-ordering rules by employing the system \"OUR DEP 2\" in Table 1 ."}
{"sent_id": "17252628fa9c03c2fe0b44763fc7a2-C001-115", "intents": ["@SIM@"], "paper_id": "ABC_17252628fa9c03c2fe0b44763fc7a2_14", "text": "The overall accuracy of this rule set is 60.0%, which is almost at the same level as the WR07 rule set (62.1%), according to the similar evaluation (200 sentences and one annotator) conducted in Wang et al. (2007) ."}
{"sent_id": "17252628fa9c03c2fe0b44763fc7a2-C001-93", "intents": ["@USE@"], "paper_id": "ABC_17252628fa9c03c2fe0b44763fc7a2_14", "text": "We implemented the constituent-based preordering rule set in Wang et al. (2007) for comparison, which is called WR07 below."}
{"sent_id": "ab8c43bf5a37c436d166960af459a8-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_ab8c43bf5a37c436d166960af459a8_14", "text": "It has been recently studied in two distinct settings: (1) Rao and Tetreault (2018) addressed the task of Formality Transfer (FT) where given an informal sentence in English, systems are asked to output a formal equivalent, or vice-versa; (2) introduced the task of FormalitySensitive Machine Translation (FSMT), where given a sentence in French and a desired formality level (approximating the intended audience of the translation), systems are asked to produce an English translation of the desired formality level."}
{"sent_id": "ab8c43bf5a37c436d166960af459a8-C001-121", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_ab8c43bf5a37c436d166960af459a8_14", "text": "2 For FT, Rao and Tetreault (2018) show that BLEU correlates well with the overall system ranking assigned by humans."}
{"sent_id": "ab8c43bf5a37c436d166960af459a8-C001-129", "intents": ["@USE@"], "paper_id": "ABC_ab8c43bf5a37c436d166960af459a8_14", "text": "For FT, we compare the top performing NMT benchmark model in Rao and Tetreault (2018) with our best FT model."}
{"sent_id": "ab8c43bf5a37c436d166960af459a8-C001-195", "intents": ["@USE@"], "paper_id": "ABC_ab8c43bf5a37c436d166960af459a8_14", "text": "We manually inspect 100 randomly selected samples from our evaluation set and compare the target-style output of our best model (MultiTask-tag-style) with that of the best baseline model (NMT-Combined) from Rao and Tetreault (2018) ."}
{"sent_id": "ab8c43bf5a37c436d166960af459a8-C001-266", "intents": ["@DIF@"], "paper_id": "ABC_ab8c43bf5a37c436d166960af459a8_14", "text": "On the FT task, the joint model significantly improves the quality of transfer between formal and informal styles in both directions, compared to prior work (Rao and Tetreault, 2018 )."}
{"sent_id": "4f111ff06afd5523d65fc1d1a9ff83-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_4f111ff06afd5523d65fc1d1a9ff83_14", "text": "Consequently, MacCartney et al. (2008) employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation."}
{"sent_id": "4f111ff06afd5523d65fc1d1a9ff83-C001-69", "intents": ["@USE@"], "paper_id": "ABC_4f111ff06afd5523d65fc1d1a9ff83_14", "text": "§3.1) using the training parameters specified in MacCartney et al. (2008) ."}
{"sent_id": "4f111ff06afd5523d65fc1d1a9ff83-C001-47", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_4f111ff06afd5523d65fc1d1a9ff83_14", "text": "We deviate from MacCartney et al. (2008) and do not introduce L2 normalization of weights during learning as this could have an unpredictable effect on the averaged parameters."}
{"sent_id": "4f111ff06afd5523d65fc1d1a9ff83-C001-72", "intents": ["@DIF@"], "paper_id": "ABC_4f111ff06afd5523d65fc1d1a9ff83_14", "text": "We first observe that our reimplemented version of MANLI improves over the results reported in MacCartney et al. (2008) , gaining 2% in precision, 1% in recall and 2-3% in the fraction of alignments that exactly matched human annotations."}
{"sent_id": "d8168f4596878807d22ddc7474ffc8-C001-18", "intents": ["@USE@"], "paper_id": "ABC_d8168f4596878807d22ddc7474ffc8_14", "text": "We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016) ; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from Kann et al. (2018) ; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3)."}
{"sent_id": "d8168f4596878807d22ddc7474ffc8-C001-47", "intents": ["@USE@"], "paper_id": "ABC_d8168f4596878807d22ddc7474ffc8_14", "text": "We use the nine grammars from Eskander et al. (2016 Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars)."}
{"sent_id": "d8168f4596878807d22ddc7474ffc8-C001-55", "intents": ["@USE@"], "paper_id": "ABC_d8168f4596878807d22ddc7474ffc8_14", "text": "We consider the three learning settings in (Eskander et al., 2016) : Standard, Scholarseeded Knowledge and Cascaded."}
{"sent_id": "d8168f4596878807d22ddc7474ffc8-C001-62", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_d8168f4596878807d22ddc7474ffc8_14", "text": "LIMS is the best-on-average AG setup obtained by Eskander et al. (2016) when trained on six languages (English, German, Finnish, Estonian, Turkish and Zulu), which is the Cascaded PrStSu+SM configuration."}
{"sent_id": "d8168f4596878807d22ddc7474ffc8-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_d8168f4596878807d22ddc7474ffc8_14", "text": "Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016 Eskander et al., , 2018 ."}
{"sent_id": "154fd8e6b625eb93da21c09906ee90-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_154fd8e6b625eb93da21c09906ee90_14", "text": "One can also assign interpretations; for example, [27] argue their LAS self-attention heads are differentiated phoneme detectors."}
{"sent_id": "154fd8e6b625eb93da21c09906ee90-C001-122", "intents": ["@DIF@"], "paper_id": "ABC_154fd8e6b625eb93da21c09906ee90_14", "text": "We see that unlike self-attentional LAS [27] , SAN-CTC works respectably even with no position en- coding; in fact, the contribution of position is relatively minor (compare with [21] , where location in an encoder-decoder system improved CER by 3% absolute)."}
{"sent_id": "23119eff3cfd71370e8ad408fc75e1-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_23119eff3cfd71370e8ad408fc75e1_15", "text": "Therefore, Lee et al. (2017) train the model end-to-end by maximizing the following marginal log-likelihood where GOLD(i) are gold antecedents for s i :"}
{"sent_id": "23119eff3cfd71370e8ad408fc75e1-C001-103", "intents": ["@BACK@"], "paper_id": "ABC_23119eff3cfd71370e8ad408fc75e1_15", "text": "(2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017) ."}
{"sent_id": "23119eff3cfd71370e8ad408fc75e1-C001-55", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_23119eff3cfd71370e8ad408fc75e1_15", "text": "Compared with the traditional FFNN approach in Lee et al. (2017) , biaffine attention directly models both the compatibility of s i and s j byŝ j U biŝi and the prior likelihood of s i having an antecedent by v biŝ i ."}
{"sent_id": "23119eff3cfd71370e8ad408fc75e1-C001-81", "intents": ["@DIF@"], "paper_id": "ABC_23119eff3cfd71370e8ad408fc75e1_15", "text": "Based on the results on the development set, λ detection = 0.1 works best from {0.05, 0.1, 0.5, 1.0}. Model is trained with ADAM optimizer (Kingma and Ba, 2015) and converges in around 200K updates, which is faster than that of Lee et al. (2017) ."}
{"sent_id": "23119eff3cfd71370e8ad408fc75e1-C001-97", "intents": ["@DIF@"], "paper_id": "ABC_23119eff3cfd71370e8ad408fc75e1_15", "text": "While Moosavi and Strube (2017) observe that there is a large overlap between the gold mentions of the training and dev (test) sets, we find that our model can correctly detect 1048 mentions which are not detected by Lee et al. (2017) , consisting of 386 mentions existing in training data and 662 mentions not existing in training data."}
{"sent_id": "c5ec401f42f79c4707770dac4f5013-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_c5ec401f42f79c4707770dac4f5013_15", "text": "• First, existing studies of utterance modeling mainly focus on representing utterances by using bidirectional GRU (Xing et al., 2017) or unidirectional GRU (Tian et al., 2017 )."}
{"sent_id": "c5ec401f42f79c4707770dac4f5013-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_c5ec401f42f79c4707770dac4f5013_15", "text": "• Utterance Representations: Bidirectional GRU vs. Unidirectional GRU Xing et al. (2017) utilized a bidirectional GRU and a word-level attention mechanism to transfer word representations to utterance representations."}
{"sent_id": "c5ec401f42f79c4707770dac4f5013-C001-126", "intents": ["@BACK@"], "paper_id": "ABC_c5ec401f42f79c4707770dac4f5013_15", "text": "• WSI and HRAN are proposed by Tian et al. (2017) and Xing et al. (2017) respectively."}
{"sent_id": "c5ec401f42f79c4707770dac4f5013-C001-60", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c5ec401f42f79c4707770dac4f5013_15", "text": "For utterance representation, we consider the advantages of the two state-of-the-art approaches to encoding contextual information for context-sensitive response generation (Xing et al., 2017; Tian et al., 2017) ."}
{"sent_id": "b7a718664f395f048abb3655fb1d8d-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_b7a718664f395f048abb3655fb1d8d_15", "text": "Similarly, when others (Tay et al., 2017; Chen et al., 2017) attempt to replicate the experiments of Tang et al. (2016a) they also produce different results to the original authors."}
{"sent_id": "b7a718664f395f048abb3655fb1d8d-C001-81", "intents": ["@BACK@"], "paper_id": "ABC_b7a718664f395f048abb3655fb1d8d_15", "text": "They did perform a comparison across different languages, domains, corpora types, and different methods; SVM with features (Kiritchenko et al., 2014) , Rec-NN (Dong et al., 2014) , TDLSTM (Tang et al., 2016a) , Memory Neural Network (MNet) (Tang et al., 2016b) and their own attention method."}
{"sent_id": "b7a718664f395f048abb3655fb1d8d-C001-167", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_b7a718664f395f048abb3655fb1d8d_15", "text": "For the LSTMs we initialised the weights using uniform distribution U(0.003, 0.003), used Stochastic Gradient Descent (SGD) a learning rate of 0.01, cross entropy loss, padded and truncated sequence to the length of the maximum sequence in the training dataset as stated in the original paper, and we did not \"set the clipping threshold of softmax layer as 200\" (Tang et al., 2016a) as we were unsure what this meant."}
{"sent_id": "b7a718664f395f048abb3655fb1d8d-C001-183", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_b7a718664f395f048abb3655fb1d8d_15", "text": "We chose these word vectors as they have very different sizes (50 and 300), also they have been shown to perform well in different text types; SSWE for social media (Tang et al., 2016a) and Glove for reviews (Chen et al., 2017) ."}
{"sent_id": "5fc7df69445712a50228d0bf80f30a-C001-25", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_5fc7df69445712a50228d0bf80f30a_15", "text": "In addition, by applying a semi-supervised label propagation procedure (Zhou et al., 2003) , we can also use the tensor embedding method for small sample humor recognition, achieving about 0.7 accuracy with only 10% of known labels on the 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day (Yang et al., 2015) datasets."}
{"sent_id": "5fc7df69445712a50228d0bf80f30a-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_5fc7df69445712a50228d0bf80f30a_15", "text": "State-of-the-art humor recognition algorithms usually require a considerable amount of training data with labels to learn effective features (Yang et al., 2015) ."}
{"sent_id": "5fc7df69445712a50228d0bf80f30a-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_5fc7df69445712a50228d0bf80f30a_15", "text": "The objective of tensor decomposition is to find an approximationŴ of W so Yang et al. (2015) that:Ŵ"}
{"sent_id": "5fc7df69445712a50228d0bf80f30a-C001-46", "intents": ["@DIF@"], "paper_id": "ABC_5fc7df69445712a50228d0bf80f30a_15", "text": "While previous works have employed data crawled from websites (Mihalcea and Strapparava, 2005; Yang et al., 2015) , Twitter (Cattle and Ma, 2016; Reyes et al., 2012) , sitcom subtitles (Bertero and Fung, 2016; Purandare and Litman, 2006) , or the New Yorker Cartoon Caption Contest (Radev et al., 2015; Shahaf et al., 2015) , these datasets are generally not released publicly."}
{"sent_id": "5fc7df69445712a50228d0bf80f30a-C001-105", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_5fc7df69445712a50228d0bf80f30a_15", "text": "Our own implementation of Yang et al. (2015) is included as a baseline."}
{"sent_id": "b13bc55709f5040cf100bd5f466ff2-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_b13bc55709f5040cf100bd5f466ff2_15", "text": "Recent approaches learn to extract prosody embedding from reference speech in an unsupervised manner and use prosody embedding to control the speech style [4, 5] ."}
{"sent_id": "b13bc55709f5040cf100bd5f466ff2-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_b13bc55709f5040cf100bd5f466ff2_15", "text": "Reference speech is encoded to prosody embedding using the reference encoder [4] ."}
{"sent_id": "b13bc55709f5040cf100bd5f466ff2-C001-12", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_b13bc55709f5040cf100bd5f466ff2_15", "text": "( We use the term prosody as defined in earlier work [4] henceforth.) Because there is no available label for prosody, learning to control prosody in TTS is a difficult problem to tackle."}
{"sent_id": "b13bc55709f5040cf100bd5f466ff2-C001-103", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_b13bc55709f5040cf100bd5f466ff2_15", "text": "Previous works [4, 5] used large amounts of data to train the prosodic TTS model (296 hours of data for the multi-speaker model)."}
{"sent_id": "b13bc55709f5040cf100bd5f466ff2-C001-41", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_b13bc55709f5040cf100bd5f466ff2_15", "text": "Although they used the same reference encoder architecture used in earlier work [4] , they did not use p itself for prosody embedding."}
{"sent_id": "d8a250a1a0495ee824837839b74f26-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_d8a250a1a0495ee824837839b74f26_15", "text": "To learn the parser, Wang et al. (2015b) define an algorithm that for each instance in the training data infers the action sequence that convert the input dependency tree into the corresponding AMR graph and train a classifier to predict the actions to be taken during testing."}
{"sent_id": "d8a250a1a0495ee824837839b74f26-C001-119", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_d8a250a1a0495ee824837839b74f26_15", "text": "The key differences to Wang et al. (2015b) are the inclusion of the brown, POSpath, NERpath, prefix and suffix feature types."}
{"sent_id": "d8a250a1a0495ee824837839b74f26-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_d8a250a1a0495ee824837839b74f26_15", "text": "In the following subsections we focus on the differences from previous work and in particular that of Wang et al. (2015b) who introduced the transitionbased dependency-to-AMR paradigm we follow."}
{"sent_id": "d8a250a1a0495ee824837839b74f26-C001-72", "intents": ["@DIF@"], "paper_id": "ABC_d8a250a1a0495ee824837839b74f26_15", "text": "Wang et al. (2015b) use all AMR concepts and relations that appear in the training set as possible parameters (l c and l r ) if they appear in any sentence containing the same lemma as σ 0 and β."}
{"sent_id": "d8a250a1a0495ee824837839b74f26-C001-116", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_d8a250a1a0495ee824837839b74f26_15", "text": "All features used are detailed in Table 2 , largely based on Wang et al. (2015b) ."}
{"sent_id": "b0083488650bc98477fb10a9c5a808-C001-23", "intents": ["@SIM@"], "paper_id": "ABC_b0083488650bc98477fb10a9c5a808_15", "text": "Our model is similar to an architecture used in machine translation described in [13] ."}
{"sent_id": "b0083488650bc98477fb10a9c5a808-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_b0083488650bc98477fb10a9c5a808_15", "text": "The additional connections are marked in blue [13] ."}
{"sent_id": "b0083488650bc98477fb10a9c5a808-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_b0083488650bc98477fb10a9c5a808_15", "text": "As written in Equation 5 , its activation is computed analogously to the other gates [13, 11] ."}
{"sent_id": "b0083488650bc98477fb10a9c5a808-C001-45", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_b0083488650bc98477fb10a9c5a808_15", "text": "We use max-pooling to select the most relevant encoder state whereas [13] uses the last horizontal state of the 2DLSTM."}
{"sent_id": "da8f30113f1126a78cefed06a15076-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_da8f30113f1126a78cefed06a15076_15", "text": "For example, in Das et al. (2018) and Lin et al. (2018) the agent obtains a reward of 1 if the correct answer entity is reached as the final state and 0 otherwise (i.e., R(s T ) = I{e T = e a })."}
{"sent_id": "da8f30113f1126a78cefed06a15076-C001-36", "intents": ["@SIM@"], "paper_id": "ABC_da8f30113f1126a78cefed06a15076_15", "text": "The closest works to ours are the works by Lin et al. (2018) , Zhang et al. (2018) and Das et al. (2018) , which consider the question answering task in a reinforcement learning setting in which the agent always chooses to answer."}
{"sent_id": "da8f30113f1126a78cefed06a15076-C001-127", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_da8f30113f1126a78cefed06a15076_15", "text": "We extend the publicly available implementation of Das et al. (2018) for our experimentation."}
{"sent_id": "da8f30113f1126a78cefed06a15076-C001-46", "intents": ["@USE@"], "paper_id": "ABC_da8f30113f1126a78cefed06a15076_15", "text": "We base our work on the recent reinforcement learning approaches introduced in Das et al. (2018) and Lin et al. (2018) ."}
{"sent_id": "da8f30113f1126a78cefed06a15076-C001-145", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_da8f30113f1126a78cefed06a15076_15", "text": "This resulted in the final QA Score of 47.58%, around 8% higher than standard RL and 12% higher than Das et al. (2018) ."}
{"sent_id": "4adcc28c6d1906d74874b8fca371dc-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_4adcc28c6d1906d74874b8fca371dc_15", "text": "There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Kiros et al., 2015; Socher et al., 2014; Donahue et al., 2015) ."}
{"sent_id": "4adcc28c6d1906d74874b8fca371dc-C001-85", "intents": ["@BACK@"], "paper_id": "ABC_4adcc28c6d1906d74874b8fca371dc_15", "text": "In Tables 1 and 2 we present the ranking results of the baseline models of Kiros et al. (2015) and Vendrov et al. (2016) and our proposed PIVOT and PARALLEL models."}
{"sent_id": "4adcc28c6d1906d74874b8fca371dc-C001-96", "intents": ["@SIM@"], "paper_id": "ABC_4adcc28c6d1906d74874b8fca371dc_15", "text": "In the semantic textual similarity task (STS), we use the textual embeddings from our model to compute the similarity between a pair of sen- (Wieting et al., 2017) − 83.7 84.5 85.0 MLMME (Calixto et al., 2017) VGG19 − 72.7 79.7 VSE (Kiros et al., 2015) VGG19 80.6 82.7 89.6 OE (Vendrov et al., 2016) VGG19 82."}
{"sent_id": "6597d733f13b06f61cb653f86c4460-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_6597d733f13b06f61cb653f86c4460_15", "text": "The permutation test used by Nerbonne and Wiersma (2006) is independent of the type of item whose frequency is measured, treating the items as atomic symbols."}
{"sent_id": "6597d733f13b06f61cb653f86c4460-C001-120", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6597d733f13b06f61cb653f86c4460_15", "text": "For comparison to the experiment conducted by Nerbonne and Wiersma (2006) , the experiment was also run with POS trigrams."}
{"sent_id": "6597d733f13b06f61cb653f86c4460-C001-52", "intents": ["@DIF@"], "paper_id": "ABC_6597d733f13b06f61cb653f86c4460_15", "text": "The principal difference between the work of Nerbonne and Wiersma (2006) and ours is the use of leaf-ancestor paths."}
{"sent_id": "6597d733f13b06f61cb653f86c4460-C001-127", "intents": ["@DIF@"], "paper_id": "ABC_6597d733f13b06f61cb653f86c4460_15", "text": "The average corpus was smaller than the Norwegian L2 English corpora of Nerbonne and Wiersma (2006) , which had two groups, one with 221,000 words and the other with 84,000."}
{"sent_id": "6597d733f13b06f61cb653f86c4460-C001-144", "intents": ["@DIF@"], "paper_id": "ABC_6597d733f13b06f61cb653f86c4460_15", "text": "In fact, even though leaf-ancestor paths should provide finer distinctions than trigrams and thus require more data for detectable significance, the regional corpora presented here were smaller than the Norwegian speakers' corpora in Nerbonne and Wiersma (2006) by up to a factor of 10."}
{"sent_id": "a7f4154081f4045390e662c6e6f3ac-C001-96", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_a7f4154081f4045390e662c6e6f3ac_15", "text": "In the sparse feature system, the highest weighted features are typically those indicating the frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q. This suggests that the system of Durrett and Klein (2014) has the power to pick the right span of a mention to resolve, but then is left to generally pick the most common link target in Wikipedia, which is not always correct."}
{"sent_id": "a7f4154081f4045390e662c6e6f3ac-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_a7f4154081f4045390e662c6e6f3ac_15", "text": "The indicator features f Q and f E are described in more detail in Durrett and Klein (2014) ."}
{"sent_id": "a7f4154081f4045390e662c6e6f3ac-C001-55", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_a7f4154081f4045390e662c6e6f3ac_15", "text": "Following Durrett and Klein (2014) , we introduce a latent variable q to capture which subset of a mention (known as a query) we resolve."}
{"sent_id": "a7f4154081f4045390e662c6e6f3ac-C001-87", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_a7f4154081f4045390e662c6e6f3ac_15", "text": "We can also compare to two ablations: using just the sparse features (a system which is a direct extension of Durrett and Klein (2014) ) or using just the CNNderived features."}
{"sent_id": "a7f4154081f4045390e662c6e6f3ac-C001-86", "intents": ["@DIF@"], "paper_id": "ABC_a7f4154081f4045390e662c6e6f3ac_15", "text": "We see that this system outperforms the results of Durrett and Klein (2014) and the AIDA-LIGHT system of Nguyen et al. (2014) ."}
{"sent_id": "a7f4154081f4045390e662c6e6f3ac-C001-75", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_a7f4154081f4045390e662c6e6f3ac_15", "text": "This corpus was used in Fahrni and Strube (2014) and Durrett and Klein (2014) ."}
{"sent_id": "57e65909baf823ff00a9a10a64fffd-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_57e65909baf823ff00a9a10a64fffd_15", "text": "To account for potential bias in the previous dataset, Waseem (2016) relabeled 2876 tweets in the dataset, along with a new sample from the tweets originally collected."}
{"sent_id": "57e65909baf823ff00a9a10a64fffd-C001-151", "intents": ["@SIM@"], "paper_id": "ABC_57e65909baf823ff00a9a10a64fffd_15", "text": "We see similar results for Waseem and Hovy (2016) and Waseem (2016) ."}
{"sent_id": "c3f6140bd69d1eef0124665e651c0c-C001-47", "intents": ["@DIF@"], "paper_id": "ABC_c3f6140bd69d1eef0124665e651c0c_15", "text": "Unigrams (u B ): Klebanov et al. (2014) use all content word forms as features without stemming or lemmatization."}
{"sent_id": "c3f6140bd69d1eef0124665e651c0c-C001-123", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_c3f6140bd69d1eef0124665e651c0c_15", "text": "Besides, the addition of N always leads to more balanced models, by compensating for the relatively lower precision of B. Due to the lack of a separate test set, as in the original setup by Klebanov et al. (2014) , and to the high dimensionality of B's lexicalized features, we cannot rule out over-fitting as an explanation for the relatively good performance of B on this benchmark."}
{"sent_id": "c3f6140bd69d1eef0124665e651c0c-C001-42", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c3f6140bd69d1eef0124665e651c0c_15", "text": "As a baseline, we use a set of features very similar to the one proposed by Klebanov et al. (2014) ."}
{"sent_id": "c3f6140bd69d1eef0124665e651c0c-C001-120", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c3f6140bd69d1eef0124665e651c0c_15", "text": "We use the same 12-fold data split as Klebanov et al. (2014) , and also in this case we perform a grid-search to optimize the meta-parameter C of the logistic regression classifier."}
{"sent_id": "34b73a56bd9b80dc415ca2c5608596-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_34b73a56bd9b80dc415ca2c5608596_15", "text": "Recent work using word embeddings-low-dimensional vector representations of words trained on large datasets to capture key semantic informationhas demonstrated that language encodes several gender, racial, and other common contemporary biases that correlate with both implicit biases (Caliskan et al., 2017) and macro-scale historical trends (Garg et al., 2018) ."}
{"sent_id": "34b73a56bd9b80dc415ca2c5608596-C001-47", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_34b73a56bd9b80dc415ca2c5608596_15", "text": "In particular, we compute linguistic bias scores for two analyses presented in (Garg et al., 2018) : the extent to which female versus male words are semantically similar to occupation-related words, and the extent to which Asian vs. White last names are semantically similar to the same, from 1910 through 1990."}
{"sent_id": "34b73a56bd9b80dc415ca2c5608596-C001-50", "intents": ["@SIM@"], "paper_id": "ABC_34b73a56bd9b80dc415ca2c5608596_15", "text": "The correlation between our scores and changes in workforce participation rates are similar to the correlation between the scores from (Garg et al., 2018) and the same (r = 0.8, p = 0.01 and r = 0.81, p < 0.01, respectively, for gender occupation bias; r = 0.84, p < 0.01 and r = 0.79, p = 0.01, respectively, for Asian/White occupation bias)."}
{"sent_id": "34b73a56bd9b80dc415ca2c5608596-C001-71", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_34b73a56bd9b80dc415ca2c5608596_15", "text": "To compute refugee bias scores with respect to the attribute set A, we use the relative norm distance metric from (Garg et al., 2018) :"}
{"sent_id": "34b73a56bd9b80dc415ca2c5608596-C001-87", "intents": ["@DIF@"], "paper_id": "ABC_34b73a56bd9b80dc415ca2c5608596_15", "text": "One possible reason for this is that the median number of words for each day in the talk radio corpus is 4 million-over 5x fewer than a median of 22 million words per decade used to train each decade-specific model in (Garg et al., 2018) ."}
{"sent_id": "93cf4d4fd9cd875e8e148d1bbd8e2c-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_93cf4d4fd9cd875e8e148d1bbd8e2c_15", "text": "Hierarchical Co-Attention Network (Lu et al., 2016) generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge 1 as of the time of this submission."}
{"sent_id": "93cf4d4fd9cd875e8e148d1bbd8e2c-C001-111", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_93cf4d4fd9cd875e8e148d1bbd8e2c_15", "text": "HieCoAtt-W (Lu et al., 2016) 0.246 ± 0.004 HieCoAtt-P (Lu et al., 2016) 0.256 ± 0.004 HieCoAtt-Q (Lu et al., 2016) 0.264 ± 0.004 Table 2 : Mean rank-correlation coefficients (higher is better); error bars show standard error of means."}
{"sent_id": "9ea14a9fe422451901ad221bee5714-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_9ea14a9fe422451901ad221bee5714_15", "text": "In Sanskrit, Krishna et al. (2016) have proposed a framework for semantic type classification of compounds in Sanskrit."}
{"sent_id": "9ea14a9fe422451901ad221bee5714-C001-47", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_9ea14a9fe422451901ad221bee5714_15", "text": "But our model relies on distributed representations or embeddings of the input as features, instead of the linguistically involved feature set proposed in Krishna et al. (2016) ."}
{"sent_id": "9ea14a9fe422451901ad221bee5714-C001-37", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_9ea14a9fe422451901ad221bee5714_15", "text": "The best system of ours, an end-to-end LSTM architecture initialised with fasttext embeddings has shown promising results in terms of F-score (0.73) compared to the state of the art classifier from Krishna et al. (2016) (0.74) and outperformed it in terms of accuracy (77.68%)."}
{"sent_id": "9ea14a9fe422451901ad221bee5714-C001-62", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_9ea14a9fe422451901ad221bee5714_15", "text": "Similar to prior computational approaches in Sanskrit compounding (Krishna et al., 2016; Kumar et al., 2010) , we follow this four class coarse level categorization of the semantic classes in compounds."}
{"sent_id": "9ea14a9fe422451901ad221bee5714-C001-148", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_9ea14a9fe422451901ad221bee5714_15", "text": "a class, Krishna et al. (2016) down-sampled it to 4,000, which takes it close to the count of the second most highly populated class Bahuvrīhi."}
{"sent_id": "b86a5a8ec1f27354057bb45ff27588-C001-20", "intents": ["@SIM@"], "paper_id": "ABC_b86a5a8ec1f27354057bb45ff27588_15", "text": "Similar to (Elfardy and Diab, 2013) , we present a sentence-level classifier that is trained in a supervised manner."}
{"sent_id": "b86a5a8ec1f27354057bb45ff27588-C001-113", "intents": ["@SIM@"], "paper_id": "ABC_b86a5a8ec1f27354057bb45ff27588_15", "text": "In addition, we have implemented a range of so-called 'meta' features similar to the ones defined in (Elfardy and Diab, 2013) ."}
{"sent_id": "4d528117dd7751d0cd6413430e1ec1-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_4d528117dd7751d0cd6413430e1ec1_15", "text": "Zhang et al. (2017) utilize adversarial training to obtain cross-lingual word embeddings without any parallel data."}
{"sent_id": "4d528117dd7751d0cd6413430e1ec1-C001-85", "intents": ["@SIM@"], "paper_id": "ABC_4d528117dd7751d0cd6413430e1ec1_15", "text": "In this section, our experiments focus on smallscale datasets and our main baseline model is adversarial autoencoder (Zhang et al., 2017) ."}
{"sent_id": "4d528117dd7751d0cd6413430e1ec1-C001-96", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4d528117dd7751d0cd6413430e1ec1_15", "text": "The baseline models are MonoGiza system (Dou et al., 2015) , translation matrix (TM) (Mikolov et al., 2013) , isometric alignment (IA) (Zhang et al., 2016b) and adversarial training approach (Zhang et al., 2017) ."}
{"sent_id": "70dc108166d6b5fb9da39c451c3229-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_70dc108166d6b5fb9da39c451c3229_16", "text": "The dataset for the shared task was introduced by Thorne et al. (2018) and consists of 185,445 claims."}
{"sent_id": "70dc108166d6b5fb9da39c451c3229-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_70dc108166d6b5fb9da39c451c3229_16", "text": "Thorne et al. (2018) used the document retrieval component from the DrQA system (Chen et al., 2017) , which returns the k nearest documents for a query using cosine similarity between binned unigram and bigram TF-IDF vectors."}
{"sent_id": "70dc108166d6b5fb9da39c451c3229-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_70dc108166d6b5fb9da39c451c3229_16", "text": "Thorne et al. (2018) used the decomposable attention model (Parikh et al., 2016) for this task."}
{"sent_id": "70dc108166d6b5fb9da39c451c3229-C001-56", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_70dc108166d6b5fb9da39c451c3229_16", "text": "For sentence selection, we used the modified document retrieval component of DrQA (Chen et al., 2017) to select sentences using bigram TF-IDF with binning as proposed by (Thorne et al., 2018) ."}
{"sent_id": "195f41862b929318787aad9d8e5a1c-C001-2", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_195f41862b929318787aad9d8e5a1c_16", "text": "In this technical report, we introduce FastFusionNet, an efficient variant of FusionNet [12] ."}
{"sent_id": "195f41862b929318787aad9d8e5a1c-C001-53", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_195f41862b929318787aad9d8e5a1c_16", "text": "Here we introduce FastFusionNet which addresses the inference bottlenecks of FusionNet [12] ."}
{"sent_id": "195f41862b929318787aad9d8e5a1c-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_195f41862b929318787aad9d8e5a1c_16", "text": "Figure 2 provides an analysis of the individual components of FusionNet that the contextual embedding layer, i.e. CoVe [21] , with several layers of wide LSTMs, takes up to 35.5% of the inference time while only contributing a 1.1% improvement of F1 Score (from 82.5% to 83.6%) Huang et al. [12] ."}
{"sent_id": "195f41862b929318787aad9d8e5a1c-C001-63", "intents": ["@SIM@"], "paper_id": "ABC_195f41862b929318787aad9d8e5a1c_16", "text": "Like others [12] we use a randomly initialized the trainable embedding layer with 12 dimensions for POS tags and 8 dimensions for NER."}
{"sent_id": "f587fc2bbbf3c1327b03d556e4bc05-C001-38", "intents": ["@SIM@"], "paper_id": "ABC_f587fc2bbbf3c1327b03d556e4bc05_16", "text": "Figure 1 illustrates that point, we do not see a clear interpretation of why the paragraph vectors (Le and Mikolov, 2014) are positioned in the space as in figure 1 ."}
{"sent_id": "6683d7b77f536b93416d985414afeb-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_6683d7b77f536b93416d985414afeb_16", "text": "Recent work (Barbieri et al., 2017) has shown that textual information can be used to predict emojis associated to text."}
{"sent_id": "6683d7b77f536b93416d985414afeb-C001-17", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_6683d7b77f536b93416d985414afeb_16", "text": "Our task and experimental framework are similar to (Barbieri et al., 2017) , however, we use different data (Instagram instead of Twitter) and, in addition, we rely on images to improve the selection of the most likely emojis to associate to a post."}
{"sent_id": "6683d7b77f536b93416d985414afeb-C001-32", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_6683d7b77f536b93416d985414afeb_16", "text": "We extend the experimental scheme of Barbieri et al. (2017) , by considering also visual information when modeling posts."}
{"sent_id": "6683d7b77f536b93416d985414afeb-C001-29", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6683d7b77f536b93416d985414afeb_16", "text": "In the experiments we also considered the subsets of the 10 (238,646 posts) and 5 most frequent emojis (184,044 posts) (similarly to the approach followed by Barbieri et al. (2017) )."}
{"sent_id": "6683d7b77f536b93416d985414afeb-C001-64", "intents": ["@SIM@"], "paper_id": "ABC_6683d7b77f536b93416d985414afeb_16", "text": "In the first experiment (Section 4.2) we compare the FastText model with the state of the art on emoji classification (B-LSTM) by Barbieri et al. (2017) ."}
{"sent_id": "6683d7b77f536b93416d985414afeb-C001-78", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6683d7b77f536b93416d985414afeb_16", "text": "To compare the FastText model with the word and character based B-LSTMs presented by Barbieri et al. (2017) , we consider the same three emoji prediction tasks they proposed: top-5, top-10 and top-20 emojis most frequently used in their Tweet datasets."}
{"sent_id": "2adb3a645a57b8f441a80bb5a46045-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_2adb3a645a57b8f441a80bb5a46045_16", "text": "For instance, in studies performed on the text genres web debate forums (Somasundaran and Wiebe, 2010; Anand et al., 2011; Walker et al., 2012; Hasan and Ng, 2013) , news paper text (Ferreira and Vlachos, 2016; Fake News Challenge, 2017) and tweets (Augenstein et al., 2016; Mohammad et al., 2017) ."}
{"sent_id": "2adb3a645a57b8f441a80bb5a46045-C001-154", "intents": ["@BACK@"], "paper_id": "ABC_2adb3a645a57b8f441a80bb5a46045_16", "text": "The eight annotators that classified each tweet in the study by Mohammad et al. (2017) were employed through a crowdsourcing platform, which was made possible by that the stance targets were chosen with the criterion that they should be commonly known in the United States."}
{"sent_id": "2adb3a645a57b8f441a80bb5a46045-C001-101", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_2adb3a645a57b8f441a80bb5a46045_16", "text": "This follows the approach of Mohammad et al. (2017) , as well as of many of the previously performed vaccine sentiment studies."}
{"sent_id": "2adb3a645a57b8f441a80bb5a46045-C001-177", "intents": ["@SIM@"], "paper_id": "ABC_2adb3a645a57b8f441a80bb5a46045_16", "text": "For instance, features constructed using an arguing lexicon (Somasun-daran and Wiebe, 2010), or word embeddings constructed in an unsupervised fashion using a large corpus from the same text genre as the text to classify (Mohammad et al., 2017) ."}
{"sent_id": "6a90ecc147618b3909609fc6c2e2b3-C001-120", "intents": ["@BACK@"], "paper_id": "ABC_6a90ecc147618b3909609fc6c2e2b3_16", "text": "It is however, quite short of the 56.1% accuracy achieved by the model of Baldridge (2008) that uses grammar informed initialization (combination of category based initialization along with category transition rules)."}
{"sent_id": "6a90ecc147618b3909609fc6c2e2b3-C001-66", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_6a90ecc147618b3909609fc6c2e2b3_16", "text": "In this experiment, we use the training and test sets used by Baldridge (2008) from CCGbank."}
{"sent_id": "6a90ecc147618b3909609fc6c2e2b3-C001-105", "intents": ["@USE@"], "paper_id": "ABC_6a90ecc147618b3909609fc6c2e2b3_16", "text": "We draw the initial sample of CCG tag sequences corresponding to the observation sequence, using probabilities based on grammar informed initialization (Baldridge, 2008) ."}
{"sent_id": "6a90ecc147618b3909609fc6c2e2b3-C001-150", "intents": ["@SIM@"], "paper_id": "ABC_6a90ecc147618b3909609fc6c2e2b3_16", "text": "Despite this, the FHMMs are suited for estimating models with less supervision, such as from tag dictionaries alone and incorporating more informative prior distributions such as those in Baldridge (2008) ."}
{"sent_id": "e7c947a02bb0e81d6b6b4b9da74024-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_e7c947a02bb0e81d6b6b4b9da74024_16", "text": "Recently, some work has been done to reduce the gender bias in word embeddings, both as a post-processing step (Bolukbasi et al., 2016b) and as part of the training procedure (Zhao et al., 2018) ."}
{"sent_id": "e7c947a02bb0e81d6b6b4b9da74024-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_e7c947a02bb0e81d6b6b4b9da74024_16", "text": "Both Bolukbasi et al. (2016b) and Zhao et al. (2018) propose methods for debiasing word embeddings, substantially reducing the bias according to the suggested definition."}
{"sent_id": "e7c947a02bb0e81d6b6b4b9da74024-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_e7c947a02bb0e81d6b6b4b9da74024_16", "text": "These works implicitly define what is good gender debiasing: according to Bolukbasi et al. (2016b) , there is no gender bias if each nonexplicitly gendered word in the vocabulary is in equal distance to both elements of all explicitly gendered pairs."}
{"sent_id": "e7c947a02bb0e81d6b6b4b9da74024-C001-95", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_e7c947a02bb0e81d6b6b4b9da74024_16", "text": "Professions We consider the list of professions used in Bolukbasi et al. (2016b) and Zhao et al. (2018) 10 in light of the neighbours-based bias definition."}
{"sent_id": "09dad2fd96cd1d48936cd5b99a38e7-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_09dad2fd96cd1d48936cd5b99a38e7_16", "text": "These two limitations define the matching problem of F-Score (Rosenberg and Hirschberg, 2007) which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality."}
{"sent_id": "09dad2fd96cd1d48936cd5b99a38e7-C001-118", "intents": ["@BACK@"], "paper_id": "ABC_09dad2fd96cd1d48936cd5b99a38e7_16", "text": "This happens when each GS class is included in all clusters with a distribution equal to the distribution of sizes (Rosenberg and Hirschberg, 2007) ."}
{"sent_id": "09dad2fd96cd1d48936cd5b99a38e7-C001-25", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_09dad2fd96cd1d48936cd5b99a38e7_16", "text": "Subsequently, we present the use of V-measure (Rosenberg and Hirschberg, 2007) as an evaluation measure that can overcome the current limitations of F-Score."}
{"sent_id": "09dad2fd96cd1d48936cd5b99a38e7-C001-46", "intents": ["@SIM@"], "paper_id": "ABC_09dad2fd96cd1d48936cd5b99a38e7_16", "text": "As it can be observed, F-Score assesses the quality of a clustering solution by considering two different angles, i.e. homogeneity and completeness (Rosenberg and Hirschberg, 2007) ."}
{"sent_id": "09dad2fd96cd1d48936cd5b99a38e7-C001-108", "intents": ["@SIM@"], "paper_id": "ABC_09dad2fd96cd1d48936cd5b99a38e7_16", "text": "Overall, in accordance with the convention of 1 being desirable and 0 undesirable, the homogeneity (h) of a clustering solution is 1 if there is only a single class, and 1− H(GS|C) H(GS) in any other case (Rosenberg and Hirschberg, 2007) ."}
{"sent_id": "7616f6f8c1c188b32cd3a8374b61dd-C001-59", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_7616f6f8c1c188b32cd3a8374b61dd_16", "text": "It is precisely for this reason that we adopt the annotation style of Vadas and Curran (2007) for the NLP Resource Metadata Questions Treebank (henceforth abbreviated as NLP-QT)."}
{"sent_id": "7616f6f8c1c188b32cd3a8374b61dd-C001-26", "intents": ["@SIM@"], "paper_id": "ABC_7616f6f8c1c188b32cd3a8374b61dd_16", "text": "Section 3 will introduce the Vadas and Curran (2007) annotation style and will motivate why it is appropriate for the QA system envisaged here."}
{"sent_id": "7616f6f8c1c188b32cd3a8374b61dd-C001-84", "intents": ["@SIM@"], "paper_id": "ABC_7616f6f8c1c188b32cd3a8374b61dd_16", "text": "As in the experiments shown in the previous subsection, the performance with a model trained purely on Penn Treebank data (with NPs annotated in the Vadas and Curran (2007) style) serves as a baseline (the model is called np-wsj in the table)."}
{"sent_id": "7616f6f8c1c188b32cd3a8374b61dd-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_7616f6f8c1c188b32cd3a8374b61dd_16", "text": "From the point of view of semantic interpretation, the more contoured Vadas and Curran (2007) annotation style is to be preferred since it reflects the type of answer that is required, namely materials for second language acquisition, but not for example acquisition materials for second language, or the second (batch) of language acquisition materials."}
{"sent_id": "2bb41cea97a0375f67eab3a77c3a97-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_2bb41cea97a0375f67eab3a77c3a97_16", "text": "While the largest corpus (Wikipedia and New York Times) employed by recent work on distant supervision (Mintz et al., 2009; Hoffmann et al., 2011) contain about 2M documents, we run experiments on a 100M-document (50X more) corpus drawn from ClueWeb."}
{"sent_id": "2bb41cea97a0375f67eab3a77c3a97-C001-86", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_2bb41cea97a0375f67eab3a77c3a97_16", "text": "As in previous work, we impose the constraint that both mentions (m 1 , m 2 ) ∈ R + i are contained in the same sentence (Mintz et al., 2009; Hoffmann et al., 2011) ."}
{"sent_id": "2bb41cea97a0375f67eab3a77c3a97-C001-91", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_2bb41cea97a0375f67eab3a77c3a97_16", "text": "Following recent work on distant supervision (Mintz et al., 2009; Hoffmann et al., 2011) , we use both lexical and syntactic features."}
{"sent_id": "4f75f73b4eac8aecdde9312a846a1d-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_4f75f73b4eac8aecdde9312a846a1d_16", "text": "To provide the awareness of errors in mt originating from src, the transformer architecture (Vaswani et al., 2017) , which is built solely upon attention mechanisms (Bahdanau et al., 2015) , makes it possible to model dependencies without regard to their distance in the input or output sequences and also captures global dependencies between input and output (for our case src, mt, and pe)."}
{"sent_id": "4f75f73b4eac8aecdde9312a846a1d-C001-55", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_4f75f73b4eac8aecdde9312a846a1d_16", "text": "Apart from enc src and enc mt , each of which is equivalent to the original transformer's encoder (Vaswani et al., 2017) , we use a joint encoder with an equivalent architecture, to maintain the homogeneity of the transformer model."}
{"sent_id": "4f75f73b4eac8aecdde9312a846a1d-C001-95", "intents": ["@SIM@"], "paper_id": "ABC_4f75f73b4eac8aecdde9312a846a1d_16", "text": "This is a similar setting to Vaswani et al. (2017) 's C − model 1 ."}
{"sent_id": "4f75f73b4eac8aecdde9312a846a1d-C001-103", "intents": ["@SIM@"], "paper_id": "ABC_4f75f73b4eac8aecdde9312a846a1d_16", "text": "For encoding the word order, our model uses learned positional embeddings (Gehring et al., 2017) , since Vaswani et al. (2017) reported nearly identical results to sinusoidal encodings."}
{"sent_id": "c5a10f46c253f0da005622661b12a1-C001-25", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_c5a10f46c253f0da005622661b12a1_16", "text": "We developed a seed-based system (Section 3), based on a state-of-the-art fully-supervised event extraction system (Li et al., 2013) ."}
{"sent_id": "c5a10f46c253f0da005622661b12a1-C001-62", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_c5a10f46c253f0da005622661b12a1_16", "text": "To implement the seed-based approach for trigger labeling, we adapt only the trigger classification part in the Li et al. (2013) fully-supervised system, ignoring arguments."}
{"sent_id": "c5a10f46c253f0da005622661b12a1-C001-77", "intents": ["@SIM@"], "paper_id": "ABC_c5a10f46c253f0da005622661b12a1_16", "text": "We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by Li et al. (2013) (Section 3) ."}
{"sent_id": "c5a10f46c253f0da005622661b12a1-C001-78", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c5a10f46c253f0da005622661b12a1_16", "text": "To maintain comparability, we use the ACE-2005 documents with the same split as in (Ji and Grishman, 2008; Liao and Grishman, 2010b; Li et al., 2013) to 40 test documents and 559 training documents."}
{"sent_id": "4da1c39dbbeaa2c9dac22118d0c698-C001-3", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_4da1c39dbbeaa2c9dac22118d0c698_16", "text": "It extends previous work by Barbu (2015) through incorporating recall-based machine translation and part-of-speech-tagging features."}
{"sent_id": "4da1c39dbbeaa2c9dac22118d0c698-C001-16", "intents": ["@SIM@"], "paper_id": "ABC_4da1c39dbbeaa2c9dac22118d0c698_16", "text": "In Section 3, we describe our method and, in Section 4, show how it compares to Barbu's (2015) approach as well as other submissions to this shared task."}
{"sent_id": "4da1c39dbbeaa2c9dac22118d0c698-C001-72", "intents": ["@SIM@"], "paper_id": "ABC_4da1c39dbbeaa2c9dac22118d0c698_16", "text": "Our feature extraction pipeline, including Barbu's (2015) as well as our own features (see Section 3.1), is implemented in Scala."}
{"sent_id": "4da1c39dbbeaa2c9dac22118d0c698-C001-90", "intents": ["@SIM@"], "paper_id": "ABC_4da1c39dbbeaa2c9dac22118d0c698_16", "text": "Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by Barbu (2015) (Baseline 2)."}
{"sent_id": "4da1c39dbbeaa2c9dac22118d0c698-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_4da1c39dbbeaa2c9dac22118d0c698_16", "text": "As outlined above, comparing machine translated source segments to their actual target segments has proven effective in Barbu's (2015) experiments."}
{"sent_id": "fa5413db2c8e0a32bc3805d25cd0e7-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_fa5413db2c8e0a32bc3805d25cd0e7_16", "text": "Based on BERT, Lample and Conneau (2019) propose a cross-lingual version called XLM and reach the state-of-the-art performance on some crosslingual NLP tasks including cross-lingual classification , machine translation, and unsupervised cross-lingual word embedding."}
{"sent_id": "fa5413db2c8e0a32bc3805d25cd0e7-C001-100", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_fa5413db2c8e0a32bc3805d25cd0e7_16", "text": "Following Devlin et al. (2018) ; Lample and Conneau (2019) , in our CMLM objective, we randomly sample 15% of the BPE ngrams from the text streams, and replace them by [MASK] tokens 70% of the time."}
{"sent_id": "fa5413db2c8e0a32bc3805d25cd0e7-C001-128", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_fa5413db2c8e0a32bc3805d25cd0e7_16", "text": "Our CMLM is optimized based on the pre-trained models released by Lample and Conneau (2019) 1 , which are trained with Wikipedia dumps."}
{"sent_id": "fa5413db2c8e0a32bc3805d25cd0e7-C001-175", "intents": ["@SIM@"], "paper_id": "ABC_fa5413db2c8e0a32bc3805d25cd0e7_16", "text": "We compare the context-unaware method (i.e., directly calculating the similarity scores between unsupervised cross-lingual embeddings (Artetxe et al., 2018a ) of source and target words), XLM (Lample and Conneau, 2019) and our proposed CMLM pre-training method in the Table 3 ."}
{"sent_id": "fa5413db2c8e0a32bc3805d25cd0e7-C001-157", "intents": ["@FUT@"], "paper_id": "ABC_fa5413db2c8e0a32bc3805d25cd0e7_16", "text": "As a result, we can further improve the translation performance significantly, compared with Lample and Conneau (2019) (with the significance level of p<0.01)."}
{"sent_id": "43a52325987ea035136a6a718389d9-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_43a52325987ea035136a6a718389d9_16", "text": "Since manual classification is costly (Kipper et al., 2008) automatic approaches have been proposed recently which could be used to learn novel classifications in a cost-effective manner (Joanis et al., 2008; Li and Brew, 2008; Ó Séaghdha and Copestake, 2008; Vlachos et al., 2009; Sun and Korhonen, 2009 )."}
{"sent_id": "43a52325987ea035136a6a718389d9-C001-28", "intents": ["@USE@"], "paper_id": "ABC_43a52325987ea035136a6a718389d9_16", "text": "We take a recent verb clustering approach developed for English (Sun and Korhonen, 2009 ) and apply it to French -a major language for which no such experiment has been conducted yet."}
{"sent_id": "43a52325987ea035136a6a718389d9-C001-114", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_43a52325987ea035136a6a718389d9_16", "text": "We employ the same measures for evaluation as previously employed e.g. byÓ Séaghdha and Copestake (2008) and Sun and Korhonen (2009) ."}
{"sent_id": "43a52325987ea035136a6a718389d9-C001-130", "intents": ["@SIM@"], "paper_id": "ABC_43a52325987ea035136a6a718389d9_16", "text": "The 4th column of the table shows, for comparison, the results of Sun and Korhonen (2009) obtained for English when they used the same features as us, clustered them using SPEC, and evaluated them against the English version of our gold standard, also using F-measure 2 ."}
{"sent_id": "be67496882917c2a44afb42e6f9f15-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_be67496882917c2a44afb42e6f9f15_16", "text": "Recently, a number of taskspecific attention variants have been proposed to deal with these issues: See et al. (2017) introduced a coverage mechanism (Tu et al., 2016 ) * Work performed while at Apple."}
{"sent_id": "be67496882917c2a44afb42e6f9f15-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_be67496882917c2a44afb42e6f9f15_16", "text": "We find, for each task, that Scratchpad attains improvements over several strong baselines: Sequence-to-Sequence with attention Bahdanau et al., 2014) , copy-enhanced approaches (Gu et al., 2016; Vinyals et al., 2015) , and coverageenhanced approaches (Tu et al., 2016; See et al., 2017) ."}
{"sent_id": "be67496882917c2a44afb42e6f9f15-C001-64", "intents": ["@SIM@"], "paper_id": "ABC_be67496882917c2a44afb42e6f9f15_16", "text": "For IWSLT15, we primarily compare to GNMT (Wu et al., 2016) , which incorporates Coverage (Tu et al., 2016) ."}
{"sent_id": "be67496882917c2a44afb42e6f9f15-C001-191", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_be67496882917c2a44afb42e6f9f15_16", "text": "In Song et al. (2017) , a seq2seq model with copynet and a coverage mechanism (Tu et al., 2016 ) is used to achieve state-of-the-art results."}
{"sent_id": "ca7db62af4457ca887fe220c43b10e-C001-105", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_ca7db62af4457ca887fe220c43b10e_16", "text": "We attempt to replicate the model described in Collobert et al. (2011) without task-specific fine-tuning, with a few exceptions: 1) we used the soft tanh activation function instead of hard tanh; 2) we use the BIO2 tagging scheme instead of BIOES; 3) we use L-BFGS optimization algorithm instead of stochastic gradient descent; 4) we did not use Gazetteer features; 5) Collobert et al. (2011) mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly."}
{"sent_id": "ca7db62af4457ca887fe220c43b10e-C001-43", "intents": ["@SIM@"], "paper_id": "ABC_ca7db62af4457ca887fe220c43b10e_16", "text": "So far we have extended the potential function used in node cliques of a CRF to a non-linear DNN. And if we keep the potential function for edge cliques the same as before, then in fact we have arrived at an identical model to the SLNN in Collobert et al. (Collobert et al., 2011) ."}
{"sent_id": "ca7db62af4457ca887fe220c43b10e-C001-101", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_ca7db62af4457ca887fe220c43b10e_16", "text": "For experiments with continuous space feature representations (a.k.a., word embeddings), we took the word embeddings (130K words, 50 dimensions) used in Collobert et al. (2011) , which were trained for 2 months over Wikipedia text."}
{"sent_id": "ca7db62af4457ca887fe220c43b10e-C001-148", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_ca7db62af4457ca887fe220c43b10e_16", "text": "We carefully compared and analyzed the nonlinear neural networks used in Collobert et al. (2011) and the widely adopted CRF, and revealed their close relationship."}
{"sent_id": "f3012301e42a4075ed6d4d2b39b528-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_f3012301e42a4075ed6d4d2b39b528_16", "text": "The past work in sarcasm detection involves rule-based and statistical approaches using: (a) unigrams and pragmatic features (such as emoticons, etc.) (Gonzalez-Ibanez et al., 2011; Carvalho et al., 2009; Barbieri et al., 2014) , (b) extraction of common patterns, such as hashtag-based sentiment (Maynard and Greenwood, 2014; Liebrecht et al., 2013) , a positive verb being followed by a negative situation (Riloff et al., 2013) , or discriminative n-grams (Tsur et al., 2010a; Davidov et al., 2010) ."}
{"sent_id": "f3012301e42a4075ed6d4d2b39b528-C001-120", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_f3012301e42a4075ed6d4d2b39b528_16", "text": "This is an improvement of about 5% over the baseline, and 40% over the algorithm by Riloff et al. (2013) ."}
{"sent_id": "f3012301e42a4075ed6d4d2b39b528-C001-114", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f3012301e42a4075ed6d4d2b39b528_16", "text": "2. Tweet-B (2278 tweets, 506 sarcastic): This dataset was manually labeled for Riloff et al. (2013 To extract the implicit incongruity features, we run the iterative algorithm described in Section 4.2, on a dataset of 4000 tweets (50% sarcastic) (also created using hashtag-based supervision)."}
{"sent_id": "c2952b2da147d5f128cdbd5d8074a5-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_c2952b2da147d5f128cdbd5d8074a5_16", "text": "Park et al. (2018) discuss different ways of mitigating gender bias, in the context of abusive language detection, ranging from debiasing a model by using the hard debiased word embeddings produced by Bolukbasi et al. (2016b) , to manipulating the data prior to training a model by swapping masculine and feminine mentions, and employing transfer learning from a model learned from less biased text."}
{"sent_id": "c2952b2da147d5f128cdbd5d8074a5-C001-104", "intents": ["@BACK@"], "paper_id": "ABC_c2952b2da147d5f128cdbd5d8074a5_16", "text": "The approach described by (Bolukbasi et al., 2016b) includes an equalize step to make all gender neutral words equidistant to each of the members of a given equality set of word pairs."}
{"sent_id": "c2952b2da147d5f128cdbd5d8074a5-C001-96", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c2952b2da147d5f128cdbd5d8074a5_16", "text": "We apply the debiasing methodology in (Bolukbasi et al., 2016b) to the pretrained embedddings."}
{"sent_id": "f29baa099b13f38badeb4cbd8789f6-C001-92", "intents": ["@USE@"], "paper_id": "ABC_f29baa099b13f38badeb4cbd8789f6_17", "text": "We have inherited features from Islam et al. (2012; and Sinha et al. (2012) , these features achieve reasonable classification accuracy."}
{"sent_id": "f29baa099b13f38badeb4cbd8789f6-C001-208", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_f29baa099b13f38badeb4cbd8789f6_17", "text": "Recently, Sinha et al. (2012) proposed few computational models that are similar to the traditional English readability formulas."}
{"sent_id": "f29baa099b13f38badeb4cbd8789f6-C001-218", "intents": ["@USE@"], "paper_id": "ABC_f29baa099b13f38badeb4cbd8789f6_17", "text": "In order to find the best performing training model, we use 20 features from Islam et al. (2012; and Sinha et al. (2012) ."}
{"sent_id": "f29baa099b13f38badeb4cbd8789f6-C001-241", "intents": ["@USE@"], "paper_id": "ABC_f29baa099b13f38badeb4cbd8789f6_17", "text": "The classification F-Score rises to 87.87 when we combine features from Islam et al. (2014) and Sinha et al. (Sinha et al., 2012) ."}
{"sent_id": "f29baa099b13f38badeb4cbd8789f6-C001-285", "intents": ["@SIM@"], "paper_id": "ABC_f29baa099b13f38badeb4cbd8789f6_17", "text": "This study also validate that features in our previous study Islam et al. (2014) and features proposed by Sinha et al. (Sinha et al., 2012) are useful for Bangla text readability analysis."}
{"sent_id": "b0a50145121eb797cf8e6ebc2f49e0-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_b0a50145121eb797cf8e6ebc2f49e0_17", "text": "N-gram-based models are Markov models over sequences of tuples or operations encapsulating tuples (Durrani et al., 2011) ."}
{"sent_id": "b0a50145121eb797cf8e6ebc2f49e0-C001-29", "intents": ["@USE@"], "paper_id": "ABC_b0a50145121eb797cf8e6ebc2f49e0_17", "text": "In order to deal with these problems, search is carried out only on a graph of pre-calculated orderings, and ad-hoc reordering limits are imposed to constrain the search space (Crego et al., 2005; , or a higher beam size is used in decoding (Durrani et al., 2011) ."}
{"sent_id": "fed51218e78d35aae39d287c95a95a-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_fed51218e78d35aae39d287c95a95a_17", "text": "More recently, Wiegand and Klakow (2010) explored convolution kernels for OH extraction and found that tree kernels outperform all other kernel types."}
{"sent_id": "fed51218e78d35aae39d287c95a95a-C001-139", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_fed51218e78d35aae39d287c95a95a_17", "text": "These are commonly accepted heuristics which have already been used in previous work as features (Choi et al., 2005; Wiegand and Klakow, 2010) ."}
{"sent_id": "fed51218e78d35aae39d287c95a95a-C001-33", "intents": ["@USE@"], "paper_id": "ABC_fed51218e78d35aae39d287c95a95a_17", "text": "2 We use the definition of OHs as described in (Wiegand and Klakow, 2010) ."}
{"sent_id": "fed51218e78d35aae39d287c95a95a-C001-105", "intents": ["@USE@"], "paper_id": "ABC_fed51218e78d35aae39d287c95a95a_17", "text": "We also augment the tree kernels themselves with additional information by 3 wordnet.princeton.edu following Wiegand and Klakow (2010) who add for each word that belongs to a predictive semantic class another node that directly dominates the pertaining leaf node and assign it a label denoting that class."}
{"sent_id": "fed51218e78d35aae39d287c95a95a-C001-132", "intents": ["@USE@"], "paper_id": "ABC_fed51218e78d35aae39d287c95a95a_17", "text": "Both resources have been found predictive for OH extraction (Bloom et al., 2007; Wiegand and Klakow, 2010) ."}
{"sent_id": "c7778abb2f1890ba896ccef2c3e13b-C001-70", "intents": ["@BACK@"], "paper_id": "ABC_c7778abb2f1890ba896ccef2c3e13b_17", "text": "Attempts have been made by Lau et al. [17] where they first introduced their topic modeling based word sense induction method to automatically detect words with emergent novel senses and in a subsequent work, Lau et al. [16] extended this task by leveraging the concept of predominant sense."}
{"sent_id": "c7778abb2f1890ba896ccef2c3e13b-C001-132", "intents": ["@USE@"], "paper_id": "ABC_c7778abb2f1890ba896ccef2c3e13b_17", "text": "We run Lau et al. [16] over these birth cases to detect 'novel' sense as per their algorithm."}
{"sent_id": "c7778abb2f1890ba896ccef2c3e13b-C001-151", "intents": ["@USE@"], "paper_id": "ABC_c7778abb2f1890ba896ccef2c3e13b_17", "text": "Similarly Table 4 shows manual evaluations results for 3 example cases, along with their novel sense as captured by Lau et al. [16] ."}
{"sent_id": "730738d63cabcd4e63ec4300a8091b-C001-29", "intents": ["@USE@"], "paper_id": "ABC_730738d63cabcd4e63ec4300a8091b_17", "text": "We present empirical studies of the error distribution of global, beam-search transition-based dependency parsing, using ZPar (Zhang and Nivre, 2011) as a representative system."}
{"sent_id": "730738d63cabcd4e63ec4300a8091b-C001-52", "intents": ["@USE@"], "paper_id": "ABC_730738d63cabcd4e63ec4300a8091b_17", "text": "Figure 1 shows the UAS of ZPar under different settings, where 'global' refers to a global model trained using the same method as Zhang and Nivre (2011) , 'local' refers to a local classifier trained using the averaged perceptron, 'base features' refers to the set of base feature templates in Zhang and Nivre (2011) , and 'all features' refers to the set of base and all extended feature templates in Zhang and Nivre (2011) ."}
{"sent_id": "730738d63cabcd4e63ec4300a8091b-C001-85", "intents": ["@USE@"], "paper_id": "ABC_730738d63cabcd4e63ec4300a8091b_17", "text": "For further evidence, we add rich non-local features in the same increments as Zhang and Nivre (2011) to both ZPar and MaltParser, and evaluate UAS on the same development data set."}
{"sent_id": "f4792ef9808a1a3c415f6f57351335-C001-7", "intents": ["@BACK@"], "paper_id": "ABC_f4792ef9808a1a3c415f6f57351335_17", "text": "Discourse structure annotations have been demonstrated to be of high utility for a number of NLP applications, including automatic text summarization (Marcu, 1998; Marcu, 1999; Cristea et al., 2005) , sentence compression (Sporleder and Lapata, 2005) , natural language generation (Prasad et al., 2005) and question answering (Verberne et al., 2006) ."}
{"sent_id": "f4792ef9808a1a3c415f6f57351335-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_f4792ef9808a1a3c415f6f57351335_17", "text": "While Sporleder and Lapata (2005) demonstrated that their finite-state system could perform as well as the SPADE system, which uses context-free parse trees, this does not directly answer the question of the utility of context-free derived features for this task."}
{"sent_id": "f4792ef9808a1a3c415f6f57351335-C001-52", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_f4792ef9808a1a3c415f6f57351335_17", "text": "Sporleder and Lapata (2005) went further and established a smaller subset of 608 sentences, which omitted sentences with only one segment, i.e., sentences which themselves are atomic edus."}
{"sent_id": "f4792ef9808a1a3c415f6f57351335-C001-194", "intents": ["@BACK@"], "paper_id": "ABC_f4792ef9808a1a3c415f6f57351335_17", "text": "This label set has been shown to be of particular utility for indicating which segments are more important to include in an automatically created summary or compressed sentence (Sporleder and Lapata, 2005; Marcu, 1998; Marcu, 1999; Cristea et al., 2005) ."}
{"sent_id": "f4792ef9808a1a3c415f6f57351335-C001-65", "intents": ["@USE@"], "paper_id": "ABC_f4792ef9808a1a3c415f6f57351335_17", "text": "Thus, in section 2.3, for comparison with reported results in Sporleder and Lapata (2005) , our F1-score is defined accordingly, i.e., seg- mentation boundaries j such that 0 ≤ j < k. In addition, we will report unlabeled bracketing precision, recall and F1-score, as defined in the PARSEVAL metrics (Black et al., 1991) and evaluated via the widely used evalb package."}
{"sent_id": "f4792ef9808a1a3c415f6f57351335-C001-126", "intents": ["@SIM@"], "paper_id": "ABC_f4792ef9808a1a3c415f6f57351335_17", "text": "This feature set is very close to that used in Sporleder and Lapata (2005) , but not identical."}
{"sent_id": "e3c735811b2ea08d92659272ddcbdd-C001-41", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_e3c735811b2ea08d92659272ddcbdd_17", "text": "In our previous work [BWDS16] , we curated what may be the largest set of gang member profiles to study how gang member Twitter profiles can be automatically identified based on the content they share online."}
{"sent_id": "e3c735811b2ea08d92659272ddcbdd-C001-71", "intents": ["@BACK@"], "paper_id": "ABC_e3c735811b2ea08d92659272ddcbdd_17", "text": "An in-depth explanation of these feature selection can be found in [BWDS16] ."}
{"sent_id": "e3c735811b2ea08d92659272ddcbdd-C001-93", "intents": ["@BACK@"], "paper_id": "ABC_e3c735811b2ea08d92659272ddcbdd_17", "text": "Further, we found that 76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre [BWDS16] ."}
{"sent_id": "e3c735811b2ea08d92659272ddcbdd-C001-156", "intents": ["@USE@"], "paper_id": "ABC_e3c735811b2ea08d92659272ddcbdd_17", "text": "We compare our results with the two best performing systems reported in [BWDS16] which are the two state-of-theart models for identifying gang members in Twitter."}
{"sent_id": "ae67018df3a74e0fd4ae90522499a3-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_ae67018df3a74e0fd4ae90522499a3_17", "text": "One can find from above discussions that the second category of approaches suffer from the data spareness problem, although there have been recent attempts (Gupta et al., 2017; Dehghani et al., 2017) trying to pseudo label query-document pairs automatically with unsupervised retrieval models such as BM25."}
{"sent_id": "ae67018df3a74e0fd4ae90522499a3-C001-122", "intents": ["@SIM@"], "paper_id": "ABC_ae67018df3a74e0fd4ae90522499a3_17", "text": "The actual probability in this paper is estimated in a similar way as in (Dehghani et al., 2017) , which is:"}
{"sent_id": "ae67018df3a74e0fd4ae90522499a3-C001-151", "intents": ["@SIM@"], "paper_id": "ABC_ae67018df3a74e0fd4ae90522499a3_17", "text": "We set the hyper-parameters of our model by following similar tasks such as (Dehghani et al., 2017) ."}
{"sent_id": "9dd9ac975c6f55797615f0e52aa296-C001-42", "intents": ["@USE@"], "paper_id": "ABC_9dd9ac975c6f55797615f0e52aa296_17", "text": "3. The real-valued embedding vector for the entity type of w i : This vector is generated by looking up the entity type embedding table (initialized randomly) for the entity type of w i . Note that we employ the BIO annotation schema to assign entity type labels to each token in the sentences using the entity mention heads as in (Nguyen and Grishman, 2015b) ."}
{"sent_id": "9dd9ac975c6f55797615f0e52aa296-C001-82", "intents": ["@USE@"], "paper_id": "ABC_9dd9ac975c6f55797615f0e52aa296_17", "text": "We apply the same parameters and resources as (Nguyen and Grishman, 2015b) to ensure the compatible comparison."}
{"sent_id": "9dd9ac975c6f55797615f0e52aa296-C001-95", "intents": ["@USE@"], "paper_id": "ABC_9dd9ac975c6f55797615f0e52aa296_17", "text": "2) The neural network models, i.e, the CNN model in (Nguyen and Grishman, 2015b) (CNN), the dynamic multi-pooling CNN model (DM-CNN) in (Chen et al., 2015) and the bidirectional recurrent neural networks (B-RNN) in (Nguyen et al., 2016a) ."}
{"sent_id": "9dd9ac975c6f55797615f0e52aa296-C001-112", "intents": ["@USE@"], "paper_id": "ABC_9dd9ac975c6f55797615f0e52aa296_17", "text": "Following (Nguyen and Grishman, 2015b), we use news (the union of bn and nw) as the source domain and bc, cts, wl and un as four different target domains 3 ."}
{"sent_id": "9dd9ac975c6f55797615f0e52aa296-C001-119", "intents": ["@USE@"], "paper_id": "ABC_9dd9ac975c6f55797615f0e52aa296_17", "text": "We emphasize that the performance of the systems MaxEnt, Joint+Local, Joint+Local+Global, B-RNN, and CNN is obtained from the actual systems in the original work (Li et al., 2013; Nguyen and Grishman, 2015b; Nguyen et al., 2016a) ."}
{"sent_id": "9dd9ac975c6f55797615f0e52aa296-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_9dd9ac975c6f55797615f0e52aa296_17", "text": "The best reported system in the DA setting for ED is (Nguyen and Grishman, 2015b) , which demonstrated that the CNN model outperformed the feature-based models in the cross-domain setting."}
{"sent_id": "8084b5077b2a8db755b1bbd0f6fe60-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_8084b5077b2a8db755b1bbd0f6fe60_17", "text": "According to Tiedemann and Ljubešić (2012) , character-based n-gram methods fail for languages with a high lexical overlap, since the more shared words between two languages, the more similar will their n-gram character frequency profiles be."}
{"sent_id": "8084b5077b2a8db755b1bbd0f6fe60-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_8084b5077b2a8db755b1bbd0f6fe60_17", "text": "This work is followed up in Tiedemann and Ljubešić (2012) where 9% of improvement over standard approaches is reported and where support for Bosnian discrimination is included."}
{"sent_id": "8084b5077b2a8db755b1bbd0f6fe60-C001-83", "intents": ["@SIM@"], "paper_id": "ABC_8084b5077b2a8db755b1bbd0f6fe60_17", "text": "We will also refer to the lists of the 10,000 most frequent words as 'white list', which have a complementary role to the 'black lists' of Tiedemann and Ljubešić (2012) ."}
{"sent_id": "2a84615479af66bbf875517a3a753b-C001-70", "intents": ["@USE@"], "paper_id": "ABC_2a84615479af66bbf875517a3a753b_17", "text": "To follow previous research [7] , we also add another prosodic feature vector, p, with each ot to generate a more informative vector representation of the signal, o A t ."}
{"sent_id": "2a84615479af66bbf875517a3a753b-C001-146", "intents": ["@SIM@"], "paper_id": "ABC_2a84615479af66bbf875517a3a753b_17", "text": "In audio-BRE (Fig. 2(a) ), most of the emotion labels are frequently misclassified as neutral class, supporting the claims of [7, 25] ."}
{"sent_id": "d2b9c678a3d4920919f59c3b5903d3-C001-66", "intents": ["@USE@"], "paper_id": "ABC_d2b9c678a3d4920919f59c3b5903d3_17", "text": "We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed \"silver\" trees (McClosky et al., 2006; Huang et al., 2010; Vinyals et al., 2015) for tritraining."}
{"sent_id": "d2b9c678a3d4920919f59c3b5903d3-C001-96", "intents": ["@USE@"], "paper_id": "ABC_d2b9c678a3d4920919f59c3b5903d3_17", "text": "We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 (HC) (Vinyals et al., 2015) ; and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (Luong et al., 2016) ."}
{"sent_id": "bd2a718f75d206ef3f2cb5648585d5-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_bd2a718f75d206ef3f2cb5648585d5_17", "text": "Wachsmuth et al. (2017a) point out that dialectical builds on rhetorical, and rhetorical builds on logical quality."}
{"sent_id": "489d0077e05269327e7fe4e7f7e4a3-C001-142", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_489d0077e05269327e7fe4e7f7e4a3_17", "text": "Both [17] and [3] use time-consuming instruction text preprocessing over the skip-thought technique [15] ."}
{"sent_id": "489d0077e05269327e7fe4e7f7e4a3-C001-151", "intents": ["@BACK@"], "paper_id": "ABC_489d0077e05269327e7fe4e7f7e4a3_17", "text": "AdaMine [3] creates more distinct class clusters than in [17] ."}
{"sent_id": "489d0077e05269327e7fe4e7f7e4a3-C001-98", "intents": ["@USE@"], "paper_id": "ABC_489d0077e05269327e7fe4e7f7e4a3_17", "text": "We have trained our model using cosine similarity loss with margin as in [17] and with the triplet loss proposed by [3] ."}
{"sent_id": "489d0077e05269327e7fe4e7f7e4a3-C001-119", "intents": ["@USE@"], "paper_id": "ABC_489d0077e05269327e7fe4e7f7e4a3_17", "text": "Since the timing of freezing layers proved not to be of importance unless the recipe path is trained first, we used the same strategy under the cosine distance objective [17] and for the triplet loss [3] ."}
{"sent_id": "489d0077e05269327e7fe4e7f7e4a3-C001-165", "intents": ["@SIM@"], "paper_id": "ABC_489d0077e05269327e7fe4e7f7e4a3_17", "text": "On the recipe retrieval task, our method performs similarly to our baseline implementation of [3] ."}
{"sent_id": "4e1b01c1faebc447891bc0b847316d-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_4e1b01c1faebc447891bc0b847316d_17", "text": "These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009) , N = 5, whereas for Newman et al. (2010) , Aletras and Stevenson (2013) and Lau et al. (2014) , N = 10."}
{"sent_id": "4e1b01c1faebc447891bc0b847316d-C001-26", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_4e1b01c1faebc447891bc0b847316d_17", "text": "The germ of this paper came when using the automatic word intrusion methodology (Lau et al., 2014) , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction."}
{"sent_id": "4e1b01c1faebc447891bc0b847316d-C001-41", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_4e1b01c1faebc447891bc0b847316d_17", "text": "2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence (Newman et al., 2010; Lau et al., 2014) ."}
{"sent_id": "4e1b01c1faebc447891bc0b847316d-C001-38", "intents": ["@USE@"], "paper_id": "ABC_4e1b01c1faebc447891bc0b847316d_17", "text": "Following Lau et al. (2014) , we use two domains: (1) WIKI, a collection of 3.3 million English Wikipedia articles (retrieved November 28th 2009); and (2) NEWS, a collection of 1.2 million New York Times articles from 1994 to 2004 (English Gigaword)."}
{"sent_id": "197b557d7b5c7c2d195be84990719b-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_197b557d7b5c7c2d195be84990719b_17", "text": "For example, Mikolov et al. (2013) utilize Skipgram NegativeSampling (SGNS) to train word embeddings using word-context pairs formed from windows moving across a text corpus."}
{"sent_id": "197b557d7b5c7c2d195be84990719b-C001-118", "intents": ["@USE@"], "paper_id": "ABC_197b557d7b5c7c2d195be84990719b_17", "text": "Word vectors are initialized to the pretrained values found in Mikolov et al. (2013) but otherwise updates are allowed to these vectors at training time."}
{"sent_id": "197b557d7b5c7c2d195be84990719b-C001-141", "intents": ["@SIM@"], "paper_id": "ABC_197b557d7b5c7c2d195be84990719b_17", "text": "The first, which we Figure 5 demonstrates that token similarities are learned in a similar fashion as in SGNS (Mikolov et al., 2013 ) but specialized to the Hacker News corpus."}
{"sent_id": "9c5baf669470fe4dd18277591591f1-C001-21", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_9c5baf669470fe4dd18277591591f1_17", "text": "We focus on the task of parsing time normalizations (Laparra et al., 2018b) , where large gains of character-level models over word-level models have been observed (Laparra et al., 2018a) ."}
{"sent_id": "9c5baf669470fe4dd18277591591f1-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_9c5baf669470fe4dd18277591591f1_17", "text": "Laparra et al. (2018a) decomposes the Parsing Time Normalizations task into two subtasks: a) time entity identification using a character-level sequence tagger which detects"}
{"sent_id": "9c5baf669470fe4dd18277591591f1-C001-36", "intents": ["@USE@"], "paper_id": "ABC_9c5baf669470fe4dd18277591591f1_17", "text": "In this paper, We focus on the character-level time entity identifier that is the foundation of Laparra et al. (2018a) 's model."}
{"sent_id": "397e593f8f282d4951402d83036c12-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_397e593f8f282d4951402d83036c12_17", "text": "End-to-end neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) has gained increasing popularity in the machine translation community."}
{"sent_id": "397e593f8f282d4951402d83036c12-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_397e593f8f282d4951402d83036c12_17", "text": "Please refer to (Bahdanau et al., 2015) for more details."}
{"sent_id": "397e593f8f282d4951402d83036c12-C001-16", "intents": ["@USE@"], "paper_id": "ABC_397e593f8f282d4951402d83036c12_17", "text": "2 The Toolkit 2.1 Model THUMT implements the standard attention-based NMT model (Bahdanau et al., 2015) on top of Theano (Bergstra et al., 2010) ."}
{"sent_id": "397e593f8f282d4951402d83036c12-C001-48", "intents": ["@USE@"], "paper_id": "ABC_397e593f8f282d4951402d83036c12_17", "text": "Our baseline system is GroundHog (Bahdanau et al., 2015) , a state-of-the-art open-source NMT toolkit."}
{"sent_id": "f633ceffdf53849159574a2891eda1-C001-10", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_f633ceffdf53849159574a2891eda1_17", "text": "Recently, Yang et al. (2019) showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) ."}
{"sent_id": "f633ceffdf53849159574a2891eda1-C001-67", "intents": ["@USE@"], "paper_id": "ABC_f633ceffdf53849159574a2891eda1_17", "text": "Following Yang et al. (2019) , to evaluate answers in an end-to-end setup, we disregard the paragraph context from the original datasets and use only the answer spans."}
{"sent_id": "f633ceffdf53849159574a2891eda1-C001-69", "intents": ["@SIM@"], "paper_id": "ABC_f633ceffdf53849159574a2891eda1_17", "text": "In addition, we compute recall (R), the fraction of questions for which the correct answer appears in any retrieved paragraph; to make our results comparable to Yang et al. (2019) , Anserini returns the top k = 100 paragraphs to feed into the BERT reader."}
{"sent_id": "04461d946dadc759e4be1207655159-C001-31", "intents": ["@SIM@"], "paper_id": "ABC_04461d946dadc759e4be1207655159_18", "text": "For rule extraction, we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase-based translation models (Och and Ney, 2004 ) and Chiang's HPB model (Chiang, 2005; Chiang, 2007) ."}
{"sent_id": "04461d946dadc759e4be1207655159-C001-40", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_04461d946dadc759e4be1207655159_18", "text": "To alleviate these problems, we filter our HD-HRs according to the same constraints as described in Chiang (2007) ."}
{"sent_id": "af9b884710f8198f008a9687153db6-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_af9b884710f8198f008a9687153db6_18", "text": "Especially, it has been shown that the combination of LSTMs (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) , convolutional neural networks (CNNs) (LeCun et al., 1989) , and word-level CRF achieves the state-of-the-art performance (Ma and Hovy, 2016) ."}
{"sent_id": "af9b884710f8198f008a9687153db6-C001-81", "intents": ["@USE@"], "paper_id": "ABC_af9b884710f8198f008a9687153db6_18", "text": "Following previous work (Ma and Hovy, 2016) , we use BIOES tagging scheme in the wordlevel tagging model."}
{"sent_id": "af9b884710f8198f008a9687153db6-C001-102", "intents": ["@SIM@"], "paper_id": "ABC_af9b884710f8198f008a9687153db6_18", "text": "This result is consistent with the result of (Ma and Hovy, 2016 In both experiments, it improves the F1 score by using segment-level CRF."}
{"sent_id": "975413dd6b3d3df9c5d111d94e8eb7-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_975413dd6b3d3df9c5d111d94e8eb7_18", "text": "Textual relation (Bunescu and Mooney, 2005) , defined as the shortest path between two entities in the dependency parse tree of a sentence, has been widely shown to be the main bearer of relational information in text and proved effective in relation extraction tasks (Xu et al., 2015; Su et al., 2018) ."}
{"sent_id": "975413dd6b3d3df9c5d111d94e8eb7-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_975413dd6b3d3df9c5d111d94e8eb7_18", "text": "(Su et al., 2018) use global co-occurrence statistics of 1 https://github.com/czyssrs/GloREPlus textual and KB relations to effectively combat the wrong labeling problem. But the global statistics in their work is limited to NYT dataset, capturing domain-specific distributions."}
{"sent_id": "975413dd6b3d3df9c5d111d94e8eb7-C001-81", "intents": ["@USE@"], "paper_id": "ABC_975413dd6b3d3df9c5d111d94e8eb7_18", "text": "We also compare with using vanilla RNN in GloRE (Su et al., 2018) ."}
{"sent_id": "975413dd6b3d3df9c5d111d94e8eb7-C001-97", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_975413dd6b3d3df9c5d111d94e8eb7_18", "text": "Same as (Su et al., 2018) , we use PCNN+ATT (Lin et al., 2016 ) as our base model."}
{"sent_id": "45d4d6f0ac4a4f3bf7b2ac70fbcf7f-C001-132", "intents": ["@USE@"], "paper_id": "ABC_45d4d6f0ac4a4f3bf7b2ac70fbcf7f_18", "text": "Following previous work [14] on multi-label classification, we adopt label-based accuracy (i.e., Hamming score) and micro-F 1 score as our main evaluation metrics."}
{"sent_id": "af0c9e20d34a080bac3304ded1f8d6-C001-117", "intents": ["@USE@"], "paper_id": "ABC_af0c9e20d34a080bac3304ded1f8d6_18", "text": "• CNN-CR [14] : The state-of-the-art approach for CDA recognition on the MSDialog-Intent dataset [14] ."}
{"sent_id": "d9c0e641f8ceb61e5d6e416bfc6492-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_d9c0e641f8ceb61e5d6e416bfc6492_18", "text": "It has been done for constituency parsing for example by Collins (1999) but also for dependency parsing for example by Nilsson et al. (2007) ."}
{"sent_id": "d9c0e641f8ceb61e5d6e416bfc6492-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_d9c0e641f8ceb61e5d6e416bfc6492_18", "text": "Nilsson et al. (2007) have shown that these same modifications as well as the modification of nonprojective structures helps parsing in four languages."}
{"sent_id": "d9c0e641f8ceb61e5d6e416bfc6492-C001-73", "intents": ["@USE@"], "paper_id": "ABC_d9c0e641f8ceb61e5d6e416bfc6492_18", "text": "We will follow the methodology from Nilsson et al. (2007) , that is, to transform, parse and then detransform the data so as to compare the original and the transformed model on the original gold standard."}
{"sent_id": "f2db88c0d4e0ec4c34fc295a5d59ba-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_f2db88c0d4e0ec4c34fc295a5d59ba_18", "text": "These constraints can be lexicalized (Collins, 1999; Charniak, 2000) , unlexicalized (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; Petrov et al., 2006) ."}
{"sent_id": "f2db88c0d4e0ec4c34fc295a5d59ba-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_f2db88c0d4e0ec4c34fc295a5d59ba_18", "text": "In previous work (Petrov et al., 2006; Petrov and Klein, 2007 ) the final grammar was chosen based on its performance on a held-out set (section 22), and corresponds to the second best grammar in Figure 3 (because only 8 different grammars were trained)."}
{"sent_id": "f2db88c0d4e0ec4c34fc295a5d59ba-C001-169", "intents": ["@BACK@"], "paper_id": "ABC_f2db88c0d4e0ec4c34fc295a5d59ba_18", "text": "The parameters of each latent variable grammar are typically smoothed in a linear fashion to prevent excessive overfitting (Petrov et al., 2006) ."}
{"sent_id": "e7b1c00e747f5bfbb96499d7223496-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_e7b1c00e747f5bfbb96499d7223496_18", "text": "These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary (Lazaridou et al., 2015; Glenberg & Robertson, 2000; )."}
{"sent_id": "e7b1c00e747f5bfbb96499d7223496-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_e7b1c00e747f5bfbb96499d7223496_18", "text": "The work introduced in Lazaridou et al. (2015) sought to address many of the drawbacks of these models."}
{"sent_id": "e7b1c00e747f5bfbb96499d7223496-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_e7b1c00e747f5bfbb96499d7223496_18", "text": "The multimodal skip-gram architecture proposed by Lazaridou et al. (2015) takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation."}
{"sent_id": "9d1699d4ca3b4026ed5aab125a737d-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_9d1699d4ca3b4026ed5aab125a737d_18", "text": "(Gillick and Favre, 2009) Several authors, e.g., Woodsend and Lapata (2012) , and Li et al. (2013) , have followed Gillick and Favre (2009) in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these."}
{"sent_id": "9d1699d4ca3b4026ed5aab125a737d-C001-70", "intents": ["@BACK@"], "paper_id": "ABC_9d1699d4ca3b4026ed5aab125a737d_18", "text": "For 'A' type documents, Gillick and Favre (2009) set this threshold to 3 and for 'B' type documents, they set this to 4."}
{"sent_id": "67b6d87aa2a943a854251fada6e183-C001-20", "intents": ["@USE@"], "paper_id": "ABC_67b6d87aa2a943a854251fada6e183_18", "text": "Specifically, we train multinomial Bayes classifiers based on location indicative words (LIWs) in tweets (Han et al., 2012) , and user-declared location and time zone metadata."}
{"sent_id": "67b6d87aa2a943a854251fada6e183-C001-55", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_67b6d87aa2a943a854251fada6e183_18", "text": "In this study, we adopt the same city-based representation and multinomial naive Bayes learner as Han et al. (2012) ."}
{"sent_id": "67b6d87aa2a943a854251fada6e183-C001-89", "intents": ["@USE@"], "paper_id": "ABC_67b6d87aa2a943a854251fada6e183_18", "text": "(1) the KL-divergence nearest prototype method of Roller et al. (2012) based on KD-tree partitioned grid cells, which we denote as KL; and (2) the multinomial naive Bayes city-level geolocation model of Han et al. (2012) , which we denote as MB."}
{"sent_id": "67b6d87aa2a943a854251fada6e183-C001-60", "intents": ["@BACK@"], "paper_id": "ABC_67b6d87aa2a943a854251fada6e183_18", "text": "In the original research of Han et al. (2012) ,"}
{"sent_id": "43622e43d6ef5291b64320d2d68b95-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_43622e43d6ef5291b64320d2d68b95_18", "text": "In addition, the performance of SANs can be improved by multi-head attention (Vaswani et al., 2017) , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace."}
{"sent_id": "43622e43d6ef5291b64320d2d68b95-C001-72", "intents": ["@BACK@"], "paper_id": "ABC_43622e43d6ef5291b64320d2d68b95_18", "text": "Multi-Head Attention Multi-head attention mechanism (Vaswani et al., 2017) employs different attention heads to capture distinct features (Raganato and Tiedemann, 2018) ."}
{"sent_id": "43622e43d6ef5291b64320d2d68b95-C001-85", "intents": ["@SIM@"], "paper_id": "ABC_43622e43d6ef5291b64320d2d68b95_18", "text": "About configurations of NMT models, we used the Base and Big settings same as Vaswani et al. (2017) , and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens."}
{"sent_id": "b49e6f8181d51a998c6c27a830b98e-C001-22", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_b49e6f8181d51a998c6c27a830b98e_18", "text": "From a synchronic perspective, Reddy et al. (2011 ), Schulte im Walde et al. (2013 and Schulte im Walde et al. (2016a) are closest to our approach, since they predict the compositionality of compounds using vector space representations."}
{"sent_id": "b49e6f8181d51a998c6c27a830b98e-C001-45", "intents": ["@USE@"], "paper_id": "ABC_b49e6f8181d51a998c6c27a830b98e_18", "text": "However, as it is not possible to survey compositionality rating for diachronic data, we instead use the synchronic data provided by Reddy et al. (2011) (henceforth referred to as REDDY) for evaluating the quality of the Google Books Ngram data as a source for investigating the compositionality of compounds in general."}
{"sent_id": "878c6cf1c47c86f36a7ff3f04e2998-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_878c6cf1c47c86f36a7ff3f04e2998_18", "text": "Recently, Thorne et al. (2018) proposed a public dataset to explore the complete process of the large-scale fact-checking."}
{"sent_id": "878c6cf1c47c86f36a7ff3f04e2998-C001-87", "intents": ["@USE@"], "paper_id": "ABC_878c6cf1c47c86f36a7ff3f04e2998_18", "text": "Dataset: FEVER dataset (Thorne et al., 2018 ) is a relatively large-scale dataset compared to other previous fact extraction and verification works, with around 5.4M Wikipedia documents and 185k samples."}
{"sent_id": "878c6cf1c47c86f36a7ff3f04e2998-C001-123", "intents": ["@USE@"], "paper_id": "ABC_878c6cf1c47c86f36a7ff3f04e2998_18", "text": "The MLP is a simple multi-layer perceptron using TF and TF-IDF cosine similarity between the claim and evidence as features as shown in Thorne et al. (2018) ."}
{"sent_id": "3bc48bea420e4977027832240450ec-C001-74", "intents": ["@BACK@"], "paper_id": "ABC_3bc48bea420e4977027832240450ec_18", "text": "Wang et al. (2017) extended the work of by using the dependency linked words from the target."}
{"sent_id": "3bc48bea420e4977027832240450ec-C001-152", "intents": ["@BACK@"], "paper_id": "ABC_3bc48bea420e4977027832240450ec_18", "text": "Wang et al. (2017) extended the NP work of and instead of using the full tweet/sentence/text contexts they used the full dependency graph of the target word."}
{"sent_id": "af6c68ef5f80eac2274bf33a894d1f-C001-15", "intents": ["@USE@"], "paper_id": "ABC_af6c68ef5f80eac2274bf33a894d1f_18", "text": "In order to to validate our methodology, we first replicate the results of Mikolov et al. (2013b) on English syntactic analogies."}
{"sent_id": "af6c68ef5f80eac2274bf33a894d1f-C001-48", "intents": ["@USE@"], "paper_id": "ABC_af6c68ef5f80eac2274bf33a894d1f_18", "text": "The first, labeled as M13, is the result of applying the vectors of Mikolov et al. (2013b) to their test set."}
{"sent_id": "af6c68ef5f80eac2274bf33a894d1f-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_af6c68ef5f80eac2274bf33a894d1f_18", "text": "The vectors of Mikolov et al. (2013b) were trained on 320M tokens of broadcast news data, as described by Mikolov et al. (2011) ."}
{"sent_id": "3356313ee5cdf186816cd6fecfce84-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_3356313ee5cdf186816cd6fecfce84_18", "text": "DSCconvs have first been introduced in the domain of Image Processing [8, 13] and have been applied to other domains since: Zhang et al. applied DSCconv to KWS [2] ."}
{"sent_id": "08d3f7a0938ab85d9a251b6a2364ed-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_08d3f7a0938ab85d9a251b6a2364ed_18", "text": "They define a deduction system for (an isomorphic variant of) Attardi's (2006) transition system, which covers a subset of non-projective trees."}
{"sent_id": "08d3f7a0938ab85d9a251b6a2364ed-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_08d3f7a0938ab85d9a251b6a2364ed_18", "text": "As shown in Fig. 1 , Attardi's (2006) system has two degree-2 transitions (re s 0 ,s 2 and re s 2 ,s 0 ) that allow it to cover 87.24% of the nonprojective trees in UD 2.1."}
{"sent_id": "08d3f7a0938ab85d9a251b6a2364ed-C001-79", "intents": ["@BACK@"], "paper_id": "ABC_08d3f7a0938ab85d9a251b6a2364ed_18", "text": "Since each such deduction rule corresponds to a reduce transition, each revision to the deduction system yields a variant of Attardi's (2006) parser."}
{"sent_id": "123d8e8ddef15fed120908c5c20656-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_123d8e8ddef15fed120908c5c20656_18", "text": "More recently, Zeghidour et al. [8] proposed an alternative learnable architecture based on a convolutional architecture that computes a scattering transform and can be initialized as an approximation of mel-filterbanks, and obtained promising results on endto-end phone recognition on TIMIT."}
{"sent_id": "123d8e8ddef15fed120908c5c20656-C001-35", "intents": ["@USE@"], "paper_id": "ABC_123d8e8ddef15fed120908c5c20656_18", "text": "The first architecture we consider is inspired by [3, 4] , the second one is taken from [8] ."}
{"sent_id": "123d8e8ddef15fed120908c5c20656-C001-69", "intents": ["@USE@"], "paper_id": "ABC_123d8e8ddef15fed120908c5c20656_18", "text": "2 [8] use 1 to prevent log(0) and [3, 4] use 0.01."}
{"sent_id": "0f66e9a5c51cff004d97e4aaddf4d0-C001-4", "intents": ["@BACK@"], "paper_id": "ABC_0f66e9a5c51cff004d97e4aaddf4d0_18", "text": "While common sentence representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in (Conneau et al., 2017) ."}
{"sent_id": "0f66e9a5c51cff004d97e4aaddf4d0-C001-75", "intents": ["@USE@"], "paper_id": "ABC_0f66e9a5c51cff004d97e4aaddf4d0_18", "text": "Following (Conneau et al., 2017) we have tested our approach on a wide array of classification tasks, including sentiment analysis (MR -Pang and Lee (2005) , SST -Socher et al. (2013) ), question-type (TREC -Li and Roth (2002) ), product reviews (CR - Hu and Liu (2004) ), subjectivity/objectivity (SUBJ - Pang and Lee (2005) ) and opinion polarity (MPQA -Wiebe et al. (2005) )."}
{"sent_id": "759c1c892361f62ad8f2c46e569e8a-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_759c1c892361f62ad8f2c46e569e8a_18", "text": "It has been widely used in the context of dialog policy learning (Fatemi et al., 2016; Dhingra et al., 2017; Casanueva et al., 2017) ."}
{"sent_id": "759c1c892361f62ad8f2c46e569e8a-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_759c1c892361f62ad8f2c46e569e8a_18", "text": "Casanueva et al. (2017) propose six different environmental models, varying in user friendliness, simulated input channel noise and the presence or absence of action masks, which, when enabled, simplify learning by masking some of the possible actions."}
{"sent_id": "759c1c892361f62ad8f2c46e569e8a-C001-76", "intents": ["@USE@"], "paper_id": "ABC_759c1c892361f62ad8f2c46e569e8a_18", "text": "Training and evaluation with the PyDial user simulator follows the PyDial benchmarking tasks (Casanueva et al., 2017) , where each task (see Table 1) is trained on 10000 dialogs split into ten training iterations of 1000 dialogs each."}
{"sent_id": "759c1c892361f62ad8f2c46e569e8a-C001-81", "intents": ["@USE@"], "paper_id": "ABC_759c1c892361f62ad8f2c46e569e8a_18", "text": "The first row of Table 3 and 4 show the results of the highest scoring policy from the PyDial benchmark (Casanueva et al., 2017) to serve as baselines."}
{"sent_id": "7293ab5db16d3fe1fee48d45154697-C001-19", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_7293ab5db16d3fe1fee48d45154697_19", "text": "As described by Esplà-Gomis et al. (2015) , a collection of features is obtained from these correspondences and then used by a binary classifier to determine the final word-level MTQE labels."}
{"sent_id": "7293ab5db16d3fe1fee48d45154697-C001-74", "intents": ["@DIF@", "@EXT@", "@BACK@"], "paper_id": "ABC_7293ab5db16d3fe1fee48d45154697_19", "text": "Following the approach by Esplà-Gomis et al. (2015) , the perceptron was built with a single hidden layer containing the same number of nodes as the number of features; this was the best performing architecture in the preliminary experiments."}
{"sent_id": "7293ab5db16d3fe1fee48d45154697-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_7293ab5db16d3fe1fee48d45154697_19", "text": "A complete description of the features used for word-level MTQE can be found in Section 2 of the paper by Esplà-Gomis et al. (2015) ."}
{"sent_id": "7293ab5db16d3fe1fee48d45154697-C001-77", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_7293ab5db16d3fe1fee48d45154697_19", "text": "dcs.shef.ac.uk/wmt16_files_qe/task2p_ en-de_test.tar.gz 7 The rest of parameters of the classifiers were also kept as in the approach by Esplà-Gomis et al. (2015) ."}
{"sent_id": "1c0d971cf771f351b51661950f4b14-C001-66", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_1c0d971cf771f351b51661950f4b14_19", "text": "All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (Steyvers and Griffiths, 2007; Vulić and Moens, 2013a; Vulić and Moens, 2013b; Kiela and Clark, 2014) ."}
{"sent_id": "1c0d971cf771f351b51661950f4b14-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_1c0d971cf771f351b51661950f4b14_19", "text": "lation in the other language (EN) according to the ground truth over the total number of ground truth translation pairs (=1000) (Gaussier et al., 2004; Tamura et al., 2012; Vulić and Moens, 2013b) ."}
{"sent_id": "f5d1c0d3ac45ea4949f7d01d1704f6-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_f5d1c0d3ac45ea4949f7d01d1704f6_19", "text": "In this section, we give a brief overview of EASYADAPT proposed in (Daumé III, 2007) ."}
{"sent_id": "f5d1c0d3ac45ea4949f7d01d1704f6-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_f5d1c0d3ac45ea4949f7d01d1704f6_19", "text": "A kernelized version of the algorithm has also been presented in (Daumé III, 2007) ."}
{"sent_id": "c2bfe3534597a8f192ec846619f6b1-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_c2bfe3534597a8f192ec846619f6b1_19", "text": "Aside from their widespread use on monolingual text, topic models have also been used to model multilingual data (Boyd-Graber and Blei, 2009; Platt et al., 2010; Jagarlamudi and Daumé, 2010; Fukumasu et al., 2012) , to name a few."}
{"sent_id": "c2bfe3534597a8f192ec846619f6b1-C001-127", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c2bfe3534597a8f192ec846619f6b1_19", "text": "We use Mallet's (McCallum, 2002) implementation of the PLTM to train and infer topics on the same data set used in Platt et al. (2010) ."}
{"sent_id": "c2bfe3534597a8f192ec846619f6b1-C001-143", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c2bfe3534597a8f192ec846619f6b1_19", "text": "Since in (Platt et al., 2010) numbers were reported on the test speeches whose word length is greater or equal to 100, we used the same subset (total of 14150 speeches) of the original test collection."}
{"sent_id": "faeac0a0e3c0cad79d39dea04ec59a-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_faeac0a0e3c0cad79d39dea04ec59a_19", "text": "Waseem and Hovy (2016) only consider \"hate speech\" without regard to any potential overlap with bullying or otherwise offensive language, while Davidson et al. (2017) distinguish hate speech from generally offensive language."}
{"sent_id": "faeac0a0e3c0cad79d39dea04ec59a-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_faeac0a0e3c0cad79d39dea04ec59a_19", "text": "Much of the work on abusive language subtasks can be synthesized in a two-fold typology that conExplicit Implicit Directed \"Go kill yourself\", \"You're a sad little f*ck\" (Van Hee et al., 2015a) , \"@User shut yo beaner ass up sp*c and hop your f*ggot ass back across the border little n*gga\" (Davidson et al., 2017) , \"Youre one of the ugliest b*tches Ive ever fucking seen\" (Kontostathis et al., 2013) ."}
{"sent_id": "faeac0a0e3c0cad79d39dea04ec59a-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_faeac0a0e3c0cad79d39dea04ec59a_19", "text": "Previous research has indicated a great deal of variation within such language (Warner and Hirschberg, 2012; Davidson et al., 2017) , with abusive terms being used in a colloquial manner or by people who are victims of abuse."}
{"sent_id": "faeac0a0e3c0cad79d39dea04ec59a-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_faeac0a0e3c0cad79d39dea04ec59a_19", "text": "Davidson et al. (2017) , for instance, show that annotators tend to code racism as hate speech at a higher rate than sexism."}
{"sent_id": "a6954db741df61f014cc622c5b8263-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_a6954db741df61f014cc622c5b8263_19", "text": "In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (Magerman, 1995; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001) ."}
{"sent_id": "a6954db741df61f014cc622c5b8263-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_a6954db741df61f014cc622c5b8263_19", "text": "The second basic deficiency is that many rule types have been seen only once (and therefore have their probabilities overestimated), and many rules which occur in test sentences will never have been seen in training (and therefore have their probabilities underestimated -see Collins (1999) for analysis)."}
{"sent_id": "a6954db741df61f014cc622c5b8263-C001-216", "intents": ["@BACK@"], "paper_id": "ABC_a6954db741df61f014cc622c5b8263_19", "text": "Collins (1999) captures this notion by introducing the notion of a base NP, in which any NP which dominates only preterminals is marked with a -B. Further, if an NP-B does not have a non-base NP parent, it is given one with a unary production."}
{"sent_id": "bb133ba3dfe483412672b44b777c4a-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_bb133ba3dfe483412672b44b777c4a_19", "text": "However, even with residual connections and layer normalization, deep Transformers are still hard to train: the original Transformer (Vaswani et al., 2017) only contains 6 encoder/decoder layers."}
{"sent_id": "bb133ba3dfe483412672b44b777c4a-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_bb133ba3dfe483412672b44b777c4a_19", "text": "v1 and v2 stand for the computation order of the proposed Transformer (Vaswani et al., 2017) and that of the official implementation respectively. \"¬\" means fail to converge, \"None\" means not reported in original works, \"*\" indicates our implementation of their approach."}
{"sent_id": "6e92b1fa4f3b78a099cb222b3eb9a9-C001-75", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6e92b1fa4f3b78a099cb222b3eb9a9_19", "text": "In this work, GMM-UBM model has 2048 gaussian components, MFCC features are extracted using a 25 ms window and the i-Vectors are 400 dimensional [2] ."}
{"sent_id": "6e92b1fa4f3b78a099cb222b3eb9a9-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_6e92b1fa4f3b78a099cb222b3eb9a9_19", "text": "Details about the phone recognizer can be found in [2] ."}
{"sent_id": "6e92b1fa4f3b78a099cb222b3eb9a9-C001-111", "intents": ["@BACK@"], "paper_id": "ABC_6e92b1fa4f3b78a099cb222b3eb9a9_19", "text": "More details about the train and test data can be found in [2, 18] ."}
{"sent_id": "866ae880aa0de1e60d306eac2e66fc-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_866ae880aa0de1e60d306eac2e66fc_19", "text": "These coefficients are then used to weight the embedding of the image regions to obtain a suitable descriptor [19, 21, 6, 25, 26] ."}
{"sent_id": "866ae880aa0de1e60d306eac2e66fc-C001-102", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_866ae880aa0de1e60d306eac2e66fc_19", "text": "Following previous work [6] , we use as candidate outputs the top 3000 most frequent answers in the VQA dataset."}
{"sent_id": "2407cfa8572ccbab7f9a081f45a4ad-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_2407cfa8572ccbab7f9a081f45a4ad_19", "text": "In section 4 we discuss the work of Khapra et al. (2009) on parameter projection for multilingual WSD."}
{"sent_id": "2407cfa8572ccbab7f9a081f45a4ad-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_2407cfa8572ccbab7f9a081f45a4ad_19", "text": "The work of Khapra et al. (2009) as described above does not attempt to reach an optimal costbenefit point in this economic system."}
{"sent_id": "2407cfa8572ccbab7f9a081f45a4ad-C001-138", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_2407cfa8572ccbab7f9a081f45a4ad_19", "text": "We used the same dataset as described in Khapra et al. (2009) for all our experiments."}
{"sent_id": "6678c19792be8d9ad66cf923d00c23-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_6678c19792be8d9ad66cf923d00c23_19", "text": "The difficulty of predicting the values is partially because of the huge amount of noise within texts (Kogan et al., 2009 ) and partially because of the weak connection between texts and the quantities."}
{"sent_id": "6678c19792be8d9ad66cf923d00c23-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_6678c19792be8d9ad66cf923d00c23_19", "text": "The distribution over ln(v) across companies tends to have a bell shape (Kogan et al., 2009) ."}
{"sent_id": "be77eed8430b6492c81ae6535f1dd5-C001-82", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_be77eed8430b6492c81ae6535f1dd5_19", "text": "We use the \"training with exploration\" strategy (Goldberg and Nivre, 2013) and the dynamic oracle mechanism described in Cross and Huang (2016) to make sure the model can handle unseen parsing configurations properly."}
{"sent_id": "983ef31a44646d8e6276ee1933e41d-C001-26", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_983ef31a44646d8e6276ee1933e41d_19", "text": "• By incorporating WikiSplit into training, we more than double (30.5 to 62.4) the BLEU score obtained on WebSplit by Aharoni and Goldberg (2018) ."}
{"sent_id": "983ef31a44646d8e6276ee1933e41d-C001-81", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_983ef31a44646d8e6276ee1933e41d_19", "text": "We use the same sequence-to-sequence architecture that produced the top result for Aharoni and Goldberg (2018) , \"Copy512\", which is a one-layer, bi-directional LSTM (cell size 512) with attention (Bahdanau et al., 2014 ) and a copying mechanism (See et al., 2017 ) that dynamically interpolates the standard word distribution with a distribution over the words in the input sentence."}
{"sent_id": "983ef31a44646d8e6276ee1933e41d-C001-101", "intents": ["@USE@"], "paper_id": "ABC_983ef31a44646d8e6276ee1933e41d_19", "text": "Our manual evaluation includes the corresponding outputs from Aharoni and Goldberg (2018) (AG18), which were 22% accurate."}
{"sent_id": "ebd4488438579946c23904cc0f5932-C001-80", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_ebd4488438579946c23904cc0f5932_19", "text": "As a further analysis, we have examined the performance of our base ME model on the same test set as that used in Gildea and Jurafsky (2000) ."}
{"sent_id": "ebd4488438579946c23904cc0f5932-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_ebd4488438579946c23904cc0f5932_19", "text": "Gildea and Jurafsky (2000) describe a system that uses completely syntactic features to classify the Frame Elements in a sentence."}
{"sent_id": "ebd4488438579946c23904cc0f5932-C001-34", "intents": ["@USE@"], "paper_id": "ABC_ebd4488438579946c23904cc0f5932_19", "text": "Training (32,251 sentences), development (3,491 sentences), and held out test sets (3,398 sentences) were generated from the June 2002 FrameNet release following the divisions used in Gildea and Jurafsky (2000) 1 ."}
{"sent_id": "79ff6e23cc951aa18ae53763e9c982-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_79ff6e23cc951aa18ae53763e9c982_19", "text": "(8) Sue grabbed one phone, as Tom darted to the other phone. (Webber et al. (2003), p. 555) Here the referent of the other phone can be inferred from the antecedent one phone."}
{"sent_id": "79ff6e23cc951aa18ae53763e9c982-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_79ff6e23cc951aa18ae53763e9c982_19", "text": "Now close your eyes and try knocking the tower, this tower¡ over with your nose. (Webber et al. (2003) , p. 552) b. Do you want an apple? Otherwise you can have a pear."}
{"sent_id": "79ff6e23cc951aa18ae53763e9c982-C001-116", "intents": ["@BACK@"], "paper_id": "ABC_79ff6e23cc951aa18ae53763e9c982_19", "text": "Following earlier proposals by Hinrichs (1986) and Kamp and Reyle (1993) , Webber et al. (2003) assume that the semantics of discourse adverbials such as then involves an anaphoric relation between two events."}
{"sent_id": "90522b5ac99d1657bf9af9d165c36e-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_90522b5ac99d1657bf9af9d165c36e_19", "text": "Reasonable metrical choices are, e.g., the Jaccard coefficient (Jaccard, 1912) between these sets (Antoniak and Mimno, 2018; Chugh et al., 2018) , or a percentage based coefficient (Hellrich and Hahn, 2016a,b; Wendlandt et al., 2018; Pierrejean and Tanguy, 2018) ."}
{"sent_id": "90522b5ac99d1657bf9af9d165c36e-C001-134", "intents": ["@BACK@"], "paper_id": "ABC_90522b5ac99d1657bf9af9d165c36e_19", "text": "Diverging reports on SVD PPMI stability-described as perfectly reliable in Hellrich and Hahn (2017) , yet not in Antoniak and Mimno (2018) -can thus be explained by their difference in down-sampling options, i.e., no down-sampling or probabilistic down-sampling."}
{"sent_id": "2e7df0912d9aac8bf97f4061de613f-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_2e7df0912d9aac8bf97f4061de613f_19", "text": "Since 2014, lipreading systems have systematically begun to use neural networks at part of the processing pipeline [5, 6] or for end-to-end-training [7, 8, 9] ."}
{"sent_id": "2e7df0912d9aac8bf97f4061de613f-C001-37", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_2e7df0912d9aac8bf97f4061de613f_19", "text": "We follow the data preprocessing protocol from [7] ."}
{"sent_id": "2e7df0912d9aac8bf97f4061de613f-C001-86", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_2e7df0912d9aac8bf97f4061de613f_19", "text": "The first experiment deals with establishing a baseline for our experiments, building on prior work [7] ."}
{"sent_id": "46b9079fb1dd6b4626f20819ccfa07-C001-91", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_46b9079fb1dd6b4626f20819ccfa07_19", "text": "We briefly review the two training algorithms described in Roark et al. (2004b) , the perceptron algorithm and global conditional log-linear models (GCLMs)."}
{"sent_id": "46b9079fb1dd6b4626f20819ccfa07-C001-82", "intents": ["@BACK@"], "paper_id": "ABC_46b9079fb1dd6b4626f20819ccfa07_19", "text": "As one example, the language modeling features might take into account n-grams, for example through definitions such as Φ 2 (a, w) = Count of the the in w Previous work (Roark et al., 2004a; Roark et al., 2004b ) considered features of this type."}
{"sent_id": "b6efc2f5239a0c5d9210d7da8466ab-C001-81", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_b6efc2f5239a0c5d9210d7da8466ab_19", "text": "We use the data splits given by Post et al. (2012) and, following that work, include the dictionaries in the training data and report results on the devtest set using case-insensitive BLEU and four references."}
{"sent_id": "320a5c79d9884e652c42f85847172b-C001-26", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_320a5c79d9884e652c42f85847172b_20", "text": "This supervision takes form of sentence-aligned parallel data [5] , pre-built word translation pairs [11, 19] or document-aligned comparable data [21] ."}
{"sent_id": "320a5c79d9884e652c42f85847172b-C001-55", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_320a5c79d9884e652c42f85847172b_20", "text": "We therefore opt for the recent model of Smith et al. [19] to serve as a baseline, due to its competitive performance, large coverage, and readily available implementation."}
{"sent_id": "320a5c79d9884e652c42f85847172b-C001-62", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_320a5c79d9884e652c42f85847172b_20", "text": "In the second step, the projection matrix W trained with adversarial objective is used to find the mutual nearest neighbors between the two vocabularies -this set of automatically obtained word translation pairs becomes a synthetic training set for the refined projection functions f S and f T computed via the SVD-based method similar to the previously described model of Smith et al. [19] ."}
{"sent_id": "320a5c79d9884e652c42f85847172b-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_320a5c79d9884e652c42f85847172b_20", "text": "This class of models [1, 11, 19] focuses on learning the projections (i.e., mappings) between independently trained monolingual embedding spaces."}
{"sent_id": "0cc576e90c5ee2af043e09234792f5-C001-63", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_0cc576e90c5ee2af043e09234792f5_20", "text": "Lexical Features: As a benchmark for comparison, the lexical features that were showed to be good predictors of the quality of the texts in this dataset (Yannakoudakis et al., 2011) were chosen."}
{"sent_id": "d66ca5ff22e508da239fc7fdf5ac29-C001-31", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_d66ca5ff22e508da239fc7fdf5ac29_20", "text": "During the selection of linguistic indicators, we have taken into consideration previously studied features of readability (François and Fairon, 2012; Heimann Mühlenbock, 2013; Vajjala and Meurers, 2012) , L2 Swedish curricula (Levy Scherrer and Lindemalm, 2009; Folkuniversitet, 2013) and aspects of Good Dictionary Examples (GDEX) (Husák, 2010; Kilgarriff et al., 2008) , being that we believe they have some properties in common with exercise items."}
{"sent_id": "d66ca5ff22e508da239fc7fdf5ac29-C001-127", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_d66ca5ff22e508da239fc7fdf5ac29_20", "text": "Features DepDepth, Mod, Sub and RightDep, PrepComp have previously been empoyed for Swedish L1 readability at the text level in Heimann Mühlenbock (2013) and Falkenjack et al. (2013) respectively."}
{"sent_id": "d66ca5ff22e508da239fc7fdf5ac29-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_d66ca5ff22e508da239fc7fdf5ac29_20", "text": "Research on L1 readability for Swedish, using machine learning, is described in Heimann Mühlenbock (2013) and Falkenjack et al. (2013) ."}
{"sent_id": "d66ca5ff22e508da239fc7fdf5ac29-C001-139", "intents": ["@BACK@"], "paper_id": "ABC_d66ca5ff22e508da239fc7fdf5ac29_20", "text": "Both NomR and PN/NN capture idea density, i.e. how complex the relation between the ideas expressed are (Heimann Mühlenbock, 2013)."}
{"sent_id": "ca98f16fa3a118f83b16586bba04c8-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_ca98f16fa3a118f83b16586bba04c8_20", "text": "With the development of deep learning, sequence-to-sequence (Seq2Seq) neural networks or more generally encoder-decoder frameworks, are among the most popular models for text-based response generation in dialog systems [1, 2, 3, 4] ."}
{"sent_id": "ca98f16fa3a118f83b16586bba04c8-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_ca98f16fa3a118f83b16586bba04c8_20", "text": "They are universally relevant to most utterances, called universal replies in [3] , and hence less desired in real-world conversation systems."}
{"sent_id": "ca98f16fa3a118f83b16586bba04c8-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_ca98f16fa3a118f83b16586bba04c8_20", "text": "This conjecture is casually expressed in previous work [3] , but is so far not supported by experiments."}
{"sent_id": "649eff228a47b484d01872a980e58f-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_649eff228a47b484d01872a980e58f_20", "text": "Recently, with the renaissance of artificial neural networks in the form of deep learning algorithms, neural network (NN) based KWS has become very popular [5, 6, 7, 8] ."}
{"sent_id": "649eff228a47b484d01872a980e58f-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_649eff228a47b484d01872a980e58f_20", "text": "Although many neural network models for KWS are presented in literature, it is difficult to make a fair comparison between them as they are all trained and evaluated on different proprietary datasets (e.g. \"TalkType\" dataset in [7] , \"Alexa\" dataset in [8] , etc.) with different input speech features and audio duration."}
{"sent_id": "649eff228a47b484d01872a980e58f-C001-149", "intents": ["@BACK@"], "paper_id": "ABC_649eff228a47b484d01872a980e58f_20", "text": "Figure 5 shows the number of operations per inference, memory requirement and test accuracy of neural network models from prior work [5, 6, 7, 8] trained on Google speech commands dataset overlayed with the memory and compute bounding boxes for the neural network classes from section 4."}
{"sent_id": "48e3715c55fcc188367dcfdc26c05f-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_48e3715c55fcc188367dcfdc26c05f_20", "text": "The work of Grefenstette and Sadrzadeh (2011a) was the first large-scale practical implementation of this framework for intransitive and transitive sentences, and thus a first step towards providing some concrete answers to these questions."}
{"sent_id": "44916cd85311c78666839a3376ccc6-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_44916cd85311c78666839a3376ccc6_20", "text": "Recent research in the area of unit segmentation (Eger et al., 2017; Ajjour et al., 2017) has lead to promising results with F1-scores of up to 0.90 for in-domain segmentation (Eger et al., 2017) ."}
{"sent_id": "3bbc588f06e326e1d75985fe253a5f-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_3bbc588f06e326e1d75985fe253a5f_20", "text": "For backward refinement (Artetxe et al., 2018b) , source synthetic data were generated from the target monolingual data using the target to source phrase table P (0) t→s and source language model LM s ."}
{"sent_id": "3bbc588f06e326e1d75985fe253a5f-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_3bbc588f06e326e1d75985fe253a5f_20", "text": "Artetxe et al. (2018b) and Lample et al. (2018) reported that the BLEU score (Papineni et al., 2002) of unsupervised MT with backward-refinement improves with increasing iterations."}
{"sent_id": "bb2609c568540390a560757dd40b32-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_bb2609c568540390a560757dd40b32_20", "text": "In this section we briefly describe the data and evaluation metrics used in [1] ."}
{"sent_id": "bb2609c568540390a560757dd40b32-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_bb2609c568540390a560757dd40b32_20", "text": "The pointwise architectures reported in [1] included (i) TF-IDF, (ii) RNN and (iii) LSTM."}
{"sent_id": "831342435ca0a4695e2a7f149891e4-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_831342435ca0a4695e2a7f149891e4_20", "text": "[21] further showed that use of synthetic training data can work better than multitask training."}
{"sent_id": "73eaa7d5a54b2d60bd8128e0270683-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_73eaa7d5a54b2d60bd8128e0270683_20", "text": "For instance, both Lample et al. (2016) and Ma and Hovy (2016) propose end-to-end models for sequence labelling task and achieve state-of-the-art results."}
{"sent_id": "73eaa7d5a54b2d60bd8128e0270683-C001-38", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_73eaa7d5a54b2d60bd8128e0270683_20", "text": "Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016) have demonstrated that CNNs are highly capable of capturing character-level features."}
{"sent_id": "4e7ee576b07a8a21a42472bf921291-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_4e7ee576b07a8a21a42472bf921291_20", "text": "This paper studies the stability of K-NRM, a recent state-of-the-art neural ranking model [10] ."}
{"sent_id": "4e7ee576b07a8a21a42472bf921291-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_4e7ee576b07a8a21a42472bf921291_20", "text": "Interaction-based models use local interactions between the query and document words and neural networks that learn matching patterns [2, 10] ."}
{"sent_id": "7cb7cfed8b7e7bf2f0a810e02e6cbc-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_7cb7cfed8b7e7bf2f0a810e02e6cbc_20", "text": "Abstractive Summarization using AMR: In Liu et al. (2015) work, the source document's sentences were parsed into AMR graphs, which were then combined through merging, collapsing and graph expansion into a single AMR graph representing the source document."}
{"sent_id": "7cb7cfed8b7e7bf2f0a810e02e6cbc-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_7cb7cfed8b7e7bf2f0a810e02e6cbc_20", "text": "In Liu et al. (2015) 's work, each of the sentence of the source document was parsed into an AMR graph, and combined into a source graph, G = (V, E) where v ∈ V and e ∈ E are the unique concepts and the relations between pairs of concepts."}
{"sent_id": "9c8c5da4cdd13efb187690e7d3aa20-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_9c8c5da4cdd13efb187690e7d3aa20_20", "text": "Previous work on paraphrase generation that used these datasets (Wang et al., 2019; Gupta et al., 2018; Li et al., 1 https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs 2018; Prakash et al., 2016) chose BLEU (Papineni et al., 2002) , METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) as evaluation metrics."}
{"sent_id": "9c8c5da4cdd13efb187690e7d3aa20-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_9c8c5da4cdd13efb187690e7d3aa20_20", "text": "There have been multiple works which use it as a paraphrase generation dataset by treating captions of the same image as paraphrases (Wang et al., 2019; Gupta et al., 2018; Prakash et al., 2016) ."}
{"sent_id": "e9779b09826d709f8851550d958df7-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_e9779b09826d709f8851550d958df7_20", "text": "These corpora, as well as deep learning models, lead to contributions in multilingual language grounding and learning of shared and multimodal representations with neural networks [4, 7, 8, 9, 10, 11, 12, 13] ."}
{"sent_id": "d0007c7f1f9ecfbdd7b6ad7c59cc92-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_d0007c7f1f9ecfbdd7b6ad7c59cc92_20", "text": "First, Chen et al. (2017) compared results across SemEval's laptop and restaurant reviews in English (Pontiki et al., 2014) , a Twitter dataset (Dong et al., 2014) and their own Chinese news comments dataset."}
{"sent_id": "bbd8c1007b573758fc78a6d16e2b77-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_bbd8c1007b573758fc78a6d16e2b77_20", "text": "Many stochastic parsing models use linguistic intuitions to find this minimal set, for example by restricting the statistical dependencies to the locality of headwords of constituents (Collins 1997 (Collins , 1999 Eisner 1997) , leaving it as an open question whether there exist important statistical dependencies that go beyond linguistically motivated dependencies."}
{"sent_id": "bbd8c1007b573758fc78a6d16e2b77-C001-122", "intents": ["@BACK@"], "paper_id": "ABC_bbd8c1007b573758fc78a6d16e2b77_20", "text": "Such \"head-lexicalized stochastic grammars\" have recently become increasingly popular (e.g. Collins 1997 Collins , 1999 Charniak 1997 Charniak , 2000 and are based on Magerman's head-percolation scheme to determine the headword of each nonterminal (Magerman 1995) ."}
{"sent_id": "2357152e66ad3ae1c23738ac95f971-C001-4", "intents": ["@BACK@"], "paper_id": "ABC_2357152e66ad3ae1c23738ac95f971_21", "text": "The most widely used technique is the use of beam search with n-gram LMs proposed by Nuhn et al. (2013) ."}
{"sent_id": "2357152e66ad3ae1c23738ac95f971-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_2357152e66ad3ae1c23738ac95f971_21", "text": "Finding this argmax is solved using a beam search algorithm (Nuhn et al., 2013) which incrementally finds the most likely substitutions using the language model scores as the ranking."}
{"sent_id": "2357152e66ad3ae1c23738ac95f971-C001-101", "intents": ["@BACK@"], "paper_id": "ABC_2357152e66ad3ae1c23738ac95f971_21", "text": "Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm (Nuhn et al., 2013) with beam size of 10M with a 6-gram LM which gives an SER of 2%."}
{"sent_id": "c91781e76a8d7d6de7d7bc4407e799-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_c91781e76a8d7d6de7d7bc4407e799_21", "text": "With the advances of deep learning, neural sequence labeling models have achieved state-ofthe-art for many tasks (Ling et al., 2015; Ma and Hovy, 2016; Peters et al., 2017) ."}
{"sent_id": "c91781e76a8d7d6de7d7bc4407e799-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_c91781e76a8d7d6de7d7bc4407e799_21", "text": "It builds a LSTM-CRF framework with CNN to encode character sequence (the same structure as Ma and Hovy (2016) ), plus POS and Cap features, within 10 lines."}
{"sent_id": "c91781e76a8d7d6de7d7bc4407e799-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_c91781e76a8d7d6de7d7bc4407e799_21", "text": "• Word RNN together with GRU and LSTM are available in NCRF++, which are popular structures in the recent literature (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2017) ."}
{"sent_id": "92e43071b2a9b05b5d96277c1aa250-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_92e43071b2a9b05b5d96277c1aa250_21", "text": "In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014) ."}
{"sent_id": "ee8163c5a76ed9f929a960b3086356-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_ee8163c5a76ed9f929a960b3086356_21", "text": "By contrast, most of the prior work depend on parallel data in the form of a small bitext (Genzel, 2005) , a gold seed lexicon (Mikolov et al., 2013b) , or document-aligned comparable corpora (Vulić and Moens, 2015) ."}
{"sent_id": "a82bdc55c15bb2bcee77c57641b1b5-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_a82bdc55c15bb2bcee77c57641b1b5_21", "text": "Classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence (Lin et al., 2006; Malouf and Mullen, 2008; Greene and Resnik, 2009; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010) ."}
{"sent_id": "a82bdc55c15bb2bcee77c57641b1b5-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_a82bdc55c15bb2bcee77c57641b1b5_21", "text": "They show that discourse relations such as concessions and the identification of argumentation triggers improves performance over sentiment features alone (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010) ."}
{"sent_id": "a82bdc55c15bb2bcee77c57641b1b5-C001-134", "intents": ["@BACK@"], "paper_id": "ABC_a82bdc55c15bb2bcee77c57641b1b5_21", "text": "Previous work suggests that the unigram baseline can be difficult to beat for certain types of debates (Somasundaran and Wiebe, 2010 )."}
{"sent_id": "a82bdc55c15bb2bcee77c57641b1b5-C001-191", "intents": ["@BACK@"], "paper_id": "ABC_a82bdc55c15bb2bcee77c57641b1b5_21", "text": "The overal lack of impact for either the POS generalized dependency features (GDepP) or the Opinion generalized dependency features (GDep0) is surprising given that they improve accuracy for other similar tasks (Joshi and Penstein-Rosé, 2009; Somasundaran and Wiebe, 2010) ."}
{"sent_id": "3bb6243de9f77fc6ebf2dc24de7faa-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_3bb6243de9f77fc6ebf2dc24de7faa_21", "text": "Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations Cheng et al., 2018; Clinchant and Perronnin, 2013; Conneau et al., 2017; Cozma et al., 2018; Fu et al., 2018; Hill et al., 2016; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 ."}
{"sent_id": "3bb6243de9f77fc6ebf2dc24de7faa-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_3bb6243de9f77fc6ebf2dc24de7faa_21", "text": "We compare VLAWE with recent stateof-the-art methods Cheng et al., 2018; Fu et al., 2018; Hill et al., 2016; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 , demonstrating the effectiveness of our approach."}
{"sent_id": "3bb6243de9f77fc6ebf2dc24de7faa-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_3bb6243de9f77fc6ebf2dc24de7faa_21", "text": "While most of these approaches are based on deep learning (Cheng et al., 2018; Conneau et al., 2017; Hill et al., 2016; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Zhao et al., 2015; Zhou et al., 2018) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (Clinchant and Perronnin, 2013) ."}
{"sent_id": "3bb6243de9f77fc6ebf2dc24de7faa-C001-101", "intents": ["@BACK@"], "paper_id": "ABC_3bb6243de9f77fc6ebf2dc24de7faa_21", "text": "We compare VLAWE with several state-of-theart methods Cheng et al., 2018; Fu et al., 2018; Hill et al., 2016; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW)."}
{"sent_id": "fe8d369d4a6f940a1eb25aa7c9b4fe-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_fe8d369d4a6f940a1eb25aa7c9b4fe_21", "text": "Alternatively, one can rerank the n-best outputs by coverage-sensitive models, but this method just affects the final output list which has a very limited scope (Wu et al., 2016) ."}
{"sent_id": "fe8d369d4a6f940a1eb25aa7c9b4fe-C001-118", "intents": ["@BACK@"], "paper_id": "ABC_fe8d369d4a6f940a1eb25aa7c9b4fe_21", "text": "Perhaps the most related work to this paper is Wu et al. (2016) ."}
{"sent_id": "f32bbd580d93f77ef764c5341b93db-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_f32bbd580d93f77ef764c5341b93db_21", "text": "One approach is to utilize unpaired text data to produce a separately trained language model (LM) to rescore the output of the end-to-end approach [18, 13, 19, 20] , but at the price of extra computation during testing."}
{"sent_id": "f32bbd580d93f77ef764c5341b93db-C001-114", "intents": ["@BACK@"], "paper_id": "ABC_f32bbd580d93f77ef764c5341b93db_21", "text": "Speech recognition performance. \"+LM\" refers to shallow fusion decoding jointly with RNN-LM [13] , \"+AT\" refers to the adversarial training proposed here, \"+Both\" indicates training with AT and joint decoding with RNN-LM, and BT is the prior work of back-translation [21] ."}
{"sent_id": "5a000efaa052588f6cfbb69f8ced2d-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_5a000efaa052588f6cfbb69f8ced2d_21", "text": "To this end, Richardson et al. (2013) proposed the Machine Comprehension Test (MCTest), a new challenge that aims at evaluating machine comprehension."}
{"sent_id": "5a000efaa052588f6cfbb69f8ced2d-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_5a000efaa052588f6cfbb69f8ced2d_21", "text": "3 Scoring function Richardson et al. (2013) proposed a sliding window algorithm that ranks the answers by forming the bag-of-words vector of each answer paired with the question text and then scoring them according to their overlap with the story text."}
{"sent_id": "5a000efaa052588f6cfbb69f8ced2d-C001-122", "intents": ["@BACK@"], "paper_id": "ABC_5a000efaa052588f6cfbb69f8ced2d_21", "text": "Richardson et al. (2013) demonstrate that the MC160 and MC500 have similar ratings for clarity and grammar, and that humans perform equally well on both."}
{"sent_id": "10de18ba49c0da530b15ff2d14f343-C001-71", "intents": ["@USE@"], "paper_id": "ABC_10de18ba49c0da530b15ff2d14f343_21", "text": "We use various types of word embeddings adapted from [3] ."}
{"sent_id": "13fe4afa75c5a02727cb8ce3a73297-C001-5", "intents": ["@USE@"], "paper_id": "ABC_13fe4afa75c5a02727cb8ce3a73297_22", "text": "When combined with our two-stage fine-tuning pipeline, our method achieves improved common sense reasoning and state-of-the-art perplexity on the WritingPrompts (Fan et al., 2018) story generation dataset."}
{"sent_id": "13fe4afa75c5a02727cb8ce3a73297-C001-22", "intents": ["@USE@"], "paper_id": "ABC_13fe4afa75c5a02727cb8ce3a73297_22", "text": "When fine-tuning is combined with multi-task learning in a two-stage pipeline, we improve the model's CSR and outperform state-of-the-art perplexity on the WritingPrompts (Fan et al., 2018) Our primary task is to perform language modeling (Elman, 1990; Bengio et al., 2003; Dai and Le, 2015) on the WritingPrompts dataset."}
{"sent_id": "2ef456a3f6b043350121c4c5cfd404-C001-100", "intents": ["@USE@"], "paper_id": "ABC_2ef456a3f6b043350121c4c5cfd404_22", "text": "Each of the models is trained using the proposed B-NCE approach and the shared noise NCE (S-NCE) [16] ."}
{"sent_id": "2ef456a3f6b043350121c4c5cfd404-C001-115", "intents": ["@USE@"], "paper_id": "ABC_2ef456a3f6b043350121c4c5cfd404_22", "text": "Following the setup proposed in [13, 16] , S-NCE uses K = 100 noise samples, whereas B-NCE uses only the target words in the batch (K=0)."}
{"sent_id": "375b9c865d9f1b559387aa01a20a78-C001-60", "intents": ["@USE@"], "paper_id": "ABC_375b9c865d9f1b559387aa01a20a78_22", "text": "As baseline systems, we use C-BOW, Skipgram (Mikolov et al., 2013) , Subword Information Skip-gram (SISG) (Bojanowski et al., 2017) and Segmentation-free word embedding for unsegmented languages (Sembei) (Oshikiri, 2017) for the word-level tasks."}
{"sent_id": "be39cfec0479ace0a7e08508239cb0-C001-15", "intents": ["@USE@"], "paper_id": "ABC_be39cfec0479ace0a7e08508239cb0_22", "text": "We specifically investigate the paragraph embedding method of Zhang et al. (2017) , which consists of a CNN-based encoder-decoder model paired with a reconstruction objective to learn powerful paragraph embeddings that are capable of accurately reconstructing long paragraphs."}
{"sent_id": "45551e674210bb9bbb56c8778d2f8c-C001-43", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_45551e674210bb9bbb56c8778d2f8c_22", "text": "We first describe the approach of Mishra et al. (2018a) that learns author embeddings using node2vec (Grover and Leskovec, 2016) ; this serves as our baseline."}
{"sent_id": "45551e674210bb9bbb56c8778d2f8c-C001-51", "intents": ["@USE@"], "paper_id": "ABC_45551e674210bb9bbb56c8778d2f8c_22", "text": "Following Mishra et al. (2018a) , we initialize these parameters to their default value of 1 and set the embedding size and number of iterations to 200 and 25 respectively."}
{"sent_id": "ebb79e6e223d4747987aa4abfd1a58-C001-116", "intents": ["@USE@"], "paper_id": "ABC_ebb79e6e223d4747987aa4abfd1a58_22", "text": "this study, we apply the USMT method of Artetxe et al. (2018b) and Marie and Fujita (2018) to GEC."}
{"sent_id": "3ced64da2c64b0963c4c3d88fd60e0-C001-81", "intents": ["@USE@"], "paper_id": "ABC_3ced64da2c64b0963c4c3d88fd60e0_22", "text": "As an evaluation measure, we follow Nagata (1994) and Kudo et al. (2004) and use Word/POS tag pair Fmeasure, so that both word boundaries and POS tags must be correct for a word to be considered correct."}
{"sent_id": "022049c0e75a490978b2c49da41deb-C001-34", "intents": ["@USE@"], "paper_id": "ABC_022049c0e75a490978b2c49da41deb_22", "text": "Our first method for building vectors is that of Salehi et al. (2014b) : the top 50 most-frequent words in the training corpus are considered to be stopwords and discarded, and words with frequency rank 51-1051 are considered to be the content-bearing words, which form the dimensions for our vectors, in the manner of Schütze (1997) ."}
{"sent_id": "742d9ca22bf801b0ade5fd1671473c-C001-21", "intents": ["@USE@"], "paper_id": "ABC_742d9ca22bf801b0ade5fd1671473c_22", "text": "In this paper, we thoroughly examine datadriven techniques on three larger algebra word problem datasets (Huang et al., 2016; Koncel-Kedziorski et al., 2016; Wang et al., 2017) ."}
{"sent_id": "742d9ca22bf801b0ade5fd1671473c-C001-72", "intents": ["@USE@"], "paper_id": "ABC_742d9ca22bf801b0ade5fd1671473c_22", "text": "Following Wang et al. (2017) we evaluate a seq2seq with LSTMs as the encoder and decoder."}
{"sent_id": "742d9ca22bf801b0ade5fd1671473c-C001-82", "intents": ["@USE@"], "paper_id": "ABC_742d9ca22bf801b0ade5fd1671473c_22", "text": "To prune these quantities, we implement a significant number identifier (SNI) as discussed in Wang et al. (2017) ."}
{"sent_id": "a5f00f524fdf18e62a4e98a92a2d82-C001-46", "intents": ["@USE@"], "paper_id": "ABC_a5f00f524fdf18e62a4e98a92a2d82_22", "text": "Our approach differs from the ones described, in that we use automatic MT to translate Arabic tweets into English and then perform SA using a stateof-the-art SA classifier for English (Socher et al., 2013) ."}
{"sent_id": "a5f00f524fdf18e62a4e98a92a2d82-C001-62", "intents": ["@USE@"], "paper_id": "ABC_a5f00f524fdf18e62a4e98a92a2d82_22", "text": "Using Socher et al. (2013) 's approach for directly training a sentiment classifier will require a larger training data-set, which is not available yet for Ara-bic 3 ."}
{"sent_id": "26658b95c9bac96f1206da96b95921-C001-60", "intents": ["@USE@"], "paper_id": "ABC_26658b95c9bac96f1206da96b95921_22", "text": "Figure 2: Principle of compositionality, illustration taken from Rudolph and Giesbrecht (2010) In compositional matrix-space models, this general idea is instantiated as follows: we have S = R n×n , i.e., the semantical space consists of quadratic matrices of real numbers."}
{"sent_id": "237ac6f9b635e56119be956d7521e1-C001-61", "intents": ["@USE@"], "paper_id": "ABC_237ac6f9b635e56119be956d7521e1_22", "text": "Therefore, for the neural models, we just picked the implementation provided in (Rei et al., 2016) ."}
{"sent_id": "cd56849805cdb43bba567f74b31b87-C001-36", "intents": ["@USE@"], "paper_id": "ABC_cd56849805cdb43bba567f74b31b87_22", "text": "We use Liu et al. (2018) 's Bi-LSTM as baseline."}
{"sent_id": "a0c0076fa8c3be914d93ec1d66d0c1-C001-38", "intents": ["@USE@"], "paper_id": "ABC_a0c0076fa8c3be914d93ec1d66d0c1_22", "text": "In addition, we apply our models to two existing collections for derivational patterns, the German dataset from Kisselew et al. (2015) , comprising six derivational patterns with 80 in-stances each (cf."}
{"sent_id": "c022b7cf4568e26c7408a835eaafb7-C001-84", "intents": ["@USE@"], "paper_id": "ABC_c022b7cf4568e26c7408a835eaafb7_22", "text": "We also compare against model combination using our reimplementation of Sagae and Lavie (2006) ."}
{"sent_id": "520588fbf0643725153b07a09430d1-C001-96", "intents": ["@USE@"], "paper_id": "ABC_520588fbf0643725153b07a09430d1_23", "text": "To compensate the lack of training data on low-resource languages, a well-trained ASR Transformer with a CER of 26.64% on HKUST dataset, a corpus of Mandarin Chinese conversational telephone speech, is adopted from our work [9] ."}
{"sent_id": "874a8d4f847aff2895deb7c7560c56-C001-133", "intents": ["@USE@"], "paper_id": "ABC_874a8d4f847aff2895deb7c7560c56_23", "text": "The dataset was provided by [9] ."}
{"sent_id": "1bcd442a685e5fb2d0f3f44d3c66c3-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_1bcd442a685e5fb2d0f3f44d3c66c3_23", "text": "Several recent works (Lazaridou, Peysakhovich, and Baroni 2016; Havrylov and Titov 2017; Lazaridou et al. 2018; Mordatch and Abbeel 2018) , have shown that in multi-agent cooperative setting of referential games, deep reinforcement learning can successfully induce communication protocols."}
{"sent_id": "497b717bc4ff6b9e2160ee823f6b42-C001-199", "intents": ["@USE@"], "paper_id": "ABC_497b717bc4ff6b9e2160ee823f6b42_24", "text": "We use HJ-dataset, which was created for RUSSE contest [10] to measure correlation between similarity of word vectors and human judgements on word pairs similarity."}
{"sent_id": "e48a1eac39987cb2f504b66d135572-C001-21", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_e48a1eac39987cb2f504b66d135572_25", "text": "Texts from biomedical publications and electronic medical record have been used to pre-train BERT models for NLP task in this domain and showed considerable improvement in many downstream tasks (Lee et al., 2019; Alsentzer et al., 2019; Si et al., 2019) ."}
{"sent_id": "1527ce2786adfe0decf8c926a3d846-C001-43", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_1527ce2786adfe0decf8c926a3d846_25", "text": "Embeddings identify words that share context in an unsupervised, scalable way and are more efficient than constructing co-occurrence matrices (Biran et al., 2011) ."}
{"sent_id": "13d3d973a4be832f66b049b364fea5-C001-54", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_13d3d973a4be832f66b049b364fea5_25", "text": "Alikaniotis et al. (2016) applied a similar idea; in their SSWE model, they trained word embeddings to distinguish between correct and noisy contexts in addition to focusing more on each word's contribution to the overall text score."}
{"sent_id": "d92e92b9a375914f3dd74868f463fc-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d92e92b9a375914f3dd74868f463fc_26", "text": "An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings (Cífka and Bojar, 2018) ."}
{"sent_id": "d42e2a9175e024e3ae44118e12fb58-C001-86", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_d42e2a9175e024e3ae44118e12fb58_26", "text": "7 https://github.com/ EducationalTestingService/skll 8 https://github.com/fchollet/keras 9 The implementation will be released with the paper On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in Yang et al. (2015) ."}
{"sent_id": "d9877fc29c2e4f20805076392a70d0-C001-38", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d9877fc29c2e4f20805076392a70d0_26", "text": "These models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor (Kim et al., 2014; Hellrich and Hahn, 2016b) , or a mapping between models for different points in time must be calculated (Kulkarni et al., 2015; Hamilton et al., 2016) ."}
{"sent_id": "cee22bd0384d3d3fd4e45833341e77-C001-99", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_cee22bd0384d3d3fd4e45833341e77_26", "text": "These numbers, however, do not show to which extent the models are able to avoid the trap of the dataset: Shekhar et al. (2017) showed that on the FOIL data, models tend to detect correct captions with reasonable accuracy but fail to identify the incorrect ones, leading to a large bias in classification."}
{"sent_id": "499580e888a1598681a8d877b07866-C001-23", "intents": ["@BACK@", "@USE@", "@EXT@"], "paper_id": "ABC_499580e888a1598681a8d877b07866_26", "text": "Recurrent neural network (RNN) based quality estimation model (Kim and Lee, 2016) consists of two parts: two bidirectional RNNs on the source and target sentences in the first part and another RNN for predicting the quality in the second part."}
{"sent_id": "35ef3eba487c3cd97d32210670678a-C001-21", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_35ef3eba487c3cd97d32210670678a_26", "text": "The ability of NLI systems to generalize and related skepticism has been raised in a recent paper by Glockner et al. (2018) ."}
{"sent_id": "28900a293048fdb0c40dc540985cf1-C001-11", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_28900a293048fdb0c40dc540985cf1_26", "text": "Therefore, Kann et al. (2017) propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language)."}
{"sent_id": "ce8997b630e9544b0f5812be319a59-C001-21", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_ce8997b630e9544b0f5812be319a59_26", "text": "Recent work by Lau et al. (2018) proposes a quatrain generation method that relies on specific domain knowledge about the dataset to train a classifier for learning the notion of rhyming: that a line ending word always rhymes with exactly one more ending word in the poem."}
{"sent_id": "9227b5afd1ef18ecf83400dc402459-C001-32", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9227b5afd1ef18ecf83400dc402459_27", "text": "They are very often observed in Mandarin spoken conversations as mentioned in Tseng (2001) and Clancy et al. (1996) ."}
{"sent_id": "6f4dc72277119f0df3d4a7155c61fc-C001-11", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_6f4dc72277119f0df3d4a7155c61fc_27", "text": "Methods for RNN compression can be divided into three groups: based on matrix factorization [6, 19] , quantization [7] or sparsification [2, 15, 20] ."}
{"sent_id": "74420437db295ca874d5c946891f69-C001-3", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_74420437db295ca874d5c946891f69_27", "text": "Tsvetkov and Wintner (2014) proposed a Bayesian network model that combines linguistically motivated features and also models their interactions."}
{"sent_id": "74420437db295ca874d5c946891f69-C001-22", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_74420437db295ca874d5c946891f69_27", "text": "Tsvetkov and Wintner (2014) showed that a manually-designed BN substantially outperforms the one whose structure is learned automatically, hypothesizing that the cause for this might be the increased model complexity."}
{"sent_id": "74420437db295ca874d5c946891f69-C001-36", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_74420437db295ca874d5c946891f69_27", "text": "The model of Tsvetkov and Wintner (2014) uses nine statistically and linguistically motivated features, computed for each MWE candidate and designed to discriminate between MWEs and ordinary word sequences."}
{"sent_id": "91723cf7f22ba6405c85a929ac2d8e-C001-24", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_91723cf7f22ba6405c85a929ac2d8e_27", "text": "This insight has been reflected by the anonymization process in construction of the dataset, in which coreferent entities (e.g. \"Robert Downey Jr.\" and \"Downey\") are replaced by randomly permuted abstract entity markers (e.g. \"@en-tity0\"), in order to prevent additional world knowledge from being attached to the surface form of the entities (Hermann et al., 2015) ."}
{"sent_id": "2e967f8560ffdb216135ae387776eb-C001-57", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_2e967f8560ffdb216135ae387776eb_27", "text": "Hancke et al. (2012) first compiled and analyzed a data set from this web resource."}
{"sent_id": "bdeffcf02a86d06f57dbfae979b098-C001-43", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_bdeffcf02a86d06f57dbfae979b098_27", "text": "In their original work Mimno et al. (2009) used the Gibbs sampling approach as a posterior inference algorithm to assign topics distributions over their test collection."}
{"sent_id": "318487ac270ca272ec11a3de6c0685-C001-30", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_318487ac270ca272ec11a3de6c0685_28", "text": "This is inspired by Mihalcea et al. (2006) and Guo and Diab (2012) , who treat sentences as pseudo-documents in an LSA framework, and identify paraphrases using similarity in the latent space."}
{"sent_id": "591e2873606d6171e48fd34a731fc7-C001-24", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_591e2873606d6171e48fd34a731fc7_29", "text": "Previous work typically considered cities with a population of at least 100K (Han et al., 2012 (Han et al., , 2014 ."}
{"sent_id": "17d44521cfdd351d29b4e5f80d41cd-C001-27", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_17d44521cfdd351d29b4e5f80d41cd_29", "text": "Recent research have focused on action sets that build projective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003; Huang and Sagae, 2010) process."}
{"sent_id": "685b0b0da37b81765bb78f0f87505b-C001-10", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_685b0b0da37b81765bb78f0f87505b_29", "text": "Neural sequence-to-sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder-aligner-decoder approaches, further enhanced via convolutional encoders, pointer-copy mechanisms, and hierarchical attention (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017) ."}
{"sent_id": "89b2b492b4319636ff2f28a4ba0d95-C001-123", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_89b2b492b4319636ff2f28a4ba0d95_30", "text": "Another important reason, as pointed out by McDonald and Nivre (2007) , is the default single-root mechanism by MaltParser: all words that have not been attached as a modifier when the shift-reduce process finishes are attached as modifiers to the pseudo-root."}
{"sent_id": "021e5dbe22bf0f4ebda4d37040d0a6-C001-98", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_021e5dbe22bf0f4ebda4d37040d0a6_31", "text": "This is in stark contrast to the results of McDonald et al. (2011) , who observe that this is rarely the case with the heterogenous CoNLL treebanks."}
{"sent_id": "61f88b86c451fb6a5e5893c8c42a24-C001-64", "intents": ["@MOT@", "@SIM@"], "paper_id": "ABC_61f88b86c451fb6a5e5893c8c42a24_31", "text": "We support the observation of Ramanand et al. (2010) that wishes for improvements and new features are implicit expression of suggestions."}
{"sent_id": "8905d5936a5b2a839bfd56783ff55d-C001-42", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_8905d5936a5b2a839bfd56783ff55d_32", "text": "Previous work shows that a simple model trained on the whole corpus of 88,855 utterances produces semantically correct outputs, but with reduced stylistic variation [5] , while a model that allocates a variable corresponding to a label for each style learns to reproduce the stylistic variation."}
{"sent_id": "b208c7180bc3f973b8616937b2801c-C001-46", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_b208c7180bc3f973b8616937b2801c_33", "text": "This model is similar to the \"flat\" model in Krause et al. (2016) , except that it incorporates attention with a top-down mechanism."}
{"sent_id": "c6ae69051a6d9111dea1a6e8405ac9-C001-63", "intents": ["@DIF@", "@BACK@"], "paper_id": "ABC_c6ae69051a6d9111dea1a6e8405ac9_34", "text": "Within forced decoding, Wuebker et al. (2010) address this problem by using a leave-one-out approach where they modify the phrase translation probabilities for each sentence pair by removing the counts of all phrases that were extracted from that particular sentence."}
{"sent_id": "8e59c2c48e27b2abd5f63d6b4ce23d-C001-106", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_8e59c2c48e27b2abd5f63d6b4ce23d_34", "text": "As the training data size increases, one would expect the likelihood of observing rare words to decrease, especially in languages with low morphological complexity, along with the significance of representing rare and unseen words (Cherry et al., 2018) ."}
{"sent_id": "3e344c590b4d5270a29054ac15efa5-C001-185", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_3e344c590b4d5270a29054ac15efa5_35", "text": "With larger datasets, this model will be able to perform significantly better [2] ."}
{"sent_id": "fd9122d20c390ea115c27092170739-C001-43", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_fd9122d20c390ea115c27092170739_35", "text": "Second, Wu and Jiang (2000) argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48)."}
{"sent_id": "196e7ca5ccd6754ac986137ec55cd3-C001-16", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_196e7ca5ccd6754ac986137ec55cd3_36", "text": "• VOILA's dialogue strategy is optimised via Reinforcement Learning to achieve an optimal trade-off between the accuracy of the concepts it learns/has learnt from users, and the effort that the dialogues incur on the users: this is a form of active learning where the agent only asks about something if it doesn't already know the answer with some appropriate confidence (see (Yu et al., 2016) for more detail)."}
{"sent_id": "db1fd6f10a3ee22e22093d50395217-C001-28", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_db1fd6f10a3ee22e22093d50395217_37", "text": "The other attempt of same 6 way PIBOSO classification on the same dataset is presented by (Verbeke et al., 2012) ."}
{"sent_id": "2db25254f275303c41f1e7ab15a5e0-C001-25", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_2db25254f275303c41f1e7ab15a5e0_38", "text": "Rutherford and Xue (2015) explore several selection heuristics for adding automatically-labeled examples from Gigaword to their system for implicit relation detection, obtaining a 2% improvement in Macro-F 1 ."}
{"sent_id": "a9897f66e05a0354c36daba0db9afe-C001-50", "intents": ["@UNSURE@"], "paper_id": "ABC_a9897f66e05a0354c36daba0db9afe_40", "text": "3 The figures confirm the claim of McClosky et al. (2006a) that self-training with a reranking parsing model is effective for improving parser accuracy in general, and the claim of Gildea (2001) that training on in-domain data is effective for parser adaption."}
{"sent_id": "f60796ff05156e81c4b183cdcb05ae-C001-61", "intents": ["@UNSURE@"], "paper_id": "ABC_f60796ff05156e81c4b183cdcb05ae_40", "text": "• Single task bi-directional LSTM • Multi-task bi-directional stacked LSTM model [6, 9] • Concept tagging model using slot label descriptions For all our experiments we use 200 dimensional word2vec embeddings trained on the GNews corpus [23] ."}
{"sent_id": "5aeb64701a6b7d9878ea5e14a87b4e-C001-17", "intents": ["@UNSURE@"], "paper_id": "ABC_5aeb64701a6b7d9878ea5e14a87b4e_40", "text": "We stand in this tradition by reporting on a recent annotation effort (Kremer et al., 2014 ) that collected lexical substitutes for content words in part of the MASC corpus (Ide et al., 2008) ."}
{"sent_id": "d6c8b712c8fe3dd87d23886d575098-C001-38", "intents": ["@UNSURE@"], "paper_id": "ABC_d6c8b712c8fe3dd87d23886d575098_40", "text": "Inspired by Schamper et al. (2018) 's work which used KenLM (Heafield, 2011) for data selection, we trained two neural language models based on self-attention networks using the 2018 part of the large monolingual News crawl corpus for English and German, respectively."}
{"sent_id": "15bacab4a8c520cfcdd7e7bd1e9ec5-C001-18", "intents": ["@UNSURE@"], "paper_id": "ABC_15bacab4a8c520cfcdd7e7bd1e9ec5_46", "text": "(Yu et al., 2017; Li et al., 2017; Wang and Lee, 2018; Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018 ), adversarial evaluation (Elliott, 2018 , and reward learning (Wang et al., 2018c) ."}
{"sent_id": "4b8de05982074c30f4d03af60827cb-C001-17", "intents": ["@UNSURE@"], "paper_id": "ABC_4b8de05982074c30f4d03af60827cb_47", "text": "Within visual dia-log, we will look at recent work that uses cooperative multi-agent tasks as a proxy for training effective visual conversational models via RL (Kottur et al., 2017; Das et al., 2017b) ."}
{"sent_id": "b3952c840ce970f0e66460ea6e145a-C001-215", "intents": ["@DIF@", "@FUT@"], "paper_id": "ABC_b3952c840ce970f0e66460ea6e145a_0", "text": "We believe that the research direction of incorporating language models and multiple levels of representations can help to provide a wide set of rich features that can capture context-dependent semantics as well as linguistic features, such as seen on ELMo [33] downstream and linguistic probing task experiments, but for sentence embeddings."}
{"sent_id": "f28720b1597ca1273303f3774167f8-C001-74", "intents": ["@UNSURE@"], "paper_id": "ABC_f28720b1597ca1273303f3774167f8_2", "text": "Rows HZP strict and HZP containment repeat the figures for the best model from (Huang et al., 2001 ) when evaluated on automatic transcriptions."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-214", "intents": ["@UNSURE@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "However, we suspect that the method of Collobert et al. (2011b) is not noise resistant and therefore unsuitable for our lexicon because it fails to distinguish exact and partial matches 34 and does not set a minimum length for partial matching."}
{"sent_id": "b56e408c53636ac5fbf5149226319f-C001-191", "intents": ["@UNSURE@"], "paper_id": "ABC_b56e408c53636ac5fbf5149226319f_3", "text": "The dashed (\"-\") performance numbers were missing in the original paper (Miwa and Bansal, 2016) ."}
{"sent_id": "4a28a289ffc730fea4114f6c71bd06-C001-267", "intents": ["@UNSURE@"], "paper_id": "ABC_4a28a289ffc730fea4114f6c71bd06_3", "text": "This approach is able to solve SCAN tasks for compositional learning that have eluded standard NLP approaches, with the exception of generalizing to longer sequences [15] ."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-172", "intents": ["@UNSURE@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "(As this has not been done in Iida et al. (2011) , direct comparison is not possible.) For the evaluation, REs are binned into short, normal, and long (1-5, 6-8, 9-14 characters, respectively, based on what the average numbers of words in REs in this corpus is), to make relative statements (\"% into the utterance\") comparable."}
{"sent_id": "c384f48d5f04ea8d63bbbb94a3b24b-C001-123", "intents": ["@UNSURE@"], "paper_id": "ABC_c384f48d5f04ea8d63bbbb94a3b24b_3", "text": "|L| = 0.9n is included to match 10-fold cross validation used by (Pang and Lee, 2005) ."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-162", "intents": ["@UNSURE@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "The length of Tamil sentences was comparatively higher than Sinhala sentences and the correlation between Sinhala and Tamil was comparatively low, hence we cannot consider mean and variance as language independent as suggested by Gale and Church (1993) ."}
{"sent_id": "c327812b2369a1dfc8e2ce4077b997-C001-167", "intents": ["@UNSURE@"], "paper_id": "ABC_c327812b2369a1dfc8e2ce4077b997_3", "text": "Also according to Gale and Church (1993) , in this method one to zero alignment is never handled correctly."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-29", "intents": ["@UNSURE@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "Also, in Yannakoudakis et al. (2011) , experiments are presented that test the validity of the system using a number of automatically-created 'outlier' texts."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-41", "intents": ["@UNSURE@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "Again, we treat AA as a rank preference learning problem and use SVMs, utilizing the SVM light package (Joachims, 2002) , to facilitate comparison with Yannakoudakis et al. (2011) ."}
{"sent_id": "f8c992a887a7b7af8b3aa45f72dca7-C001-197", "intents": ["@UNSURE@"], "paper_id": "ABC_f8c992a887a7b7af8b3aa45f72dca7_4", "text": "20 See Yannakoudakis et al. (2011) for details."}
{"sent_id": "845c66e6dfafc21ab90e5aa5cbf947-C001-176", "intents": ["@UNSURE@"], "paper_id": "ABC_845c66e6dfafc21ab90e5aa5cbf947_4", "text": "This may raise the question of whether some simple fully connected model may be all that is needed; however, previous work has shown that this approach is not competitive [14] , and convolutional or recurrent layers are needed to summarize arbitrarylength segments into a fixed-dimensional representation."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-80", "intents": ["@UNSURE@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "Specifically, we are considering the question-answering agent ABOT from (Das et al. 2017b) as ABOT is the agent more likely to be deployed with a human partner in real applications (e.g. to answer questions about visual content to aid a visually impaired user)."}
{"sent_id": "9795a839cb79ed971de4c325e01e74-C001-89", "intents": ["@UNSURE@"], "paper_id": "ABC_9795a839cb79ed971de4c325e01e74_4", "text": "We refer to (Das et al. 2017b) for complete model details."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-4", "intents": ["@UNSURE@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "We consider the generative semantics-text correspondence model (Liang et al., 2009) and demonstrate that exploiting the noncontradiction relation between texts leads to substantial improvements over natural baselines on a problem of analyzing human-written weather forecasts."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-36", "intents": ["@UNSURE@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "In this paper, we do not consider models which aim to produce complete formal meaning of text (Zettlemoyer and Collins, 2005; Mooney, 2007; Poon and Domingos, 2009) , instead focusing on a simpler problem studied in (Liang et al., 2009) ."}
{"sent_id": "d1decbc03929cbf67a412d0a3a2a66-C001-129", "intents": ["@UNSURE@"], "paper_id": "ABC_d1decbc03929cbf67a412d0a3a2a66_4", "text": "For details on these emission models, as well as for details on modeling record and field transitions, we refer the reader to the original publication (Liang et al., 2009 )."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-153", "intents": ["@UNSURE@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "Table 8 lists the transfer learning results for our models with 1200D and 2400D hidden dimensionality and compares our model to the InferSent and SkipThought scores reported by Conneau et al. (2017) ."}
{"sent_id": "57ef27eefdf272bead22212863a8a8-C001-185", "intents": ["@UNSURE@"], "paper_id": "ABC_57ef27eefdf272bead22212863a8a8_4", "text": "Our primary comparison is with the heuristic LPR model of Brooke et al. (2015) , which is scalable to large corpora and includes gapped n-grams."}
{"sent_id": "34346688a7e5166ee7b559ccbfe8e3-C001-96", "intents": ["@UNSURE@"], "paper_id": "ABC_34346688a7e5166ee7b559ccbfe8e3_4", "text": "There are two possible explanations for why we only see a slight increase when adding more data: 1) AGs are able to generalize from small data and 2) the added Bible data represents a domain that is different from those of the datasets we are experimenting with as only 4.8% and 9% of the words in the training sets from Kann et al. (2018) appear in the augmented data of NH and WX, respectively."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-113", "intents": ["@UNSURE@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "We compare our method against multisense approaches in (Huang et al., 2012; Neelakantan et al., 2015; ."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-135", "intents": ["@UNSURE@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "For (Huang et al., 2012) different rows correspond to various types of distance used to get the contextual embedding."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-165", "intents": ["@UNSURE@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "We note that the cosine similarity is also used in (Neelakantan et al., 2015; Reisinger and Mooney, 2010a) Of course, this is disadvantageous for the embeddings of (Huang et al., 2012) , and our scores of their embedding are closer to that reported in (Neelakantan et al., 2015) , which also does not use averaging."}
{"sent_id": "50d065b6b187f361f8e456df0a0bbe-C001-141", "intents": ["@UNSURE@"], "paper_id": "ABC_50d065b6b187f361f8e456df0a0bbe_5", "text": "Koppel and Ordan (2011) claim that Toury's (1995) findings of interference of a translation hold true; we find the assumption to be too simplistic, since for Slavic text either as a source or target language this statement cannot supported."}
{"sent_id": "cb64ba694c37df9ebc1065a1deac0f-C001-261", "intents": ["@UNSURE@"], "paper_id": "ABC_cb64ba694c37df9ebc1065a1deac0f_5", "text": "However, for the Texas corpus, Mohler et al. (2011) have opted to use the arithmetic mean of the two graders as gold standard."}
{"sent_id": "46a23364b7bc51493d83f874a824ad-C001-127", "intents": ["@UNSURE@"], "paper_id": "ABC_46a23364b7bc51493d83f874a824ad_5", "text": "In order for our results to be comparable to those reported in previous evaluations (Rimell et al., 2009; Nivre et al., 2010) , we ran the parser \"out of the box\" directly on the test sentences, without using the development sentences to finetune."}
{"sent_id": "d8e73e9c00acffc34ade1331709d92-C001-161", "intents": ["@UNSURE@"], "paper_id": "ABC_d8e73e9c00acffc34ade1331709d92_5", "text": "We compare the translation performance to a baseline PBSMT system and to a baseline augmented with the original BiLMs from (Niehues et al., 2011) ."}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-56", "intents": ["@UNSURE@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "\"baseline\" is the heuristic rules in (Wang et al., 2007) ."}
{"sent_id": "3dbdf61d07a3e35ac1b6ecc7ab3999-C001-87", "intents": ["@UNSURE@"], "paper_id": "ABC_3dbdf61d07a3e35ac1b6ecc7ab3999_5", "text": "The 2-class accuracy is still lower than using the heuristic rules in (Wang et al., 2007) , which is reasonable because their rules encode more information than just the POS tags of DEs."}
{"sent_id": "518d8a8395e38d9971bd51344cf1b8-C001-109", "intents": ["@UNSURE@"], "paper_id": "ABC_518d8a8395e38d9971bd51344cf1b8_5", "text": "The results of the Eskander et al. (2013) CEC system are also presented for the purpose of comparison."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-130", "intents": ["@UNSURE@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "Besides the MIMIC-III dataset, we also leveraged the MIMIC-II dataset to compare our models with the ones in previous work (Perotte et al. 2013; Mullenbach et al. 2018; Baumel et al. 2018) ."}
{"sent_id": "f2ff155003d139b3677f746baf3807-C001-197", "intents": ["@UNSURE@"], "paper_id": "ABC_f2ff155003d139b3677f746baf3807_5", "text": "Following prior work (Mullenbach et al. 2018) , we compared our model with existing work using the MIMIC-III and MIMIC-II dataset."}
{"sent_id": "4cb16f436d910d82c3661052c1fa30-C001-200", "intents": ["@UNSURE@"], "paper_id": "ABC_4cb16f436d910d82c3661052c1fa30_5", "text": "Moreover, the fact that the two implementations are comparable is not inconceivable once we consider that (Luo et al., 2004 ) never compared their system to another coreference resolver and reported their competitive results on true mentions only."}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-130", "intents": ["@UNSURE@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "Observe that the results on our dataset are in general lower than those reported in Levy et al. (2017) ."}
{"sent_id": "b71321a9252376308d627c439e85b7-C001-92", "intents": ["@UNSURE@"], "paper_id": "ABC_b71321a9252376308d627c439e85b7_5", "text": "The syntactic GCNs (Bastings et al., 2017) appear stronger than semantic GCNs."}
{"sent_id": "920f2b94270c0711fcc19ad23dbb0d-C001-97", "intents": ["@UNSURE@"], "paper_id": "ABC_920f2b94270c0711fcc19ad23dbb0d_6", "text": "For both Wikipedia and Gazzetta, the tuning selected character n-grams, as in the work of Wulczyn et al. Also, for both Wikipedia and Gazzetta, it preferred LR to MLP, whereas Wulczyn et al. reported slightly higher performance 8 Two of the co-authors of Wulczyn et al. (2017) are with Jigsaw, who recently announced Perspective, a system to detect 'toxic' comments."}
{"sent_id": "920f2b94270c0711fcc19ad23dbb0d-C001-175", "intents": ["@UNSURE@"], "paper_id": "ABC_920f2b94270c0711fcc19ad23dbb0d_6", "text": "Following Wulczyn et al. (2017) , we report in Table 2 AUC scores (area under ROC curve), along with Spearman correlations between systemgenerated probabilities P (accept|c) and human probabilistic gold labels (Section 2.2) when probabilistic gold labels are available."}
{"sent_id": "a3dbc3362016cdcfc0c4da429b98cc-C001-202", "intents": ["@UNSURE@"], "paper_id": "ABC_a3dbc3362016cdcfc0c4da429b98cc_6", "text": "Compared to reported baseline result in Wiseman et al. (2017) , we achieved improvement of 22.27% in terms of RG, 26.84% in terms of CS F1%, 35.28% in terms of CO and 18.75% in terms of BLEU on test set."}
{"sent_id": "d3672a2d7129beef6703598f1558c4-C001-148", "intents": ["@UNSURE@"], "paper_id": "ABC_d3672a2d7129beef6703598f1558c4_6", "text": "Both our Transformer-based models -TLT-TS and CATS -outperform the competing supervised model of Koshorek et al. (2018) , a hierarchical encoder based on recurrent components, across the board."}
{"sent_id": "5596207b89d917db38c04af49c08aa-C001-104", "intents": ["@UNSURE@"], "paper_id": "ABC_5596207b89d917db38c04af49c08aa_10", "text": "We first compare our proposed model PhaseCond with Iterative Aligner, which is employed by two top ranked systems MEMEN (Pan et al., 2017) and MReader (Hu et al., 2017) on the SQuAD leaderboard 1 ."}
{"sent_id": "9e0a44722390d0508fbe56785701e6-C001-201", "intents": ["@UNSURE@"], "paper_id": "ABC_9e0a44722390d0508fbe56785701e6_10", "text": "As a final step in our exploration, we wanted to measure the impact of each of the features used by the system of Shwartz et al. (2016) ."}
{"sent_id": "1cd671c60486a137377096cae435ec-C001-115", "intents": ["@UNSURE@"], "paper_id": "ABC_1cd671c60486a137377096cae435ec_10", "text": "Results marked with 1 are from (Hill et al., 2015) and those marked with 2 are from (Kadlec et al., 2016) ."}
{"sent_id": "d9567072d2df6c0010b32e1d1eb676-C001-143", "intents": ["@UNSURE@"], "paper_id": "ABC_d9567072d2df6c0010b32e1d1eb676_10", "text": "We tuned hyper-parameters on the validation set, including sequence length to generate at test time (7 for Press et al. (2017) , 1000 for )."}
{"sent_id": "d9567072d2df6c0010b32e1d1eb676-C001-165", "intents": ["@UNSURE@"], "paper_id": "ABC_d9567072d2df6c0010b32e1d1eb676_10", "text": "We indeed observe that adversarial training slightly reduces the quality of generated text for SeqGAN, and find that the quality of 100-character long sequences generated from Press et al. (2017) is low."}
{"sent_id": "dbe1f1bdf7d94824f6f7cd176a4f6d-C001-97", "intents": ["@UNSURE@"], "paper_id": "ABC_dbe1f1bdf7d94824f6f7cd176a4f6d_10", "text": "The more indepth comparison of our work and recursive neural network model by Ji and Eisenstein (2015) is provided in the discussion section."}
{"sent_id": "60cc075e5351a756de8f9919d5a84e-C001-87", "intents": ["@UNSURE@"], "paper_id": "ABC_60cc075e5351a756de8f9919d5a84e_10", "text": "The parser is described in detail in Clark and Curran (2004) ."}
{"sent_id": "60cc075e5351a756de8f9919d5a84e-C001-192", "intents": ["@UNSURE@"], "paper_id": "ABC_60cc075e5351a756de8f9919d5a84e_10", "text": "Here we simply use the exponential of the inside score of a category as the beam score; the inside score for a category c is the sum over all sub-derivations dominated by c of the weights of the features in those sub-derivations (see Clark and Curran (2004) ."}
{"sent_id": "60cc075e5351a756de8f9919d5a84e-C001-200", "intents": ["@UNSURE@"], "paper_id": "ABC_60cc075e5351a756de8f9919d5a84e_10", "text": "The new strategy increases the F-score over labelled dependencies by approximately 0.5%, leading to the figures reported in Clark and Curran (2004) ."}
