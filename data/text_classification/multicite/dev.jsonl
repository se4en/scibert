{"sent_id": "8abffa3f807bad5ae2073aa7db215d-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_8abffa3f807bad5ae2073aa7db215d_0", "text": "Another different context type is dependency-based word embedding [11, 12, 13] , which considers syntactic contexts rather"}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-24", "intents": ["@MOT@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "Recently, Ott et al. (2011) have proposed an approach for generating positive deceptive opinion spam using Amazon's popular Mechanical Turk crowdsourcing service."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-41", "intents": ["@SIM@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "Following Ott et al. (2011) , we sample a subset of the available truthful reviews so that we retain an equal number of truthful and deceptive reviews (20 each) for each hotel."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-63", "intents": ["@USE@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "Following Ott et al. (2011) , we evaluate the performance of linear Support Vector Machine (SVM) classifiers trained with unigram and bigram term-frequency features on our novel negative deceptive opinion spam dataset."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-64", "intents": ["@USE@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "We employ the same 5-fold stratified cross-validation (CV) procedure as Ott et al. (2011) , whereby for each cross-validation iteration we train our model on all reviews for 16 hotels, and test our model on all reviews for the remaining 4 hotels."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-25", "intents": ["@EXT@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "In this section we discuss our efforts to extend Ott et al. (2011) 's dataset to additionally include negative deceptive opinion spam."}
{"sent_id": "5fe872d8e15ac38f845bc244f7bf5f-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_5fe872d8e15ac38f845bc244f7bf5f_0", "text": "Indeed, Ott et al. (2011) found that two out of three human judges were unable to perform statistically significantly better than chance (at the p < 0.05 level) at detecting positive deceptive opinion spam."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-10", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "Unlike earlier work (e.g., Briscoe, 2002; Cangelosi and Parisi, 2002; Steels, 2012) , many recent simulations consider realistic visual input, for example, by playing referential games with real-life pictures (e.g., Jorge et al., 2016; Lazaridou et al., 2017; Havrylov and Titov, 2017; Lee et al., 2018; Evtimova et al., 2018) ."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-13", "intents": ["@MOT@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "We study here agent representations following the model and setup of Lazaridou et al. (2017) ."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "Lazaridou and colleagues present preliminary evidence suggesting that, indeed, agents are now developing conceptual symbol meanings."}
{"sent_id": "2b148e376c39eae7f674610118e588-C001-45", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_2b148e376c39eae7f674610118e588_0", "text": "As we faithfully reproduced the setup of Lazaridou et al. (2017) , we refer the reader there for hyper-parameters and training details."}
{"sent_id": "737a452057be3e254b35bd8df492be-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_737a452057be3e254b35bd8df492be_0", "text": "[2] in their work, experiment with multiple deep learning architectures for the task of hate speech detection on Twitter using the same data set by [10] ."}
{"sent_id": "737a452057be3e254b35bd8df492be-C001-51", "intents": ["@USE@"], "paper_id": "ABC_737a452057be3e254b35bd8df492be_0", "text": "Once u i is obtained we calculate the importance of the word as the similarity Data Set Tweets Count [10] 15,844 [9] 25,112 [4] 20,362 Table 2 : Data sets and their total tweets count of u i with u c and get a normalized importance weight α i through a softmax function."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "al. [18] found that if they tie the key vectors to entities in the text then the memories contain information about the state of those entities."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "We demonstrate the utility of the model through experiments on the bAbI tasks [19] and find that the model achieves smaller mean error across the tasks than the best previously published result [18] in the 10k examples regime and achieves 0% error on 11 of the 20 tasks."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-30", "intents": ["@SIM@", "@EXT@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "We model the dynamic memory in a fashion similar to Recurrent Entity Networks [18] and then equip it with an additional relational memory."}
{"sent_id": "92f4cc0d6516a19a860d5b9af80f59-C001-65", "intents": ["@SIM@"], "paper_id": "ABC_92f4cc0d6516a19a860d5b9af80f59_0", "text": "Output Module This is a standard attention module used in memory networks [17, 18] ."}
{"sent_id": "b5097b3d901d073bfe06bcd88318ac-C001-6", "intents": ["@USE@"], "paper_id": "ABC_b5097b3d901d073bfe06bcd88318ac_0", "text": "We compare them with the original model released by Mikolov."}
{"sent_id": "b5097b3d901d073bfe06bcd88318ac-C001-88", "intents": ["@USE@"], "paper_id": "ABC_b5097b3d901d073bfe06bcd88318ac_0", "text": "In both tasks, the default pytorch embedding was tested before being replaced by pre-trained embeddings released by Mikolov et al. (2013a) and ours."}
{"sent_id": "b5097b3d901d073bfe06bcd88318ac-C001-117", "intents": ["@DIF@"], "paper_id": "ABC_b5097b3d901d073bfe06bcd88318ac_0", "text": "2 With regards to NER, most pretrained embeddings outperformed the default pytorch embedding, with our BW w4s1h0 model (which is best in BW analogy score) performing best in F1 score and closely followed by Mikolov et al. (2013a) model."}
{"sent_id": "b5097b3d901d073bfe06bcd88318ac-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_b5097b3d901d073bfe06bcd88318ac_0", "text": "Besides, Mikolov et al. (2013a) showed with vector space algebra that relationships among words can be evaluated, expressing the quality of vectors produced from the model."}
{"sent_id": "b5097b3d901d073bfe06bcd88318ac-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_b5097b3d901d073bfe06bcd88318ac_0", "text": "Mikolov et al. (2013a) tried various hyper-parameters with both architectures of their model, ranging from 50 to 1,000 dimensions, 30,000 to 3,000,000 vocabulary sizes, 1 to 3 epochs, among others."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-23", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "Figure 2 lists some of the rules which can be extracted from the sentence pair in Figure 1 by the system used in (Zhang et al., 2008a) ."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-67", "intents": ["@BACK@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "In (Zhang et al., 2008a) , they are described as follows: Definition 1: The Structure Reordering Rule (SRR) refers to the structure reordering rule that has at least two non-terminal leaf nodes with inverted order in the source and target side."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-76", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "The definition of DPR in (Zhang et al., 2008a ) is explicit but somewhat rough and not very accurate."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-70", "intents": ["@MOT@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "Although these two definitions are easy implemented in practice, we argue that the definition of SRR is not complete."}
{"sent_id": "d70b9838e8a32a8638d7aed0adc80a-C001-100", "intents": ["@DIF@"], "paper_id": "ABC_d70b9838e8a32a8638d7aed0adc80a_0", "text": "Through Definition 3, we know that the DPR is a sub-set of the SRR NT-T."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-59", "intents": ["@BACK@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "**GONG ET AL. (2018)'S APPROACH**"}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "Gong et al. (2018) motivate their approach mainly with the length mismatch argument, which they claim makes approaches relying on document representations (incl. vector averaging) unsuitable."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-67", "intents": ["@BACK@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "Gong et al. (2018) use two different sets of word embeddings: One (topic wiki) was trained on a full English Wikipedia dump, the other (wiki science) on a smaller subset of the former dump which only contained science articles."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-54", "intents": ["@DIF@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "Instead, they create gold standard annotations based on a majority vote of three manual annotations."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-147", "intents": ["@DIF@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "Another result is that, contrary to the claim made by Gong et al. (2018) , the standard averaging approach does indeed work very well even for heterogeneous document collections, if appropriate weighting is applied."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-70", "intents": ["@EXT@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "We implement this standard measure (AVG COS SIM) as a baseline for both our method and for the method by Gong et al. (2018) ."}
{"sent_id": "f3282df3adadf78320e99c09d8384f-C001-111", "intents": ["@SIM@"], "paper_id": "ABC_f3282df3adadf78320e99c09d8384f_0", "text": "The second, more important finding is that our proposed TOP n COS SIM AVG measure is also very competitive, as it also outperforms both systems by Gong et al. (2018) in two out of three settings."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-75", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "Viewing these results holistically, we conclude that current methods for the EmbodiedQA task are not effective at using context from the environment, and in fact this negatively hampers them."}
{"sent_id": "f1e5584a2139160943d9f0338e6ce0-C001-67", "intents": ["@DIF@"], "paper_id": "ABC_f1e5584a2139160943d9f0338e6ce0_0", "text": "The Nearest Neighbour method also does pretty well, and only falls behind to PACMAN (BC+REINFORCE) and NMC(BC+A3C) in the T 10 case."}
{"sent_id": "07a2b256766020450c85eae2839db8-C001-20", "intents": ["@BACK@", "@FUT@", "@MOT@"], "paper_id": "ABC_07a2b256766020450c85eae2839db8_1", "text": "Most importantly, it is clear from some recent studies [8, 9 ] that novel topological measurements should be introduced to capture a wider range of linguistic features."}
{"sent_id": "07a2b256766020450c85eae2839db8-C001-25", "intents": ["@USE@"], "paper_id": "ABC_07a2b256766020450c85eae2839db8_1", "text": "December 4, 2014 the conception of measurements that are able to capture semantic aspects, since the topological measurements of co-occurrence networks capture mostly syntactic factors [8] ."}
{"sent_id": "53bc4206427e95d600f787e0531df1-C001-21", "intents": ["@USE@"], "paper_id": "ABC_53bc4206427e95d600f787e0531df1_1", "text": "Applying the PMI score to the β-priors is the only one choice because we can adjust the degree of the word co-occurrence by modifying the distributions in the β-priors."}
{"sent_id": "53bc4206427e95d600f787e0531df1-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_53bc4206427e95d600f787e0531df1_1", "text": "Since the PMI score not only considers the co-occurrence frequency of the two words, but also normalizes by the single word frequency."}
{"sent_id": "cf2cc67035107f5bdaab85a760e56e-C001-60", "intents": ["@USE@"], "paper_id": "ABC_cf2cc67035107f5bdaab85a760e56e_1", "text": "We found that (1) in general, accuracies on the analogy tasks were low, suggesting that improvements for Indonesian word embeddings are still possible and KaWAT is hard enough to be the benchmark dataset for that purpose, (2) on syntactic analogies, embedding by (Bojanowski et al., 2017) performed best and yielded 20% fewer training epochs when employed for POS tagging, and (3) on semantic analogies, GloVe embedding trained on Tempo corpus performed best and produced significant gains on ROUGE-1 and ROUGE-L scores when used for text summarization."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-20", "intents": ["@BACK@", "@MOT@", "@EXT@", "@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "In particular, Artetxe et al. (2018b) show that the adversarial methods of Conneau et al. (2018) and Zhang et al. (2017a,b) fail for many language pairs."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-263", "intents": ["@BACK@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "However, this is not surprising as it has been shown that iterative fine-tuning with Procrustes solution is a robust method that can recover many errors made in the initial mapping (Conneau et al., 2018) ."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-85", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Our aim is to learn a mapping f (x) in an unsupervised way (i.e., no bilingual dictionary given) such that for every x i , f (x) corresponds to its translation in Y. Our overall approach follows the same sequence of steps as Conneau et al. (2018): 1."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-171", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "The first one is from Conneau et al. (2018) , which consists of FastText monolingual embeddings of (d =) 300 dimensions (Bojanowski et al., 2017) trained on Wikipedia monolingual corpus and gold dictionaries for 110 language pairs."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-181", "intents": ["@USE@", "@DIF@", "@SIM@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "We compare our method with the unsupervised models of Conneau et al. (2018) , Artetxe et al. (2018b) , Alvarez-Melis and Jaakkola (2018) , Xu et al. (2018a) , and Hoshen and Wolf (2018) ."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-182", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "To evaluate how our unsupervised method compares with methods that rely on a bilingual seed dictionary, we follow Conneau et al. (2018) , and compute a supervised baseline that uses the Procrustes solution directly on the seed dictionary (5000 pairs) to learn the mapping function, and then uses CSLS to do the nearest neighbor search."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-200", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "In Table 1 , we see that our Adversarial autoencoder + Conneau et al. (2018) Refinement outperforms Conneau et al. (2018) in all the six translation tasks involving European language pairs, yielding gains in the range 0.3 -1.3%."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-211", "intents": ["@USE@", "@DIF@", "@SIM@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "In Section 5.3, we compare our model with Conneau et al. (2018) more rigorously by evaluating them with and without fine-tuning and measuring their performance on P@1, P@5, and P@10."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-217", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "Let us first consider the results for European language pairs on the dataset of Conneau et al. (2018) in Table 1 ."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-252", "intents": ["@USE@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "However, it is important Table 5 : Ablation study of our adversarial autoencoder model on the dataset of Conneau et al. (2018)."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-204", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "We found their model to be very fragile for En from/to Ms, and does not converge at all for Ms→En."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-209", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "For example, their method had difficulties in converging for En from/to Es translations; for En→Es, it converges only 2 times out of 10 attempts, while for Es→En it did not converge a single time in 10 attempts."}
{"sent_id": "5b14e259a557aa3cbfcbd6265f04c8-C001-253", "intents": ["@DIF@"], "paper_id": "ABC_5b14e259a557aa3cbfcbd6265f04c8_1", "text": "to note that in contrast to Conneau et al. (2018) , our mapping is performed at the code space."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-37", "intents": ["@USE@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "The dataset includes phrase-level labels in addition to sentence-level labels (see Table 1 for detailed statistics); following Hu et al. (2016) , we use both types of labels for the comparisons in Section 3.2."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-41", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "In this section we reanalyze the effectiveness of the techniques of Hu et al. (2016) and find that most of the performance gain is due to projection and not knowledge distillation."}
{"sent_id": "d3122aab8960a7c89afe87c73faa59-C001-57", "intents": ["@USE@"], "paper_id": "ABC_d3122aab8960a7c89afe87c73faa59_1", "text": "In Figure 2 , the first two columns show the reported accuracies in Hu et al. (2016) for models trained with and without distillation (corresponding to using values π = 1 and π = 0.95 t in the t th epoch, respectively)."}
{"sent_id": "3b0a82129333203eca96a7473095f3-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_3b0a82129333203eca96a7473095f3_1", "text": "Oostdijk's work provides an excellent example of the strengths and weaknesses of the approach advocated by Black et al. In addition, she discusses issues such as sampling and tokenization of corpus material, as well as the exploitation of the analyzed corpus in studies of language variation."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017) ."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "For example, Reimers and Gurevych (2017b) conduct a large number of experiments using the code of Ma and Hovy (2016) , but cannot obtain comparable results as reported in the paper."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-42", "intents": ["@MOT@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "Others report the best result among different trials (Ma and Hovy, 2016) , which cannot be compared directly."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-174", "intents": ["@SIM@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "The GloVe 100-dimension embeddings give higher F1-scores than SENNA (Collobert et al., 2011) on both models, which is consistent with the observation of Ma and Hovy (2016) ."}
{"sent_id": "d52a1a26cbf8a6f528be5494f05e45-C001-137", "intents": ["@USE@"], "paper_id": "ABC_d52a1a26cbf8a6f528be5494f05e45_1", "text": "We re-implement the structure of several reports (Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017) , which take the CCNN+WLSTM+CRF architecture."}
{"sent_id": "2b6dd9388c43df4416c738b2d1ed5f-C001-61", "intents": ["@USE@"], "paper_id": "ABC_2b6dd9388c43df4416c738b2d1ed5f_1", "text": "For comparison purposes, in Table 4 we have also evaluated our results on the dataset by (Davidson et al. 2017 )."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-5", "intents": ["@DIF@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "The proposed system outperforms the baseline keyword spotting model in [1] due to increased optimizability."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-131", "intents": ["@DIF@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "Especially the proposed Max4 model reduces FR rate to nearly half of the baseline in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-136", "intents": ["@DIF@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the baseline."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, 1] ."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "In [1] , target label sequence consists of intervals of repeated labels which we call runs."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-86", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "We used the same frontend feature extract as the baseline [1] in our experiments."}
{"sent_id": "e6fe4c6c32294072dbc1ee5bb0a606-C001-114", "intents": ["@SIM@"], "paper_id": "ABC_e6fe4c6c32294072dbc1ee5bb0a606_2", "text": "Data augmentation similar to [1] has been used for better robustness."}
{"sent_id": "0da20f60adaff9637ebdbe2a27f2a4-C001-46", "intents": ["@MOT@"], "paper_id": "ABC_0da20f60adaff9637ebdbe2a27f2a4_2", "text": "Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works (Bowman et al., 2016; Yang et al., 2017; Xu and Durrett, 2018; Dieng et al., 2019) ."}
{"sent_id": "0da20f60adaff9637ebdbe2a27f2a4-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_0da20f60adaff9637ebdbe2a27f2a4_2", "text": "Bowman et al. (2016) uses KL annealing, where a variable weight is added to the KL term in the cost function at training time."}
{"sent_id": "0da20f60adaff9637ebdbe2a27f2a4-C001-68", "intents": ["@USE@"], "paper_id": "ABC_0da20f60adaff9637ebdbe2a27f2a4_2", "text": "For the PTB dataset, we used the train-test split following (Bowman et al., 2016; Xu and Durrett, 2018) ."}
{"sent_id": "12c5d72fad925c8ec025cda87a0fd9-C001-71", "intents": ["@BACK@"], "paper_id": "ABC_12c5d72fad925c8ec025cda87a0fd9_2", "text": "Fazly et al. report results over DEV and TEST separately."}
{"sent_id": "12c5d72fad925c8ec025cda87a0fd9-C001-21", "intents": ["@USE@"], "paper_id": "ABC_12c5d72fad925c8ec025cda87a0fd9_2", "text": "In this paper we further incorporate knowledge of the lexico-syntactic fixedness of VNCs -automatically acquired from corpora using the method of Fazly et al. (2009) -into our various embedding-based approaches."}
{"sent_id": "15dd59368074f3473b57d86568807f-C001-99", "intents": ["@DIF@"], "paper_id": "ABC_15dd59368074f3473b57d86568807f_2", "text": "We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works (Phandi et al., 2015; Dong and Zhang, 2016; Tay et al., 2018) ."}
{"sent_id": "15dd59368074f3473b57d86568807f-C001-118", "intents": ["@DIF@"], "paper_id": "ABC_15dd59368074f3473b57d86568807f_2", "text": "We concluded that this simple apSource→Target Method n t = 0 n t = 10 n t = 25 n t = 50 n t = 100 1→2 (Phandi et al., 2015) Table 3 : Corss-domain automatic essay scoring results of our approach versus two state-of-the-art methods (Phandi et al., 2015; Dong and Zhang, 2016) ."}
{"sent_id": "15dd59368074f3473b57d86568807f-C001-69", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_15dd59368074f3473b57d86568807f_2", "text": "We closely followed the same settings for data preparation as (Phandi et al., 2015; Dong and Zhang, 2016) ."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "The quality of an answer depends on how ambiguous and latent notions of reference frames and intentions are understood [27, 44] ."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-71", "intents": ["@BACK@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "DAQUAR's language scope is beyond the nouns or tuples that are typical to recognition datasets [51, 52, 53] ."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-81", "intents": ["@BACK@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "Moreover, ambiguities naturally emerge due to fine grained categories that exist in DAQUAR."}
{"sent_id": "2606ecb66287c0199f3aa6d95f6774-C001-113", "intents": ["@FUT@"], "paper_id": "ABC_2606ecb66287c0199f3aa6d95f6774_2", "text": "The authors of [27] have proposed using WUP similarity [62] as the membership measure µ in the WUPS score."}
{"sent_id": "4821d5a283c1d4ba162b58e5fac8bc-C001-110", "intents": ["@DIF@"], "paper_id": "ABC_4821d5a283c1d4ba162b58e5fac8bc_2", "text": "Our highest improvement (more than 54%, from 0.187 to 0.728) over (Phandi et al., 2015) is recorded for the pair 5→6, when n t = 0."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-3", "intents": ["@MOT@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "Tu and Gimpel (2018) developed an efficient framework for energy-based models by training \"inference networks\" to approximate structured inference instead of using gradient descent."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-67", "intents": ["@MOT@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "Tu and Gimpel (2018) used the same inference network for solving both problems, which led them to fine-tune the network at test-time with a different objective."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "Tu and Gimpel (2018) propose an alternative that replaces gradient descent with a neural network trained to do inference, i.e., to mimic the function performed in equation (1)."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-91", "intents": ["@BACK@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "However, Tu and Gimpel (2018) found that the trained cost-augmented network was barely affected by fine-tuning for the test-time inference objective."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-162", "intents": ["@BACK@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "The importance of CE in prior work (Tu and Gimpel, 2018) is likely due to the fact that truncation was being used."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-173", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "The \"marginrescaled\" row uses a SPEN with the local CE term and without zero truncation, where A Ψ is obtained by finetuning F Φ as done by Tu and Gimpel (2018) ."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-187", "intents": ["@USE@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "Adding the backward (b) and word-augmented TLMs (c) improves over only using the forward TLM from Tu and Gimpel (2018) ."}
{"sent_id": "79b3933e51c5fd412d00829815a958-C001-148", "intents": ["@SIM@"], "paper_id": "ABC_79b3933e51c5fd412d00829815a958_2", "text": "We experiment with three settings for the global energy: GE(a): forward TLM as in Tu and Gimpel (2018) ; GE(b): forward and backward TLMs (γ = 0); GE(c): all four TLMs in Eq. (7)."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-95", "intents": ["@EXT@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "We start by outlining the features proposed by Schulder et al. (2017) and how we adapt them for use with German ( §4.1)."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "However, while there has been significant research on negation in sentiment analysis (Wiegand et al., 2010) , current classifiers fail to handle polarity shifters adequately (Schulder et al., 2017) ."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Their approach includes both features that rely on semantic resources and datadriven ones."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Schulder et al. (2017) show that the explicit knowledge provided by a shifter lexicon can improve polarity classification in such cases."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-116", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Schulder et al. (2017) hypothesize that this phenomenon correlates with shifting, which can be seen as producing a new (negative) end state."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-225", "intents": ["@DIF@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "The most feature-rich SVM configuration, SVM data+resource+dict+embed , provides a significant improvement over SVM data+resource , the best classifier of Schulder et al. (2017) ."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-44", "intents": ["@SIM@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Focusing on verbs also allows us a closer comparison with Schulder et al. (2017) and to investigate cross-lingual similarities between verbal shifters."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-203", "intents": ["@SIM@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "Analogous to Schulder et al. (2017) we evaluate a supervised SVM classifier as well as a graph-based label propagation (LP) classifier that requires no labeled training data."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-153", "intents": ["@USE@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "We choose to use the bootstrapped lexicon of Schulder et al. (2017) , rather than the manually created one of Schulder et al. (2018) , to show that bootstrapping is sufficient for all stages of the learning process."}
{"sent_id": "31e8c524f05495fdd87bfac6fbecc8-C001-275", "intents": ["@USE@"], "paper_id": "ABC_31e8c524f05495fdd87bfac6fbecc8_2", "text": "In reproducing the work of Schulder et al. (2017) , we limited ourselves to verbs."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "Similarly to the approach of Kiros et al. [1] , most image annotation and image retrieval approaches rely on the use of CNN features for image representation."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-65", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "To combine both embeddings, Kiros et al. [1] use an affine transformation on the image representation (in our case, the FNE) identical to a fully connected neural network layer."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-29", "intents": ["@EXT@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "This paper builds upon the methodology described by Kiros et al. [1] , which is in turn based on previous works in the area of Neural Machine Translation [14] ."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-63", "intents": ["@DIF@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "To do so we use the FNE to obtain an image representation instead of the output of the last layer of a CNN, as the original model does."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-127", "intents": ["@DIF@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "For both image annotation and image retrieval tasks on the Flickr8k dataset, Table 1 shows the results of the proposed FN-MME, the reported results of the original model CNN-MME, the results of the original model when using our configuration CNN-MME*, and the current state-of-the-art (SotA)."}
{"sent_id": "e90c9a93ec445a636fcee924306d95-C001-149", "intents": ["@FUT@"], "paper_id": "ABC_e90c9a93ec445a636fcee924306d95_2", "text": "If the boost in performance obtained by the FNE on the Kiros et al. [1] pipeline translates to these other methods, such combination would be likely to define new state-of-the-art results on both tasks."}
{"sent_id": "95883b369c4b019fa98493a728c1a0-C001-8", "intents": ["@DIF@"], "paper_id": "ABC_95883b369c4b019fa98493a728c1a0_2", "text": "Specifically, we find that the bias metric proposed by (Bolukbasi et al. 2016 ) is highly sensitive to embedding hyperparameter selection, and that in many cases, the variance due to the selection of some hyper-parameters, notably the embedding space dimensionality, is greater than the variance in the metric due to corpus selection, while in fewer cases, even the relative rankings of the bias measured in the embedding spaces of various corpora varies with hyper-parameter selection."}
{"sent_id": "95883b369c4b019fa98493a728c1a0-C001-59", "intents": ["@DIF@"], "paper_id": "ABC_95883b369c4b019fa98493a728c1a0_2", "text": "While it is the case that the bias metrics in (Bolukbasi et al. 2016 ) may provide meaningful rankings of corpora when controlling for model hyper-parameter configuration, publishing the average absolute value of the metric without a complete account for model configuration is suspect."}
{"sent_id": "95883b369c4b019fa98493a728c1a0-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_95883b369c4b019fa98493a728c1a0_2", "text": "For the sake of com-pleteness, we repeat the definition of (Bolukbasi et al. 2016 )'s bias metric:"}
{"sent_id": "822b2010b07d3e1103f904fa45388a-C001-66", "intents": ["@MOT@"], "paper_id": "ABC_822b2010b07d3e1103f904fa45388a_2", "text": "The parent source vocabulary (and hence embeddings) is randomly mapped to child source vocabulary since it was shown that NMT is less sensitive to it (Zoph et al., 2016"}
{"sent_id": "822b2010b07d3e1103f904fa45388a-C001-23", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_822b2010b07d3e1103f904fa45388a_2", "text": "It is essentially the same as described in (Zoph et al., 2016) where we learn a model (parent model) for a resource rich language pair (Hindi-English) and use it to initialize the model (child model) for the resource poor pair (Marathi-English)."}
{"sent_id": "822b2010b07d3e1103f904fa45388a-C001-64", "intents": ["@SIM@"], "paper_id": "ABC_822b2010b07d3e1103f904fa45388a_2", "text": "The model and training details are the same as that in the original work (Zoph et al., 2016) Note that the target language (English) vocabulary is same for all settings and the WPM is learned on the English side of the French-English corpus since it is the largest one amongst all our pairs."}
{"sent_id": "6b147afca676882878e67bc10abd58-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_6b147afca676882878e67bc10abd58_2", "text": "The most prominent approaches include: (1) temporal ordering in terms of publication date (Barzilay, 2003) , (2) temporal ordering in terms of textual cues in sentences (Bollegala et al., 2006) , (3) the topic of the sentences (Barzilay, 2003) , (4) coherence theories (Barzilay and Lapata, 2008) , e.g., Centering Theory, (5) content models (Barzilay and Lee, 2004) , and (6) ordering(s) in the underlying documents in the case of summarisation (Bollegala et al., 2006; Barzilay, 2003) ."}
{"sent_id": "6b147afca676882878e67bc10abd58-C001-73", "intents": ["@DIF@"], "paper_id": "ABC_6b147afca676882878e67bc10abd58_2", "text": "The source furthermore differs from Barzilay and Lapata (2008) 's datasets in that the content of each text is not based on one individual event (an earthquake or accident), but on more complex topics followed over a period of time (e.g., the espionage case between GM and VW along with the various actions taken to resolve it)."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-3", "intents": ["@EXT@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "Our work is based on the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017) in the fashion domain."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-69", "intents": ["@USE@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "We compare our results against Saha et al. (2017) by using their code and data-generation scripts."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "In contrast to simple QA tasks in visually grounded dialogue, e.g. (Antol et al., 2015) , it contains conversations with a clear end-goal."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-35", "intents": ["@SIM@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "Saha et al. (2017) propose a similar baseline model for the MMD dataset, extending HREDs to include the visual modality."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-76", "intents": ["@DIF@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "To summarize, our best performing model (M-HRED-attn) outperforms the model of Saha et al. by 7 BLEU points."}
{"sent_id": "0bb68718667b8850dc0110d10d1d3a-C001-84", "intents": ["@MOT@"], "paper_id": "ABC_0bb68718667b8850dc0110d10d1d3a_2", "text": "In this research, we address the novel task of response generation in search-based multimodal dialogue by learning from the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017) ."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-130", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "Wang et al. (2014a) confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as (Wang et al., 2014a) to investigate the performance of our new alignment model."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-150", "intents": ["@DIF@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "The above four experiments are consistent in results: without using any anchor information, alignment by entity description is able to achieve better or comparable performance, compared to alignment by Wikipedia anchors proposed by Wang et al. (2014a) ."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-96", "intents": ["@USE@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "For triplet classification, a large dataset provided by (Wang et al., 2014a ) is used as the knowledge base."}
{"sent_id": "08c64c92b77dbd9e999092a2fec3d1-C001-131", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_08c64c92b77dbd9e999092a2fec3d1_2", "text": "We use the same public dataset NYT+FB, released by Riedel et al. (2010) and used in and (Wang et al., 2014a) ."}
{"sent_id": "c60a1131c6b1639b772b0e5c59588e-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_c60a1131c6b1639b772b0e5c59588e_2", "text": "This reduces feature sparsity and has been shown to improve PRA inference (Gardner et al., 2013) , (Gardner et al., 2014) ."}
{"sent_id": "c60a1131c6b1639b772b0e5c59588e-C001-96", "intents": ["@DIF@"], "paper_id": "ABC_c60a1131c6b1639b772b0e5c59588e_2", "text": "Improvements in PRA-ODA over PRA-SVO is statistically significant with p < 0.007, with PRA-SVO as null hypothesis."}
{"sent_id": "7f78697390e28cc7798f8cb183cb59-C001-73", "intents": ["@USE@"], "paper_id": "ABC_7f78697390e28cc7798f8cb183cb59_2", "text": "Salton et al., on the other hand, merge DEV and TEST, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data."}
{"sent_id": "7f78697390e28cc7798f8cb183cb59-C001-103", "intents": ["@DIF@"], "paper_id": "ABC_7f78697390e28cc7798f8cb183cb59_2", "text": "Nevertheless, it is remarkable that the relatively simple approach to averaging word embeddings used by word2vec performs as well as, or better than, the much more complex skipthoughts model used by Salton et al. (2016) ."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-183", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "The Pearson coefficient shows that our dataset correlates with human annotation better than the dataset of Kajiwara and Yamamoto (2015) , possibly because we controlled each sentence to include only one complex word."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-184", "intents": ["@DIF@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "Because our dataset is balanced, the accuracy of Web corpus-based metrics (Frequency and Number of Users) closer than the dataset of Kajiwara and Yamamoto (2015) ."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-158", "intents": ["@USE@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "We calcu- late 1-best accuracy in our dataset and the dataset of Kajiwara and Yamamoto (2015) ."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-160", "intents": ["@USE@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "Then, we calculate correlation between the accuracies of annotated data and either those of Kajiwara and Yamamoto (2015) or those of our dataset."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "They gathered substitutes from five annotators using crowdsourcing."}
{"sent_id": "2d3ec2e77947cb23af773926ec917b-C001-55", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_2d3ec2e77947cb23af773926ec917b_3", "text": "Because Kajiwara and Yamamoto (2015) extracted sentences from a newswire corpus, their dataset has a poor variety of expression."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-101", "intents": ["@DIF@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "Moreover, the unsupervised CFORM method of Fazly et al. (2009) gives substantially higher accuracies than this supervised approach."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-116", "intents": ["@DIF@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "In this paper we proposed supervised and unsupervised approaches, based on word embeddings, to identifying token instances of VNICs that performed better than the supervised approach, and unsupervised CFORM approach, of Fazly et al. (2009) , respectively."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "Many studies, however, have taken a word sense disambiguation-inspired approach to MWE identification (e.g., Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010) , treating literal combinations and MWEs as different word senses, and have exploited linguistic knowledge of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005; Hashimoto and Kawahara, 2008; Fazly et al., 2009; Fothergill and Baldwin, 2012) ."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "Fazly et al. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-47", "intents": ["@USE@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "Finally, to form the feature vector representing a VNIC instance, we subtract e from c, and append to this vector a single binary feature representing whether the VNIC instance occurs in its canonical form, as determined by Fazly et al. (2009) ."}
{"sent_id": "491879c73f8aa9f11bfae01abc795d-C001-87", "intents": ["@SIM@"], "paper_id": "ABC_491879c73f8aa9f11bfae01abc795d_3", "text": "A smaller window size for the word embedding features might be better able to capture similar information to CFORM, which could explain the good performance of the model using a window size of 1."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-12", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "(Herbelot and Vecchi, 2015) investigated a method to map word embeddings to formal semantics, which is the center of interest of this paper."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "The transformation used in (Herbelot and Vecchi, 2015) is based on Partial Least Squares Regression (PLSR)."}
{"sent_id": "f7e80cf0a6724675cab2825cbf7e10-C001-19", "intents": ["@USE@"], "paper_id": "ABC_f7e80cf0a6724675cab2825cbf7e10_3", "text": "In this section, we summarize the task presented in (Herbelot and Vecchi, 2015) ."}
{"sent_id": "7e380d496bb253885218465b778cc1-C001-48", "intents": ["@USE@"], "paper_id": "ABC_7e380d496bb253885218465b778cc1_3", "text": "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from Boito et al. (2019a) ."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-4", "intents": ["@USE@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-63", "intents": ["@USE@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "Then we split all such instances into training and test, with 539 and 62 instances, respectively and augment these sentences by swapping all the gendered words with words of the opposite gender such that the numbers of male 1 We use the list collected in (Zhao et al., 2018a) and female entities are balanced."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-100", "intents": ["@USE@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "Table 2 summarizes our results on WinoBias."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-78", "intents": ["@BACK@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "WinoBias dataset is split Semantics Only and w/ Syntactic Cues subsets."}
{"sent_id": "3d6df70136820f74ce76f60a59cc42-C001-44", "intents": ["@MOT@"], "paper_id": "ABC_3d6df70136820f74ce76f60a59cc42_3", "text": "The analysis shows that the Billion Word corpus contains a significant skew with respect to gender: (1) male pronouns occur three times more than female pronouns and (2) male pronouns co-occur more frequently with occupation words, irrespective of whether they are prototypically male or female."}
{"sent_id": "311b238406da4891c09cb9c3c0334d-C001-41", "intents": ["@DIF@"], "paper_id": "ABC_311b238406da4891c09cb9c3c0334d_3", "text": "The original system [3] used more features, which could not be easily applied on Czech commentaries."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "They provided CCG categories from a supertagger as features to Malt and obtained overall improvements of 0.3% and 0.4% in unlabelled and labelled attachment scores respectively."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-47", "intents": ["@USE@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "We used Clark and Curran (2004) 's supertagger for English, and Ambati et al. (2013) 's supertagger for Hindi."}
{"sent_id": "21c2160667b3ff919e39285cd1ece7-C001-102", "intents": ["@USE@"], "paper_id": "ABC_21c2160667b3ff919e39285cd1ece7_3", "text": "Ambati et al. (2013) showed that for Hindi, providing CCG categories as features improved Malt in better handling of long distance dependencies."}
{"sent_id": "4dbf6e2bfa30e96816b7f6d9c6e069-C001-32", "intents": ["@USE@"], "paper_id": "ABC_4dbf6e2bfa30e96816b7f6d9c6e069_3", "text": "Each edge of the merged structure is represented as a variable in the ILP objective function, and the solution will decide whether the edge has to be preserved or discarded."}
{"sent_id": "4dbf6e2bfa30e96816b7f6d9c6e069-C001-34", "intents": ["@EXT@"], "paper_id": "ABC_4dbf6e2bfa30e96816b7f6d9c6e069_3", "text": "Further, to solve the ILP, we introduce several constraints, such as desired length of the output, etc."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-73", "intents": ["@EXT@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "We then extended the method further so that it is more suitable to evaluate embedding techniques designed for polysemous words with regard to their ability to embed diverse senses."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "As the approach relies on human interpretation of words, it is important to take into account how humans interpret or understand the meaning of a word."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-48", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "Also, the evaluated word embedding techniques in (Schnabel et al., 2015) except TSCCA (Dhillon et al., 2015)-generate one vector for each word, and that makes comparisons between two related words from two embedding techniques difficult."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-95", "intents": ["@BACK@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "Note that this is not needed in (Schnabel et al., 2015) where query words are not annotated."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-152", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "We argued that their evaluation was in expectation with respect to subjective preferences of the Turkers."}
{"sent_id": "e9404db1fbda5dd8c55a40711d06ec-C001-22", "intents": ["@MOT@"], "paper_id": "ABC_e9404db1fbda5dd8c55a40711d06ec_3", "text": "Specifically, it does not explicitly consider word context."}
{"sent_id": "fe539365c7bb4555280fd1a5478aba-C001-58", "intents": ["@USE@"], "paper_id": "ABC_fe539365c7bb4555280fd1a5478aba_3", "text": "In order to evaluate our proposal, we have performed some experimental study on the first publicly available opinion spam dataset gathered and presented in (Ott et al., 2011; Ott et al., 2013) ."}
{"sent_id": "fe539365c7bb4555280fd1a5478aba-C001-114", "intents": ["@BACK@"], "paper_id": "ABC_fe539365c7bb4555280fd1a5478aba_3", "text": "In (Ott et al., 2011 ) the authors used the 80 dimensions of LIWC2007, unigrams and bigrams as set of features with a SVM classifier."}
{"sent_id": "fe539365c7bb4555280fd1a5478aba-C001-127", "intents": ["@SIM@"], "paper_id": "ABC_fe539365c7bb4555280fd1a5478aba_3", "text": "Regarding the experiments with the 5 fold cross-validation, we obtained similar results to those of (Ott et al., 2011) and slightly lower than the ones of (Feng and Hirst, 2013) ."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-110", "intents": ["@BACK@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "The evaluation in Rimell et al. (2009) took into account a wide variety of parser output formats, some of which differed significantly from the gold-standard."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-81", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "In comparison to the five parsers evaluated in Rimell et al. (2009) , it is worth noting that MSTParser and MaltParser were trained on the same basic data as four of the five, but with a different kind of syntactic representation -dependency trees instead of phrase structure trees or theoryspecific representations from CCG and HPSG."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-159", "intents": ["@DIF@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "Perhaps more interestingly, the accuracies of MSTParser and MaltParser are only slightly below the best performing systems in Rimell et al. (2009) -C&C and Enju ."}
{"sent_id": "9158f716efae0d1f38510dd0847c45-C001-46", "intents": ["@USE@"], "paper_id": "ABC_9158f716efae0d1f38510dd0847c45_3", "text": "In this paper we repeat the study of Rimell et al. (2009) for two dependency parsers, with the goal of evaluating how parsers based on dependency grammars perform on unbounded dependencies."}
{"sent_id": "9f1d2be80dbfd726a24fb2a05e130b-C001-63", "intents": ["@USE@"], "paper_id": "ABC_9f1d2be80dbfd726a24fb2a05e130b_3", "text": "In this paper we proposed Neural Machine Transliteration based on successful studies in sequence to sequence learning (Sutskever et al., 2014) and Neural Machine Translation (Ling et al., 2015; Costa-Jussà and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a) ."}
{"sent_id": "fca14f99953b9dc30a594525ee92b5-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_fca14f99953b9dc30a594525ee92b5_3", "text": "In [6] and subsequent efforts [8, 9, 10] , the authors further showed that the disentangled representation is also helpful in the speech recognition task."}
{"sent_id": "2eaa48dbc5e42a5934e905ec2288ac-C001-104", "intents": ["@DIF@"], "paper_id": "ABC_2eaa48dbc5e42a5934e905ec2288ac_3", "text": "The average QWK score of HISK and BOSWE (0.785) is more than 2% better the average scores of the best-performing state-of-the-art approaches Tay et al., 2018) ."}
{"sent_id": "e7f972baa73e7ababa28eded3adad9-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_e7f972baa73e7ababa28eded3adad9_3", "text": "Numerous initiatives such as the Digital Corpus of Sanskrit 1 , GRETIL 2 , The Sanskrit Library 3 and others from the Sanskrit Linguistic and Computational Linguistic community is a fine example of such efforts (Goyal et al., 2012; Krishna et al., 2017) ."}
{"sent_id": "e7f972baa73e7ababa28eded3adad9-C001-183", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_e7f972baa73e7ababa28eded3adad9_3", "text": "Krishna et al. (2017) states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit."}
{"sent_id": "e7f972baa73e7ababa28eded3adad9-C001-41", "intents": ["@EXT@"], "paper_id": "ABC_e7f972baa73e7ababa28eded3adad9_3", "text": "Our approach will help to scale the segmentation process in comparison with the challenges posed by knowledge involved processes in the current systems (Krishna et al., 2017) ."}
{"sent_id": "1540b0b172971ac75771b414765f1d-C001-45", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_1540b0b172971ac75771b414765f1d_3", "text": "Bollmann et al. (2017) also describe a multi-task learning (MTL) scenario where the encoder-decoder model is trained on two datasets in parallel."}
{"sent_id": "1540b0b172971ac75771b414765f1d-C001-24", "intents": ["@USE@"], "paper_id": "ABC_1540b0b172971ac75771b414765f1d_3", "text": "We consider 10 datasets from 8 different languages: German, using the Anselm dataset (taken from Bollmann et al., 2017) and texts from the RIDGES corpus (Odebrecht et al., 2016) Bollmann et al. (2017) to obtain a single dataset."}
{"sent_id": "91e4fd2556d4a04e477ea97208b218-C001-98", "intents": ["@DIF@"], "paper_id": "ABC_91e4fd2556d4a04e477ea97208b218_4", "text": "Evaluated by human graders, questions generated by our system are significantly better than these from Serban et al. (2016) on 500 random-selected triples from Freebase."}
{"sent_id": "91e4fd2556d4a04e477ea97208b218-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_91e4fd2556d4a04e477ea97208b218_4", "text": "To tackle this challenge, previous work (Seyler et al., 2015; Serban et al., 2016 ) relies on massive human-labeled data."}
{"sent_id": "91e4fd2556d4a04e477ea97208b218-C001-58", "intents": ["@USE@"], "paper_id": "ABC_91e4fd2556d4a04e477ea97208b218_4", "text": "We first compare our system with Serban et al. (2016) on 500 randomly selected triples from Freebase (Bollacker et al., 2008) 2 ."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "More recent models introduced a latent variable for the sense of a word, with the assumption that a sense has multiple concepts (HC, HC+Zipf) (Chang, Pei, and Chen 2014) and that topics and senses should be inferred jointly (STM) (Wang et al. 2015) ."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-112", "intents": ["@USE@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "Following (Wang et al. 2015), we set the local context window to 10, with a maximum number of words of 21 (i.e. 10 words before and 10 words after)."}
{"sent_id": "75b2aa54c363151130ca2146044922-C001-128", "intents": ["@USE@"], "paper_id": "ABC_75b2aa54c363151130ca2146044922_4", "text": "In order to get a common ground for comparison, we do a geometric average AVG of both metrics, following (Wang et al. 2015) ."}
{"sent_id": "55160a7ab2df9a86e677bcc72d9842-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_55160a7ab2df9a86e677bcc72d9842_4", "text": "Iyer et al. (2018) use a similar decoding approach but use a specialized context encoder for the task of context-dependent code generation."}
{"sent_id": "55160a7ab2df9a86e677bcc72d9842-C001-21", "intents": ["@USE@"], "paper_id": "ABC_55160a7ab2df9a86e677bcc72d9842_4", "text": "We evaluate our approach on a context dependent semantic parsing task (Iyer et al., 2018) using the CONCODE dataset, where we improve the state of the art by 2.2% of BLEU score."}
{"sent_id": "55160a7ab2df9a86e677bcc72d9842-C001-101", "intents": ["@USE@"], "paper_id": "ABC_55160a7ab2df9a86e677bcc72d9842_4", "text": "Since idiom aware training is significantly faster than without idioms, we are also able to train on an additional 400K training examples that Iyer et al. (2018) released as part of CONCODE."}
{"sent_id": "55160a7ab2df9a86e677bcc72d9842-C001-85", "intents": ["@EXT@"], "paper_id": "ABC_55160a7ab2df9a86e677bcc72d9842_4", "text": "We follow the approach of Iyer et al. (2018) with three major modifications in their encoder, which yields improvements in speed and accuracy (IyerSimp) ."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "That is, if a subject judges that a sentence generated by automatically changing the word order in the same way as the previous work (Yoshida et al., 2014 ) may have spontaneously written by a native."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-79", "intents": ["@USE@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "We compared our method to Yoshida's method (Yoshida et al., 2014) and two conventional sequential methods."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-93", "intents": ["@DIF@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "The dependency accuracy of our method was significantly lower than that of the two sequential methods, and was higher than that of Yoshida's method although there was no significant difference."}
{"sent_id": "ddd23a034c366b62b53d15128edd45-C001-95", "intents": ["@SIM@"], "paper_id": "ABC_ddd23a034c366b62b53d15128edd45_4", "text": "As a result of analysis, especially, our method and Yoshida's method tended to improve the sentence accuracy very well in case of short sentences."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "Hayashi (2016) represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet)."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-24", "intents": ["@EXT@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "Following Hayashi (2016)'s work on Evocation prediction, we frame word association prediction as a supervised regression task and introduce several new and modified features, including the first use of Gaussian embeddings (Vilnis and McCallum, 2014) to better capture the asymmetric nature of word associations."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-67", "intents": ["@DIF@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "We instead consider a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1."}
{"sent_id": "a99a393c83e47393400c72338faf80-C001-75", "intents": ["@DIF@"], "paper_id": "ABC_a99a393c83e47393400c72338faf80_4", "text": "Instead, we experiment with offsets calculated using w2v, GloVe, and w2g embeddings."}
{"sent_id": "87a87855d67d90c691ab5bedb4d460-C001-16", "intents": ["@EXT@"], "paper_id": "ABC_87a87855d67d90c691ab5bedb4d460_4", "text": "This paper is a follow-up of our previous work [2] ."}
{"sent_id": "87a87855d67d90c691ab5bedb4d460-C001-21", "intents": ["@USE@"], "paper_id": "ABC_87a87855d67d90c691ab5bedb4d460_4", "text": "Section 4 describes our evaluation on two datasets: the synthetic dataset used in [2] and the audiobook dataset described in section 2."}
{"sent_id": "87a87855d67d90c691ab5bedb4d460-C001-53", "intents": ["@USE@"], "paper_id": "ABC_87a87855d67d90c691ab5bedb4d460_4", "text": "For the three tasks, we use encoder-decoder models with attention [9, 10, 11, 2, 3] ."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-3", "intents": ["@DIF@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambiguation accuracy on a large sense-tagged corpus first used in (Ng and Lee, 1996) ."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-83", "intents": ["@BACK@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "Local collocations have been found to be the single most informative set of features for WSD (Ng and Lee, 1996) ."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-100", "intents": ["@USE@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "The accuracy figures of LEXAS as reported in (Ng and Lee, 1996) are reproduced in the third row of Table 1 ."}
{"sent_id": "d160d5a44f795f2b694d5ee538d713-C001-92", "intents": ["@SIM@"], "paper_id": "ABC_d160d5a44f795f2b694d5ee538d713_4", "text": "2The first five of these seven features were also used in (Ng and Lee, 1996) . , 1996) ."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "Learning representations (Mikolov et al., 2013) of natural language and language model pre-training (Devlin et al., 2018; Radford et al., 2019) has shown promising results recently."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-113", "intents": ["@USE@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "Following the original BERT setup (Devlin et al., 2018) , we format the inputs as \"[CLS] TRANS/BERT  108M  12  768  768  12  6.0X  Base  TRANS-BLSTM-SMALL  152M  12  768  768  12  3.3X  TRANS-BLSTM  237M  12  768  768  12  2.5X  Large TRANS/BERT  334M  24  1024  1024  16  2.8X  TRANS-BLSTM-SMALL  487M  24  1024  1024  16  1.4X  TRANS-BLSTM  789M  24  1024  1024  16  1   Table 1 : Parameter size and training speed for TRANS/BERT, TRANS-BLSTM-SMALL, and TRANS-BLSTM on base and large settings respectively."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-176", "intents": ["@USE@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "Following (Devlin et al., 2018) , we use a batch size of 32 and 3-epoch fine-tuning over the data for all GLUE tasks."}
{"sent_id": "caa0ffb1d4e3e5310a28b921333d1e-C001-128", "intents": ["@DIF@"], "paper_id": "ABC_caa0ffb1d4e3e5310a28b921333d1e_4", "text": "1 Nevertheless, our implementation of baseline BERT model obtained higher accuracy than that reported by the original BERT paper (Devlin et al., 2018) ."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-2", "intents": ["@MOT@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "It is shown that many published models for the Stanford Question Answering Dataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50% decrease in F1 score during adversarial evaluation based on the AddSent (Jia and Liang, 2017) algorithm."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-11", "intents": ["@MOT@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "However, as shown recently by Jia and Liang (2017) , these models are very fragile when presented with adversarially generated data."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-48", "intents": ["@MOT@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "Lastly, to address the antonymstyle semantic perturbations used in AddSent, we show that we need to improve model capabilities by adding indicator features for semantic relationships (but only when) in tandem with the addition of diverse adversarial data (Sec. 3.3)."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-26", "intents": ["@EXT@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "Although we focused on the AddSent adversary (Jia and Liang, 2017) , our method of effective adversarial training by eliminating superficial statistical correlations (with joint model capability improvements) are generalizable to other similar insertion-based adversaries for Q&A tasks."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-42", "intents": ["@EXT@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "Our 'AddSentDiverse' algorithm is a modified version of AddSent (Jia and Liang, 2017) , aimed at producing good adversarial examples for robust training purposes."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "They proposed AddSent, which creates a semantically-irrelevant sentence containing a fake answer that resembles the question syntactically, and appends it to the context."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "Jia and Liang (2017) presented some initial attempts to fix this problem by retraining the BiDAF model (Seo et al., 2017) with adversaries generated with AddSent. But they showed that the method is not very effective, as slight modifications (e.g., different positioning of the distractor sentence in the paragraph and different fake answer set) to the adversary generation algorithm at test time have drastic impact on the retrained model's performance."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "During training done by Jia and Liang (2017) , the distractor is always added as the last sentence, creating a very skewed distribution for Y ."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-21", "intents": ["@DIF@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "We show that an AddSentDiverse-based adversariallytrained model beats an AddSent-trained model across 3 different adversarial test sets, showing an average improvement of 24.22% in F1 score, demonstrating a general increase in robustness."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-81", "intents": ["@USE@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "Experiments measure the soft F1 score and all of the adversarial evaluations are modeldependent, following the style of AddSent, where multiple adversaries are generated for each exam-ple in the evaluation set and the model's worst performance among the variants is recorded."}
{"sent_id": "6ca6283ae23bbd6d0827d8f5f2947a-C001-129", "intents": ["@USE@"], "paper_id": "ABC_6ca6283ae23bbd6d0827d8f5f2947a_4", "text": "This explains why in our experiment studying the effect of distractor placement strategies (see Table 2), InsMid's performance was not skewed towards either AddSent or AddSentPrepend, but was worse on both when compared to InsRandom."}
{"sent_id": "15c8ca572430c214d9c571fbe0db95-C001-44", "intents": ["@SIM@"], "paper_id": "ABC_15c8ca572430c214d9c571fbe0db95_4", "text": "We use a DP-based beam search procedure similar to the one presented in (Tillmann and Xia, 2003) ."}
{"sent_id": "91c1a4ab0347fb8b11ff213a97e864-C001-19", "intents": ["@USE@"], "paper_id": "ABC_91c1a4ab0347fb8b11ff213a97e864_4", "text": "We empirically validate the benefit of this approach within the cube-pruning dependency parser of Zhang and McDonald (2012) ."}
{"sent_id": "91c1a4ab0347fb8b11ff213a97e864-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_91c1a4ab0347fb8b11ff213a97e864_4", "text": "All other highorder features of Zhang and McDonald (2012) only look at arcs on the same side of their head."}
{"sent_id": "91c1a4ab0347fb8b11ff213a97e864-C001-84", "intents": ["@DIF@"], "paper_id": "ABC_91c1a4ab0347fb8b11ff213a97e864_4", "text": "It also improves over the baseline cube-pruning parser without max-violation update strategies (Zhang and McDonald, 2012) , showing the importance of update strategies in inexact hypergraph search."}
{"sent_id": "bdd7a4dabf8a8d7c0a2b638eb6eb72-C001-105", "intents": ["@USE@"], "paper_id": "ABC_bdd7a4dabf8a8d7c0a2b638eb6eb72_4", "text": "We used the implementation of PRA provided by the authors of (Gardner et al., 2014) ."}
{"sent_id": "a127218cca5653f1700c0de6c8318a-C001-23", "intents": ["@USE@"], "paper_id": "ABC_a127218cca5653f1700c0de6c8318a_4", "text": "We built the database by applying the bert-base-uncased model of the PyTorch Pretrained the BERT project 1 (Devlin et al., 2019) to the corpus."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-2", "intents": ["@BACK@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "Transformer with self-attention has achieved great success in the area of nature language processing."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "In [16, 17, 18] , self-attention is only applied in a chunk of acoustic input restricted by a time window, which makes the transformer model easier to train as it does not need to consider very long term correlations."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "In [8] , the authors used the dot-production attention [20] rather than the conventional additive attention [19] in favor of the low computational complexity, which is rewritten here as:"}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-17", "intents": ["@MOT@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "While there have been many studies on end-to-end speech recognition using transformers [10, 11, 12, 13, 14] , their applications for hybrid acoustic models are less well understood."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-33", "intents": ["@EXT@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "We propose an interleaved self-attention and convolution structure for transformer model, with the motivation that convolution can learn local feature correlations and maintain the ordering information of the sequence while self-attention can capture longterm correlations."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-60", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "For hybrid models, our preliminary experiments show that transformers with multiple self-attention layers alone is hard to train without time restriction, and it can easily diverge after a few epochs."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-92", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "We first evaluated the positional encoding discussed in section 3.2 and dropout training for the transformer model."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-110", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "We also have to insert another layer normalization to the output of the transformer before the output linear layer to stabilize the training."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-153", "intents": ["@USE@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "We are more interested in transformers for hybrid acoustic models as there is no theoretical issues for online streaming speech recognition."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-132", "intents": ["@DIF@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "The table shows that when we limited the future context information for the transformer model, we obtained slightly worse results."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-108", "intents": ["@SIM@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "This corresponds to a vanilla transformer as in [8] ."}
{"sent_id": "42854f204c8d2a62822e12f731ad08-C001-158", "intents": ["@FUT@"], "paper_id": "ABC_42854f204c8d2a62822e12f731ad08_4", "text": "For our future works, we shall study training much deeper transformer with low frame rate to get rid of the GPU memory constraint, as well as evaluate the model in the setting with a very large amount of training data."}
{"sent_id": "1dd3adcb79c8bc4b5187b85d836ceb-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_1dd3adcb79c8bc4b5187b85d836ceb_4", "text": "A generative parsing model can be used on its own, and it was shown in Collins and Roark (2004) that a discriminative parsing model can be used on its own."}
{"sent_id": "3251c6cd1afccf6ad8d5391a4360b0-C001-55", "intents": ["@USE@"], "paper_id": "ABC_3251c6cd1afccf6ad8d5391a4360b0_4", "text": "Following previous work (Post et al., 2013; Sperber et al., 2017) , we lowercase and remove punctuation from the English translations."}
{"sent_id": "3251c6cd1afccf6ad8d5391a4360b0-C001-81", "intents": ["@DIF@"], "paper_id": "ABC_3251c6cd1afccf6ad8d5391a4360b0_4", "text": "Most importantly, we are able to reach those results while being two orders of magnitude faster at training time: Sperber et al. (2017) report taking 1.5 days for each epoch while our architecture can process each epoch in 15min."}
{"sent_id": "d2c95c3198f21e793549d7b16bdaf8-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_d2c95c3198f21e793549d7b16bdaf8_4", "text": "Independently of the chosen graph algorithm, the highest candidate score among the set of candidates is chosen as correct disambiguation for a given mention [4] ."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "They model the semantics of natural language sentences and KB triples as the sum of the embeddings of the associated words and KB elements respectively."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "The embeddings learned in (Bordes et al., 2014b ) also encode context information."}
{"sent_id": "357667e192057a48dff60edad86bf0-C001-83", "intents": ["@USE@"], "paper_id": "ABC_357667e192057a48dff60edad86bf0_4", "text": "The model is introduced in (Bordes et al., 2014b) and we use the same scoring function."}
{"sent_id": "9272b3f7e628a156caed328d475d0c-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_9272b3f7e628a156caed328d475d0c_5", "text": "Sinha et al. classified English readability formulas into three broad categories -traditional methods, cognitively motivated methods, and machine learning methods [21] ."}
{"sent_id": "9272b3f7e628a156caed328d475d0c-C001-45", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9272b3f7e628a156caed328d475d0c_5", "text": "We found only three lines of work that specifically looked into Bengali readability [6] , [11] , [21] ."}
{"sent_id": "9272b3f7e628a156caed328d475d0c-C001-93", "intents": ["@DIF@"], "paper_id": "ABC_9272b3f7e628a156caed328d475d0c_5", "text": "Note also that our dataset is larger than both Sinha et al.'s 16document dataset [21] , and Das and Roychoudhury's seven document dataset [6] ."}
{"sent_id": "55f67c918001335974608200a87cfc-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_55f67c918001335974608200a87cfc_5", "text": "This linking is used in GRAFT-Net as well which also performs question answering through fusing learned knowledge graph and linked document representations [17] ."}
{"sent_id": "55f67c918001335974608200a87cfc-C001-12", "intents": ["@MOT@"], "paper_id": "ABC_55f67c918001335974608200a87cfc_5", "text": "Text corpora have high coverage but extracting information from them is challenging whereas knowledge graphs are incomplete but are easier to extract answers from [17] ."}
{"sent_id": "55f67c918001335974608200a87cfc-C001-80", "intents": ["@USE@"], "paper_id": "ABC_55f67c918001335974608200a87cfc_5", "text": "Following [17] , we apply the same pre-processing and report average F 1 and Hits@1, as well as micro-average, and macro-average F 1 scores."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-3", "intents": ["@USE@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "We show that such representation together with nCCA, a successful multimodal embedding technique, achieves state-of-the-art performance on the Visual Madlibs task."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-128", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "We evaluate our method on the multiple choice task of the Visual Madlibs dataset."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-178", "intents": ["@USE@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "We study an image representation formed by averaging over representations of object proposals, and show its effectiveness through experimental evaluation on the Visual Madlibs dataset [32] ."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-30", "intents": ["@DIF@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "This contrasts with the prior approach of [32] that uses a post-hoc comparison between the discrete output of the CNN+LSTM method and all four candidates."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-115", "intents": ["@DIF@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "Although the CNN+LSTM models we trained on Madlibs were not quite as accurate as nCCA for selecting the correct multiple-choice answer, they did result in better, sometimes much better, accuracy (as measured by BLEU scores) for targeted generation."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "The recently introduced Visual Madlibs task [32] removes ambiguities in question or scene interpretations by introducing a multiple choice \"filling the blank\" task, where a c 2016."}
{"sent_id": "866cc7036c626f07fba10ab2a839d8-C001-28", "intents": ["@EXT@"], "paper_id": "ABC_866cc7036c626f07fba10ab2a839d8_5", "text": "Our approach in the combination with the Normalized Correlation Analysis embedding technique improves on the state-of-the-art of the Visual Madlibs task."}
{"sent_id": "247bbc4eb671895222065ed425f968-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_247bbc4eb671895222065ed425f968_5", "text": "The CTS acoustic training corpus consists of approximately 2000 hours of speech with human transcriptions [2] ."}
{"sent_id": "247bbc4eb671895222065ed425f968-C001-30", "intents": ["@MOT@", "@SIM@"], "paper_id": "ABC_247bbc4eb671895222065ed425f968_5", "text": "In [2, 3] we describe state-of-the-art speech recognition systems on the CTS task using multiple LSTM and ResNet acoustic models trained on various acoustic features along with word and character LSTMs and convolutional WaveNet-style language models."}
{"sent_id": "247bbc4eb671895222065ed425f968-C001-47", "intents": ["@SIM@"], "paper_id": "ABC_247bbc4eb671895222065ed425f968_5", "text": "Similar to [2] , human performance measurements on two broadcast news tasks -RT04 and DEV04F -are carried out by Appen."}
{"sent_id": "247bbc4eb671895222065ed425f968-C001-161", "intents": ["@SIM@"], "paper_id": "ABC_247bbc4eb671895222065ed425f968_5", "text": "5. Similar to telephone conversation confusions reported in [2] , humans performance is much higher because the number of deletions is significantly lower -compare 2.3% vs 0.8%/0.6% for deletion errors in Table 5 ."}
{"sent_id": "247bbc4eb671895222065ed425f968-C001-90", "intents": ["@USE@"], "paper_id": "ABC_247bbc4eb671895222065ed425f968_5", "text": "To complement the LSTM acoustic model, we train a deep Residual Network based on the best performing architecture proposed in [2] ."}
{"sent_id": "1056d36c5ed22c7a34f6fe82b4962f-C001-43", "intents": ["@USE@"], "paper_id": "ABC_1056d36c5ed22c7a34f6fe82b4962f_5", "text": "Finally, for spelling mistakes, we, again, follow Bryant and Briscoe (2018) and use CyHunSpell 3 to generate alternatives for non-words."}
{"sent_id": "1056d36c5ed22c7a34f6fe82b4962f-C001-69", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_1056d36c5ed22c7a34f6fe82b4962f_5", "text": "However, we use the development sets used by Bryant and Briscoe (2018) to tune the hyperparameter τ ."}
{"sent_id": "1056d36c5ed22c7a34f6fe82b4962f-C001-13", "intents": ["@EXT@", "@MOT@"], "paper_id": "ABC_1056d36c5ed22c7a34f6fe82b4962f_5", "text": "In our work, we extend this work by substituting the n-gram model for several publicly available implementations of state-of-the-art Transformer language models trained on large linguistic corpora and assess their performance on GEC without any supervised training."}
{"sent_id": "f6694f359ae948b6e4563b927a672c-C001-83", "intents": ["@BACK@"], "paper_id": "ABC_f6694f359ae948b6e4563b927a672c_5", "text": "Moreover, thanks to the orthogonality constraint their monolingual performance in the word analogy task does not degrade, whereas the accuracy of Mikolov et al. (2013b) drops by 2.86% in absolute terms with respect to the original embeddings."}
{"sent_id": "f6694f359ae948b6e4563b927a672c-C001-39", "intents": ["@DIF@"], "paper_id": "ABC_f6694f359ae948b6e4563b927a672c_5", "text": "This last optimization objective coincides with Xing et al. (2015) , but their work was motivated by an hypothetical inconsistency in Mikolov et al. (2013b) , where the optimization objective to learn word embeddings uses dot product, the objective to learn mappings uses Euclidean distance and the similarity computations use cosine."}
{"sent_id": "f6694f359ae948b6e4563b927a672c-C001-85", "intents": ["@DIF@"], "paper_id": "ABC_f6694f359ae948b6e4563b927a672c_5", "text": "This beats the method by Xing et al. (2015) in the bilingual task, although it comes at the price of a considerable degradation in monolingual quality."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-26", "intents": ["@MOT@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "First, Miwa and Bansal (2016) rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008) ."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-185", "intents": ["@BACK@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "For the local model, we apply scheduled sampling (Bengio et al., 2015) , which has been shown to improve the performance of relation extraction by Miwa and Bansal (2016) ."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-32", "intents": ["@DIF@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "Second, Miwa and Bansal (2016) did not explicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies (Wang and Chang, 2016; ."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-114", "intents": ["@DIF@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "On the one hand, incorrect dependency paths between entity pairs can propagate to relation classification in Miwa and Bansal (2016) , because these paths rely on explicit discrete outputs from a syntactic parser."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-83", "intents": ["@USE@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "We follow Miwa and Bansal (2016) , learning global context representations using LSTMs."}
{"sent_id": "580713b57ae47692af0d0c86a07fd1-C001-219", "intents": ["@USE@"], "paper_id": "ABC_580713b57ae47692af0d0c86a07fd1_5", "text": "Miwa and Bansal (2016) exploit the shortest dependency path, which can make the distance between two entities closer compared with their sequential dis-tance, thus facilitating relation extraction."}
{"sent_id": "3fd7a249a8fa7a71a4c6aa2e79fecf-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_3fd7a249a8fa7a71a4c6aa2e79fecf_5", "text": "Most literature compares results with others by citing the scores directly Lample et al., 2016) without re-implementing them under the same setting, resulting in less persuasiveness on the advantage of their models."}
{"sent_id": "13091dd4d06e11957a5cb7785b92d4-C001-38", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_13091dd4d06e11957a5cb7785b92d4_5", "text": "Substitutions and swaps were experimented with extensively in previous work (Heigold et al., 2017; Belinkov and Bisk, 2018) , but deletion and insertion were not."}
{"sent_id": "13091dd4d06e11957a5cb7785b92d4-C001-104", "intents": ["@BACK@", "@DIF@", "@EXT@"], "paper_id": "ABC_13091dd4d06e11957a5cb7785b92d4_5", "text": "Most relevant for us is the work of Belinkov and Bisk (2018) , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) ."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-13", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "In this work, we propose to address both these issues by learning simple dependency link embeddings on 'head-argument' pairs (as a single concatenated unit), which allows us to work directly with linguistically-intuitive, higher-order substructures, and also fire significantly fewer and simpler features in dependency parsing, as opposed to word cluster and embedding features in previous work (Koo et al., 2008; Bansal et al., 2014) , while still maintaining their strong accuracies."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-88", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "However, their embedding features are still based on the sparse set of n-ary, word-based templates from previous work (McDonald et al., 2005a; Koo et al., 2008) ."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-23", "intents": ["@SIM@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "2 We use the original skip-gram model and simply change the context tuple data on which the model is trained, similar to Bansal et al. (2014) and Levy and Goldberg (2014) ."}
{"sent_id": "d0b4d9566f16915cb5a5244f351e61-C001-48", "intents": ["@USE@"], "paper_id": "ABC_d0b4d9566f16915cb5a5244f351e61_5", "text": "For all experiments (unless otherwise noted), we follow the 2nd-order MSTParser setup of Bansal et al. (2014) , in terms of data splits, parameters, preprocessing, and feature thresholding."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "One of the studies in this area has been presented by Mitra et al. [19] where the authors show that at earlier times, the sense of the word 'sick' was mostly associated to some form of illness; however, over the years, a new sense associating the same word to something that is 'cool' or 'crazy' has emerged."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-90", "intents": ["@BACK@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "The authors then apply multi-stage filtering in order to obtain meaningful candidate words."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-40", "intents": ["@EXT@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "We propose a method based on the network features to reduce the number of false positives and thereby, increase the overall precision of the method proposed by Mitra et al. [19] ."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-148", "intents": ["@USE@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "Table 3 shows three example words from T 1 , their 'birth' clusters as reported in Mitra et al. [19] and the manual evaluation result."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-159", "intents": ["@USE@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "Table 7 represents some of the examples which were declared as 'birth' by Mitra et al. [19] but SVM filtering correctly flagged them as 'false birth'."}
{"sent_id": "8a1d4802c170fa8a71504533437e8f-C001-158", "intents": ["@DIF@"], "paper_id": "ABC_8a1d4802c170fa8a71504533437e8f_5", "text": "Even then Mitra et al. [19] 's F-measure ranges from 0.37-0.48 while ours is 0.67-0.68."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-3", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "The model addresses an important limitation associated with our previous stateof-the-art discriminative semantic parsing model -the relaxed hybrid tree model by introducing our constrained semantic forests."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-15", "intents": ["@MOT@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Thus, the model is unable to identify joint representations for certain sentence-semantics pairs during the training process, and is unable to produce desired outputs for certain inputs during the evaluation process."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-60", "intents": ["@MOT@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Therefore we can construct a relaxed hybrid tree representation that contains the given natural language sentence w 1 w 2 with an infinite number of nodes."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-78", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "where M refers to the set of all possible semantic trees whose heights are less than or equal to c, and H (n, m ) refers to the set of possible relaxed hybrid tree representations where the pattern X is allowed."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-54", "intents": ["@DIF@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "One important difference between the hybrid tree representations and the relaxed hybrid tree representations is the exclusion of the pattern X in the latter."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-105", "intents": ["@DIF@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "We would like to highlight two potential advantages of our new model over the old RHT model."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-110", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "As we have mentioned in (Lu, 2014) , although the RHT model is able to capture unbounded long-distance dependencies, for certain languages such as German such longdistance features appeared to be detrimental to the performance of the system (Lu, 2014, Table  4 )."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "It relies on representations called relaxed hybrid trees that can jointly represent both the sentences and semantics."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "We briefly discuss our previously proposed relaxed hybrid tree model (Lu, 2014) in this section."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Figure 1 gives an example of a hybrid tree and a relaxed hybrid tree representation encoding the sentence w 1 w 2 w 3 w 4 w 5 w 6 w 7 w 8 w 9 w 10 and the se-"}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-117", "intents": ["@USE@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "In practice, to make the overall training process faster, we implemented a parallel version of the original RHT algorithm."}
{"sent_id": "88aca1aa7ab73cde8492adc0e7a059-C001-83", "intents": ["@SIM@"], "paper_id": "ABC_88aca1aa7ab73cde8492adc0e7a059_5", "text": "Setting the maximum height to c effectively guarantees that all semantic trees contained in the constrained semantic forest have a height no greater than c. We then constructed the (exponentially many) relaxed hybrid tree representations based on the constrained semantic forest M and each input sentence n. We used a single packed forest representation to represent all such relaxed hybrid tree representations."}
{"sent_id": "4b0aab00a99547791bff0597aabc06-C001-92", "intents": ["@USE@"], "paper_id": "ABC_4b0aab00a99547791bff0597aabc06_5", "text": "The 0 line represents a plain word2vec baseline and the dashed line represents the 3-tensor baseline of Cotterell et al. (2017) ."}
{"sent_id": "4b0aab00a99547791bff0597aabc06-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_4b0aab00a99547791bff0597aabc06_5", "text": "Cotterell et al. (2017) showed that SG is a form of exponential family PCA that factorizes the matrix of word/context cooccurrence counts (rather than shifted positive PMI values)."}
{"sent_id": "4b0aab00a99547791bff0597aabc06-C001-110", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_4b0aab00a99547791bff0597aabc06_5", "text": "Both Cotterell et al. (2017) and Levy and Goldberg (2014a) incorporate additional syntactic and morphological information in their word embeddings."}
{"sent_id": "d5144370a9361ff870dd3cb2e064ff-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_d5144370a9361ff870dd3cb2e064ff_5", "text": "Linzen et al. (2016) start with existing sentences from wikipedia that contain a present-tense verb."}
{"sent_id": "d5144370a9361ff870dd3cb2e064ff-C001-20", "intents": ["@EXT@"], "paper_id": "ABC_d5144370a9361ff870dd3cb2e064ff_5", "text": "I use the stimuli provided by (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model."}
{"sent_id": "e5886e138ce8d84a48e44db3f3d6a1-C001-43", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_e5886e138ce8d84a48e44db3f3d6a1_5", "text": "In a recent work, Jana and Goyal (2018b) used network features extracted from the DT to detect co-hyponyms."}
{"sent_id": "e5886e138ce8d84a48e44db3f3d6a1-C001-71", "intents": ["@USE@"], "paper_id": "ABC_e5886e138ce8d84a48e44db3f3d6a1_5", "text": "The relation between word pair holds if the lin similarity (Lin, 1998) of the word vectors is greater than some threshold p Table 3 : Accuracy scores on a ten-fold cross validation for cohyponym BLESS dataset of our models along with the top two baseline models (one supervised, one semisupervised) described in (Weeds et al., 2014) and models described in (Jana and Goyal, 2018b )"}
{"sent_id": "e5886e138ce8d84a48e44db3f3d6a1-C001-96", "intents": ["@USE@"], "paper_id": "ABC_e5886e138ce8d84a48e44db3f3d6a1_5", "text": "This dataset is extracted from BLESS (Baroni and Lenci, 2011) and divided into three small datasets-Co-Hypo vs Hyper, Co-Hypo vs Mero, Co-Hypo Vs Random."}
{"sent_id": "e5886e138ce8d84a48e44db3f3d6a1-C001-80", "intents": ["@DIF@"], "paper_id": "ABC_e5886e138ce8d84a48e44db3f3d6a1_5", "text": "Here, the best model proposed by Jana and Goyal (2018b) uses SVM classifier which is fed with structural similarity of the words in the given word pair from the distributional thesaurus network."}
{"sent_id": "bebcad79900e9a4a25020ed0d886b5-C001-2", "intents": ["@MOT@"], "paper_id": "ABC_bebcad79900e9a4a25020ed0d886b5_5", "text": "I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) \"coloreless green ideas\" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena."}
{"sent_id": "bebcad79900e9a4a25020ed0d886b5-C001-10", "intents": ["@DIF@", "@BACK@"], "paper_id": "ABC_bebcad79900e9a4a25020ed0d886b5_5", "text": "(Gulordava et al., 2018 ) also consider subject-verb agreement, but in a \"colorless green ideas\" setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues."}
{"sent_id": "bebcad79900e9a4a25020ed0d886b5-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_bebcad79900e9a4a25020ed0d886b5_5", "text": "Gulordava et al. (2018) also start with existing sentences."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "RL-based methods formulate the task of KG completion as a sequential decision-making process in which the goal is to train an RL agent to walk over the graph by taking a sequence of actions (i.e., choosing the next entity) that connects the source to the target entity."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Lin et al. [13] implement reward shaping to address the problem of the sparse reward signal and action dropout to reduce the effect of incorrect paths."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-68", "intents": ["@BACK@", "@SIM@", "@EXT@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "For example, the authors of [13] build an agent-based model on top of pre-trained embeddings generated by ComplEx [27] or ConvE [4] ."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "For example, [13] pre-computes PageRank scores for each node, and narrows the action space to a fixed number of highest-ranking neighbors."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-121", "intents": ["@USE@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Following [13] , we use an LST M to encode the history h t = {e t−k , r t−k+1 , ..., e t−1 , r t } of the past k steps taken by the agent in solving the query."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-155", "intents": ["@USE@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "We compare against several baseline methods: ConvE (embeddingbased) [4] , ComplEx (embedding-based) [27] , MINERVA (agent-based) [3] , and MultiHopKG (agent-based) using both ConvE and ComplEx for pre-trained embeddings [13] ."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-197", "intents": ["@USE@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "For NELL-995 data, We quote the results reported in [3, 13] ."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-227", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "We compare the ablation models along with the best RL baseline performance on seen and unseen queries."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-203", "intents": ["@DIF@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Our method results in a 4% improvement in MRR (and 5.43% in Hits@1) over the best RL baseline on Amazon Cellphones and a 3.9% improvement in MRR (and 5.5% in Hits@1) on Amazon Beauty."}
{"sent_id": "666cc3c936358c5e9b2f7d0eb8d0e4-C001-242", "intents": ["@DIF@"], "paper_id": "ABC_666cc3c936358c5e9b2f7d0eb8d0e4_6", "text": "Our proposed model consistently shows a better performance than Lin et al., except for the NELL-995 dataset where the improvement is marginal."}
{"sent_id": "0c233d68fb2ccdf033fc6a08c8f4bf-C001-25", "intents": ["@USE@"], "paper_id": "ABC_0c233d68fb2ccdf033fc6a08c8f4bf_6", "text": "Following the PDTB annotation manual (Prasad et al. 2008b ), we conducted a pilot annotation of discourse connectivity in biomedical text."}
{"sent_id": "0c233d68fb2ccdf033fc6a08c8f4bf-C001-47", "intents": ["@EXT@"], "paper_id": "ABC_0c233d68fb2ccdf033fc6a08c8f4bf_6", "text": "After the completion of the pilot annotation and the discussion, we decided to add the following conventions to the PDTB annotation guidelines to address the characteristics of biomedical text:"}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-172", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "Since for these models, phrase-level training signals are not available-the nodes here do not correspond to that in the original Standford Sentiment Tree Bank, but the roots and leafs annotations are still the same, so we run two versions of our experiments: one uses only training signals from roots and the other includes also leaf annotations."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "In recent years, recursive neural networks (RvNN) have been introduced and demonstrated to achieve state-of-the-art performances on different problems such as semantic analysis in natural language processing and image segmentation (Socher et al., 2013; 2011) ."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-84", "intents": ["@USE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "Backpropagation over structures During training, the gradient of the objective function with respect to each parameter can be calculated efficiently via backpropagation over structures (Goller & Kchler, 1996; Socher et al., 2013) ."}
{"sent_id": "e8c60c9fc3a2d74df632f3b423adae-C001-116", "intents": ["@USE@"], "paper_id": "ABC_e8c60c9fc3a2d74df632f3b423adae_6", "text": "Following (Socher et al., 2013) , we also use the classification accuracy to measure the performances."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "In their experiments, semantic knowledge was acquired from a corpus containing the materials to be segmented in the test phase."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-17", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "Second, Choi et al. employed a large number of small test samples to evaluate their algorithm, each making up-on average-0.15% of the LSA corpus."}
{"sent_id": "7c8ec9e38bf4c0c458d60af014e102-C001-60", "intents": ["@USE@"], "paper_id": "ABC_7c8ec9e38bf4c0c458d60af014e102_6", "text": "Words were not stemmed, as in Choi et al. (2001) ."}
{"sent_id": "9567cb276162a6e9d445f13f06f5a2-C001-31", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9567cb276162a6e9d445f13f06f5a2_6", "text": "More recently, we showed (Zapirain et al., 2009 ) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored."}
{"sent_id": "9567cb276162a6e9d445f13f06f5a2-C001-32", "intents": ["@EXT@"], "paper_id": "ABC_9567cb276162a6e9d445f13f06f5a2_6", "text": "In this work we apply a subset of the SP methods proposed in (Zapirain et al., 2009 )."}
{"sent_id": "9567cb276162a6e9d445f13f06f5a2-C001-36", "intents": ["@USE@"], "paper_id": "ABC_9567cb276162a6e9d445f13f06f5a2_6", "text": "The other model is an in-house method (Zapirain et al., 2009 ), referred as wn, which only takes into account the depth of the most common ancestor, and returns SPs that are as specific as possible."}
{"sent_id": "c856f5ce5d2cdcfc71027d6fa4c6b3-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_c856f5ce5d2cdcfc71027d6fa4c6b3_6", "text": "The segmentation algorithm proposed by Choi (2000) is made up of the three steps usually found in any segmentation procedure based on lexical cohesion."}
{"sent_id": "c856f5ce5d2cdcfc71027d6fa4c6b3-C001-48", "intents": ["@USE@"], "paper_id": "ABC_c856f5ce5d2cdcfc71027d6fa4c6b3_6", "text": "This experiment was based on the procedure and test materials designed by Choi (2000) , which was also used by several authors as a benchmark for comparing segmentation systems (Brants et al. 2002; Ferret 2002; Kehagias et al. 2003; Utiyama and Isahara 2001) ."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-161", "intents": ["@DIF@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "Cumulatively up to 20% error reduction is achieved relative to the standard Boulis and Ostendorf (2005) algorithm for classifying individual conversations on Switchboard, and accuracy for gender detection on the Switchboard corpus (aggregate) and Gulf Arabic exceeds 95%."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-20", "intents": ["@EXT@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "Sociolinguistic features: The paper explores a rich set of lexical and non-lexical features motivated by the sociolinguistic literature for gender classification, and show how they can effectively augment the standard ngrambased model of Boulis and Ostendorf (2005) ."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-44", "intents": ["@USE@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "Consistent with Boulis and Ostendorf (2005) , we utilized the Fisher telephone conversation corpus (Cieri et al., 2004) and we also evaluated performance on the standard Switchboard conversational corpus (Godfrey et al., 1992) , both collected and annotated by the Linguistic Data Consortium."}
{"sent_id": "ff7bafb8f21118ca3c908603ef32d0-C001-55", "intents": ["@USE@"], "paper_id": "ABC_ff7bafb8f21118ca3c908603ef32d0_6", "text": "We reimplemented this model as our reference for gender classification, further details of which are given below:"}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Waseem and Hovy (2016) and Ross et al. (2016) focus on building corpora which they annotate for containing hate speech."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-37", "intents": ["@USE@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "We present the annotators with the tests from Waseem and Hovy (2016) ."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-67", "intents": ["@EXT@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "To counteract the low coverage in Waseem and Hovy (2016) , we use a lexicon trained on Twitter (Sap et al., 2014) to calculate the probability of gender."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-24", "intents": ["@SIM@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "The order of the tweets is selected by our database connection, thus allowing for an overlap with the data set released by Waseem and Hovy (2016) ."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-74", "intents": ["@SIM@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Results Running our system on the Waseem and Hovy (2016) data set, we find that our best performing system does not substantially outperform on the binary classification task Waseem and Hovy (2016 Interestingly, the main cause of error is false positives."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-48", "intents": ["@DIF@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Interestingly, we see that the vast majority of disagreements between our annotators and Waseem and Hovy (2016) , are disagreements where our annotators do not find hate speech but Waseem and Hovy (2016) the influence of the features listed in Table 4 for each annotator group."}
{"sent_id": "1c51e45e2917268e0ab5ce43a69655-C001-101", "intents": ["@MOT@"], "paper_id": "ABC_1c51e45e2917268e0ab5ce43a69655_6", "text": "Finally, we will review the negative class in Waseem and Hovy (2016) ."}
{"sent_id": "04b525b91b48e31258287a015d0401-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_04b525b91b48e31258287a015d0401_6", "text": "(Gliozzo et al., 2005) succeeded eliminating this requirement by using the category name alone as the initial keyword, yet obtaining superior performance within the keywordbased approach."}
{"sent_id": "04b525b91b48e31258287a015d0401-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_04b525b91b48e31258287a015d0401_6", "text": "In this space, words that tend to cooccur together, or occur in similar contexts, are represented by similar vectors."}
{"sent_id": "04b525b91b48e31258287a015d0401-C001-22", "intents": ["@MOT@"], "paper_id": "ABC_04b525b91b48e31258287a015d0401_6", "text": "This behavior is quite typical for query expansion methods, which expand a query with contextually correlated terms."}
{"sent_id": "04b525b91b48e31258287a015d0401-C001-70", "intents": ["@USE@"], "paper_id": "ABC_04b525b91b48e31258287a015d0401_6", "text": "We therefore measure the contextual similarity between a category c and a document d utilizing LSA space, replicating the method in (Gliozzo et al., 2005) : c con and d LSA are taken as the LSA vectors of the category name and the document, respectively, yielding Sim con (c, d) = cos( c con , d LSA ))."}
{"sent_id": "04b525b91b48e31258287a015d0401-C001-78", "intents": ["@USE@"], "paper_id": "ABC_04b525b91b48e31258287a015d0401_6", "text": "As in their work, non-standard category names were adjusted, such as Foreign exchange for Money-fx."}
{"sent_id": "5e34591c2a7b1664e1275372c40b79-C001-69", "intents": ["@USE@"], "paper_id": "ABC_5e34591c2a7b1664e1275372c40b79_6", "text": "We define S e as the sentences where some system extracted a relation and S F as the sentences that match the arguments of a fact in ∆. The sentential precision and recall is computed on a randomly sampled set of sentences from S e ∪ S F , in which each sentence is manually labeled whether it expresses any relation in R. Figure 3 shows the precision/recall curves for MULTIR with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by Hoffmann et al. (2011) ."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-76", "intents": ["@USE@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "For our evaluation, we used three widely popular datasets: WordSim-353 (Finkelstein et al., 2001) , MEN (Bruni et al., 2014) , SimLex-999 (Hill et al., 2015) ."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-122", "intents": ["@USE@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "In the bottom the performance of the state-of-the-art models of Collobert and Weston (2008) , Huang et al. (2012), Mikolov et al. (2013) , as reported in Hill et al. (2015) ."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-141", "intents": ["@USE@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by Schnabel et al. (2015) , using the words of the SimLex-999 dataset as query words and collecting for each of them the top 1000 nearest neighbors."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-79", "intents": ["@BACK@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "However, Hill et al. (2015) claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater)."}
{"sent_id": "7ce85e3c3f58cee33015409b74f99e-C001-93", "intents": ["@BACK@"], "paper_id": "ABC_7ce85e3c3f58cee33015409b74f99e_6", "text": "Hill et al. (2015) claim that differently from other datasets, SimLex-999 interannotator agreement has not been surpassed by any automatic approach."}
{"sent_id": "14529822630fb469f5fc8f37aaf473-C001-14", "intents": ["@USE@"], "paper_id": "ABC_14529822630fb469f5fc8f37aaf473_6", "text": "In this paper, we use a compound noun compositionality dataset (Reddy et al., 2011) to investigate the extent to which the underlying definition of context has an effect on a model's ability to support composition."}
{"sent_id": "14529822630fb469f5fc8f37aaf473-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_14529822630fb469f5fc8f37aaf473_6", "text": "Reddy et al. (2011) carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100)."}
{"sent_id": "14529822630fb469f5fc8f37aaf473-C001-52", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_14529822630fb469f5fc8f37aaf473_6", "text": "For consistency with the experiments of Reddy et al. (2011) , the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008) ."}
{"sent_id": "14529822630fb469f5fc8f37aaf473-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_14529822630fb469f5fc8f37aaf473_6", "text": "However, these results were still significantly lower than those reported by Reddy et al. (2011) using the larger ukWaC corpus."}
{"sent_id": "14529822630fb469f5fc8f37aaf473-C001-99", "intents": ["@DIF@"], "paper_id": "ABC_14529822630fb469f5fc8f37aaf473_6", "text": "We see that the results using standard PPMI (α = 1) significantly outperform the result reported in Reddy et al. (2011) , which demonstrates the superiority of a typed dependency space over an untyped dependency space."}
{"sent_id": "4176674f83dec5389a23d9d45654c7-C001-4", "intents": ["@USE@"], "paper_id": "ABC_4176674f83dec5389a23d9d45654c7_6", "text": "Our system is based on Marton et al. (2013) ."}
{"sent_id": "a6564c4b215e6c5ad4f53eeb5dd69c-C001-139", "intents": ["@BACK@"], "paper_id": "ABC_a6564c4b215e6c5ad4f53eeb5dd69c_6", "text": "We note that combinations such as \"Ling 50ms\" with \"Acous 10ms\" are not possible when using the \"no subnets\" and \"one subnet\" configurations since the features are being input into the same LSTM and cannot operate at different temporal resolutions."}
{"sent_id": "780b96afa8d417aa241e01ad594ce9-C001-46", "intents": ["@USE@"], "paper_id": "ABC_780b96afa8d417aa241e01ad594ce9_6", "text": "Following Mihalcea and Strapparava (2005) and Yang et al. (2015) , we selected the same numbers (n = 4726) of 'Laughter' and 'NoLaughter' sentences."}
{"sent_id": "780b96afa8d417aa241e01ad594ce9-C001-76", "intents": ["@SIM@"], "paper_id": "ABC_780b96afa8d417aa241e01ad594ce9_6", "text": "The Pun data allows us to verify that our implementation of the conventional model is consistent with the work reported in Yang et al. (2015) ."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-25", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "In the following sections, we first summarize the BERT architecture, then give details of shared task data set, and then describe experimental setups we used to train BERT model."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-63", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "In order to show how BERT performs in news domain, our first attempt was to use the training data to only fine-tune the pretrained model for classification."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-85", "intents": ["@USE@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "Table 2 presents the combined loss of two unsupervised tasks on the held-out data for original BERT-Base and further pretrained model with the generated data."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "However, recent studies (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018) showed that contextual word embeddings perform quite better than traditional word embeddings in many different NLP tasks as a result of their superior capacity of meaning representation."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-22", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "Though it is quite new, BERT has been tried in many different domains than the one proposed in Devlin et al. (2018) ."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-23", "intents": ["@MOT@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "However, almost all of these studies have two things in common: they don't start training BERT from scratch and the target domain contains very limited data (Zhu et al., 2018; Yang et al., 2019; Alberti et al., 2019) ."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-101", "intents": ["@DIF@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "Looking at the results of second and third models on \"by-article-test-set\" shows us, although we fine-tune BERT with supervised data for the same classification task, fine-tuning on \"byarticle-train-set\" improves the results drastically."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-110", "intents": ["@DIF@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "We demonstrated that pretraining BERT in an unseen domain improves the performance of the model on the domain specific supervised task."}
{"sent_id": "b27150a3506730c61dc78b3034887e-C001-113", "intents": ["@EXT@", "@FUT@"], "paper_id": "ABC_b27150a3506730c61dc78b3034887e_6", "text": "From our findings, we believe that domain adaptation is important for the BERT architecture and we would like to investigate the effect of from scratch unsupervised pretraining on the supervised task as future work."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "One recent notable work (Ganea and Hofmann 2017) instead pioneers to rely on pre-trained entity embeddings, learnable context representation and differentiable joint inference stage to learn basic features and their combinations from scratch."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "So it is natural for the model of Ganea and Hofmann (2017) to make type errors when it is trained to fit such entity embeddings."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-180", "intents": ["@BACK@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Ganea and Hofmanns entity embeddings are bootstrapped from word embeddings which mainly capture topic level entity relatedness, while BERT-based context representation is derived from BERT which naturally captures type information."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-28", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "To verify this, we conduct error analysis of the well known DeepED 1 model (Ganea and Hofmann 2017) on the development set of AIDA-CoNLL (Hoffart et al. 2011) , and found that more than half of their error cases fall into the category of type errors where the predicted entity's type is different from the golden entity's type, although some predictive contextual cue for them can be found in their local context."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-144", "intents": ["@MOT@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "The main goal of this work is to introduce a BERT-based entity similarity to capture latent entity type information which is supplementary to existing SOTA local context model (Ganea and Hofmann 2017) ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-45", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "What's more, we integrate a BERT-based entity similarity feature into the local model of Ganea and Hofmann (2017) to better capture entity type information."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-129", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Then we adopt exactly the same global model as Ganea and Hofmann (2017) which is already introduced in the Background section."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-130", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Specifically, we adopt loopy belief propagation (LBP) to estimate the max-marginal probabilityĝ i (e|D) and then combine it with the priorp(e|m i ) using a two-layer neural network to get the final score ρ i (e) for m i ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-148", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "To verify the contribution of our proposed BERT-based entity embeddings, we also compare with a straightforward baseline which directly replaces the encoder of Ganea and Hofmann (2017) utilizing pre-trained BERT."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-211", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Besides, there are 22.95% remaining type errors which are due to global modeling problem which shows the limitation of the global modeling method of Ganea and Hofmann (2017) ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-263", "intents": ["@USE@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Then we integrate a BERT-based entity similarity into the local model of the state-of-the-art method by (Ganea and Hofmann 2017) ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-194", "intents": ["@DIF@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "Table 5 : Performance of two state-of-the-art fine grained entity typing systems on AIDA-CoNLL development set order to verify our claim that the entity embeddings from BERT better capture entity type information than those from Ganea and Hofmann (2017) , we carry out an entity type prediction task based on its entity embedding."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-201", "intents": ["@DIF@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "This demonstrates that our proposed entity embeddings from BERT indeed capture better latent entity type information than Ganea and Hofmann (2017) ."}
{"sent_id": "eb5ef34dd9c3845cd27c33242d5316-C001-176", "intents": ["@EXT@"], "paper_id": "ABC_eb5ef34dd9c3845cd27c33242d5316_6", "text": "The model of Le and Titov (2018) is a multirelational extension of Ganea and Hofmann (2017)'s global modeling method while keeps exactly the same local context model."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-17", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "1 Our contributions are: (a) In §2, we show that the monolingual word embeddings used in Conneau et al. (2018) are not approximately isomorphic, using the VF2 algorithm (Cordella et al., 2001 ) and we therefore introduce a metric for quantifying the similarity of word embeddings, based on Laplacian eigenvalues."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-47", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "We discuss the correlation between unsupervised BDI performance and approximate isospectrality or eigenvector similarity in §4.7."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-55", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "We describe the learning algorithm in §3.2."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-58", "intents": ["@USE@", "@MOT@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "3 We evaluate Conneau et al. (2018) on (English to) Estonian (ET), Finnish (FI), Greek (EL), Hungarian (HU), Polish (PL), and Turkish (TR) in §4.2, to test whether the selection of languages in the original study introduces a bias."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-70", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "We now introduce the method of Conneau et al. (2018) ."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-94", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "We use bilingual dictionaries compiled by Conneau et al. (2018) as gold standard, and adopt their evaluation procedure: each test set in each language consists of 1500 gold translation pairs."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-131", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "In general, unsupervised BDI, using the approach in Conneau et al. (2018) , seems challenged when pairing En-glish with languages that are not isolating and do not have dependent marking."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-146", "intents": ["@USE@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "8 We show the results of unsupervised BDI in Figures 2g-i ."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Conneau et al. (2018) consider a specific set of learning scenarios:"}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-173", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "BDI models are evaluated on a held-out set of query words."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-206", "intents": ["@BACK@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Zhang et al. (2017) , in addition, use different forms of regularization for convergence, while Conneau et al. (2018) uses additional steps to refine the induced embedding space."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-120", "intents": ["@MOT@", "@EXT@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "The approach achieves impressive performance for Spanish, one of the languages Conneau et al. (2018) include in their paper."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-157", "intents": ["@MOT@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "This is of course always practically possible, but we are interested in seeing whether their approach works on pre-trained embeddings induced with possibly very different hyper-parameters."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-193", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "Since we already established that the monolingual word embeddings are far from isomorphic-in contrast with the intuitions motivating previous work (Mikolov et al., 2013b; Barone, 2016; Zhang et al., 2017; Conneau et al., 2018 )-we would like to establish another diagnostic metric that identifies embedding spaces for which the approach in Conneau et al. (2018) is likely to work."}
{"sent_id": "d91913d0c8e669153e8d477e19aef2-C001-79", "intents": ["@SIM@"], "paper_id": "ABC_d91913d0c8e669153e8d477e19aef2_6", "text": "In the experiments in Conneau et al. (2018) , as well as in ours, the iterative Procrustes refinement improves performance across the board."}
{"sent_id": "c3f71bea55f85633568c7ba57f6fd5-C001-4", "intents": ["@EXT@"], "paper_id": "ABC_c3f71bea55f85633568c7ba57f6fd5_6", "text": "In order to promote research in this area, we propose a data generation paradigm adapted from CLEVR [11] ."}
{"sent_id": "c3f71bea55f85633568c7ba57f6fd5-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_c3f71bea55f85633568c7ba57f6fd5_6", "text": "Text-based QA, use text corpora as context ( [19, 20, 17, 9, 10, 16] ); in visual question answering (VQA), instead, the questions are related to a scene depicted in still images (e.g. [11, 2, 25, 7, 1, 23, 8, 10, 16] ."}
{"sent_id": "c3f71bea55f85633568c7ba57f6fd5-C001-97", "intents": ["@USE@"], "paper_id": "ABC_c3f71bea55f85633568c7ba57f6fd5_6", "text": "It is a good candidate as it has been shown to work well on the CLEVR VQA task [11] that shares the same structure of questions as our CLEAR dataset."}
{"sent_id": "b7278824bdae498021b899fbc6c638-C001-88", "intents": ["@DIF@"], "paper_id": "ABC_b7278824bdae498021b899fbc6c638_6", "text": "Table 1 show the results of the comparison: EA-GER significantly improves precision and recall over the baseline system and outperforms (Zhang and Iria, 2009 ) in all cases."}
{"sent_id": "6ed955baf28ad1c7fd6d590e660c20-C001-14", "intents": ["@USE@"], "paper_id": "ABC_6ed955baf28ad1c7fd6d590e660c20_7", "text": "Abstract denotation is proposed to capture the meaning of this abstract version of DCS tree, and a textual inference system based on abstract denotation is built (Tian et al., 2014) ."}
{"sent_id": "6ed955baf28ad1c7fd6d590e660c20-C001-58", "intents": ["@USE@", "@UNSURE@", "@SIM@"], "paper_id": "ABC_6ed955baf28ad1c7fd6d590e660c20_7", "text": "We plot the running time of Tian et al. (2014) 's inference engine (single-threaded) on a 2.27GHz Xeon CPU, with respect to the weighted sum of all statements 2 , as shown in Figure 3 ."}
{"sent_id": "6ed955baf28ad1c7fd6d590e660c20-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_6ed955baf28ad1c7fd6d590e660c20_7", "text": "To elaborate the above discussion and to provide more topics to the literature, in this paper we discuss the following four questions: ( §2) How well can tree transformation approximate logical inference? ( §3) With rigorous inference on DCS trees, where does logic contribute in the system of Tian et al. (2014) In the tree transformation based approach to RTE, it has been realized that some gaps between T and H cannot be filled even by a large number of tree transformation rules extracted from corpus (BarHaim et al., 2007a) ."}
{"sent_id": "6ed955baf28ad1c7fd6d590e660c20-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_6ed955baf28ad1c7fd6d590e660c20_7", "text": "Logical inference is shown to be useful for RTE, as Tian et al. (2014) demonstrates a system with competitive results."}
{"sent_id": "6ed955baf28ad1c7fd6d590e660c20-C001-74", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_6ed955baf28ad1c7fd6d590e660c20_7", "text": "Improvement of similarity score To calculate phrase similarities, Tian et al. (2014) use the cosine similarity of sums of word vectors, which ignores syntactic information."}
{"sent_id": "926e7df3c367ae29da574ba465504f-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_926e7df3c367ae29da574ba465504f_7", "text": "The method presented by Aletras and Stevenson (2013) selects an image from a small set of candidates by re-ranking them using an unsupervised graph-based method."}
{"sent_id": "926e7df3c367ae29da574ba465504f-C001-63", "intents": ["@USE@"], "paper_id": "ABC_926e7df3c367ae29da574ba465504f_7", "text": "We evaluate our model on the publicly available data set provided by Aletras and Stevenson (2013) ."}
{"sent_id": "2eeffe385539b28c7d31eeb176e926-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_2eeffe385539b28c7d31eeb176e926_7", "text": "These approaches train a deep-learning language model on large volumes of unlabeled text, which is subsequently fine-tuned for particular NLP tasks."}
{"sent_id": "2eeffe385539b28c7d31eeb176e926-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_2eeffe385539b28c7d31eeb176e926_7", "text": "As such, their computational cost can thus be prohibitively high at both training and prediction time (Devlin et al., 2019) ."}
{"sent_id": "2eeffe385539b28c7d31eeb176e926-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_2eeffe385539b28c7d31eeb176e926_7", "text": ". EduBERT and EduDistilBERT are fine-tuned on millions of tokens, in contrast to the billions of tokens required to make the most of the architecture potential (Devlin et al., 2019) ."}
{"sent_id": "2eeffe385539b28c7d31eeb176e926-C001-21", "intents": ["@EXT@"], "paper_id": "ABC_2eeffe385539b28c7d31eeb176e926_7", "text": "We apply the BERT approach to the following three previously explored LAK tasks on MOOC forum data (Wei et al., 2017) : Confusion detection, urgency of teacher intervention and sentimentality classification."}
{"sent_id": "c7c9266b5063ec85494fde45d1dce1-C001-20", "intents": ["@USE@"], "paper_id": "ABC_c7c9266b5063ec85494fde45d1dce1_7", "text": "As baselines, we compare with feature spaces comprising of char n-grams [6] , TF-IDF vectors, and Bag of Words vectors (BoWV)."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-24", "intents": ["@USE@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "We build upon previous work and use the dataset from [5] : this way we can investigate hyperpartisan-biased news (i.e., extremely one-sided) that have been manually fact-checked by professional journalists from BuzzFeed."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-77", "intents": ["@USE@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "In order to compare our results with those reported in [5] , we report the same measures the authors used."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-88", "intents": ["@SIM@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "In line with what was already pointed out in [5] , the left-wing orientation is harder to predict, possibly because this class is represented with fewer examples in the dataset."}
{"sent_id": "55bcdca5052745160dc861e22e7401-C001-101", "intents": ["@DIF@"], "paper_id": "ABC_55bcdca5052745160dc861e22e7401_7", "text": "These results confirm that perhaps the performance of our approach overcomes the models proposed in [5] because of the length of the n-grams 7 ."}
{"sent_id": "ec0ae4e56c069e3efb4a2dc12199cd-C001-108", "intents": ["@BACK@"], "paper_id": "ABC_ec0ae4e56c069e3efb4a2dc12199cd_7", "text": "This effect is truly beneficial, especially for real-world annotation projects, due to much lower training times and, by this, shorter annotator idle times (Tomanek et al., 2007a) ."}
{"sent_id": "74e7b114ae968e196ea87f529f5eff-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_74e7b114ae968e196ea87f529f5eff_7", "text": "The former have been shown to outperform the latter in Weeds et al. (2014) , even though Levy et al. (2015) have recently claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x."}
{"sent_id": "74e7b114ae968e196ea87f529f5eff-C001-56", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_74e7b114ae968e196ea87f529f5eff_7", "text": "Our classifier is competitive with the state-of-the-art (Weeds et al., 2014) ."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "Lifelong FewRel is derived from the FewRel (Han et al., 2018) dataset, by partitioning its 80 relations into 10 distinct clusters made up of 8 relations each, with each cluster serving as a task where a sentence must be labeled with the correct relation."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-18", "intents": ["@EXT@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "In this work, we extend the work of Wang et al. (2019) by exploiting ideas from both lifelong learning and meta-learning."}
{"sent_id": "681c3e59adbfc09a28d267a4885598-C001-66", "intents": ["@USE@"], "paper_id": "ABC_681c3e59adbfc09a28d267a4885598_7", "text": "Hyperparameters Apart from the hyperparameters specific to meta-learning (such as the step size ), all other hyperparameters we use for the learner model are the same as used by Wang et al. (2019) ."}
{"sent_id": "6fbfc9f887e736472510bce30c9228-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_6fbfc9f887e736472510bce30c9228_7", "text": "In Ivanovic (2005b) , the MSN Shopping corpus was collected and a gold standard produced by labelling the utterances with dialogue acts."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-67", "intents": ["@USE@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "We review three approaches that are directly relevant to us; we refer to the excellent literature review in (Zitouni et al., 2006) for a general review."}
{"sent_id": "c126f8b9a5fcb2687494a8c0b1e859-C001-81", "intents": ["@BACK@"], "paper_id": "ABC_c126f8b9a5fcb2687494a8c0b1e859_7", "text": "Furthermore, Zitouni et al. (2006) do not use a morphological lexicon."}
{"sent_id": "fd5a6307b398f37d8729c21cfce6c1-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_fd5a6307b398f37d8729c21cfce6c1_7", "text": "These models achieve state-of-the-art results in the literature."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-19", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "In our first experiment, we examine the utility of a standard QA dataset as training data for the slotfilling model of Levy et al. (2017) ."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "The UWRE data is derived from WikiReading (Hewlett et al., 2016) , which is itself derived from WikiData (Vrandečić, 2012) , and consists of a set of positive and negative examples for relation extraction from Wikipedia sentences."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-60", "intents": ["@EXT@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "Thus, each positive example from the original UWRE entity split test set is turned into a negative example by pairing the sentence with an equivalent question about another entity."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-46", "intents": ["@USE@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "Results Table 1 reports the F1 scores for zeroshot relation extraction on the relation split test set, using models trained on the original UWRE and SQuAD datasets."}
{"sent_id": "5428f8c196308c90618abfdbdf856a-C001-73", "intents": ["@USE@"], "paper_id": "ABC_5428f8c196308c90618abfdbdf856a_7", "text": "Looking first at the effect of adding the original UWRE training instances, performance drops dramatically as the size of this expansion increases."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-5", "intents": ["@USE@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "Furthermore, we reproduce multisource projection (Tyers et al., 2018) , in which dependency trees of multiple sources are combined."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-22", "intents": ["@DIF@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "The former differs from the approach of Tyers et al. (2018) , who use multiple discrete, monolingual models to parse the translated sentences, whereas in this work we use a single model trained on multiple source treebanks."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-51", "intents": ["@DIF@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "In contrast to Tyers et al. (2018) , they translate a target sentence and project the source parse tree back to the target during test time instead of using this approach to obtain training data for the target language."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-76", "intents": ["@BACK@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "Consequently, to create parallel source sentences, Tyers et al. (2018) use a rule-based machine translation system available in Apertium 8 to translate from Faroese to Norwegian Bokmål."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-110", "intents": ["@BACK@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "Synthetic Source Treebanks Source translations are tokenized with UDPipe (Straka and Straková, 2017) by Tyers et al. (2018) ."}
{"sent_id": "c684a2be8ca8ed8db25be6e080f921-C001-174", "intents": ["@BACK@"], "paper_id": "ABC_c684a2be8ca8ed8db25be6e080f921_7", "text": "Tyers et al. (2018) report an LAS of 64.43 with a monolingual multi-source approach."}
{"sent_id": "05fe3e9c1598f5b36b6efa79216309-C001-44", "intents": ["@USE@"], "paper_id": "ABC_05fe3e9c1598f5b36b6efa79216309_7", "text": "• Attention mechanism The attention mechanism, adaptively learns soft alignments c t between character dependencies H t and attention inputs a. Eq. 1 formally defines the new character dependencies using attention layer H attention t [1] ."}
{"sent_id": "008d5261ee7385a2b7e39772938f51-C001-40", "intents": ["@USE@"], "paper_id": "ABC_008d5261ee7385a2b7e39772938f51_7", "text": "To compare our work with Athar (2011) , we also applied a three-class annotation scheme."}
{"sent_id": "d06a49ad232f73328874282d91cde0-C001-6", "intents": ["@EXT@"], "paper_id": "ABC_d06a49ad232f73328874282d91cde0_7", "text": "Building on the work reported in Zhang et al. (2007a) , we further propose a new partial parsing model that splits the parsing process into two stages, both of which use the bottom-up chart-based parsing algorithm."}
{"sent_id": "d06a49ad232f73328874282d91cde0-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_d06a49ad232f73328874282d91cde0_7", "text": "Section 2. provides background knowledge about the DELPH-IN HPSG grammars, the semantic and syntactic representations, and the partial parsing model presented in Kasper et al. (1999) and Zhang et al. (2007a) ."}
{"sent_id": "d06a49ad232f73328874282d91cde0-C001-96", "intents": ["@BACK@"], "paper_id": "ABC_d06a49ad232f73328874282d91cde0_7", "text": "One common shortcoming of the partial parsing models proposed in both (Kasper et al., 1999) and (Zhang et al., 2007a) is that the results of partial parsing are sets of disjoint sub-analyses, either in the form of derivation subtrees, or in the form of MRS fragments."}
{"sent_id": "9731b4cea1405b7cbf3792aed5b1e4-C001-121", "intents": ["@BACK@"], "paper_id": "ABC_9731b4cea1405b7cbf3792aed5b1e4_7", "text": "Entity recognition systems are traditionally based on a sequential model, for example a CRF, and involve feature engineering (Lafferty et al., 2001; Ratinov and Roth, 2009 )."}
{"sent_id": "a2b945e18ab6b73b4021a2db8bda4f-C001-65", "intents": ["@USE@"], "paper_id": "ABC_a2b945e18ab6b73b4021a2db8bda4f_7", "text": "We use the approach proposed by Stab and Gurevych (2014b) ."}
{"sent_id": "a2b945e18ab6b73b4021a2db8bda4f-C001-60", "intents": ["@SIM@"], "paper_id": "ABC_a2b945e18ab6b73b4021a2db8bda4f_7", "text": "In addition, we notice that attack relations are sparse, as was the case in Stab and Gurevych (2014b) dataset and thus the coefficients for attack relations features (#10, #11 in Table 1 ) are negligible."}
{"sent_id": "a2b945e18ab6b73b4021a2db8bda4f-C001-76", "intents": ["@DIF@"], "paper_id": "ABC_a2b945e18ab6b73b4021a2db8bda4f_7", "text": "One explanation of having lower performance on the P (premise) category is that the S&G dataset used for training has higher quality essays, while 2/3 of our T OEF L arg dataset consists of medium and low scoring essays (the writing style for providing reasons or example can differ between high and low scoring essays)."}
{"sent_id": "78afdf391c70d7992200b4071e4ac2-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_78afdf391c70d7992200b4071e4ac2_7", "text": "The aliasing process introduced by Bonial et al. (2014) tries to extend the coverage of PB for CPs while keeping the number of rolesets that should be newly created to a minimum."}
{"sent_id": "78afdf391c70d7992200b4071e4ac2-C001-107", "intents": ["@DIF@"], "paper_id": "ABC_78afdf391c70d7992200b4071e4ac2_7", "text": "The simple inter-annotator agreement 5 was 67% for annotator A%B, 51% for A&C and 44% for A&D. These agreement figures are higher than the figures in Bonial et al. (2014) , and actual agreement is probably even higher, because synonymous rolesets are regarded as disagreements."}
{"sent_id": "78afdf391c70d7992200b4071e4ac2-C001-171", "intents": ["@EXT@"], "paper_id": "ABC_78afdf391c70d7992200b4071e4ac2_7", "text": "We set up an annotation effort to gather a frequency-balanced, contextualized evaluation set that is more natural, varied and larger than the pilot annotations provided by Bonial et al. (2014) ."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "b. Product Reviews: 1235 sentences, out of which 12% are annotated as wishes."}
{"sent_id": "950263323d351bcb483be7cdf15a7e-C001-112", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_950263323d351bcb483be7cdf15a7e_8", "text": "Table 2 compares the AUC values obtained with unigrams, subjunctive features, a combination of both, and the results from Goldberg et al. (2009) for wish detection."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "Previous attempts to annotate QA-SRL initially involved trained annotators (He et al., 2015) but later resorted to crowdsourcing (Fitzgerald et al., 2018) to achieve scalability."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-113", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "For each predicate, the parser classifies every span for being an argument, independently of the other spans."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-138", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "As mentioned in the paper body, the Fitzgerald et al. parser generates redundant role questions and answers."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-120", "intents": ["@MOT@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "Based on this inspection, the parser completely misses 23% of the 154 roles present in the gold-data, out of which, 17% are implied."}
{"sent_id": "211b889125682f2596f708be1e83b9-C001-24", "intents": ["@USE@"], "paper_id": "ABC_211b889125682f2596f708be1e83b9_8", "text": "To foster future research, we release an assessed high-quality gold dataset along with our reproducible protocol and evaluation scheme, and report the performance of the existing parser (Fitzgerald et al., 2018) as a baseline."}
{"sent_id": "d72f0608fddd1bf1cdef7ca6a20bdf-C001-24", "intents": ["@USE@"], "paper_id": "ABC_d72f0608fddd1bf1cdef7ca6a20bdf_8", "text": "For the sake of simplicity, we used a CNN acoustic model in our experiment, where the baseline system's performance is directly comparable to the state-of-the-art CNN performance reported in [8] ."}
{"sent_id": "d72f0608fddd1bf1cdef7ca6a20bdf-C001-92", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_d72f0608fddd1bf1cdef7ca6a20bdf_8", "text": "Table 1 and 2 also demonstrates that sequence training always gave additive performance gain over crossentropy training, supporting the in [8, 21] ."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "Problem Formulation: The hredGAN [7] formulates multi-turn dialogue response generation as: given a dialogue history of sequence of utterances,"}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-74", "intents": ["@USE@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "We train both the generator and the discriminator (with a shared encoder) of phredGAN using the same training procedure in Algorithm 1 with λ G = λ M = 1 [7] ."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-79", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "The number of attributes, V c is dataset-dependent Compute the generator output similar to Eq. (11) in [7] ."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-138", "intents": ["@USE@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "Also for fair comparison, we use the same UDC dataset as reported in [7] ."}
{"sent_id": "264bdb348c13f167768fd859b047e8-C001-66", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_264bdb348c13f167768fd859b047e8_8", "text": "Noise Injection: Although [7] demonstrated that injecting noise at the word level seems to perform better than at the utterance level for hredGAN , we found that this is datasetdependent for phredGAN ."}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-6", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "Our method is based on the method described in (Hoshino et al., 2013) , and extends their rules to handle abbreviation and passivization frequently found in scientific papers."}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-82", "intents": ["@USE@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "Also, following (Hoshino et al., 2013) , we did not consider event nouns as predicates."}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-7", "intents": ["@DIF@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "Experimental results show that our proposed method improves performance of both (Hoshino et al., 2013) 's system and our phrase-based SMT baseline without preordering."}
{"sent_id": "668e8967d702d4538c85935de083f7-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_668e8967d702d4538c85935de083f7_8", "text": "Specifically, previous work in the literature uses morphological analysis (Katz-Brown and Collins, 2008) , dependency structure (Katz-Brown and Collins, 2008) and predicate-argument structure (Komachi et al., 2006; Hoshino et al., 2013) for preordering in Japanese-English statistical machine translation."}
{"sent_id": "e9e733d38affa8a39a633ffb4d9d71-C001-178", "intents": ["@MOT@"], "paper_id": "ABC_e9e733d38affa8a39a633ffb4d9d71_8", "text": "Such tools are put to use by some of the existing systems (Krishna et al., 2016; Mittal, 2010 ) to obtain additional morphological or syntactic information about the sentences."}
{"sent_id": "e9e733d38affa8a39a633ffb4d9d71-C001-180", "intents": ["@MOT@"], "paper_id": "ABC_e9e733d38affa8a39a633ffb4d9d71_8", "text": "Scalability of such systems is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing."}
{"sent_id": "e9e733d38affa8a39a633ffb4d9d71-C001-200", "intents": ["@DIF@"], "paper_id": "ABC_e9e733d38affa8a39a633ffb4d9d71_8", "text": "Our model with attention outperforms the current state of the art (Krishna et al., 2016) ."}
{"sent_id": "fdfb8fbdb8544dca17b1aeba768124-C001-2", "intents": ["@USE@"], "paper_id": "ABC_fdfb8fbdb8544dca17b1aeba768124_8", "text": "An empirical comparison of CFG filtering techniques for LTAG and HPSG is presented."}
{"sent_id": "fdfb8fbdb8544dca17b1aeba768124-C001-91", "intents": ["@USE@", "@FUT@"], "paper_id": "ABC_fdfb8fbdb8544dca17b1aeba768124_8", "text": "We are going to integrate the advantage of the CF approximation of HPSG into that of LTAG in order to establish another CFG filtering for LTAG."}
{"sent_id": "3e2fb3d4c1e224c084117c22a5db78-C001-48", "intents": ["@USE@"], "paper_id": "ABC_3e2fb3d4c1e224c084117c22a5db78_8", "text": "Following the methodology described in (Zampieri et al., 2019a) and others, including a recent comparable Danish dataset (Sigurbergsson and Derczynski, 2020) , we collected tweets using keywords such as sensitive or obscene language."}
{"sent_id": "3e2fb3d4c1e224c084117c22a5db78-C001-52", "intents": ["@USE@"], "paper_id": "ABC_3e2fb3d4c1e224c084117c22a5db78_8", "text": "The final query for data collection was for tweets containing \"είσαι\" (eisai, \"you are\") as a keyword, inspired by (Zampieri et al., 2019a) ."}
{"sent_id": "3e2fb3d4c1e224c084117c22a5db78-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_3e2fb3d4c1e224c084117c22a5db78_8", "text": "(Zampieri et al., 2019a) model distinguishes targeted from general profanity, and considers the target of offensive posts as indicators of potential hate speech posts (insults targeted at groups) and cyberbulling posts (insults targeted at individuals)."}
{"sent_id": "91c82c4a49815fb2de300d99312754-C001-83", "intents": ["@USE@"], "paper_id": "ABC_91c82c4a49815fb2de300d99312754_8", "text": "And the baseline models are Seq2Seq+Attn (Luong et al., 2015) , Pointer to Unknown (Ptr-Unk, Gulcehre et al. (2016) ), Mem2Seq (Madotto et al., 2018) , Hierarchical Pointer Generator Memory Network (HyP-MN, Raghu et al. (2018) ) and Global-to-Local Memory Pointer (GLMP, Wu et al. (2019) )."}
{"sent_id": "91c82c4a49815fb2de300d99312754-C001-105", "intents": ["@USE@"], "paper_id": "ABC_91c82c4a49815fb2de300d99312754_8", "text": "We adopt Mem2Seq as the baseline for human evaluation considering its good performance and code release 3 ."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-88", "intents": ["@USE@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "Depression is selected as the most prevalent disorder in the SMHD dataset with a number of studies in the field (Rude et al., 2004; Chung and Pennebaker, 2007; De Choudhury et al., 2013b; Park et al., 2012) ."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-94", "intents": ["@BACK@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "The importance of personal pronouns in distinguishing depressed authors from the control group is supported by multiple studies (Rude et al., 2004; Chung and Pennebaker, 2007; De Choudhury et al., 2013b; Cohan et al., 2018) ."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-101", "intents": ["@BACK@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "While most studies use Twitter data (Coppersmith et al., 2015a (Coppersmith et al., , 2014 Benton et al., 2017; Coppersmith et al., 2015b) , a recent stream turns to Reddit as a richer source of high-volume data (De Choudhury and De, 2014; Shen and Rudzicz, 2017; Gjurković andŠnajder, 2018; Cohan et al., 2018; Sekulić et al., 2018; Zirikly et al., 2019) ."}
{"sent_id": "10f17930192132077f0d4526e7d755-C001-76", "intents": ["@DIF@"], "paper_id": "ABC_10f17930192132077f0d4526e7d755_8", "text": "In contrast to Cohan et al. (2018) , supervised FastText yields worse results than tuned linear models."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-20", "intents": ["@MOT@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "To address this we extend a suitable stateof-the-art technique that is robust to such corpora, i.e. (Barzilay and McKeown, 2001) ."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-70", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "In (Barzilay and McKeown, 2001 ), a paraphrase is reported as long as there is a single good supporting pair of sentences."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-117", "intents": ["@MOT@", "@USE@", "@EXT@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "In the experiment, we find that running BL using their default threshold of 0.95 on the 5,935 parallel bug reports only gives us 18 paraphrases."}
{"sent_id": "2f3e2c81bed66fd020731b2475bb98-C001-78", "intents": ["@USE@"], "paper_id": "ABC_2f3e2c81bed66fd020731b2475bb98_8", "text": "We compute the set of patterns with affixed pattern scores based on (Barzilay and McKeown, 2001 )."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-3", "intents": ["@EXT@", "@MOT@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "The original HyTER metric relied on hand-crafted paraphrase networks which restricted its applicability to new data."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-98", "intents": ["@EXT@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "We have proposed a method for automatic paraphrase lattice creation which makes the HyTER metric applicable to new datasets."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-73", "intents": ["@MOT@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "The benefits of using a metric like HyTER, which focuses on the word level, are much clearer in the sentence-based evaluation (Table 2) ."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-4", "intents": ["@USE@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "We test, for the first time, HyTER with automatically built paraphrase lattices."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-68", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "Overall HyTERA scores are highly correlated with HyTER scores (ρ = 0.766 for Arabic and ρ = 0.756 for Chinese)."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "To evaluate the performance of HyTER, Dreyer and Marcu (2012) examine whether it can approximate the hTER score (Snover et al., 2006) that measures the number of edits required to change a system output into its post-edition."}
{"sent_id": "45ba2841e91a2fd62f0534aeaf7491-C001-94", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_45ba2841e91a2fd62f0534aeaf7491_8", "text": "While in the (Dreyer and Marcu, 2012) evaluation, the systems came from the 2010 Open MT NIST evaluation and were selected to cover a variety of architectures and performances, the systems that participated in WMT15 are, for the large part, neural MT systems (Bojar et al., 2015) ."}
{"sent_id": "c6bae8dbdb66092865945e776148e6-C001-84", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_c6bae8dbdb66092865945e776148e6_8", "text": "We also compare to the recent biLSTM-Max Encoder of Conneau et al. (2017) , which served as our model's 1-layer starting point."}
{"sent_id": "c6bae8dbdb66092865945e776148e6-C001-39", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_c6bae8dbdb66092865945e776148e6_8", "text": "Then, assuming we have m layers of biLSTM, the final vector representation will be obtained by applying row-max-pool over the output of the last biLSTM layer, similar to Conneau et al. (2017) ."}
{"sent_id": "c6bae8dbdb66092865945e776148e6-C001-69", "intents": ["@DIF@"], "paper_id": "ABC_c6bae8dbdb66092865945e776148e6_8", "text": "As shown, each added layer model improves the accuracy and we achieve a substantial improvement in accuracy (around 2%) on both matched and mismatched settings, compared to the single-layer biLSTM in Conneau et al. (2017) ."}
{"sent_id": "c6bae8dbdb66092865945e776148e6-C001-42", "intents": ["@SIM@", "@EXT@"], "paper_id": "ABC_c6bae8dbdb66092865945e776148e6_8", "text": "The closest encoder architecture to ours is that of Conneau et al. (2017) , whose model consists of a single-layer biLSTM with a max-pooling layer, which we treat as our starting point."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "Siahbani et al. (2013) propose an augmented version of LR decoding to address some limitations in the original LR-Hiero algorithm in terms of translation quality and time efficiency."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-19", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "We introduce two improvements to LR decoding of GNF grammars: (1) We add queue diversity to the cube pruning algorithm for LR-Hiero, and (2) We extend the LR-Hiero decoder to capture all the hierarchical phrasal alignments that are reachable in CKY-Hiero (restricted to using GNF grammars)."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-66", "intents": ["@USE@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "d=1 in standard cube pruning for LR-Hiero (Siahbani et al., 2013) ."}
{"sent_id": "1542325bbf9bed87c22d34d12ee40e-C001-97", "intents": ["@USE@", "@DIF@", "@MOT@"], "paper_id": "ABC_1542325bbf9bed87c22d34d12ee40e_8", "text": "Row 3 is from (Siahbani et al., 2013) 5 . As we discussed in Section 2, LR-Hiero+CP suffers from severe search errors on Zh-En (1.5 BLEU) but using queue diversity (QD=15) we fill this gap."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-71", "intents": ["@BACK@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "We refer the reader to Naseem et al. (2012) for detailed information on the different treebanks."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-207", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "In this study, we combine the arc sets of two base parsers: first, the arc-marginal ambiguity set of the base parser ( §5.2); and second, the Viterbi arc set from the NBG parser of Naseem et al. (2012) in Table 2 ."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-24", "intents": ["@DIF@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "The resulting parser provides significant improvements over a strong baseline parser and achieves a 13% relative error reduction on average with respect to the best model of Naseem et al. (2012) , outperforming it on 15 out of the 16 evaluated languages."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-78", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "3 This model is trained without target language data, but we investigate the use of such data in §5.4."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-84", "intents": ["@DIF@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "However, for languages 3 Model \"D-,To\" in Table 2 from Naseem et al. (2012) ."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-234", "intents": ["@DIF@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "While the best generative transfer model of Naseem et al. (2012) approaches its upper-bounding supervised accuracy (60.4% vs. 67.1%), our relaxed selftraining model is still far below its supervised counterpart (64.0% vs. 84.1%)."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-49", "intents": ["@USE@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "Inspired by the superiority of discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of Naseem et al. (2012) on selective parameter sharing can be incorporated into such models in the transfer scenario."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-113", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "Like Naseem et al. (2012) , we instead share parameters more selectively."}
{"sent_id": "52c52f6ce3663de49d5784630af1e7-C001-96", "intents": ["@SIM@"], "paper_id": "ABC_52c52f6ce3663de49d5784630af1e7_8", "text": "These features only model selectional preferences and dependency length, analogously to the selection component of NBG."}
{"sent_id": "87af486eb2e968d2055eeab094b3f9-C001-15", "intents": ["@MOT@"], "paper_id": "ABC_87af486eb2e968d2055eeab094b3f9_8", "text": "The work showed promising results for further improving general sentence representations by grounding them visually."}
{"sent_id": "87af486eb2e968d2055eeab094b3f9-C001-16", "intents": ["@MOT@"], "paper_id": "ABC_87af486eb2e968d2055eeab094b3f9_8", "text": "However, according to the model, visual association only occurs at the final hidden state of the encoder, potentially limiting the effect of visual grounding."}
{"sent_id": "87af486eb2e968d2055eeab094b3f9-C001-83", "intents": ["@USE@"], "paper_id": "ABC_87af486eb2e968d2055eeab094b3f9_8", "text": "We evaluate sentence representation quality using SentEval 2 [8, 10] scripts."}
{"sent_id": "dcf84cf05e3e7950cabbdd8d8f304c-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_dcf84cf05e3e7950cabbdd8d8f304c_8", "text": "One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi et al., 2015) ."}
{"sent_id": "dcf84cf05e3e7950cabbdd8d8f304c-C001-72", "intents": ["@USE@"], "paper_id": "ABC_dcf84cf05e3e7950cabbdd8d8f304c_8", "text": "The binary compositionality judgements are converted to continuous values as in Salehi et al. (2015) by dividing the number of judgements that an expression is compositional by the total number of judgements."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-71", "intents": ["@BACK@", "@MOT@", "@EXT@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "In (Guo and Diab, 2012b) we use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003) ."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-94", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "Table 1 shows WTMF is already a very strong baseline: it outperforms LSA and LDA by a large margin."}
{"sent_id": "048944feaff977c8cf057d52594c72-C001-45", "intents": ["@USE@"], "paper_id": "ABC_048944feaff977c8cf057d52594c72_8", "text": "The graphical model of WTMF is illustrated in Figure 2a ."}
{"sent_id": "0f5c87e5434785a612c6578244543d-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_0f5c87e5434785a612c6578244543d_8", "text": "Faruqui and Dyer (2014) use canonical correlation analysis to project the embeddings in both languages to a shared vector space."}
{"sent_id": "0f5c87e5434785a612c6578244543d-C001-46", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_0f5c87e5434785a612c6578244543d_8", "text": "where C m denotes the centering matrix This equivalence reveals that the method proposed by Faruqui and Dyer (2014) is closely related to our framework."}
{"sent_id": "0f5c87e5434785a612c6578244543d-C001-87", "intents": ["@DIF@"], "paper_id": "ABC_0f5c87e5434785a612c6578244543d_8", "text": "Moreover, it does not suffer from any considerable degradation in monolingual quality, with an anecdotal drop of only 0.07% in contrast with 2.86% for Mikolov et al. (2013b) and 7.02% for Faruqui and Dyer (2014) ."}
{"sent_id": "0f5c87e5434785a612c6578244543d-C001-91", "intents": ["@DIF@"], "paper_id": "ABC_0f5c87e5434785a612c6578244543d_8", "text": "However, our model performs considerably better than any configuration from Faruqui and Dyer (2014) in both the monolingual and the bilingual task, supporting our hypothesis that these two constraints that are implicit in their method are not only conceptually confusing, 2292 but also have a negative impact."}
{"sent_id": "520437f612e678dcd4ec9c043cf701-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_520437f612e678dcd4ec9c043cf701_8", "text": "For instance, the work of Ott et al. (2011) in the hotel review domain results in very insightful observations that deceptive reviewers tend to use verbs and personal pronouns (e.g., \"I\", \"my\") more often, while truthful reviewers tend to use more of nouns, adjectives, prepositions."}
{"sent_id": "520437f612e678dcd4ec9c043cf701-C001-38", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_520437f612e678dcd4ec9c043cf701_8", "text": "Note that Ott et al. (2011) found that even though POS tags are effective in detecting fake product reviews, they are not as effective as words."}
{"sent_id": "520437f612e678dcd4ec9c043cf701-C001-15", "intents": ["@USE@"], "paper_id": "ABC_520437f612e678dcd4ec9c043cf701_8", "text": "Our results improve the best published result on the hotel review data of Ott et al. (2011) reaching 91.2% accuracy with 14% error reduction."}
{"sent_id": "425148e63eb84bba50326e362cc5b8-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_425148e63eb84bba50326e362cc5b8_9", "text": "Many sources give methodological guidance on how to apply LDA topic modeling in software engineering [1, 3, 19] ."}
{"sent_id": "425148e63eb84bba50326e362cc5b8-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_425148e63eb84bba50326e362cc5b8_9", "text": "Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution [1] ."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-2", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Transformer (Vaswani et al., 2017 ) is a relative new architecture which outperforms traditional deep learning models such as Recurrent Neural Networks (RNNs) (Sutskever et al., 2014) and Temporal Convolutional Networks (TCNs) (Bai et al., 2018) for sequence modeling tasks across neural machine translations (Vaswani et al., 2017) , language understanding (Devlin et al., 2018) , sequence prediction (Dai et al., 2019) , image generation (Child et al., 2019) , video activity classification (Wang et al., 2018) , music generation (Huang et al., 2018a) , and multimodal sentiment analysis (Tsai et al., 2019a) ."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "As a result, Transformer (Vaswani et al., 2017) introduced positional embedding to indicate the positional relation for the inputs."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Given the introduced notation, the attention mechanism in original Transformer (Vaswani et al., 2017) can be presented as:"}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-105", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "with k fq t q , t k being an asymmetric kernel with coefficients inferred by f q : log k fq t q , t k = ∑ (iii) Relative Positional Embedding of Shaw et al. (2018) and Music Transformer (Huang et al., 2018b) : t ⋅ represents the indicator of the position in the sequence, and the kernel is modified to be indexed by a look-up table:"}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-124", "intents": ["@BACK@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "(iii) Decoder Self-Attention in original Transformer (Vaswani et al., 2017) : For each query x q in the decoded sequence, M (x q , S x k ) returns a subset of S x k (M (x q , S x k ) ⊂ S x k )."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-6", "intents": ["@MOT@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "This new formulation gives us a better way to understand individual components of the Transformer's attention, such as the better way to integrate the positional embedding."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-24", "intents": ["@MOT@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Therefore, in the paper, we present a new formulation for Transformer's attention via the lens of kernel."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-245", "intents": ["@MOT@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Alternatively, our work focuses on presenting a new formulation of Transformer's attention mechanism that gains us the possibility for understanding the attention mechanism better."}
{"sent_id": "04f6b9d4296dee4bbf965f9911bf98-C001-161", "intents": ["@SIM@"], "paper_id": "ABC_04f6b9d4296dee4bbf965f9911bf98_9", "text": "Similar to prior work (Vaswani et al., 2017; Dai et al., 2019) , we report BLEU score for NMT and perplexity for SP."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "Ninomiya et al. (2006) showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-132", "intents": ["@BACK@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "Their parser ran around 6 times faster than Ninomiya et al. (2006) 's model 3, 9 times faster than 'our model 1' and 60 times faster than 'our model 2.' Instead, our models achieved better accuracy."}
{"sent_id": "c14d918f3b1b1248dc1d25a7e0b2e4-C001-115", "intents": ["@DIF@"], "paper_id": "ABC_c14d918f3b1b1248dc1d25a7e0b2e4_9", "text": "We must admit that the difference between our models and Ninomiya et al. (2006) 's model 3 was not as great as the difference from 's model, but 'our model 1' achieved 0.56 points higher F-score, and 'our model 2' achieved 0.8 points higher F-score."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "The basic assumption of such combination is that DS method performs better on IV words and Zhang derives this belief from the fact that DS achieves higher IV recall rate as Table 1 shows."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-120", "intents": ["@MOT@", "@USE@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "We repeat the experiments about CM in Zhang\"s paper (Zhang et al., 2006a) and show that there is a representation flaw in the CM formula."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-171", "intents": ["@MOT@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "Furthermore, our experiments show that confidence measure in Zhang\"s paper has a representation flaw and we propose an EIV tag method to revise the combination."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-90", "intents": ["@USE@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "Here, we repeat two experiments described in (Zhang et al., 2006a) , namely dictionary-based approach and subword-based tagging."}
{"sent_id": "0c8a99cac11953f26308128bfc058b-C001-162", "intents": ["@DIF@"], "paper_id": "ABC_0c8a99cac11953f26308128bfc058b_9", "text": "Even on IV word, this pure CT approach outperforms Zhang\"s CT method and produces comparable results with combination with EIV tags, which shows that pure CT method can perform well on IV words too."}
{"sent_id": "64d11e9efeaa735f74585d6998bab7-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_64d11e9efeaa735f74585d6998bab7_9", "text": "In our first experiment, we examine the utility of a QA dataset in relation to the slot-filling task of Levy et al. (2017) ."}
{"sent_id": "64d11e9efeaa735f74585d6998bab7-C001-41", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_64d11e9efeaa735f74585d6998bab7_9", "text": "Models We employ the same modified BiDAF (Seo et al., 2016) model as Levy et al. (2017) , which uses an additional bias term to allow the model to signal when no answer is predicted within the text."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "ABS proposed in Rush et al. (2015) has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007) ."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-107", "intents": ["@BACK@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "The first phase learns the parameters of the ABS."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-16", "intents": ["@EXT@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "Our proposed method encodes results obtained from an AMR parser by using a modified version of Tree-LSTM encoder (Tai et al., 2015) as additional information of the baseline ABS model."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-98", "intents": ["@USE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the Gigaword corpus as our additional test data."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-99", "intents": ["@USE@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in Rush et al. (2015) as \"ABS\", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as \"ABS+AMR\"."}
{"sent_id": "eb51af7d0487fc0795616aecfae9fb-C001-129", "intents": ["@DIF@"], "paper_id": "ABC_eb51af7d0487fc0795616aecfae9fb_9", "text": "We found the statistical difference between ABS(re-run) and ABS+AMR on ROUGE-1 and ROUGE-2."}
{"sent_id": "0a93feafef3ba2d4bb5360ff215171-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_0a93feafef3ba2d4bb5360ff215171_9", "text": "Being simple to compute and interpret, this metric (hence, VQA3+) is the standard evaluation criterion for open-ended VQA [4, 1, 16, 47, 14] ."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Thus, only speaker information that is directly related to the dialog, such as turn-taking (Liu et al., 2017) , is typically considered."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-101", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Still, Khanpour et al. (2016) reported 73.9% accuracy on the validation set and 80.1% on the test set, while Liu et al. (2017) reported 74.5% and 76.9% accuracy on the two sets used to evaluate their experiments."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-159", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "As stated in Section 3, of the two state-of-the-art approaches on dialog act recognition, one uses a RNN-based approach (Khanpour et al., 2016) for segment representation, while the other uses one based on CNNs (Liu et al., 2017) ."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-184", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "To achieve the results presented in their paper, Liu et al. (2017) used three CNNs with 100 filters and 1, 2, and 3 as context window sizes."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-379", "intents": ["@BACK@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "However, information concerning the speakers and, more specifically, turn-taking has also been proved important (Liu et al., 2017) ."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-121", "intents": ["@SIM@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "The resulting word embeddings are 200-dimensional as in the study by Liu et al. (2017) ."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-421", "intents": ["@SIM@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "Starting with the reproduction of the flat label sequence approach, in Table 9 we can see that the results follow the same pattern as in our previous study and that by Liu et al. (2017) ."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-268", "intents": ["@MOT@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "In their study, Liu et al. (2017) used pre-trained embeddings but let them adapt to the task during the training phase."}
{"sent_id": "1ab7893c2a930bc5af3c34a5912dd2-C001-416", "intents": ["@MOT@"], "paper_id": "ABC_1ab7893c2a930bc5af3c34a5912dd2_9", "text": "In fact, this has been confirmed in the study by Liu et al. (2017) ."}
{"sent_id": "e803782890224294066ce447671981-C001-131", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "We extend the pattern matching approach of Johnson (2002) with machine learning techniques, and use dependency structures instead of constituency trees."}
{"sent_id": "e803782890224294066ce447671981-C001-90", "intents": ["@SIM@"], "paper_id": "ABC_e803782890224294066ce447671981_9", "text": "This corresponds to the row \"Overall\" of Table 4 in (Johnson, 2002) , repeated here in Table 4 ."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-4", "intents": ["@BACK@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "The recent advances in image captioning as well as the release of large-scale movie description datasets such as MPII-MD [28] allow to study this task in more depth."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-50", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "To extract labels from sentences we rely on the semantic parser of [28] , however we treat the labels differently to handle the weak supervision (see Section 3.1)."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-67", "intents": ["@DIF@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "However, in contrast to [28] we do not want to keep all of these initial labels as they are noisy, but select only visual ones which actually can be robustly recognized."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-244", "intents": ["@DIF@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "We find that our method is best for all topics except \"communication\", where [28] wins."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-66", "intents": ["@USE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "As in [28] we parse the sentences to obtain a set of labels (single words or short phrases, e.g. look up) to train our visual classifiers."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-115", "intents": ["@USE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "We build on the labels discovered by our semantic parser [28] and additionally match these labels to sentences which the parser failed to process."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-195", "intents": ["@USE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "In the following we evaluate all three methods on the MPII-MD test set."}
{"sent_id": "5fa570cf5f37c7aae3b428a17de3e3-C001-268", "intents": ["@USE@"], "paper_id": "ABC_5fa570cf5f37c7aae3b428a17de3e3_9", "text": "As the result we obtain the highest performance on the MPII-MD dataset as shown by all automatic evaluation measures and extensive human evaluation."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-2", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "We follow the step-by-step approach to neural data-to-text generation we proposed in Moryossef et al. (2019) , in which the generation process is divided into a text-planning stage followed by a plan-realization stage."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "In recent work (Moryossef et al., 2019) we proposed to adopt ideas from \"traditional\" language generation approaches (i.e. Reiter and Dale (2000) ; Walker et al. (2007) ; Gatt and Krahmer (2017) ) that separate the generation into a planning stage that determines the order and structure of the expressed facts, and a realization stage that maps the plan to natural language text."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-92", "intents": ["@BACK@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "Moryossef et al. (2019) suggests the possibility of handling this with a post-processing referring-expression generation step (REG)."}
{"sent_id": "5d68c07f716cd3c9861921d7e515ea-C001-157", "intents": ["@DIF@"], "paper_id": "ABC_5d68c07f716cd3c9861921d7e515ea_9", "text": "We conclude that these extensions not only improve the system of Moryossef et al. (2019) but also highlight the flexibility and advantages of the step-by-step framework for text generation."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-14", "intents": ["@MOT@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "Currently, most of the state-of-the-art VQA models (Antol et al. 2015; Chen et al. 2016; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) only focus on how to improve accuracy."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-37", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "Then, we verify the above claim by the detailed experiments and use the VQABQ method to analyze the robustness of 6 available pretrained stateof-the-art VQA models, provided by papers' authors, (Antol et al. 2015; Lu et al. 2016; Ben-younes et al. 2017; Fukui et al. 2016; Kim et al. 2017) ."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-103", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "That is to say, our main question candidates and the number of main question candidates are exactly the same as the testing question set of VQA dataset (Antol et al. 2015 )."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-133", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "Finally, we find the ranked BQ of all 244302 testing questions from the testing question set of VQA dataset (Antol et al. 2015) and collect them together, with the format {Image, M Q, 21 (BQ + corresponding similarity score)}, as our Basic Question Dataset (BQD)."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-138", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "(Antol et al. 2015) , and the corresponding similarity scores of BQ are generated by our basic question generation method, referring to Section 3. Note that we preprocess the training and validation question datasets from the VQA dataset (Antol et al. 2015) , so the total number of basic question candidates is less than the total number of training and validation question datasets in VQA dataset (Antol et al. 2015) ."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-166", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "\"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and VQA dataset and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and VQA dataset."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-201", "intents": ["@USE@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "Note that Figure 2 : The accuracy of state-of-the-art VQA models evaluated on BQD and VQA dataset (Antol et al. 2015) ."}
{"sent_id": "c8cf2d615cc47395a55bc8737cd9fd-C001-122", "intents": ["@BACK@"], "paper_id": "ABC_c8cf2d615cc47395a55bc8737cd9fd_9", "text": "Our idea is the BQ generation for MQ and, at the same time, we only want the minimum number of BQ to represent the MQ, so modeling our problem as LASSO optimization problem is an appropriate way: (Antol et al. 2015) . \"-\" indicates the results are not available, \"-std\" means the accuracy of VQA model evaluated on the complete testing set of BQD and VQA dataset and \"-dev\" means the accuracy of VQA model evaluated on the partial testing set of BQD and VQA dataset."}
{"sent_id": "40d73d5fc22686c13a14946946dd18-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_40d73d5fc22686c13a14946946dd18_9", "text": "Despite the foreseen importance, character-level embeddings have not been used in previous work (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018) ."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "This model is denoted as Back-translated Style Transfer (BST) in the future."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-21", "intents": ["@EXT@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "We introduce two approaches which extend the back-translation models proposed by Prabhumoye et al. (2018) exploring back-translation setups that preserve the content of the sentence better."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-100", "intents": ["@UNSURE@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "The results in (Prabhumoye et al., 2018) were reproduced for comparing the CAE model with the BST model."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-102", "intents": ["@UNSURE@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "We present the results for the comparison between BST and MBST models; and the MBST and the MBST+F models."}
{"sent_id": "28eeecadd8d3348de6daec3c801ae4-C001-88", "intents": ["@USE@"], "paper_id": "ABC_28eeecadd8d3348de6daec3c801ae4_9", "text": "We have reproduced the classifiers described in (Prabhumoye et al., 2018) ."}
{"sent_id": "b71da01fb46900d81162b3a3c3cd41-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_b71da01fb46900d81162b3a3c3cd41_9", "text": "Using this technique, the system of Nivre (2009) processes unrestricted non-projective structures with state-ofthe-art accuracy in observed linear time."}
{"sent_id": "b71da01fb46900d81162b3a3c3cd41-C001-34", "intents": ["@UNSURE@"], "paper_id": "ABC_b71da01fb46900d81162b3a3c3cd41_9", "text": "Figure 2 defines the original training oracle τ 1 proposed by Nivre (2009) ."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "On the other hand, studies in (Neubig et al., 2012; Lerner and Petrov, 2013; Hoshino et al., 2015; Nakagawa, 2015) apply machine learning to the preordering problem."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-108", "intents": ["@SIM@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "These scores were comparable (statistically insignificant at p < 0.05) between RvNN and BTG, 12 indicating that the proposed method achieves a translation quality comparable to BTG."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-106", "intents": ["@UNSURE@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "The distortion limit of SMT systems trained using preordered sentences by RvNN and BTG was set to 0, while that without preordering was set to 6."}
{"sent_id": "9ee702243b3976ee4261f433d75528-C001-109", "intents": ["@UNSURE@"], "paper_id": "ABC_9ee702243b3976ee4261f433d75528_9", "text": "In contrast to the case of PB-SMT, NMT without preordering achieved a significantly higher BLEU score than NMT models with preordering by RvNN and BTG."}
{"sent_id": "27aeffca1f7a9a6b40743284a2871d-C001-6", "intents": ["@BACK@"], "paper_id": "ABC_27aeffca1f7a9a6b40743284a2871d_9", "text": "The adaptor grammar framework is a nonparametric extension of probabilistic context-free grammars (Johnson et al., 2007) , which was initially intended to allow fast prototyping of models of unsupervised language acquisition (Johnson, 2008), but it has been shown to have applications in text data mining and information retrieval as well Hardisty et al., 2010) ."}
{"sent_id": "9b203bfa690c4a79c1324360a4b8dc-C001-63", "intents": ["@USE@"], "paper_id": "ABC_9b203bfa690c4a79c1324360a4b8dc_11", "text": "We perform the analysis of three datasets to investigate the extent of overlap present in these publicly available benchmark datasets and find out that 45.7% word pairs of dataset prepared by Weeds et al. (2014) are present in dataset ROOT9 prepared by Santus et al. (2016) ."}
{"sent_id": "3128481fa4e5d2c4af7deba2c28950-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_3128481fa4e5d2c4af7deba2c28950_11", "text": "However, Hearst (1994 Hearst ( , 1997 and Foltz et al. (1998) differ on how text units are defined and on how to interpret the results of a comparison."}
{"sent_id": "1c1b524d2bfe00c62a5a2e1a05ffc7-C001-71", "intents": ["@BACK@"], "paper_id": "ABC_1c1b524d2bfe00c62a5a2e1a05ffc7_11", "text": "Vaswani et al. (2017) state three reasons for the preference: (a) computational complexity of each layer, (b) concurrency, and (c) path length between long-range dependencies."}
{"sent_id": "1c1b524d2bfe00c62a5a2e1a05ffc7-C001-81", "intents": ["@USE@"], "paper_id": "ABC_1c1b524d2bfe00c62a5a2e1a05ffc7_11", "text": "In Equations (3) and (4), we described the attention layer proposed in Vaswani et al. (2017) comprising the multi-head attention sub-layer and a FFN sub-layer."}
{"sent_id": "1c1b524d2bfe00c62a5a2e1a05ffc7-C001-151", "intents": ["@USE@"], "paper_id": "ABC_1c1b524d2bfe00c62a5a2e1a05ffc7_11", "text": "Transformer (large) (Vaswani et al., 2017) 28.4 41.0 Weighted Transformer (large) 28.9 41.4"}
{"sent_id": "1c1b524d2bfe00c62a5a2e1a05ffc7-C001-128", "intents": ["@DIF@"], "paper_id": "ABC_1c1b524d2bfe00c62a5a2e1a05ffc7_11", "text": "Further, we do not use any averaging strategies employed in Vaswani et al. (2017) and simply return the final model for testing purposes."}
{"sent_id": "79e96060492c3978dc5a7a0d5f293f-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_79e96060492c3978dc5a7a0d5f293f_11", "text": "Glorot et al. (2011) investigate a deep learning approach to domain adaptation and report increased accuracy across domains both on the Blitzer et al. (2007) 4-domain data set and the larger Amazon review data set (25 domains) also made available in that release."}
{"sent_id": "79e96060492c3978dc5a7a0d5f293f-C001-162", "intents": ["@USE@"], "paper_id": "ABC_79e96060492c3978dc5a7a0d5f293f_11", "text": "We compare the results of the all-in-one and the ensemble classifier to the SCL and its variation SCL-MI adaptation techniques using the four datasets used to evaluate SCL and SCL-MI in Blitzer et al. (2007) ."}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "Learning such embeddings using powerful neural networks has led to significant advancements in image-caption retrieval and generation Kiros et al. (2014) ; Karpathy & Fei-Fei (2015) , video-to-text alignment Zhu et al. (2015) , and question-answering Malinowski et al. (2015) ."}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-72", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "The rank loss used in Kiros et al. (2014) , Socher et al. (2014) , and Karpathy & Fei-Fei (2015) is defined as follows:"}
{"sent_id": "9aa9fa6b94aa24939b50effa0e575b-C001-101", "intents": ["@SIM@"], "paper_id": "ABC_9aa9fa6b94aa24939b50effa0e575b_11", "text": "Note that since we use SGD to learn our parameters, we take the max over each mini-batch (this is similar to Kiros et al. (2014) , while empty circles are negative samples for the query i. The dashed circles on the two sides are drawn at the same radii."}
{"sent_id": "4a90cd18be0df0c41a94febe2f68ef-C001-11", "intents": ["@USE@"], "paper_id": "ABC_4a90cd18be0df0c41a94febe2f68ef_11", "text": "Specifically, we focus on a new reading comprehension dataset called DROP (Dua et al., 2019) , which requires Discrete Reasoning Over the content of Paragraphs to obtain the final answer."}
{"sent_id": "4a90cd18be0df0c41a94febe2f68ef-C001-70", "intents": ["@USE@"], "paper_id": "ABC_4a90cd18be0df0c41a94febe2f68ef_11", "text": "Following Dua et al. (2019) , we design a multi-type answer predictor to selectively produce different kinds of answers such as span, count number, and arithmetic expression."}
{"sent_id": "fa3d20d5975ec59454abfca68f8935-C001-13", "intents": ["@MOT@"], "paper_id": "ABC_fa3d20d5975ec59454abfca68f8935_11", "text": "Even with recent progress (Gehrmann et al., 2018; Chen and Bansal, 2018) , there is still some work to be done in the field."}
{"sent_id": "fa3d20d5975ec59454abfca68f8935-C001-70", "intents": ["@USE@"], "paper_id": "ABC_fa3d20d5975ec59454abfca68f8935_11", "text": "Hence it is customary to create one from the abstractive ground-truth summaries (Chen and Bansal, 2018; Nallapati et al., 2017) ."}
{"sent_id": "fa3d20d5975ec59454abfca68f8935-C001-84", "intents": ["@SIM@"], "paper_id": "ABC_fa3d20d5975ec59454abfca68f8935_11", "text": "Different from Nallapati et al. (2017) 's approach to greedily add sentences to the summary that maximizes the ROUGE score, our approach is more similar to Chen and Bansal (2018)'s model that calculates the individual reference sentence-level score as per its similarity with each sentence in the corresponding document."}
{"sent_id": "81bdddc7d6b04c88407537f57c0580-C001-64", "intents": ["@USE@"], "paper_id": "ABC_81bdddc7d6b04c88407537f57c0580_11", "text": "We use the WIKIQA dataset (Yang et al., 2015) for evaluation."}
{"sent_id": "81bdddc7d6b04c88407537f57c0580-C001-108", "intents": ["@USE@"], "paper_id": "ABC_81bdddc7d6b04c88407537f57c0580_11", "text": "We further detach the QA matching component QAM in a similar way: We directly use the matching score between a question and a candidate answer obtained by Yang et al. (2015) , and concatenate it with Cnt features as input to the Softmax layer, which is our framework without ENC or QAM, denoted as -ENC -QAM, and trained by our group-level objective."}
{"sent_id": "81bdddc7d6b04c88407537f57c0580-C001-15", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_81bdddc7d6b04c88407537f57c0580_11", "text": "Previous work (Yang et al., 2015; Jurczyk et al., 2016) attack the problem via a pipeline approach: First solve P 1 as a ranking task and then solve P 2 by choosing an optimal threshold upon the previous step's highest ranking score."}
{"sent_id": "81bdddc7d6b04c88407537f57c0580-C001-92", "intents": ["@DIF@"], "paper_id": "ABC_81bdddc7d6b04c88407537f57c0580_11", "text": "Through the comparison between Naive and GAT, Model Prec Rec F1 (Yang et al., 2015) 27.96 37.86 32.17 (Jurczyk et al., 2016) we can see that our proposed objective function has a great advantage over the Naive one which does not model the complexity of answer triggering for positive candidate sets."}
{"sent_id": "81bdddc7d6b04c88407537f57c0580-C001-101", "intents": ["@DIF@"], "paper_id": "ABC_81bdddc7d6b04c88407537f57c0580_11", "text": "This directly shows an end-to-end learning strategy works better than the pipeline approach in (Yang et al., 2015) ."}
{"sent_id": "69857bcd5ba67cb7ca0b4344a3a85f-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_69857bcd5ba67cb7ca0b4344a3a85f_11", "text": "In contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical (Lapata, 2003) and global levels (Barzilay and Lee, 2004) , while preserving regularities recognized by classic discourse theories (Barzilay and Lapata, 2005) ."}
{"sent_id": "69857bcd5ba67cb7ca0b4344a3a85f-C001-165", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_69857bcd5ba67cb7ca0b4344a3a85f_11", "text": "At the other end of the spectrum, the exhaustive search of Barzilay and Lee (2004) , while ensuring optimal solutions, is prohibitively expensive, and cannot be used to perform utility-based training."}
{"sent_id": "69857bcd5ba67cb7ca0b4344a3a85f-C001-123", "intents": ["@USE@"], "paper_id": "ABC_69857bcd5ba67cb7ca0b4344a3a85f_11", "text": "We evaluated the performance of several search algorithms across four stochastic models of document coherence: the IBM £ and IBM £ coherence models, the content model of Barzilay and Lee (2004) (CM) , and the entity-based model of Barzilay and Lapata (2005) (EB) (Section 2)."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-57", "intents": ["@USE@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "Prior to each experiment with Malt, we used MaltOptimizer to obtain settings and a feature model; for MST we exploited default configuration; for the Bohnet and Nivre (2012) parser we set the beam parameter to 80 and otherwise employed the default setup."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-104", "intents": ["@USE@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "VBP (verb, non-3rd person singular present), VBZ (verb, 3rd person singular present) and VBG (verb, gerund or present participle) are the PTB tags that have error rates in 10 highest error rates list for each parser (Malt, MST and the Bohnet and Nivre (2012) parser) with each dependency format (SB, CD and DT) and with each PoS tag set (PTB PoS and supertags) when PTB tags are included as CPOSTAG feature."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-56", "intents": ["@DIF@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "For Malt and MST we perform the experiments on gold PoS tags, whereas the Bohnet and Nivre (2012) parser predicts PoS tags during testing."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-125", "intents": ["@DIF@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "From the parser perspective, the Bohnet and Nivre (2012) parser performs better than Malt and MST not only on conventional formats but also on the new representation, although this parser solves a harder task than Malt and MST."}
{"sent_id": "5d3c08596677a1f8ac48fa17766bb4-C001-75", "intents": ["@EXT@"], "paper_id": "ABC_5d3c08596677a1f8ac48fa17766bb4_11", "text": "The addition of gold PTB tags as a feature lifts the performance of the Bohnet and Nivre (2012) parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2 , Predicted supertags + gold PTB, and Table 1 , Gold supertags)."}
{"sent_id": "547551e556d8aa919f731da99424c9-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_547551e556d8aa919f731da99424c9_11", "text": "More recently, Conneau et al. (2017) show that a completely supervised approach to learning sentence representations from natural language inference data outperforms all previous approaches on transfer learning benchmarks."}
{"sent_id": "547551e556d8aa919f731da99424c9-C001-196", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_547551e556d8aa919f731da99424c9_11", "text": "For example, use the last hidden state while Conneau et al. (2017) perform max-pooling across all of the hidden states."}
{"sent_id": "547551e556d8aa919f731da99424c9-C001-101", "intents": ["@USE@"], "paper_id": "ABC_547551e556d8aa919f731da99424c9_11", "text": "This is the same classification strategy adopted by Conneau et al. (2017) ."}
{"sent_id": "547551e556d8aa919f731da99424c9-C001-207", "intents": ["@USE@"], "paper_id": "ABC_547551e556d8aa919f731da99424c9_11", "text": "In tables 3 and 5 we do not concatenate the representations of multiple models. and Conneau et al. (2017) provide a detailed description of tasks that are typically used to evaluate sentence representations."}
{"sent_id": "28038a4fa4182ccdc6134f2138c0da-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_28038a4fa4182ccdc6134f2138c0da_11", "text": "In the case of Noraset et al. (2017) , the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters derived from a characterlevel CNN, and its \"hypernym embedding\"."}
{"sent_id": "28038a4fa4182ccdc6134f2138c0da-C001-148", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_28038a4fa4182ccdc6134f2138c0da_11", "text": "The dataset of Noraset et al. (2017) (henceforth D Nor ) maps definienda to their respective definientia, as well as additional information not used here."}
{"sent_id": "28038a4fa4182ccdc6134f2138c0da-C001-193", "intents": ["@SIM@"], "paper_id": "ABC_28038a4fa4182ccdc6134f2138c0da_11", "text": "A manual analysis of definitions produced by our system reveals issues similar to those discussed by Noraset et al. (2017) , namely selfreference, 7 POS-mismatches, over-and underspecificity, antonymy, and incoherence."}
{"sent_id": "fa7475b6025d010dd6814dfb3905ef-C001-87", "intents": ["@BACK@"], "paper_id": "ABC_fa7475b6025d010dd6814dfb3905ef_11", "text": "This loss function needs to be combined with taskspecific loss [33, 17] ."}
{"sent_id": "fa7475b6025d010dd6814dfb3905ef-C001-140", "intents": ["@USE@"], "paper_id": "ABC_fa7475b6025d010dd6814dfb3905ef_11", "text": "We test our approach under a task-specific compression setting [33, 37] instead of a pretraining compression setting [28, 34] ."}
{"sent_id": "d51bf6d22d21dcd91e080f6f0b5dcb-C001-86", "intents": ["@USE@"], "paper_id": "ABC_d51bf6d22d21dcd91e080f6f0b5dcb_11", "text": "(1): 1) A graph of source phrases is constructed as in (Razmara et al., 2013) ; 2) translations are propagated as labels through the graph as explained in Fig. 2 ; and 3) new translation rules obtained from graph-propagation are integrated with the original phrase table."}
{"sent_id": "d51bf6d22d21dcd91e080f6f0b5dcb-C001-223", "intents": ["@USE@"], "paper_id": "ABC_d51bf6d22d21dcd91e080f6f0b5dcb_11", "text": "In this experiment we use a setup similar to (Razmara et al., 2013 we use 10K French-English parallel sentences, randomly chosen from Europarl to train translation system, as reported in (Razmara et al., 2013) ."}
{"sent_id": "b124e65938672691a5589fb5cdb21e-C001-33", "intents": ["@USE@"], "paper_id": "ABC_b124e65938672691a5589fb5cdb21e_11", "text": "In this study, we have used the stance-annotated tweet data set in Turkish (Küçük, 2017) which includes 700 random tweets related to two sports clubs and these clubs constitute the stance targets."}
{"sent_id": "b124e65938672691a5589fb5cdb21e-C001-64", "intents": ["@USE@"], "paper_id": "ABC_b124e65938672691a5589fb5cdb21e_11", "text": "As can be observed in Table 3 , using the existence of hashtags as an additional feature improves stance detection performance in terms of average F-Measure for Target-2 although it leads to a slight decrease in F-Measure for Target-1 (Küçük, 2017) ."}
{"sent_id": "b124e65938672691a5589fb5cdb21e-C001-82", "intents": ["@SIM@"], "paper_id": "ABC_b124e65938672691a5589fb5cdb21e_11", "text": "• As has been reported in (Küçük, 2017) , the overall evaluation results of the stance detection task are considerably higher for the Favor class when compared with the results for the Against class, in all settings given in Table 4 and 5."}
{"sent_id": "76476d80e1d3f65818592ec4caab0e-C001-17", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_76476d80e1d3f65818592ec4caab0e_11", "text": "1 Code available at https://github.com/ sweetpeach/ReCode Tree-based approaches (Yin and Neubig, 2017; Rabinovich et al., 2017) represent code as Abstract Syntax Trees (ASTs), which has proven effective in improving accuracy as it enforces the well-formedness of the output code."}
{"sent_id": "76476d80e1d3f65818592ec4caab0e-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_76476d80e1d3f65818592ec4caab0e_11", "text": "Interested readers can reference Yin and Neubig (2017) for more detail of the neural model, which consists of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) encoder-decoder with action embeddings, context vectors, parent feeding, and a copy mechanism using pointer networks."}
{"sent_id": "66392c3b6fa3744de79f056f615a75-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_66392c3b6fa3744de79f056f615a75_11", "text": "As mentioned above, Moldovan et al. (2004) showed that the sense collocation of NCs is a key feature when interpreting NCs."}
{"sent_id": "66392c3b6fa3744de79f056f615a75-C001-69", "intents": ["@USE@"], "paper_id": "ABC_66392c3b6fa3744de79f056f615a75_11", "text": "At first, we describe the principal idea of sense collocation method on NC interpretation and the probability model proposed in (Moldovan et al., 2004) ."}
{"sent_id": "66392c3b6fa3744de79f056f615a75-C001-113", "intents": ["@USE@"], "paper_id": "ABC_66392c3b6fa3744de79f056f615a75_11", "text": ".217 .496 .544 .552 .573 .562 .588 .568 .557 .197 .142 .547 .547 533 .573 .600 .606 .586 .607 .630 .467 .453 IA .507 .581 .595 .608 .649 .671 .653 .629 .645 .500 .500 PP .655 .667 .679 .691 .679 .737 .700 .690 .687 .655 .655 OE .558 .636 .623 .610 .662 .645 .662 .625 .712 .558 .558 TT .636 .697 .727 .712 .742 .766 .732 .717 .650 .515 .394 PW .634 .620 .690 .690 .629 .657 .585 .731 .630 .633 .634 CC .514 .676 .703 .689 .689 .676 .667 .647 .698 .446 .514 All .579 .632 .649 .653 .662 .679 .654 .661 .667 .541 .534 Table 4 : Results for each of the 2-way classification tasks: B = baseline, M+ = Moldovan et al. (2004) method, H i = ith-order Hypernym, O = Hyponym and S = Sister word; the best performing system is indicated in boldface and that of extended sense collocation as proposed in this paper."}
{"sent_id": "66392c3b6fa3744de79f056f615a75-C001-61", "intents": ["@MOT@"], "paper_id": "ABC_66392c3b6fa3744de79f056f615a75_11", "text": "The original method described in Moldovan et al. (2004) only relies on observed sense collocations."}
{"sent_id": "74623c8d812e3c84e7bc6b46e982f5-C001-171", "intents": ["@USE@"], "paper_id": "ABC_74623c8d812e3c84e7bc6b46e982f5_11", "text": "Second, Elsner and Charniak (2008) and Lowe et al. (2017) per-form similarly, with one doing better on VI and the other on 1-1, though Elsner and Charniak (2008) do consistently better across the exact conversation extraction metrics."}
{"sent_id": "74623c8d812e3c84e7bc6b46e982f5-C001-260", "intents": ["@DIF@"], "paper_id": "ABC_74623c8d812e3c84e7bc6b46e982f5_11", "text": "How far apart consecutive messages in a conversation are: Elsner and Charniak (2008) and Mehri and Carenini (2017) use a limit of 129 seconds, Jiang et al. (2018) limit to within 1 hour, Guo et al. (2017) limit to within 8 messages, and we limit to within 100 messages."}
{"sent_id": "e0df566d073649431c3454a52813e9-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_e0df566d073649431c3454a52813e9_11", "text": "Inspired by Pitler and Nenkova [20] , Alsaif and Markert [4] proposed an approach for the disambiguation of Arabic Discourse connectives."}
{"sent_id": "e0df566d073649431c3454a52813e9-C001-91", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_e0df566d073649431c3454a52813e9_11", "text": "In addition to the four features above, Pitler and Nenkova [20] used the discourse connective itself (case sensitive) as an additional feature for the classifier."}
{"sent_id": "e0df566d073649431c3454a52813e9-C001-97", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_e0df566d073649431c3454a52813e9_11", "text": "These two features are as informative as the case-sensitive connective string proposed by Pitler and Nenkova [20] , however, separating these features gives the classifier more flexibility when building its model."}
{"sent_id": "e0df566d073649431c3454a52813e9-C001-115", "intents": ["@SIM@"], "paper_id": "ABC_e0df566d073649431c3454a52813e9_11", "text": "Similarly to Pitler and Nenkova [20] , we report results using a maximum entropy classifier using ten-fold cross-validation over the extracted datasets."}
{"sent_id": "eee36102d3feac0f673cd33562d40f-C001-9", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_eee36102d3feac0f673cd33562d40f_11", "text": "However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (Finkel et al., 2005; Sarawagi and Cohen, 2004) are not robust for unknown named entities because their feature space is very sparse (Primadhanty et al., 2015) ."}
{"sent_id": "eee36102d3feac0f673cd33562d40f-C001-86", "intents": ["@USE@"], "paper_id": "ABC_eee36102d3feac0f673cd33562d40f_11", "text": "Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of Primadhanty et al. (2015) ."}
{"sent_id": "950cc4a7fa2db3aa6786cc0ae802b5-C001-57", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_950cc4a7fa2db3aa6786cc0ae802b5_12", "text": "5 The Yahoo! Answers manner question dataset prepared by Jansen et al. (2014) and described in the previous paragraph, was initially sampled from this larger dataset."}
{"sent_id": "950cc4a7fa2db3aa6786cc0ae802b5-C001-73", "intents": ["@USE@"], "paper_id": "ABC_950cc4a7fa2db3aa6786cc0ae802b5_12", "text": "In Table 1 , we report best development P@1 and MRR of the multilayer perceptron trained on Yahoo! Answers (Jansen et al., 2014) 200 outperforming the rest by a small margin."}
{"sent_id": "950cc4a7fa2db3aa6786cc0ae802b5-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_950cc4a7fa2db3aa6786cc0ae802b5_12", "text": "The best performance on this dataset -33.01 P@1 and 53.96 MRR -is reported by Fried et al. (2015) who improve on the lexical semantic models of Jansen et al. (2014) by exploiting indirect associations between words using higher-order models."}
{"sent_id": "d68bb5264d157cc4c2d9fa9c8f82b6-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_d68bb5264d157cc4c2d9fa9c8f82b6_12", "text": "The Google Ngram corpus [37] provides new support for these statistical regularities in word frequency dynamics at timescales from decades to centuries [22, 41, 42, 1, 28] of n-grams -an n-gram being n consecutive character strings, separated by spaces -derived from millions of books over multiple centuries [35] , the n-gram data now covers English books from the year 1500 to year 2008."}
{"sent_id": "d68bb5264d157cc4c2d9fa9c8f82b6-C001-75", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_d68bb5264d157cc4c2d9fa9c8f82b6_12", "text": "Fig 3b shows how 100 runs of the PNM yields a Heaps law exponent within the range derived by [42] for several different n-grams corpora (all English, English fiction, English GB, English US and English 1M)."}
{"sent_id": "d68bb5264d157cc4c2d9fa9c8f82b6-C001-150", "intents": ["@USE@"], "paper_id": "ABC_d68bb5264d157cc4c2d9fa9c8f82b6_12", "text": "Although we track 1-grams from the year 1700, for turnover statistics we follow other studies [42] in being cautious about the n-grams record before the year 1800, due to misspelled words before 1800 that were surely digital scanning errors related to antique printing styles of that may conflate letters such as 's' and 'f' (e.g., myfelf, yourfelf, provifions, increafe, afked etc)."}
{"sent_id": "392cbe849c1b8a69aae9923ade41aa-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_392cbe849c1b8a69aae9923ade41aa_12", "text": "Despite its simplicity and the lack of global optimisation, Zhang et al. (2017) report competitive results for English, Czech, and German."}
{"sent_id": "392cbe849c1b8a69aae9923ade41aa-C001-23", "intents": ["@EXT@"], "paper_id": "ABC_392cbe849c1b8a69aae9923ade41aa_12", "text": "Dependency Parsing as Head Selection Our labelling model is an extension of the parsing model of Zhang et al. (2017) ."}
{"sent_id": "392cbe849c1b8a69aae9923ade41aa-C001-75", "intents": ["@USE@"], "paper_id": "ABC_392cbe849c1b8a69aae9923ade41aa_12", "text": "Unless stated otherwise, all parameters are set according to Zhang et al. (2017) , and tag embedding size was set to 40 for all languages."}
{"sent_id": "392cbe849c1b8a69aae9923ade41aa-C001-84", "intents": ["@DIF@"], "paper_id": "ABC_392cbe849c1b8a69aae9923ade41aa_12", "text": "While we tried to reimplement the model of Zhang et al. (2017) following the details in the paper, our reimplemented model yields higher scores for German, compared to the results in the paper."}
{"sent_id": "392cbe849c1b8a69aae9923ade41aa-C001-106", "intents": ["@DIF@"], "paper_id": "ABC_392cbe849c1b8a69aae9923ade41aa_12", "text": "All our models outperform the original labeller of Zhang et al. (2017) and give results in the same range as the best system from the SPMRL-2014 shared task (without the reranker), but with a much simpler model."}
{"sent_id": "2c3a2999390b82f4e29b00d59f90f2-C001-85", "intents": ["@BACK@"], "paper_id": "ABC_2c3a2999390b82f4e29b00d59f90f2_12", "text": "Hidden Markov Models were employed by four of the systems that took part in the shared task (Florian et al., 2003; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003) ."}
{"sent_id": "2c3a2999390b82f4e29b00d59f90f2-C001-134", "intents": ["@BACK@"], "paper_id": "ABC_2c3a2999390b82f4e29b00d59f90f2_12", "text": "For English, the combined classifier of Florian et al. (2003) achieved the highest overall F β=1 rate."}
{"sent_id": "2c3a2999390b82f4e29b00d59f90f2-C001-142", "intents": ["@BACK@"], "paper_id": "ABC_2c3a2999390b82f4e29b00d59f90f2_12", "text": "A majority vote of five systems (Chieu and Ng, 2003; Florian et al., 2003; Klein et al., 2003; McCallum and Li, 2003; Whitelaw and Patrick, 2003) performed best on the English development data."}
{"sent_id": "e3ee86bbaca6ae00906e7ec64f0ac0-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12", "text": "• Knowledge-Base Learner (KBL): The Knowledge-Based Learner [2] aims to retrieve and transfer previous knowledge to the current task."}
{"sent_id": "c705c0533600b9b93d2c89bcbc292b-C001-33", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_c705c0533600b9b93d2c89bcbc292b_12", "text": "Most recently, Ma et al. (2019) introduce a visual and textual co-attention mechanism and a route progress predictor."}
{"sent_id": "35233406ffd78d87743478454432d5-C001-66", "intents": ["@USE@"], "paper_id": "ABC_35233406ffd78d87743478454432d5_12", "text": "We also bring out the test dataset used in (Xu et al., 2016) for a comparison (Section 5)."}
{"sent_id": "35233406ffd78d87743478454432d5-C001-91", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_35233406ffd78d87743478454432d5_12", "text": "We briefly introduce the method used in (Xu et al., 2016) and how the dependency paths can be used in questions of PCQA (details of dependency paths can be found in the original paper)."}
{"sent_id": "35233406ffd78d87743478454432d5-C001-180", "intents": ["@USE@"], "paper_id": "ABC_35233406ffd78d87743478454432d5_12", "text": "CER6K: This method is the method proposed in (Xu et al., 2016) ."}
{"sent_id": "808e0a94b877182dc06447c8682a63-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_808e0a94b877182dc06447c8682a63_12", "text": "In Bansal and Klein (2011) , paraphrase features are generated for all full-parse attachment ambiguities from the surface n-gram corpus."}
{"sent_id": "808e0a94b877182dc06447c8682a63-C001-13", "intents": ["@USE@"], "paper_id": "ABC_808e0a94b877182dc06447c8682a63_12", "text": "We compare the performance of our syntactic n-gram features against the surface n-gram features of Bansal and Klein (2011) in-domain on newswire and out-of-domain on the English Web Treebank (Petrov and McDonald, 2012) across CoNLL-style (LTH) dependencies."}
{"sent_id": "808e0a94b877182dc06447c8682a63-C001-29", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_808e0a94b877182dc06447c8682a63_12", "text": "We also develop paraphrase-style features like those of Bansal and Klein (2011) based on the most frequently occurring words and POS tags before, in between, and after each head-argument ambiguity (see Section 3.2)."}
{"sent_id": "da2429450c8d1f1f3e72383c86ec73-C001-93", "intents": ["@EXT@", "@USE@"], "paper_id": "ABC_da2429450c8d1f1f3e72383c86ec73_12", "text": "We used the same set of lexicons as in (Mohammad et al., 2013) , with one addition:"}
{"sent_id": "da2429450c8d1f1f3e72383c86ec73-C001-66", "intents": ["@USE@"], "paper_id": "ABC_da2429450c8d1f1f3e72383c86ec73_12", "text": "All text was transformed to lowercase (except for those features in (Mohammad et al., 2013) which use case information)."}
{"sent_id": "da2429450c8d1f1f3e72383c86ec73-C001-110", "intents": ["@DIF@"], "paper_id": "ABC_da2429450c8d1f1f3e72383c86ec73_12", "text": "While in (Mohammad et al., 2013) , the score for each n-gram was computed using point-wise mutual information (PMI) with the labels, we trained a linear classifier on the same labels instead."}
{"sent_id": "06db17253d76150772c0926e11131d-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_06db17253d76150772c0926e11131d_12", "text": "Several large real image VQA datasets have recently emerged [8] [9] [10] [11] [12] [13] [14] ."}
{"sent_id": "06db17253d76150772c0926e11131d-C001-34", "intents": ["@MOT@"], "paper_id": "ABC_06db17253d76150772c0926e11131d_12", "text": "However, when evaluated on a test set that displays different statistical regularities, they usually suffer from a significant drop in accuracy [10, 25] ."}
{"sent_id": "7b9fc52e4479dc5ff9b8796a558981-C001-11", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_7b9fc52e4479dc5ff9b8796a558981_12", "text": "Artetxe et al. (2017) use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; Zhang et al. (2017) use an adversarial training method to learn a similar mapping."}
{"sent_id": "7b9fc52e4479dc5ff9b8796a558981-C001-50", "intents": ["@EXT@", "@USE@"], "paper_id": "ABC_7b9fc52e4479dc5ff9b8796a558981_12", "text": "This method modifies the similarity score for each word pair during the dictionary induction phase of the self-learning framework of Artetxe et al. (2017) , which uses the dot product of two words' embeddings to quantify similarity."}
{"sent_id": "7b9fc52e4479dc5ff9b8796a558981-C001-21", "intents": ["@USE@"], "paper_id": "ABC_7b9fc52e4479dc5ff9b8796a558981_12", "text": "This work is directly based on the work of Artetxe et al. (2017) ."}
{"sent_id": "c34bbed419bddb6d63b3e3bccf595d-C001-136", "intents": ["@DIF@"], "paper_id": "ABC_c34bbed419bddb6d63b3e3bccf595d_12", "text": "By using a cascading minority preference system, we show that our approach outperforms the bridging recognition in Markert et al. (2012) by a large margin without impairing the performance on other IS classes."}
{"sent_id": "c34bbed419bddb6d63b3e3bccf595d-C001-25", "intents": ["@MOT@"], "paper_id": "ABC_c34bbed419bddb6d63b3e3bccf595d_12", "text": "Previous work on recognition is either limited to definite NPs based on heuristics evaluated on small datasets (Hahn et al., 1996; Vieira and Poesio, 2000) , or models it as a subtask of learning fine-grained IS (Rahman and Ng, 2012; Markert et al., 2012; Cahill and Riester, 2012) ."}
{"sent_id": "c34bbed419bddb6d63b3e3bccf595d-C001-113", "intents": ["@EXT@"], "paper_id": "ABC_c34bbed419bddb6d63b3e3bccf595d_12", "text": "rbls uses the same features as Markert et al. (2012) (Table 1) but replaces the local decision tree classifier with LibSVM as we will need to include lexical features."}
{"sent_id": "bcf19914bb67ded47785d298969a7a-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_bcf19914bb67ded47785d298969a7a_12", "text": "As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information (Shekhar et al., 2017b; Johnson et al., 2017; Antol et al., 2015; Chen et al., 2015; Gao et al., 2015; Yu et al., 2015; Zhu et al., 2016) ."}
{"sent_id": "bcf19914bb67ded47785d298969a7a-C001-40", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_bcf19914bb67ded47785d298969a7a_12", "text": "The Verb, Adjective, Adverb and Preposition subsets were obtained using a slightly different methodology (see Shekhar et al. (2017a) ) than that used for Nouns (Shekhar et al., 2017b) ."}
{"sent_id": "710ec6f6d6d4c7c8c148833c0adfef-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_710ec6f6d6d4c7c8c148833c0adfef_12", "text": "Among the automatic methods, Mohammad et al. (2013) proposed to use tweets with emoticons or hashtags as training data."}
{"sent_id": "710ec6f6d6d4c7c8c148833c0adfef-C001-26", "intents": ["@USE@"], "paper_id": "ABC_710ec6f6d6d4c7c8c148833c0adfef_12", "text": "The method can leverage the same data as Mohammad et al. (2013) and therefore benefits from both scale and annotation independence."}
{"sent_id": "fc5de471ba4cc82a2156ed25d2c78b-C001-10", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_fc5de471ba4cc82a2156ed25d2c78b_12", "text": "An end-to-end approach [1] [2] [3] [4] [5] [6] [7] is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions [8] ."}
{"sent_id": "fc5de471ba4cc82a2156ed25d2c78b-C001-24", "intents": ["@EXT@", "@USE@"], "paper_id": "ABC_fc5de471ba4cc82a2156ed25d2c78b_12", "text": "To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. [5] , but pretrain the encoder using a number of different ASR datasets: the 150hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data."}
{"sent_id": "fc5de471ba4cc82a2156ed25d2c78b-C001-33", "intents": ["@USE@"], "paper_id": "ABC_fc5de471ba4cc82a2156ed25d2c78b_12", "text": "For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure 1 : the encoder-decoder model from [5] , which itself is adapted from [2] , [4] and [3] ."}
{"sent_id": "fc5de471ba4cc82a2156ed25d2c78b-C001-90", "intents": ["@DIF@"], "paper_id": "ABC_fc5de471ba4cc82a2156ed25d2c78b_12", "text": "Our baseline 20-hour AST system obtains a BLEU score of 10.3 ( Table 1 , first row), 0.5 BLEU point lower than that reported by [5] ."}
{"sent_id": "fc5de471ba4cc82a2156ed25d2c78b-C001-101", "intents": ["@SIM@"], "paper_id": "ABC_fc5de471ba4cc82a2156ed25d2c78b_12", "text": "This is nearly as much as the 6 point improvement reported by [5] when pretraining on 100 hours of English data, which is especially surprising given not only that Chinese is very different from Spanish, but also that the Spanish data contains some English words."}
{"sent_id": "24ee9b2bd8c97cbe923bc747b09806-C001-129", "intents": ["@BACK@"], "paper_id": "ABC_24ee9b2bd8c97cbe923bc747b09806_12", "text": "[12] is a convolutional approach, whereas [15] is an approach using recurrent highway networks with scalar attention."}
{"sent_id": "26fbf9f4ae740513d8889160ad9f63-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_26fbf9f4ae740513d8889160ad9f63_12", "text": "As shown in Figure 1 , the contingency relation subsumes several more specific relations: explanation, evidence, reason, cause, result, consequence, background, condition, hypothetical, enablement, and purpose relations according to the Penn Discourse TreeBank (Prasad et al., 2008) ."}
{"sent_id": "26fbf9f4ae740513d8889160ad9f63-C001-109", "intents": ["@USE@"], "paper_id": "ABC_26fbf9f4ae740513d8889160ad9f63_12", "text": "To measure the usefulness of discourse relations for the summarization of informal texts, we have tested the effect of each relation with four different summarizers: BlogSum (Mithun, 2012) , MEAD (Radev et al., 2004) , the best scoring system at TAC 2008 5 and the best scoring system at DUC 2007 6 ."}
{"sent_id": "7d5c01ec5d744747413e42dcbc1a3c-C001-24", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_7d5c01ec5d744747413e42dcbc1a3c_12", "text": "These algorithms represent the ma-jor directions in this research area, including TfIdf and four recently proposed systems, namely, TextRank (Mihalcea and Tarau, 2004) , SingleRank (Wan and Xiao, 2008) , ExpandRank (Wan and Xiao, 2008) , and a clustering-based approach (Liu et al., 2009b) ."}
{"sent_id": "7d5c01ec5d744747413e42dcbc1a3c-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_7d5c01ec5d744747413e42dcbc1a3c_12", "text": "This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by Hulth (2003) and later by Mihalcea and Tarau (2004) and Liu et al. (2009b) ."}
{"sent_id": "7d5c01ec5d744747413e42dcbc1a3c-C001-89", "intents": ["@BACK@"], "paper_id": "ABC_7d5c01ec5d744747413e42dcbc1a3c_12", "text": "According to Mihalcea and Tarau (2004) , TextRank's best score on the Inspec dataset is achieved when only nouns and adjectives are used to create a uniformly weighted graph for the text under consideration, where an edge connects two word types only if they co-occur within a window of two words."}
{"sent_id": "7d5c01ec5d744747413e42dcbc1a3c-C001-159", "intents": ["@DIF@"], "paper_id": "ABC_7d5c01ec5d744747413e42dcbc1a3c_12", "text": "This certainly gives more insight into TextRank since it was evaluated on Inspec only for T=33% by Mihalcea and Tarau (2004) ."}
{"sent_id": "40742bac72bbbaed4755ff0b74d599-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_40742bac72bbbaed4755ff0b74d599_12", "text": "The performance of the PPMI matrix on word similarity tasks can be further improved by using context-distribution smoothing (Levy et al., 2015) and subsampling the corpus (Mikolov et al., 2013b) ."}
{"sent_id": "40742bac72bbbaed4755ff0b74d599-C001-64", "intents": ["@USE@"], "paper_id": "ABC_40742bac72bbbaed4755ff0b74d599_12", "text": "The PPMI* matrix used by both PPMI-SVD and LexVec was constructed using smoothing of α = 3/4 suggested in (Levy et al., 2015) and an unweighted window of size 2."}
{"sent_id": "0732eaa37366d7ae092f4de0ed72cb-C001-75", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_0732eaa37366d7ae092f4de0ed72cb_12", "text": "In constituency parsing, several systems also incorporate pre-trained word embeddings, such as Vinyals et al. (2015) ; Durrett and Klein (2015) ."}
{"sent_id": "0732eaa37366d7ae092f4de0ed72cb-C001-12", "intents": ["@USE@"], "paper_id": "ABC_0732eaa37366d7ae092f4de0ed72cb_12", "text": "Our aim is to update the Seq2seq approach proposed in Vinyals et al. (2015) as a stronger baseline of constituency parsing."}
{"sent_id": "0732eaa37366d7ae092f4de0ed72cb-C001-98", "intents": ["@USE@"], "paper_id": "ABC_0732eaa37366d7ae092f4de0ed72cb_12", "text": "As explained in Vinyals et al. (2015) , we did not apply any parse tree binarization or special unary treatment, which were used as common techniques in the literature."}
{"sent_id": "4d2488844c1f6f39f1f4b8f3487288-C001-28", "intents": ["@USE@"], "paper_id": "ABC_4d2488844c1f6f39f1f4b8f3487288_13", "text": "The DPMM used in Kawahara et al. (2014) for clustering verb senses."}
{"sent_id": "4d2488844c1f6f39f1f4b8f3487288-C001-113", "intents": ["@USE@"], "paper_id": "ABC_4d2488844c1f6f39f1f4b8f3487288_13", "text": "Therefore, to compare apples-to-apples, we calculate the nPU, niPU, and F1 of the Kawahara et al. (2014) full clustering against the evaluation set."}
{"sent_id": "9655fb9abfb1c30b39f3261680fafc-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_9655fb9abfb1c30b39f3261680fafc_13", "text": "datato-document generation is a slightly more challenging setting in which a system generates multisentence summaries based on input data (Wiseman et al., 2017) ."}
{"sent_id": "9655fb9abfb1c30b39f3261680fafc-C001-99", "intents": ["@USE@"], "paper_id": "ABC_9655fb9abfb1c30b39f3261680fafc_13", "text": "the previous work (Wiseman et al., 2017) ."}
{"sent_id": "9655fb9abfb1c30b39f3261680fafc-C001-46", "intents": ["@SIM@"], "paper_id": "ABC_9655fb9abfb1c30b39f3261680fafc_13", "text": "To extract information describing the input data from the generated texts, we apply a simple information extraction system similar to (Wiseman et al., 2017) ."}
{"sent_id": "e5bff4a27468139762496abdff3436-C001-48", "intents": ["@USE@"], "paper_id": "ABC_e5bff4a27468139762496abdff3436_13", "text": "Both the original data and the most updated version used here are publicly available (Danescu-NiculescuMizil et al. 2012) ."}
{"sent_id": "8c202e3610599c9eee23724ef213de-C001-131", "intents": ["@BACK@"], "paper_id": "ABC_8c202e3610599c9eee23724ef213de_13", "text": "In online argumentative discourse, claims often serve as implicit arguments with inappropriate or missing justification (Park and Cardie, 2014) ."}
{"sent_id": "8ef47e16cd41aa3a606cf21c41adb7-C001-14", "intents": ["@MOT@"], "paper_id": "ABC_8ef47e16cd41aa3a606cf21c41adb7_13", "text": "However, as pointed out in (Murray et al., 2010) and (Oya et al., 2014) , abstractive summaries are preferred to extractive ones by human judges."}
{"sent_id": "8ef47e16cd41aa3a606cf21c41adb7-C001-92", "intents": ["@USE@"], "paper_id": "ABC_8ef47e16cd41aa3a606cf21c41adb7_13", "text": "For AMI corpus, following (Oya et al., 2014) , we report ROUGE-2 F-measures on 3-fold cross-validation."}
{"sent_id": "8ef47e16cd41aa3a606cf21c41adb7-C001-48", "intents": ["@EXT@"], "paper_id": "ABC_8ef47e16cd41aa3a606cf21c41adb7_13", "text": "The clustered templates are further generalized using a word graph algorithm extended to templates in (Oya et al., 2014) ."}
{"sent_id": "c1eefe276c0ed46d7cd50f3f3bc3f3-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_c1eefe276c0ed46d7cd50f3f3bc3f3_13", "text": "Conversation modelling is one such domain where end-to-end trained systems have matched or surpassed traditional dialog systems in both open-ended (Dodge et al., 2016) and goal-oriented applications (Bordes et al., 2017 )."}
{"sent_id": "c1eefe276c0ed46d7cd50f3f3bc3f3-C001-140", "intents": ["@USE@"], "paper_id": "ABC_c1eefe276c0ed46d7cd50f3f3bc3f3_13", "text": "The embedding vectors are trained specifically for the task of predicting the next response given the previous conversation: a candidate response y is scored against the input (Dodge et al., 2016; Bordes et al., 2017) ."}
{"sent_id": "c1eefe276c0ed46d7cd50f3f3bc3f3-C001-145", "intents": ["@DIF@"], "paper_id": "ABC_c1eefe276c0ed46d7cd50f3f3bc3f3_13", "text": "Unlike Bordes et al. (2017), we do not make use of any match type features."}
{"sent_id": "1f77b780c98093cd85966243471a1d-C001-27", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_1f77b780c98093cd85966243471a1d_13", "text": "Furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible (Fried et al., 2014; Culotta, 2014) ."}
{"sent_id": "1f77b780c98093cd85966243471a1d-C001-123", "intents": ["@DIF@"], "paper_id": "ABC_1f77b780c98093cd85966243471a1d_13", "text": "The table shows that our best model performs 2% better than the best model of (Fried et al., 2014) ."}
{"sent_id": "1f77b780c98093cd85966243471a1d-C001-108", "intents": ["@USE@"], "paper_id": "ABC_1f77b780c98093cd85966243471a1d_13", "text": "Majority baseline 50.89 SVM (Fried et al., 2014) 80.39 RF (food + hashtags) 82.35 Discretized RF (food + hashtags) 78.43 Table 2 : Random forest (RF) classifier performance on state-level data relative to majority baseline and Fried et al. (2014) 's best classifier."}
{"sent_id": "45238fe9b493ccdf5921c8f5284097-C001-53", "intents": ["@USE@"], "paper_id": "ABC_45238fe9b493ccdf5921c8f5284097_13", "text": "Most recent work uses a single human judgment to capture all linguistic qualities of the summary (Hsu et al., 2018; Kryściński et al., 2018; Narayan et al., 2018b; Song et al., 2018; Guo et al., 2018) ; we group them under \"Fluency\" in Table 1 with an exception of \"Clarity\" which was evaluated in the DUC evaluation campaigns (Dang, 2005) ."}
{"sent_id": "45238fe9b493ccdf5921c8f5284097-C001-131", "intents": ["@USE@"], "paper_id": "ABC_45238fe9b493ccdf5921c8f5284097_13", "text": "Following Narayan et al. (2018b) , we didn't use the whole test set portion, but sampled 50 articles from it for our highlight-based evaluation."}
{"sent_id": "45238fe9b493ccdf5921c8f5284097-C001-65", "intents": ["@MOT@"], "paper_id": "ABC_45238fe9b493ccdf5921c8f5284097_13", "text": "However, summarization datasets are limited to a single reference summary per document (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018b) thus evaluations using them is prone to reference bias (Louis and Nenkova, 2013) , also a known issue in machine translation evaluation (Fomicheva and Specia, 2016) ."}
{"sent_id": "71a72cfca17b0b15938ed590f9c868-C001-49", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_71a72cfca17b0b15938ed590f9c868_13", "text": "Nissim (2006) and Rahman and Ng (2011) both present algorithms for IS detection on Nissim et al.'s (2004) Switchboard corpus."}
{"sent_id": "71a72cfca17b0b15938ed590f9c868-C001-121", "intents": ["@BACK@"], "paper_id": "ABC_71a72cfca17b0b15938ed590f9c868_13", "text": "Both Nissim (2006) and Rahman and Ng (2011) classify each mention individually in a standard supervised ML setting, not considering potential dependencies between the IS categories of different mentions."}
{"sent_id": "71a72cfca17b0b15938ed590f9c868-C001-33", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_71a72cfca17b0b15938ed590f9c868_13", "text": "We reimplement Nissim's (2006) and Rahman and Ng's (2011) approaches as baselines and show that our approach outperforms these by a large margin for both coarse-and finegrained IS classification."}
{"sent_id": "71a72cfca17b0b15938ed590f9c868-C001-193", "intents": ["@DIF@"], "paper_id": "ABC_71a72cfca17b0b15938ed590f9c868_13", "text": "We show that our collective classification approach outperforms the state-of-the-art in coarse-grained IS classification by about 10% (Nissim, 2006) and 5% (Rahman and Ng, 2011) accuracy."}
{"sent_id": "71a72cfca17b0b15938ed590f9c868-C001-154", "intents": ["@USE@"], "paper_id": "ABC_71a72cfca17b0b15938ed590f9c868_13", "text": "We reimplemented the algorithms in Nissim (2006) and Rahman and Ng (2011) as comparison baselines, using their feature and algorithm choices."}
{"sent_id": "03c57679549ff600a024d436d5a107-C001-209", "intents": ["@BACK@"], "paper_id": "ABC_03c57679549ff600a024d436d5a107_13", "text": "Wikifier (Ratinov et al., 2011) analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in Ratinov and Roth (2012) ."}
{"sent_id": "03c57679549ff600a024d436d5a107-C001-179", "intents": ["@SIM@"], "paper_id": "ABC_03c57679549ff600a024d436d5a107_13", "text": "Our static linking matches the performance of Ratinov and Roth (2012) on the non-transcripts."}
{"sent_id": "46050691971ea46ce7e18fef5f6d2d-C001-97", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_46050691971ea46ce7e18fef5f6d2d_13", "text": "Goldberg (2019) , following Linzen et al. (2016) , considered a correct prediction to be one in which the masked word receives a higher probability than other inflected forms of the lemma."}
{"sent_id": "46050691971ea46ce7e18fef5f6d2d-C001-137", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_46050691971ea46ce7e18fef5f6d2d_13", "text": "Previous work by Goldberg (2019) showed that BERT captures English subject-verb number agreement well despite this lack of explicit structural representation."}
{"sent_id": "46050691971ea46ce7e18fef5f6d2d-C001-106", "intents": ["@USE@"], "paper_id": "ABC_46050691971ea46ce7e18fef5f6d2d_13", "text": "Following Goldberg (2019), we use the pretrained BERT models from the original authors 2 , but through the PyTorch implementation."}
{"sent_id": "e264c45391853fb008c838aa7ccca8-C001-2", "intents": ["@EXT@"], "paper_id": "ABC_e264c45391853fb008c838aa7ccca8_13", "text": "This paper proposes an extension of Sumida and Torisawa's method of acquiring hyponymy relations from hierachical layouts in Wikipedia (Sumida and Torisawa, 2008) ."}
{"sent_id": "e264c45391853fb008c838aa7ccca8-C001-56", "intents": ["@DIF@"], "paper_id": "ABC_e264c45391853fb008c838aa7ccca8_13", "text": "We obtain HRCs by considering the title of each marked-up item as a hypernym candidate, and titles of its all subordinate marked-up items as its hyponym candidates; for example, we extract 'England', 'France', 'Wedgwood', 'Lipton', and 'Fauchon' as hyponym candidates of 'Common tea brands' from the hierarchical structure in Figure 1 . Note that Sumida and Torisawa (2008) extracted HRCs by regarding the title of each marked-up item as a hypernym candidate and titles of its direct subordinate marked-up items as its hyponyms; for example, they extracted only 'England' and 'France' as hyponym candidates of 'Common tea brands' from the hierarchical structure in Figure 1 ."}
{"sent_id": "e264c45391853fb008c838aa7ccca8-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_e264c45391853fb008c838aa7ccca8_13", "text": "Many NLP researchers have attempted to automatically acquire hyponymy relations from texts (Hearst, 1992; Caraballo, 1999; Mann, 2002; Fleischman et al., 2003; Morin and Jacquemin, 2004; Shinzato and Torisawa, 2004; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Sumida et al., 2006; Sumida and Torisawa, 2008) ."}
{"sent_id": "e264c45391853fb008c838aa7ccca8-C001-42", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_e264c45391853fb008c838aa7ccca8_13", "text": "Although the above studies extracted hyponymy relations from the English version of Wikipedia, Sumida and Torisawa (2008) extracted hyponymy relations from definition sentences, category labels, and hierarchical structures in Wikipedia articles."}
{"sent_id": "e264c45391853fb008c838aa7ccca8-C001-114", "intents": ["@USE@"], "paper_id": "ABC_e264c45391853fb008c838aa7ccca8_13", "text": "The row titled 'S & T (2008) ' shows the performance of the method proposed by Sumida and Torisawa (2008) ."}
{"sent_id": "c3c09df34cf9f81c1cc4fc63a18bf0-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_c3c09df34cf9f81c1cc4fc63a18bf0_13", "text": "Given a set of sentences S = {s i }; i ∈ [1 . . . N ], where each sentence s i contains both the entities, the task of relation extraction with distantly supervised dataset is to learn a function F r : F r (S, (e 1 , e 2 )) = 1 if relation r is true for pair(e 1 , e 2 ) 0 Otherwise PCNN: [Zeng et al., 2015] proposed the Piecewise Convolution Neural Network (PCNN), a successful model for distantly supervised relation extraction."}
{"sent_id": "8ae44e74146d3f40845741fac4dff9-C001-91", "intents": ["@BACK@"], "paper_id": "ABC_8ae44e74146d3f40845741fac4dff9_13", "text": "Shutova (2010) annotated metaphorical expressions in a subset of the BNC sampling various genres: literature, newspaper/journal articles, essays on politics, international relations and sociology, radio broadcast (transcribed speech)."}
{"sent_id": "8ae44e74146d3f40845741fac4dff9-C001-30", "intents": ["@USE@"], "paper_id": "ABC_8ae44e74146d3f40845741fac4dff9_13", "text": "We focus on paraphrasing metaphorical verbs and evaluate our system using the dataset of Shutova (2010) especially designed for this task."}
{"sent_id": "8ae44e74146d3f40845741fac4dff9-C001-119", "intents": ["@USE@"], "paper_id": "ABC_8ae44e74146d3f40845741fac4dff9_13", "text": "The gold standard was created by Shutova (2010) as follows."}
{"sent_id": "8ae44e74146d3f40845741fac4dff9-C001-161", "intents": ["@USE@"], "paper_id": "ABC_8ae44e74146d3f40845741fac4dff9_13", "text": "Following Shutova (2010) , the current experimental design and test set focuses on subject-verb and verb-object metaphors only, but we expect the method to be equally applicable to other parts of speech and a wider range of syntactic constructions."}
{"sent_id": "2504d707a8123774791d98b755551a-C001-153", "intents": ["@DIF@"], "paper_id": "ABC_2504d707a8123774791d98b755551a_13", "text": "We also compare our performance against (Bunescu and Mooney, 2004) and (Finkel et al., 2005) and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF."}
{"sent_id": "2504d707a8123774791d98b755551a-C001-110", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_2504d707a8123774791d98b755551a_13", "text": "• Most existing work to capture labelconsistency, has attempted to create all n 2 pairwise dependencies between the different occurrences of an entity, (Finkel et al., 2005; Sutton and McCallum, 2004) , where n is the number of occurrences of the given entity."}
{"sent_id": "a3ad95d75b7750b8a879fa183e30f6-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_a3ad95d75b7750b8a879fa183e30f6_13", "text": "Research on unsupervised adaptation for acoustic models can be roughly divided into three categories: (1) constrained model adaptation [5, 6, 7] , (2) domain-invariant feature extraction [8, 9, 10] , and (3) labeled in-domain data augmentation by synthesis [11, 12, 13] ."}
{"sent_id": "a3ad95d75b7750b8a879fa183e30f6-C001-101", "intents": ["@SIM@"], "paper_id": "ABC_a3ad95d75b7750b8a879fa183e30f6_13", "text": "Therefore, we adopt a similar soft perturbation scheme as in [13] ."}
{"sent_id": "4bc12aca138835b5ed80b0cf69febf-C001-67", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_4bc12aca138835b5ed80b0cf69febf_13", "text": "Otherwise, we follow the procedure in Devlin et al. (2018) ."}
{"sent_id": "518b281a4bc04b3504d3b385a5dc62-C001-17", "intents": ["@EXT@"], "paper_id": "ABC_518b281a4bc04b3504d3b385a5dc62_13", "text": "We present a system that adapts the unsupervised MorphoChains segmentation system (Narasimhan et al., 2015) to provide morphological analyses that aim to abstract over spelling differences in functionally similar morphemes."}
{"sent_id": "518b281a4bc04b3504d3b385a5dc62-C001-27", "intents": ["@USE@"], "paper_id": "ABC_518b281a4bc04b3504d3b385a5dc62_13", "text": "We base our work on the MorphoChains segmentation system (Narasimhan et al., 2015) , 1 which defines a morphological chain as a sequence of child-parent pairs."}
{"sent_id": "518b281a4bc04b3504d3b385a5dc62-C001-57", "intents": ["@USE@"], "paper_id": "ABC_518b281a4bc04b3504d3b385a5dc62_13", "text": "Following Narasimhan et al. (2015), we do so using Contrastive Estimation (CE) (Smith and Eisner, 2005) ."}
{"sent_id": "518b281a4bc04b3504d3b385a5dc62-C001-162", "intents": ["@USE@"], "paper_id": "ABC_518b281a4bc04b3504d3b385a5dc62_13", "text": "Using these features gives gains of +1.0%, 3.8% and 0.6% F-1 absolute on English, German and Turkish respectively over using the raw affix occurrence frequencies as used by Narasimhan et al. (2015) ."}
{"sent_id": "c4e2f43e223f61d81d81ac2c9aaa3f-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_c4e2f43e223f61d81d81ac2c9aaa3f_13", "text": "Multi-task learning (MTL) in deep neural networks is typically a result of parameter sharing between two networks (of usually the same dimensions) (Caruana 1993) ."}
{"sent_id": "c4e2f43e223f61d81d81ac2c9aaa3f-C001-195", "intents": ["@BACK@"], "paper_id": "ABC_c4e2f43e223f61d81d81ac2c9aaa3f_13", "text": "Hard parameter sharing (Caruana 1993 ) is easy to implement, reduces overfitting, but is only guaranteed to work for (certain types of) closely related tasks (Baxter 2000; Maurer 2007 )."}
{"sent_id": "c4e2f43e223f61d81d81ac2c9aaa3f-C001-152", "intents": ["@USE@"], "paper_id": "ABC_c4e2f43e223f61d81d81ac2c9aaa3f_13", "text": "Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing (Caruana 1993) ; and iv) cross-stitch networks (Misra et al. 2016) ."}
{"sent_id": "c4e2f43e223f61d81d81ac2c9aaa3f-C001-76", "intents": ["@SIM@"], "paper_id": "ABC_c4e2f43e223f61d81d81ac2c9aaa3f_13", "text": "For now just observe that if all α-values are set to 0.25 (or any other constant), we obtain hard parameter sharing (Caruana 1993) , which is equivalent to a heavy L 0 matrix regularizer."}
{"sent_id": "c4e2f43e223f61d81d81ac2c9aaa3f-C001-160", "intents": ["@DIF@"], "paper_id": "ABC_c4e2f43e223f61d81d81ac2c9aaa3f_13", "text": "In contrast to previous studies on MTL (Martínez Alonso and Plank 2017; Bingel and Søgaard 2017; Augenstein, Ruder, and Søgaard 2018) , our model also consistently outperforms single-task learning."}
{"sent_id": "412c2daf6d060f520850d187c6eb36-C001-150", "intents": ["@DIF@"], "paper_id": "ABC_412c2daf6d060f520850d187c6eb36_13", "text": "4. The sense marked with ** is defined in LDOCE but not used in Yarowsky (1992) ."}
{"sent_id": "1c89c8f4849d1c8214a3e5f6b9ff1a-C001-55", "intents": ["@USE@"], "paper_id": "ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14", "text": "A window center might contain 1 or more words (e.g., \"8 AM\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL (Sproat and Jaitly, 2016) ."}
{"sent_id": "22da24997f66a6dafa911f83f061e5-C001-65", "intents": ["@BACK@"], "paper_id": "ABC_22da24997f66a6dafa911f83f061e5_14", "text": "Enhanced Sequential Inference Model (ESIM) (Chen et al., 2016) with some small modifications has been used in (Nie et al., 2019; Hanselowski et al., 2018) ."}
{"sent_id": "22da24997f66a6dafa911f83f061e5-C001-98", "intents": ["@USE@"], "paper_id": "ABC_22da24997f66a6dafa911f83f061e5_14", "text": "Following the UKP-Athene promising document retrieval component (Hanselowski et al., 2018) , which results in more than 93% development set document recall, we exactly use their method to collect a set of top documents D c l top for the claim c l ."}
{"sent_id": "22da24997f66a6dafa911f83f061e5-C001-164", "intents": ["@DIF@"], "paper_id": "ABC_22da24997f66a6dafa911f83f061e5_14", "text": "The BERT claim verification system even if it is trained on the UKP-Athene sentence retrieval component (Hanselowski et al., 2018) , the state of the art method with the highest recall, improves both label accuracy and FEVER score."}
{"sent_id": "260489da0fb3f7a201a6a1cce8f03b-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_260489da0fb3f7a201a6a1cce8f03b_14", "text": "The encoder-decoder framework [Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015] usually uses a recurrent neural network (RNN) to encode the source sentence into a sequence of hidden states h = h 1 , . . . , h m , . . . , h M :"}
{"sent_id": "260489da0fb3f7a201a6a1cce8f03b-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_260489da0fb3f7a201a6a1cce8f03b_14", "text": "Bahdanau et al. [2015] define the conditional probability in Eq."}
{"sent_id": "260489da0fb3f7a201a6a1cce8f03b-C001-76", "intents": ["@USE@"], "paper_id": "ABC_260489da0fb3f7a201a6a1cce8f03b_14", "text": "We follow Bahdanau et al. [2015] to restrict that sentences are no longer than 50 words."}
{"sent_id": "260489da0fb3f7a201a6a1cce8f03b-C001-124", "intents": ["@MOT@"], "paper_id": "ABC_260489da0fb3f7a201a6a1cce8f03b_14", "text": "After analyzing the alignment matrices generated by GroundHog [Bahdanau et al., 2015] , we find that modeling the structural divergence of natural languages is so challenging that unidirectional models can only capture part of alignment regularities."}
{"sent_id": "8853d810b364ae47a2da71c2502b3e-C001-60", "intents": ["@USE@"], "paper_id": "ABC_8853d810b364ae47a2da71c2502b3e_14", "text": "Hyperparameters We use the same setup as Plank and Agić (2018) , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions."}
{"sent_id": "8853d810b364ae47a2da71c2502b3e-C001-207", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_8853d810b364ae47a2da71c2502b3e_14", "text": "We replicated the results of Plank and Agić (2018) , showing that the more implicit use of embedding user-generated dictionaries turns out to be more beneficial than approaches that rely more explicitly on symbolic knowledge, such a type constraints or retrofitting."}
{"sent_id": "bdd0ebe147e277f8f7f04fc351464a-C001-49", "intents": ["@BACK@"], "paper_id": "ABC_bdd0ebe147e277f8f7f04fc351464a_14", "text": "Concerning the SDRs, in order to formally define a relation the following four fields ought to be defined (see also [5] ):"}
{"sent_id": "bdd0ebe147e277f8f7f04fc351464a-C001-146", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_bdd0ebe147e277f8f7f04fc351464a_14", "text": "In [1] and [5] we thoroughly presented a methodology (and applied it in two different case studies) which aims towards the creation of summaries from descriptions of evolving events which are emitted from multiple sources."}
{"sent_id": "cc66b46b34a0d716414e8b845707f9-C001-160", "intents": ["@BACK@"], "paper_id": "ABC_cc66b46b34a0d716414e8b845707f9_14", "text": "We attribute this improvement to the LSTM-RNN's ability to capture the relationships that exist among the records, which is known to be essential to selective generation (Barzilay and Lapata, 2005; Angeli et al., 2010) ."}
{"sent_id": "cc66b46b34a0d716414e8b845707f9-C001-18", "intents": ["@MOT@"], "paper_id": "ABC_cc66b46b34a0d716414e8b845707f9_14", "text": "Further, our memorybased model captures the long-range contextual dependencies among records and descriptions, which are integral to this task (Angeli et al., 2010) ."}
{"sent_id": "cc66b46b34a0d716414e8b845707f9-C001-122", "intents": ["@SIM@"], "paper_id": "ABC_cc66b46b34a0d716414e8b845707f9_14", "text": "Beam search offers a way to perform approximate joint inference -however, we empirically found that beam search does not perform any better than greedy search on the datasets that we consider, an observation that is shared with previous work (Angeli et al., 2010) ."}
{"sent_id": "cc66b46b34a0d716414e8b845707f9-C001-127", "intents": ["@USE@"], "paper_id": "ABC_cc66b46b34a0d716414e8b845707f9_14", "text": "Following Angeli et al. (2010) , we use WEATHERGOV training, development, and test splits of size 25000, 1000, and 3528, respectively."}
{"sent_id": "7f234ecfb4cf880502faa8b89cd07b-C001-76", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_7f234ecfb4cf880502faa8b89cd07b_14", "text": "The architecture is very flexible and can be seen as a generalization over several existing algorithms for transfer and multi-task learning, including [7, 9, 25, 21] ."}
{"sent_id": "7f234ecfb4cf880502faa8b89cd07b-C001-66", "intents": ["@DIF@"], "paper_id": "ABC_7f234ecfb4cf880502faa8b89cd07b_14", "text": "predefining it [25, 11] , we enable our model to learn hierarchical relations by associating different tasks with different layers if this is beneficial for learning."}
{"sent_id": "a0af9cf22996a245af9d66cf1d358f-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_a0af9cf22996a245af9d66cf1d358f_14", "text": "While this shift appears to be due to time (Staliūnaitė et al. 2018) , the shift in descriptor use may also stem from non-temporal factors as well, such as an author's expectations of their audience (audience design) and additional information such as links to external news articles, included in the same sentence with the location (a micro-level aspect of the discussion)."}
{"sent_id": "a0af9cf22996a245af9d66cf1d358f-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_a0af9cf22996a245af9d66cf1d358f_14", "text": "Collective attention is an important component in the spread of information (Wu and Huberman 2007) , and it can shift either vary rapidly or gradually in response to particular events such as sports games (Lehmann et al. 2012) , natural disasters (Varga et al. 2013) , and political controversy (Garimella et al. 2017) ."}
{"sent_id": "a0af9cf22996a245af9d66cf1d358f-C001-151", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_a0af9cf22996a245af9d66cf1d358f_14", "text": "Here, the definition of peak in collective attention is critical, because it determines the point at which an entity is expected to become shared knowledge (Staliūnaitė et al. 2018) ."}
{"sent_id": "a0af9cf22996a245af9d66cf1d358f-C001-99", "intents": ["@USE@"], "paper_id": "ABC_a0af9cf22996a245af9d66cf1d358f_14", "text": "Following Staliūnaitė et al. (2018) , we used a small set of dependencies to capture the \"MODIFIER\" phrase type in a subclause (adjectival clause, appositional modifier, prepositional modifier, numeric modifier) and another set of dependencies to capture the \"COMPOUND\" type in a super-clause (nominal modifier, compound, appositional modifier)."}
{"sent_id": "8ff1560ac0241a763b4b0d93718b40-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_8ff1560ac0241a763b4b0d93718b40_14", "text": "From another point of view, two mainstreams for extracting relations in KBQA are studied: (a) using a classifier which chooses the most probable relation among all (Mohammed et al., 2018) ; (b) matching questions and relations through learning of an embedding space for representing all relations and question words (Bordes et al., 2015; Dai et al., 2016; Yin et al., 2016; He and Golub, 2016; Yu et al., 2017) , in which each relation is considered either as a meaningful sequence of words or as a unique entity."}
{"sent_id": "de9eb9b7dff69743252b3ff0ef8894-C001-10", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_de9eb9b7dff69743252b3ff0ef8894_14", "text": "Recent work demonstrated that word embeddings induced from large text collections encode many human biases (e.g., Bolukbasi et al., 2016; Caliskan et al., 2017) ."}
{"sent_id": "de9eb9b7dff69743252b3ff0ef8894-C001-27", "intents": ["@USE@"], "paper_id": "ABC_de9eb9b7dff69743252b3ff0ef8894_14", "text": "We first introduce the WEAT dataset (Caliskan et al., 2017) and then describe XWEAT, our multilingual and cross-lingual extension of WEAT designed for comparative bias analyses across languages and in cross-lingual embedding spaces."}
{"sent_id": "de9eb9b7dff69743252b3ff0ef8894-C001-122", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_de9eb9b7dff69743252b3ff0ef8894_14", "text": "To this end, we have extended previous analyses based on the WEAT test (Caliskan et al., 2017; McCurdy and Serbetci, 2017) in multiple dimensions: across seven languages, four embedding models, and three different types of text."}
{"sent_id": "de9eb9b7dff69743252b3ff0ef8894-C001-103", "intents": ["@SIM@"], "paper_id": "ABC_de9eb9b7dff69743252b3ff0ef8894_14", "text": "5 This is consistent with the original results obtained by Caliskan et al. (2017) ."}
{"sent_id": "e68d09937d522dc5acac9637eb2a8b-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_e68d09937d522dc5acac9637eb2a8b_14", "text": "The noun phrases in this data set are the same as in the Treebank and therefore the baseNPs in this data set are slightly different from the ones in the (Ramshaw and Marcus, 1995) data sets."}
{"sent_id": "e68d09937d522dc5acac9637eb2a8b-C001-141", "intents": ["@BACK@"], "paper_id": "ABC_e68d09937d522dc5acac9637eb2a8b_14", "text": "(Ramshaw and Marcus, 1995) have build a chunker by applying transformation-based learning to sections of the Penn Treebank."}
{"sent_id": "e68d09937d522dc5acac9637eb2a8b-C001-160", "intents": ["@DIF@"], "paper_id": "ABC_e68d09937d522dc5acac9637eb2a8b_14", "text": "They compare two data representations and report that a representation with bracket structures outperforms the IOB tagging representation introduced by (Ramshaw and Marcus, 1995) ."}
{"sent_id": "26b00c6e5b499eea30e9cef0bbaf9f-C001-8", "intents": ["@MOT@"], "paper_id": "ABC_26b00c6e5b499eea30e9cef0bbaf9f_14", "text": "Though Baroni et al. (2014) suggested that predictive models which use neural networks to generate the distributed word representations (also known as embeddings in this context) outperform counting models which work on co-occurrence matrices, recent work shows evidence to the contrary (Levy et al., 2014; Salle et al., 2016) ."}
{"sent_id": "26b00c6e5b499eea30e9cef0bbaf9f-C001-25", "intents": ["@USE@"], "paper_id": "ABC_26b00c6e5b499eea30e9cef0bbaf9f_14", "text": "with α = 3/4 (Mikolov et al., 2013b; Salle et al., 2016) , and #(w) the unigram frequency of w. Two methods were defined for the minimization of eqs. (2) and (3): Mini-batch and Stochastic (Salle et al., 2016) ."}
{"sent_id": "26b00c6e5b499eea30e9cef0bbaf9f-C001-79", "intents": ["@USE@"], "paper_id": "ABC_26b00c6e5b499eea30e9cef0bbaf9f_14", "text": "Therefore, we perform the exact same evaluation as Salle et al. (2016) , namely the WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001) , MEN (Bruni et al., 2012) , MTurk (Radinsky et al., 2011) , RW (Luong et al., 2013) , SimLex-999 (Hill et al., 2015) , MC (Miller and Charles, 1991) , RG (Rubenstein and Goodenough, 1965) , and SCWS (Huang et al., 2012) word similarity tasks 1 , and the Google semantic (GSem) and syntactic (GSyn) analogy (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c) tasks."}
{"sent_id": "9e8af6ca401cd74adc9a4137ae05ec-C001-4", "intents": ["@EXT@", "@MOT@"], "paper_id": "ABC_9e8af6ca401cd74adc9a4137ae05ec_14", "text": "Our model of pronunciation variation is used to extend a pronouncing dictionary for use in the spelling correction algorithm developed by Toutanova and Moore (2002), which includes models for both orthography and pronunciation."}
{"sent_id": "9e8af6ca401cd74adc9a4137ae05ec-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_9e8af6ca401cd74adc9a4137ae05ec_14", "text": "The spelling correction models from Brill and Moore (2000) and Toutanova and Moore (2002) use the noisy channel model approach to determine the types and weights of edit operations."}
{"sent_id": "17252628fa9c03c2fe0b44763fc7a2-C001-88", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_17252628fa9c03c2fe0b44763fc7a2_14", "text": "The training data, which included those data used in Wang et al. (2007) , contained 1 million pairs of sentences extracted from the Linguistic Data Consortium's parallel news corpora."}
{"sent_id": "ab8c43bf5a37c436d166960af459a8-C001-61", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_ab8c43bf5a37c436d166960af459a8_14", "text": "Rao and Tetreault (2018) used independent neural machine translation models for each formality transfer direction (informal→formal and formal→informal)."}
{"sent_id": "ab8c43bf5a37c436d166960af459a8-C001-154", "intents": ["@BACK@"], "paper_id": "ABC_ab8c43bf5a37c436d166960af459a8_14", "text": "Rao and Tetreault (2018) use a pre-processing step to make source informal sentences more formal and source formal sentences more informal by rules such as re-casing."}
{"sent_id": "ab8c43bf5a37c436d166960af459a8-C001-99", "intents": ["@USE@"], "paper_id": "ABC_ab8c43bf5a37c436d166960af459a8_14", "text": "FT data: We use the GYAFC corpus introduced by Rao and Tetreault (2018) as our FT data."}
{"sent_id": "ab8c43bf5a37c436d166960af459a8-C001-126", "intents": ["@USE@"], "paper_id": "ABC_ab8c43bf5a37c436d166960af459a8_14", "text": "Following Rao and Tetreault (2018), we assess model outputs on three criteria: formality, fluency and meaning preservation."}
{"sent_id": "4f111ff06afd5523d65fc1d1a9ff83-C001-11", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_4f111ff06afd5523d65fc1d1a9ff83_14", "text": "Previous work (MacCartney et al., 2008) has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006) ."}
{"sent_id": "4f111ff06afd5523d65fc1d1a9ff83-C001-121", "intents": ["@DIF@"], "paper_id": "ABC_4f111ff06afd5523d65fc1d1a9ff83_14", "text": "Our final system improves over the results reported in MacCartney et al. (2008) by about 4.5% in precision and 1% in recall, with a large gain in the number of perfect alignments over the test corpus."}
{"sent_id": "4f111ff06afd5523d65fc1d1a9ff83-C001-109", "intents": ["@SIM@", "@FUT@"], "paper_id": "ABC_4f111ff06afd5523d65fc1d1a9ff83_14", "text": "An examination of the alignments produced by our system reveals that many remaining errors can be tackled by the use of named-entity recognition and better paraphrase corpora; this was also noted by MacCartney et al. (2008) with regard to the original MANLI system."}
{"sent_id": "d8168f4596878807d22ddc7474ffc8-C001-4", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_d8168f4596878807d22ddc7474ffc8_14", "text": "We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) (Eskander et al., 2016) ."}
{"sent_id": "d8168f4596878807d22ddc7474ffc8-C001-45", "intents": ["@USE@"], "paper_id": "ABC_d8168f4596878807d22ddc7474ffc8_14", "text": "In this paper, we experiment with the grammars and the learning setups proposed by Eskander et al. (2016) , which we outline briefly below."}
{"sent_id": "d8168f4596878807d22ddc7474ffc8-C001-48", "intents": ["@USE@"], "paper_id": "ABC_d8168f4596878807d22ddc7474ffc8_14", "text": "For example, the PrStSu+SM grammar models the Table 2 : Sample grammar setups used by Eskander et al. (2018 Eskander et al. ( , 2016 ."}
{"sent_id": "d8168f4596878807d22ddc7474ffc8-C001-128", "intents": ["@USE@"], "paper_id": "ABC_d8168f4596878807d22ddc7474ffc8_14", "text": "We worked with the AG grammars developed by Eskander et al. (2016 Eskander et al. ( , 2018 for languages that are not polysynthetic."}
{"sent_id": "154fd8e6b625eb93da21c09906ee90-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_154fd8e6b625eb93da21c09906ee90_14", "text": "Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) [27] , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems."}
{"sent_id": "154fd8e6b625eb93da21c09906ee90-C001-126", "intents": ["@DIF@"], "paper_id": "ABC_154fd8e6b625eb93da21c09906ee90_14", "text": "In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike [27] we still see mild differentiation of variances."}
{"sent_id": "23119eff3cfd71370e8ad408fc75e1-C001-76", "intents": ["@BACK@"], "paper_id": "ABC_23119eff3cfd71370e8ad408fc75e1_15", "text": "F1 Our model (single) 67.8 without mention detection loss 67.5 without biaffine attention 67.4 Lee et al. (2017) 67.3 Table 2 : Ablation study on the development set."}
{"sent_id": "23119eff3cfd71370e8ad408fc75e1-C001-71", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_23119eff3cfd71370e8ad408fc75e1_15", "text": "Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017) ."}
{"sent_id": "c5ec401f42f79c4707770dac4f5013-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_c5ec401f42f79c4707770dac4f5013_15", "text": "• Inter-utterance Representations: Attention vs. Sequential Integration Xing et al. (2017) proposed a hierarchical attention mechanism to feed the utterance representations to a backward RNN to obtain contextual representation."}
{"sent_id": "c5ec401f42f79c4707770dac4f5013-C001-65", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c5ec401f42f79c4707770dac4f5013_15", "text": "weighing the importance of utterances for generating open-domain conversational responses (Xing et al., 2017) , we thus model the inter-utterance representation to obtain the context vector in two measures, namely static and dynamic attention, as shown in Figure 2 ."}
{"sent_id": "c5ec401f42f79c4707770dac4f5013-C001-82", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_c5ec401f42f79c4707770dac4f5013_15", "text": "Rather than use a hierarchical attention neural network (Xing et al., 2017) to obtain the contextual representation of a conversation, we propose two utterance-level attentions for weighting the importance of each utterance in the context, which is more simple in structure and has less number of parameters than the hierarchical attention approach."}
{"sent_id": "b7a718664f395f048abb3655fb1d8d-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_b7a718664f395f048abb3655fb1d8d_15", "text": "In some cases, the code was initially made available, then removed, and is now back online (Tang et al., 2016a) ."}
{"sent_id": "5fc7df69445712a50228d0bf80f30a-C001-6", "intents": ["@MOT@"], "paper_id": "ABC_5fc7df69445712a50228d0bf80f30a_15", "text": "In addition, we further show that this approach is also beneficial for small sample humor recognition tasks through a semi-supervised label propagation procedure, which achieves about 0.7 accuracy on the 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day (Yang et al., 2015) humour classification datasets using only 10% of known labels."}
{"sent_id": "5fc7df69445712a50228d0bf80f30a-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_5fc7df69445712a50228d0bf80f30a_15", "text": "Semantic features range from attempts to measure incongruity (Cattle and Ma, 2018; Shahaf et al., 2015; Yang et al., 2015) to the use of word embeddings as inputs to neural models (Bertero and Fung, 2016; Donahue et al., 2017) ."}
{"sent_id": "5fc7df69445712a50228d0bf80f30a-C001-45", "intents": ["@DIF@"], "paper_id": "ABC_5fc7df69445712a50228d0bf80f30a_15", "text": "Although learning-based methods have shown significant performance improvement recently (Yang et al., 2015) , one of their main bottlenecks is the lack of appropriate training corpora."}
{"sent_id": "5fc7df69445712a50228d0bf80f30a-C001-106", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_5fc7df69445712a50228d0bf80f30a_15", "text": "While Yang et al. (2015) uses a large portion of data for training and combine different features, we find that at similar portion of training data (90%), the results of our method are comparable to it."}
{"sent_id": "b13bc55709f5040cf100bd5f466ff2-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_b13bc55709f5040cf100bd5f466ff2_15", "text": "Since Tacotron [1] paved the way for end-to-end Text-To-Speech (TTS) using neural networks, researchers have attempted to generate more naturally sounding speech by conditioning a TTS model via speaker and prosody embedding [2, 3, 4, 5, 6] ."}
{"sent_id": "b13bc55709f5040cf100bd5f466ff2-C001-128", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_b13bc55709f5040cf100bd5f466ff2_15", "text": "For the quantitative comparison, we used the Mean Cepstral Distortion (MCD) with the first 13 MFCCs, as proposed in earlier work [4] ."}
{"sent_id": "d8a250a1a0495ee824837839b74f26-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_d8a250a1a0495ee824837839b74f26_15", "text": "Later work by Wang et al. (2015b) adopted a different strategy based on the similarity between the dependency parse of a sentence and the semantic AMR graph."}
{"sent_id": "d8a250a1a0495ee824837839b74f26-C001-15", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_d8a250a1a0495ee824837839b74f26_15", "text": "In our submission to SemEval Task 8 on AMR parsing, we follow the transition-based paradigm of Wang et al. (2015b) with modifications to the parsing algorithm, and also use the DAGGER imitation learning algorithm (Ross et al., 2011) to generalise better to unseen data."}
{"sent_id": "d8a250a1a0495ee824837839b74f26-C001-43", "intents": ["@DIF@"], "paper_id": "ABC_d8a250a1a0495ee824837839b74f26_15", "text": "Flanigan et al. (2014) and Wang et al. (2015b) , both use AMR fragments as their smallest unit, which may consist of more than one AMR concept."}
{"sent_id": "b0083488650bc98477fb10a9c5a808-C001-42", "intents": ["@SIM@", "@DIF@", "@EXT@"], "paper_id": "ABC_b0083488650bc98477fb10a9c5a808_15", "text": "Similar to [13] , we apply a 2DLSTM layer to combine the acoustic model (the LSTM encoder) and the language model (the decoder) without any attention components."}
{"sent_id": "b0083488650bc98477fb10a9c5a808-C001-77", "intents": ["@SIM@"], "paper_id": "ABC_b0083488650bc98477fb10a9c5a808_15", "text": "Similar to [13] , we then equip the network by a 2DLSTM layer to relate the encoder and the decoder states."}
{"sent_id": "b0083488650bc98477fb10a9c5a808-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_b0083488650bc98477fb10a9c5a808_15", "text": "Recently, the 2DLSTM layer also has been used for sequence-to-sequence modeling in machine translation [13] where it implicitly updates the source representation conditioned on the generated target words."}
{"sent_id": "da8f30113f1126a78cefed06a15076-C001-108", "intents": ["@BACK@"], "paper_id": "ABC_da8f30113f1126a78cefed06a15076_15", "text": "In the framework of Das et al. (2018) a binary reward is used which rewards the learner for the answer being wrong or correct."}
{"sent_id": "da8f30113f1126a78cefed06a15076-C001-132", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_da8f30113f1126a78cefed06a15076_15", "text": "Unlike Das et al. (2018) , we also train entity embeddings after initializing them with random values."}
{"sent_id": "6597d733f13b06f61cb653f86c4460-C001-26", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_6597d733f13b06f61cb653f86c4460_15", "text": "A measure of syntactic distance is the obvious next step: Nerbonne and Wiersma (2006) provide one such method."}
{"sent_id": "6597d733f13b06f61cb653f86c4460-C001-92", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6597d733f13b06f61cb653f86c4460_15", "text": "However, to find out if the value of R is significant, we must use a permutation test with a Monte Carlo technique described by Good (1995) , following closely the same usage by Nerbonne and Wiersma (2006) ."}
{"sent_id": "6597d733f13b06f61cb653f86c4460-C001-57", "intents": ["@DIF@"], "paper_id": "ABC_6597d733f13b06f61cb653f86c4460_15", "text": "Fortunately, this is not the case; Nerbonne and Wiersma (2006) generate N − 2 POS trigrams from each sentence of length N ; we generate N leaf-ancestor paths from each parsed sentence in the corpus."}
{"sent_id": "a7f4154081f4045390e662c6e6f3ac-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_a7f4154081f4045390e662c6e6f3ac_15", "text": "Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014) ."}
{"sent_id": "a7f4154081f4045390e662c6e6f3ac-C001-70", "intents": ["@EXT@", "@DIF@"], "paper_id": "ABC_a7f4154081f4045390e662c6e6f3ac_15", "text": "Adding tf-idf indicators is the only modification we made to the features of Durrett and Klein (2014) ."}
{"sent_id": "57e65909baf823ff00a9a10a64fffd-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_57e65909baf823ff00a9a10a64fffd_15", "text": "While not directly measuring bias, prior work has explored how annotation schemes and the identity of the annotators (Waseem, 2016 ) might be manipulated to help to avoid bias."}
{"sent_id": "57e65909baf823ff00a9a10a64fffd-C001-170", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_57e65909baf823ff00a9a10a64fffd_15", "text": "Given this result, and the gender biases identified in these data by Park et al. (2018), it not apparent that the purportedly expert annotators were any less biased than amateur annotators (Waseem, 2016) ."}
{"sent_id": "57e65909baf823ff00a9a10a64fffd-C001-137", "intents": ["@SIM@"], "paper_id": "ABC_57e65909baf823ff00a9a10a64fffd_15", "text": "For Waseem (2016) we see that there is no significant difference in the estimated rates at which tweets are classified as racist across groups, although the rates remain low."}
{"sent_id": "c3f6140bd69d1eef0124665e651c0c-C001-15", "intents": ["@DIF@"], "paper_id": "ABC_c3f6140bd69d1eef0124665e651c0c_15", "text": "On the proverb data, the novel features result in compact models that significantly outperform existing features designed for word-level metaphor detection in other genres (Klebanov et al., 2014) , such as news and essays."}
{"sent_id": "c3f6140bd69d1eef0124665e651c0c-C001-125", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_c3f6140bd69d1eef0124665e651c0c_15", "text": "In particular, our implementation of the B features performs better than reported by Klebanov et al. (2014) on all four genres, namely: 0.52 vs. 0.51 for \"news\", 0.51 vs. 0.28 for \"academic\", 0.39 vs. 0.28 for \"conversation\" and 0.42 vs. 0.33 for \"fiction\"."}
{"sent_id": "c3f6140bd69d1eef0124665e651c0c-C001-43", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c3f6140bd69d1eef0124665e651c0c_15", "text": "To obtain results more easily comparable with Klebanov et al. (2014), we use the same classifier, i.e., logistic regression, in the implementation bundled with the scikit-learn package (Pedregosa et al., 2011) ."}
{"sent_id": "34b73a56bd9b80dc415ca2c5608596-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_34b73a56bd9b80dc415ca2c5608596_15", "text": "For example, the historical biases presented in (Garg et al., 2018) are computed using decade-specific word embeddings produced by training different Word2Vec (Mikolov et al., 2013 ) models on a large corpus of historical text from that decade."}
{"sent_id": "34b73a56bd9b80dc415ca2c5608596-C001-45", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_34b73a56bd9b80dc415ca2c5608596_15", "text": "To validate our model, we compare our results to those produced via the decade-by-decade models trained in (Garg et al., 2018) using the Corpus of Historical American English (Davies, 2010) ."}
{"sent_id": "34b73a56bd9b80dc415ca2c5608596-C001-100", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_34b73a56bd9b80dc415ca2c5608596_15", "text": "We validated our model by replicating gender and ethnic stereotypes produced in (Garg et al., 2018) by training multiple word embedding models and applied it to a novel corpus of talk radio data to analyze how perceptions of refugees as \"outsiders\" vary by geography and over time."}
{"sent_id": "34b73a56bd9b80dc415ca2c5608596-C001-51", "intents": ["@DIF@"], "paper_id": "ABC_34b73a56bd9b80dc415ca2c5608596_15", "text": "Qualitative inspection of Figure 2 suggests that our model also produces smoother decade-by-decade scores, suggesting that it not only identifies attribute- (Garg et al., 2018) and our model (blue dotted and green dashed lines, respectively) compared to actual workforce participation rates (solid lines) for gender (top) and Asian/White (bottom) linguistic biases."}
{"sent_id": "9ea14a9fe422451901ad221bee5714-C001-50", "intents": ["@SIM@"], "paper_id": "ABC_9ea14a9fe422451901ad221bee5714_15", "text": "In Sanskrit, Krishna et al. (2016) proposed a similar statistical approach which combined lexical and distributional information by using information from the lexical network Amarakos ."}
{"sent_id": "9ea14a9fe422451901ad221bee5714-C001-143", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_9ea14a9fe422451901ad221bee5714_15", "text": "We have used the same experimental setting as Krishna et al. (2016) for the classification task."}
{"sent_id": "b86a5a8ec1f27354057bb45ff27588-C001-117", "intents": ["@DIF@"], "paper_id": "ABC_b86a5a8ec1f27354057bb45ff27588_15", "text": "Contrary to (Elfardy and Diab, 2013) we find that those features do not improve accuracy of our best model in the cross-validation experiments."}
{"sent_id": "b86a5a8ec1f27354057bb45ff27588-C001-214", "intents": ["@DIF@"], "paper_id": "ABC_b86a5a8ec1f27354057bb45ff27588_15", "text": "Similarly, we use linear SVMs to train a classification model at the sentence level without access to sentence length statistics, i.e. our best performing classifier does not compute features like the percentage of punctuation, numbers, or averaged word length as has been proposed previously (Elfardy and Diab, 2013) ."}
{"sent_id": "4d528117dd7751d0cd6413430e1ec1-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_4d528117dd7751d0cd6413430e1ec1_15", "text": "Zhang et al. (2017) apply adversarial training to align monolingual word vector spaces with no supervision."}
{"sent_id": "4d528117dd7751d0cd6413430e1ec1-C001-81", "intents": ["@SIM@"], "paper_id": "ABC_4d528117dd7751d0cd6413430e1ec1_15", "text": "In the first part, we conduct experiments on smallscale datasets and our main baseline is Zhang et al. (2017) ."}
{"sent_id": "4d528117dd7751d0cd6413430e1ec1-C001-103", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4d528117dd7751d0cd6413430e1ec1_15", "text": "Again, we use the same dataset with Zhang et al. (2017) . and the statistics are shown in The experimental results are shown in Table 4 ."}
{"sent_id": "70dc108166d6b5fb9da39c451c3229-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_70dc108166d6b5fb9da39c451c3229_16", "text": "Table 1 : Examples of claims, the extracted evidence from Wikipedia and the verdicts from the shared task dataset (Thorne et al., 2018) The baseline system described by Thorne et al. (2018) uses 3 major components:"}
{"sent_id": "70dc108166d6b5fb9da39c451c3229-C001-69", "intents": ["@DIF@"], "paper_id": "ABC_70dc108166d6b5fb9da39c451c3229_16", "text": "Unlike Thorne et al. (2018) , we did not concatenate evidences, but trained our model for each claim-evidence pair."}
{"sent_id": "195f41862b929318787aad9d8e5a1c-C001-9", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_195f41862b929318787aad9d8e5a1c_16", "text": "In this technical report, we analyze the inference bottlenecks of FusionNet [12] and introduce FastFusionNet that tackles them."}
{"sent_id": "195f41862b929318787aad9d8e5a1c-C001-131", "intents": ["@DIF@"], "paper_id": "ABC_195f41862b929318787aad9d8e5a1c_16", "text": "[12] here since our reimplementation is about 0.5% F1 score worse."}
{"sent_id": "195f41862b929318787aad9d8e5a1c-C001-78", "intents": ["@SIM@"], "paper_id": "ABC_195f41862b929318787aad9d8e5a1c_16", "text": "The Question-Context Attention Layer is a fully-aware attention module [12] which takes the history (concatenation of GloVe, low-level, and high-level features) of each context word and question words as query and key for three attention modules, and represents each context word as three different vectors:"}
{"sent_id": "f587fc2bbbf3c1327b03d556e4bc05-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_f587fc2bbbf3c1327b03d556e4bc05_16", "text": "Most of the presented works study the interrelationship between words in a text snip- pet (Hill et al., 2016; Kiros et al., 2015; Le and Mikolov, 2014) in an unsupervised fashion."}
{"sent_id": "f587fc2bbbf3c1327b03d556e4bc05-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_f587fc2bbbf3c1327b03d556e4bc05_16", "text": "We show a toy example to highlight the differences between DoCoV vector, the Mean vector and paragraph vector (Le and Mikolov, 2014) ."}
{"sent_id": "f587fc2bbbf3c1327b03d556e4bc05-C001-168", "intents": ["@DIF@"], "paper_id": "ABC_f587fc2bbbf3c1327b03d556e4bc05_16", "text": "We further observe that DoCoV is consistently better than the paragraph vectors (Le and Mikolov, 2014) , Fastsent and SDAE (Hill et al., 2016) ."}
{"sent_id": "6683d7b77f536b93416d985414afeb-C001-27", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_6683d7b77f536b93416d985414afeb_16", "text": "Moreover, as done by Barbieri et al. (2017) , we considered only the posts which include one and only one of the 20 most frequent emojis (the most frequent emojis are shown in Table 3 )."}
{"sent_id": "6683d7b77f536b93416d985414afeb-C001-66", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6683d7b77f536b93416d985414afeb_16", "text": "Moreover, we evaluate a multimodal combination of both models respectively based on visual and Barbieri et al. (2017) , using the same Twitter dataset."}
{"sent_id": "6683d7b77f536b93416d985414afeb-C001-100", "intents": ["@SIM@"], "paper_id": "ABC_6683d7b77f536b93416d985414afeb_16", "text": "Another relevant confusion scenario related to emoji prediction has been spotted by Barbieri et al. (2017) : relying on Twitter textual data they showed that the emoji was hard to predict as it was used similarly to ."}
{"sent_id": "2adb3a645a57b8f441a80bb5a46045-C001-162", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_2adb3a645a57b8f441a80bb5a46045_16", "text": "The choice of machine learning model was primarily based on that a linear support vector machine was successful on data from the previously mentioned shared task of stance detection of tweets (Mohammad et al., 2017) ."}
{"sent_id": "6a90ecc147618b3909609fc6c2e2b3-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_6a90ecc147618b3909609fc6c2e2b3_16", "text": "Previous work has used Bayesian HMMs to learn taggers for both POS tagging and supertagging (Baldridge, 2008) separately."}
{"sent_id": "6a90ecc147618b3909609fc6c2e2b3-C001-79", "intents": ["@DIF@"], "paper_id": "ABC_6a90ecc147618b3909609fc6c2e2b3_16", "text": "The accuracy of our HMM is lower than the performance of Baldridge (2008) for supertags."}
{"sent_id": "6a90ecc147618b3909609fc6c2e2b3-C001-119", "intents": ["@DIF@"], "paper_id": "ABC_6a90ecc147618b3909609fc6c2e2b3_16", "text": "However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of Baldridge (2008) ."}
{"sent_id": "6a90ecc147618b3909609fc6c2e2b3-C001-127", "intents": ["@FUT@"], "paper_id": "ABC_6a90ecc147618b3909609fc6c2e2b3_16", "text": "We expect that CCG transition rules (Baldridge, 2008) when encoded as category specific transition priors, will lead to better performance with the FHMMs."}
{"sent_id": "6a90ecc147618b3909609fc6c2e2b3-C001-130", "intents": ["@EXT@"], "paper_id": "ABC_6a90ecc147618b3909609fc6c2e2b3_16", "text": "This paper follows the work of Duh (2005) , Baldridge (2008) and Goldwater and Griffiths (2007) ."}
{"sent_id": "e7c947a02bb0e81d6b6b4b9da74024-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_e7c947a02bb0e81d6b6b4b9da74024_16", "text": "Bolukbasi et al. (2016b) define the gender bias of a word w by its projection on the \"gender direction\": − → w · ( − → he − −→ she), assuming all vectors are normalized."}
{"sent_id": "e7c947a02bb0e81d6b6b4b9da74024-C001-64", "intents": ["@SIM@"], "paper_id": "ABC_e7c947a02bb0e81d6b6b4b9da74024_16", "text": "We refer to the word embeddings of the previous works as HARD-DEBIASED (Bolukbasi et al., 2016b) and GN-GLOVE (gender-neutral GloVe) counterparts in a predefined set."}
{"sent_id": "09dad2fd96cd1d48936cd5b99a38e7-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_09dad2fd96cd1d48936cd5b99a38e7_16", "text": "A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class (Rosenberg and Hirschberg, 2007) ."}
{"sent_id": "09dad2fd96cd1d48936cd5b99a38e7-C001-71", "intents": ["@BACK@"], "paper_id": "ABC_09dad2fd96cd1d48936cd5b99a38e7_16", "text": "However, F-Score suffers from the matching problem, which manifests itself either by not evaluating the entire membership of a cluster, or by not evaluating every cluster (Rosenberg and Hirschberg, 2007) ."}
{"sent_id": "09dad2fd96cd1d48936cd5b99a38e7-C001-141", "intents": ["@SIM@"], "paper_id": "ABC_09dad2fd96cd1d48936cd5b99a38e7_16", "text": "This is due to the fact that V-measure considers as the worst solution in terms of completeness the one, in which each class is represented by every cluster, and specifically with a distribution equal to the distribution of cluster sizes (Rosenberg and Hirschberg, 2007) ."}
{"sent_id": "7616f6f8c1c188b32cd3a8374b61dd-C001-25", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_7616f6f8c1c188b32cd3a8374b61dd_16", "text": "Rather, we will follow a more hierarchical annotation style for NPs that has been proposed by Vadas and Curran (2007) and that provides an easier interface for semantic interpretation."}
{"sent_id": "7616f6f8c1c188b32cd3a8374b61dd-C001-70", "intents": ["@BACK@"], "paper_id": "ABC_7616f6f8c1c188b32cd3a8374b61dd_16", "text": "Using Bikel (2004)'s parser, Vadas and Curran (2007) report that the parsing results slightly decrease when the parser is trained on the Penn Treebank with the modified annotation style for NPs."}
{"sent_id": "2bb41cea97a0375f67eab3a77c3a97-C001-69", "intents": ["@SIM@"], "paper_id": "ABC_2bb41cea97a0375f67eab3a77c3a97_16", "text": "At each step of the distant supervision process, we closely follow the recent literature (Mintz et al., 2009; ."}
{"sent_id": "2bb41cea97a0375f67eab3a77c3a97-C001-77", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_2bb41cea97a0375f67eab3a77c3a97_16", "text": "Following recent work (Mintz et al., 2009; Hoffmann et al., 2011) , we use Freebase 5 as the knowledge base for seed facts."}
{"sent_id": "2bb41cea97a0375f67eab3a77c3a97-C001-136", "intents": ["@DIF@"], "paper_id": "ABC_2bb41cea97a0375f67eab3a77c3a97_16", "text": "Interestingly, the Freebase held-out metric (Mintz et al., 2009; Hoffmann et al., 2011 ) turns out to be heavily biased toward distantly labeled data (e.g., increasing human feedback hurts precision; see Section 4.6)."}
{"sent_id": "4f75f73b4eac8aecdde9312a846a1d-C001-33", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_4f75f73b4eac8aecdde9312a846a1d_16", "text": "Our APE system extends this transformer-based NMT architecture (Vaswani et al., 2017) by using two encoders, a joint encoder, and a single decoder."}
{"sent_id": "4f75f73b4eac8aecdde9312a846a1d-C001-165", "intents": ["@DIF@"], "paper_id": "ABC_4f75f73b4eac8aecdde9312a846a1d_16", "text": "Unfortunately, we could not test either the 'big' or the 'base' hyper-parameter configuration in Vaswani et al. (2017) due to unavailable computing resources at the time of submission."}
{"sent_id": "c5a10f46c253f0da005622661b12a1-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_c5a10f46c253f0da005622661b12a1_16", "text": "Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm."}
{"sent_id": "c5a10f46c253f0da005622661b12a1-C001-48", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_c5a10f46c253f0da005622661b12a1_16", "text": "Therefore, we implemented our system by adapting the state-of-the-art fully-supervised event extraction system of Li et al. (2013) , modifying mechanisms relevant for features and for trigger labels, as described below."}
{"sent_id": "c5a10f46c253f0da005622661b12a1-C001-52", "intents": ["@SIM@"], "paper_id": "ABC_c5a10f46c253f0da005622661b12a1_16", "text": "The event extraction system of Li et al. (2013) labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided."}
{"sent_id": "4da1c39dbbeaa2c9dac22118d0c698-C001-14", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_4da1c39dbbeaa2c9dac22118d0c698_16", "text": "The system is based on previous work by Barbu (2015) and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification."}
{"sent_id": "4da1c39dbbeaa2c9dac22118d0c698-C001-27", "intents": ["@SIM@"], "paper_id": "ABC_4da1c39dbbeaa2c9dac22118d0c698_16", "text": "To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as \"a neglected research area\" (Barbu, 2015) ."}
{"sent_id": "4da1c39dbbeaa2c9dac22118d0c698-C001-91", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4da1c39dbbeaa2c9dac22118d0c698_16", "text": "More importantly, however, we compared our system to Barbu's (2015) approach, using the classification algorithms which reportedly worked best with the 17 features in his work."}
{"sent_id": "4da1c39dbbeaa2c9dac22118d0c698-C001-100", "intents": ["@SIM@"], "paper_id": "ABC_4da1c39dbbeaa2c9dac22118d0c698_16", "text": "Again, we compared our system's performance to Barbu's (2015) method, using 2 /3-1 /3 splits of the training data (5-fold cross-validation)."}
{"sent_id": "fa5413db2c8e0a32bc3805d25cd0e7-C001-16", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_fa5413db2c8e0a32bc3805d25cd0e7_16", "text": "Previous approaches benefit mostly from crosslingual n-gram embeddings, but recent work proves that cross-lingual language model pretraining could be a more effective way to build initial unsupervised machine translation models (Lample and Conneau, 2019) ."}
{"sent_id": "43a52325987ea035136a6a718389d9-C001-82", "intents": ["@BACK@"], "paper_id": "ABC_43a52325987ea035136a6a718389d9_16", "text": "Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew and Schulte im Walde, 2002; Sun and Korhonen, 2009 ) and other similar NLP tasks involving high dimensional feature space (Chen et al., 2006) ."}
{"sent_id": "43a52325987ea035136a6a718389d9-C001-155", "intents": ["@SIM@"], "paper_id": "ABC_43a52325987ea035136a6a718389d9_16", "text": "Those results suggest that when 2000 or more occurrences per verb are used, most features perform like they performed for English in the experiment of Sun and Korhonen (2009) which is not typical to many other classes."}
{"sent_id": "43a52325987ea035136a6a718389d9-C001-180", "intents": ["@DIF@"], "paper_id": "ABC_43a52325987ea035136a6a718389d9_16", "text": "When considering the general level of performance, our best performance for French (65.4 F) is lower than the best performance for English in the experiment of Sun and Korhonen (2009) ."}
{"sent_id": "43a52325987ea035136a6a718389d9-C001-186", "intents": ["@FUT@"], "paper_id": "ABC_43a52325987ea035136a6a718389d9_16", "text": "However, parser and feature extraction performance can also play a big role in overall accuracy, and should therefore be investigated further (Sun and Korhonen, 2009 )."}
{"sent_id": "be67496882917c2a44afb42e6f9f15-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_be67496882917c2a44afb42e6f9f15_16", "text": "Copynet (Gu et al., 2016) and pointer-generator networks (Vinyals et al., 2015) , for example, aim to reduce input-output vocabulary mismatch and, thereby, improve specificity, while the coveragebased techniques of Tu et al. (2016) tackle repetition and under-generation."}
{"sent_id": "be67496882917c2a44afb42e6f9f15-C001-90", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_be67496882917c2a44afb42e6f9f15_16", "text": "Accordingly, we compare our Scratchpad Mechanism against three baselines: (1) Seq2Seq, (2) Copynet and (3) Coverage, a method introduced by Tu et al. (2016) that aims to solve attention-related problems."}
{"sent_id": "be67496882917c2a44afb42e6f9f15-C001-193", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_be67496882917c2a44afb42e6f9f15_16", "text": "Attention Closest to our work, in the general paradigm of seq2seq learning, is the coverage mechanism introduced in Tu et al. (2016) and later adapted for summarization in See et al. (2017) ."}
{"sent_id": "ca7db62af4457ca887fe220c43b10e-C001-2", "intents": ["@DIF@"], "paper_id": "ABC_ca7db62af4457ca887fe220c43b10e_16", "text": "If we compare the widely used Conditional Random Fields (CRF) with newly proposed \"deep architecture\" sequence models (Collobert et al., 2011) , there are two things changing: from linear architecture to non-linear, and from discrete feature representation to distributional."}
{"sent_id": "ca7db62af4457ca887fe220c43b10e-C001-98", "intents": ["@SIM@"], "paper_id": "ABC_ca7db62af4457ca887fe220c43b10e_16", "text": "However, overall we found that the feature set we used is competitive with CRF results from earlier literature (Turian et al., 2010; Collobert et al., 2011) ."}
{"sent_id": "f3012301e42a4075ed6d4d2b39b528-C001-22", "intents": ["@DIF@"], "paper_id": "ABC_f3012301e42a4075ed6d4d2b39b528_16", "text": "• Our sarcasm detection system outperforms two state-of-art sarcasm detection systems (Riloff et al., 2013; Maynard and Greenwood, 2014) ."}
{"sent_id": "f3012301e42a4075ed6d4d2b39b528-C001-100", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_f3012301e42a4075ed6d4d2b39b528_16", "text": "For this, we modify the algorithm given in Riloff et al. (2013) in two ways: (a) they extract only positive verbs and negative noun situation phrases."}
{"sent_id": "f3012301e42a4075ed6d4d2b39b528-C001-126", "intents": ["@DIF@"], "paper_id": "ABC_f3012301e42a4075ed6d4d2b39b528_16", "text": "Table 4 shows that we achieve a 10% higher F-score than the best reported F-score of Riloff et al. (2013) ."}
{"sent_id": "c2952b2da147d5f128cdbd5d8074a5-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_c2952b2da147d5f128cdbd5d8074a5_16", "text": "While the seminal work by Bolukbasi et al. (2016a Bolukbasi et al. ( , 2016b concerns the identification and mitigation of gender bias in pretrained word embeddings, Zhao et al. (2018) provide insights into the possibilities of learning embeddings that are gender neutral."}
{"sent_id": "c2952b2da147d5f128cdbd5d8074a5-C001-39", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c2952b2da147d5f128cdbd5d8074a5_16", "text": "We also provide further evidence of the inability of the debiasing method proposed by Bolukbasi et al. (2016b) to handle the type of bias we are concerned with."}
{"sent_id": "c2952b2da147d5f128cdbd5d8074a5-C001-101", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c2952b2da147d5f128cdbd5d8074a5_16", "text": "We use the same methodology for growing a seed set of gender specific words into a larger set as described in (Bolukbasi et al., 2016b) , and end up with 486 manually curated gender specific words, including e.g., farfar (paternal grandfather), tvillingsystrar (twin sisters), and matriark (matriarch)."}
{"sent_id": "c2952b2da147d5f128cdbd5d8074a5-C001-141", "intents": ["@USE@"], "paper_id": "ABC_c2952b2da147d5f128cdbd5d8074a5_16", "text": "We now turn to investigate the effect the hard debiasing operation has on the embedding spaces, using the intrinsic evaluation methodology of Bolukbasi et al. (2016b) ."}
{"sent_id": "c2952b2da147d5f128cdbd5d8074a5-C001-156", "intents": ["@DIF@"], "paper_id": "ABC_c2952b2da147d5f128cdbd5d8074a5_16", "text": "The number of stereotypical analogy pairs output by the Swedish models is small compared to the numbers reported by Bolukbasi et al. (2016b) ."}
{"sent_id": "f29baa099b13f38badeb4cbd8789f6-C001-226", "intents": ["@USE@"], "paper_id": "ABC_f29baa099b13f38badeb4cbd8789f6_17", "text": "At first we build a classifier using two readability models from Sinha et al(2012) ."}
{"sent_id": "b0a50145121eb797cf8e6ebc2f49e0-C001-90", "intents": ["@BACK@"], "paper_id": "ABC_b0a50145121eb797cf8e6ebc2f49e0_17", "text": "Durrani et al. (2011) recently addressed these problems by proposing an operation sequence Ngram model which strongly couples translation and reordering, hypothesizes all possible reorderings and does not require POS-based rules."}
{"sent_id": "b0a50145121eb797cf8e6ebc2f49e0-C001-111", "intents": ["@USE@"], "paper_id": "ABC_b0a50145121eb797cf8e6ebc2f49e0_17", "text": "Several count-based features such as gap and open gap penalties and distance-based features such as gap-width and reordering distance are added to the model, along with the lexical weighting and length penalty features in a standard log-linear framework (Durrani et al., 2011) ."}
{"sent_id": "b0a50145121eb797cf8e6ebc2f49e0-C001-168", "intents": ["@EXT@"], "paper_id": "ABC_b0a50145121eb797cf8e6ebc2f49e0_17", "text": "We extended the training steps in Durrani et al. (2011) to extract a phrase lexicon from the parallel data."}
{"sent_id": "b0a50145121eb797cf8e6ebc2f49e0-C001-177", "intents": ["@EXT@"], "paper_id": "ABC_b0a50145121eb797cf8e6ebc2f49e0_17", "text": "We extended the decoder developed by Durrani et al. (2011) and tried three ideas."}
{"sent_id": "fed51218e78d35aae39d287c95a95a-C001-106", "intents": ["@DIF@"], "paper_id": "ABC_fed51218e78d35aae39d287c95a95a_17", "text": "While Wiegand and Klakow (2010) made use of manually built lexicons, we use our predictive predicates extracted from contexts of protoOHs."}
{"sent_id": "c7778abb2f1890ba896ccef2c3e13b-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_c7778abb2f1890ba896ccef2c3e13b_17", "text": "Recently, with the arrival of large-scale collections of historic texts and online libraries such as Google books, a new paradigm has been added to this research area, whereby the prime interest is in identifying the temporal scope of a sense [10, 14, 16, 25] which, in turn, can give further insights to the phenomenon of language evolution."}
{"sent_id": "c7778abb2f1890ba896ccef2c3e13b-C001-33", "intents": ["@USE@"], "paper_id": "ABC_c7778abb2f1890ba896ccef2c3e13b_17", "text": "We then explore another unsupervised approach presented in Lau et al. [16] over the same Google books corpus 1 , apply topic modeling for sense induction and directly adapt their similarity measure to get the new senses."}
{"sent_id": "c7778abb2f1890ba896ccef2c3e13b-C001-136", "intents": ["@USE@"], "paper_id": "ABC_c7778abb2f1890ba896ccef2c3e13b_17", "text": "For the same 100 random samples, we now use the outputs of Lau et al. [16] and the proposed approach, and estimate the precision as well as recall of these."}
{"sent_id": "730738d63cabcd4e63ec4300a8091b-C001-47", "intents": ["@USE@"], "paper_id": "ABC_730738d63cabcd4e63ec4300a8091b_17", "text": "Global learning is implemented in the same way as Zhang and Nivre (2011) , using the averaged perceptron algorithm (Collins, 2002) and early update (Collins and Roark, 2004) ."}
{"sent_id": "f4792ef9808a1a3c415f6f57351335-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_f4792ef9808a1a3c415f6f57351335_17", "text": "In Sporleder and Lapata (2005) , they were primarily interested in labeled segmentation, where the segment initial boundary was labeled with the segment type."}
{"sent_id": "f4792ef9808a1a3c415f6f57351335-C001-54", "intents": ["@USE@"], "paper_id": "ABC_f4792ef9808a1a3c415f6f57351335_17", "text": "First, in Section 2.3, we compare SPADE results under our configuration with results from Sporleder and Lapata (2005) in order to establish comparability, and this is done on their 608 sentence subset."}
{"sent_id": "e3c735811b2ea08d92659272ddcbdd-C001-20", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_e3c735811b2ea08d92659272ddcbdd_17", "text": "To help agencies monitor gang activity on social media, our past work investigated how features from Twitter profiles, including profile text, profile images, tweet text, emjoi use, and their links to YouTube, may be used to reliably find gang member profiles [BWDS16] ."}
{"sent_id": "e3c735811b2ea08d92659272ddcbdd-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_e3c735811b2ea08d92659272ddcbdd_17", "text": "Gang member tweets and profile descriptions tend to have few textual indicators that demonstrate their gang affiliations or their tweets/profile text may carry acronyms which can only be deciphered by others involved in gang culture [BWDS16] ."}
{"sent_id": "ae67018df3a74e0fd4ae90522499a3-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_ae67018df3a74e0fd4ae90522499a3_17", "text": "Dehghani et al. (2017) use BM25 to obtain relevant documents for a large set of AOL queries (Pass et al., 2006) which are then used as weakly supervised signals for joint embedding and ranking model training."}
{"sent_id": "ae67018df3a74e0fd4ae90522499a3-C001-162", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_ae67018df3a74e0fd4ae90522499a3_17", "text": "• NRMS: It is a weakly-supervised neural IR model learned with automatically annotated querydocument pairs (Dehghani et al., 2017) ."}
{"sent_id": "ae67018df3a74e0fd4ae90522499a3-C001-65", "intents": ["@SIM@"], "paper_id": "ABC_ae67018df3a74e0fd4ae90522499a3_17", "text": "employ similar supervision signals as (Dehghani et al., 2017) to train an embedding network similar to Word2vec and use the obtained embeddings for query expansion and query classification."}
{"sent_id": "ae67018df3a74e0fd4ae90522499a3-C001-158", "intents": ["@SIM@"], "paper_id": "ABC_ae67018df3a74e0fd4ae90522499a3_17", "text": "More importantly, since our model learns from weakly supervised signals by BM25, we are more interested in the comparisons to BM25 and similar models using weakly supervised signals, an experimental strategy also employed in (Dehghani et al., 2017) ."}
{"sent_id": "ae67018df3a74e0fd4ae90522499a3-C001-141", "intents": ["@DIF@"], "paper_id": "ABC_ae67018df3a74e0fd4ae90522499a3_17", "text": "In order to build the labeled query-document pairs for supervised learning, we choose to use the more general methodology in (Gupta et al., 2017) instead of the one in (Dehghani et al., 2017) to relieve from data (i.e. AOL queries) only available from industrial labs."}
{"sent_id": "9dd9ac975c6f55797615f0e52aa296-C001-35", "intents": ["@USE@"], "paper_id": "ABC_9dd9ac975c6f55797615f0e52aa296_17", "text": "In order to make it compatible with the previous work, we follow the procedure in (Nguyen and Grishman, 2015b) to process the trigger candidates for CNN."}
{"sent_id": "9dd9ac975c6f55797615f0e52aa296-C001-89", "intents": ["@USE@"], "paper_id": "ABC_9dd9ac975c6f55797615f0e52aa296_17", "text": "All the data preprocessing and evaluation criteria follow those in (Nguyen and Grishman, 2015b)."}
{"sent_id": "9dd9ac975c6f55797615f0e52aa296-C001-107", "intents": ["@USE@"], "paper_id": "ABC_9dd9ac975c6f55797615f0e52aa296_17", "text": "In this section, we compare NC-CNN with the CNN model in (Nguyen and Grishman, 2015b) (as well as the other models above) in the DA setting to further investigate their effectiveness."}
{"sent_id": "9dd9ac975c6f55797615f0e52aa296-C001-100", "intents": ["@DIF@"], "paper_id": "ABC_9dd9ac975c6f55797615f0e52aa296_17", "text": "Finally, comparing NC-CNN and the CNN model in (Nguyen and Grishman, 2015b), we see that the non-consecutive mechanism significantly improves the performance of the traditional CNN model for ED (up to 2.3% in absolute Fmeasures with p < 0.05)."}
{"sent_id": "8084b5077b2a8db755b1bbd0f6fe60-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_8084b5077b2a8db755b1bbd0f6fe60_17", "text": "There are some interesting problems around written language identification that have attracted some attention recently, as native language identification (NLI, Tetreault et al., 2013) , the identification of the country of origin or the discrimination between similar or closely related languages (DSL, Tiedemann and Ljubešić, 2012) ."}
{"sent_id": "2a84615479af66bbf875517a3a753b-C001-84", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_2a84615479af66bbf875517a3a753b_17", "text": "Previous research used multi-modal information independently using neural network model by concatenating features from each modality [7, 21] ."}
{"sent_id": "2a84615479af66bbf875517a3a753b-C001-112", "intents": ["@EXT@"], "paper_id": "ABC_2a84615479af66bbf875517a3a753b_17", "text": "As this research is extended work from previous research [7] , we use the same feature extraction method as done in our previous work."}
{"sent_id": "2a84615479af66bbf875517a3a753b-C001-125", "intents": ["@USE@"], "paper_id": "ABC_2a84615479af66bbf875517a3a753b_17", "text": "We use the same dataset and features as other researchers [7, 18] ."}
{"sent_id": "d2b9c678a3d4920919f59c3b5903d3-C001-115", "intents": ["@DIF@"], "paper_id": "ABC_d2b9c678a3d4920919f59c3b5903d3_17", "text": "Note that the numbers of Vinyals et al. (2015) and Luong et al. (2016) are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees."}
{"sent_id": "bd2a718f75d206ef3f2cb5648585d5-C001-137", "intents": ["@USE@"], "paper_id": "ABC_bd2a718f75d206ef3f2cb5648585d5_17", "text": "On one hand, we compared the mean of all 10 crowd ratings to the mean of the three ratings of Wachsmuth et al. (2017a) ."}
{"sent_id": "489d0077e05269327e7fe4e7f7e4a3-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_489d0077e05269327e7fe4e7f7e4a3_17", "text": "Carvalho et al. [3] proposed a similar multi-modal embedding method for aligning text and image representations in a shared latent space."}
{"sent_id": "489d0077e05269327e7fe4e7f7e4a3-C001-127", "intents": ["@SIM@"], "paper_id": "ABC_489d0077e05269327e7fe4e7f7e4a3_17", "text": "Similarly to [17] and [3] , we evaluated our model on 10 subsets of 1000 samples each."}
{"sent_id": "4e1b01c1faebc447891bc0b847316d-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_4e1b01c1faebc447891bc0b847316d_17", "text": "More recently, Lau et al. (2014) proposed a methodology to automate the word intrusion task directly."}
{"sent_id": "197b557d7b5c7c2d195be84990719b-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_197b557d7b5c7c2d195be84990719b_17", "text": "Mikolov et al. (2013) provide the intuition that word vectors can be summed together to form a semantically meaningful combination of both words."}
{"sent_id": "197b557d7b5c7c2d195be84990719b-C001-40", "intents": ["@USE@"], "paper_id": "ABC_197b557d7b5c7c2d195be84990719b_17", "text": "As in Mikolov et al. (2013) , pairs of pivot and target words (j, i) are extracted when they cooccur in a moving window scanning across the corpus."}
{"sent_id": "9c5baf669470fe4dd18277591591f1-C001-23", "intents": ["@EXT@"], "paper_id": "ABC_9c5baf669470fe4dd18277591591f1_17", "text": "We first take a state-of-the-art neural network for parsing time normalizations (Laparra et al., 2018a) and replace its randomly initialized character embeddings with pre-trained contextual character embeddings."}
{"sent_id": "0fd26c6dffab3fba2d120d2c58dff6-C001-146", "intents": ["@SIM@"], "paper_id": "ABC_0fd26c6dffab3fba2d120d2c58dff6_17", "text": "In audio-BRE ( Fig. 2(a) ), most of the emotion labels are frequently misclassified as neutral class, supporting the claims of [7, 25] ."}
{"sent_id": "397e593f8f282d4951402d83036c12-C001-11", "intents": ["@USE@"], "paper_id": "ABC_397e593f8f282d4951402d83036c12_17", "text": "On top of Theano (Bergstra et al., 2010) , THUMT implements the standard attention-based encoder-decoder framework for NMT (Bahdanau et al., 2015) ."}
{"sent_id": "397e593f8f282d4951402d83036c12-C001-21", "intents": ["@USE@"], "paper_id": "ABC_397e593f8f282d4951402d83036c12_17", "text": "1. Maximum likelihood estimation (MLE) (Bahdanau et al., 2015) : the default training criterion in THUMT, which aims to find a set of model parameters that maximizes the likelihood of training data."}
{"sent_id": "397e593f8f282d4951402d83036c12-C001-15", "intents": ["@DIF@"], "paper_id": "ABC_397e593f8f282d4951402d83036c12_17", "text": "We compare THUMT with the state-of-the-art opensource toolkit GroundHog (Bahdanau et al., 2015) and achieve significant improvements on ChineseEnglish translation tasks by introducing new training criteria and optimizers."}
{"sent_id": "f633ceffdf53849159574a2891eda1-C001-11", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_f633ceffdf53849159574a2891eda1_17", "text": "Interestingly, the approach of Yang et al. (2019) represents a simple method to combining BERT with off-the-shelf IR."}
{"sent_id": "f633ceffdf53849159574a2891eda1-C001-13", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_f633ceffdf53849159574a2891eda1_17", "text": "Experiments show that, using the same reader model as Yang et al. (2019) , our simple data-augmentation techniques yield additional large improvements."}
{"sent_id": "f633ceffdf53849159574a2891eda1-C001-91", "intents": ["@USE@", "@UNSURE@"], "paper_id": "ABC_f633ceffdf53849159574a2891eda1_17", "text": "BERTserini (Yang et al., 2019) 38.6 46.1 85.9 SRC 41.8 49.5 85.9 DS(+) 44.0 51.4 85.9 DS(±) 48.7 56.5 85.9 SRC + DS(±) 45.7 53.5 85.9 DS(±) → SRC 47.4 55.0 85.9 SRC → DS(±) 50.2 58.2 85.9 Table 2 : Results on SQuAD helps, an even larger boost comes from leveraging negative examples using DS(±)."}
{"sent_id": "f633ceffdf53849159574a2891eda1-C001-84", "intents": ["@DIF@"], "paper_id": "ABC_f633ceffdf53849159574a2891eda1_17", "text": "The row marked \"SRC\" indicates fine tuning with SQuAD data only and matches the BERTserini condition of Yang et al. (2019) ; we report higher scores due to engineering improvements (primarily a Lucene version upgrade)."}
{"sent_id": "04461d946dadc759e4be1207655159-C001-6", "intents": ["@BACK@"], "paper_id": "ABC_04461d946dadc759e4be1207655159_18", "text": "Chiang's hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 2007) and has been widely adopted in statistical machine translation (SMT)."}
{"sent_id": "04461d946dadc759e4be1207655159-C001-53", "intents": ["@SIM@"], "paper_id": "ABC_04461d946dadc759e4be1207655159_18", "text": "Given e for the translation output in the target language, s and t for strings of terminals and nonterminals on the source and target side, respectively, we use a feature set analogous to the default feature set of Chiang (2007) , including:"}
{"sent_id": "04461d946dadc759e4be1207655159-C001-24", "intents": ["@DIF@"], "paper_id": "ABC_04461d946dadc759e4be1207655159_18", "text": "Instead of collapsing all non-terminals in the source language into a single symbol X as in Chiang (2007) , given a word sequence f i j from position i to position j, we first find heads and then concatenate the POS tags of these heads as f i j 's non-terminal symbol."}
{"sent_id": "04461d946dadc759e4be1207655159-C001-45", "intents": ["@UNSURE@"], "paper_id": "ABC_04461d946dadc759e4be1207655159_18", "text": "Given an initial phrase pair on the source side, there are four possible positional relationships for their target side translations (we use Y as a variable for nonterminals on the source side while all non-terminals on the target side are labeled as X): Chiang (2007) and HD-HRs."}
{"sent_id": "af9b884710f8198f008a9687153db6-C001-47", "intents": ["@DIF@"], "paper_id": "ABC_af9b884710f8198f008a9687153db6_18", "text": "While it has been shown that the CRF layer is required to achieve the state-ofthe-art performance in Ma and Hovy (2016) , we observe that the CRF has no significant effect on the final performance for the lattice construction."}
{"sent_id": "975413dd6b3d3df9c5d111d94e8eb7-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_975413dd6b3d3df9c5d111d94e8eb7_18", "text": "Recently Su et al. (2018) propose to leverage global co-occurrence statistics of textual and KB relations to learn embeddings of textual relations, and show that it can effectively combat the wrong labeling problem of distant supervision (see Figure 1 for example)."}
{"sent_id": "975413dd6b3d3df9c5d111d94e8eb7-C001-94", "intents": ["@USE@"], "paper_id": "ABC_975413dd6b3d3df9c5d111d94e8eb7_18", "text": "Following GloRE (Su et al., 2018) , we aim at augmenting existing relation extractors with the textual relation embeddings."}
{"sent_id": "45d4d6f0ac4a4f3bf7b2ac70fbcf7f-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_45d4d6f0ac4a4f3bf7b2ac70fbcf7f_18", "text": "Qu et al. [14] apply a CNN-based text classifier proposed by Kim [8] using a fixed window to represent the context."}
{"sent_id": "45d4d6f0ac4a4f3bf7b2ac70fbcf7f-C001-110", "intents": ["@DIF@"], "paper_id": "ABC_45d4d6f0ac4a4f3bf7b2ac70fbcf7f_18", "text": "Besides, unlike Qu et al. [14] , we keep all the DA annotations in the dataset to preserve the meaningful DA structures within and across utterances."}
{"sent_id": "d9c0e641f8ceb61e5d6e416bfc6492-C001-209", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_d9c0e641f8ceb61e5d6e416bfc6492_18", "text": "In this paper, we have attempted to reproduce a study by Nilsson et al. (2007) that has shown that making auxiliaries heads in verb groups improves parsing but failed to show that those results port to parsing with Universal Dependencies."}
{"sent_id": "d9c0e641f8ceb61e5d6e416bfc6492-C001-125", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_d9c0e641f8ceb61e5d6e416bfc6492_18", "text": "For comparability with the study in Nilsson et al. (2007) , and because we used a slightly modified version of their algorithm, we also tested the approach on the versions of the Czech and Slovenian treebanks that they worked on, respectively version 1.0 of the PDT (Hajic et al., 2001 ) and the 2006 version of SDT (Deroski et al., 2006) ."}
{"sent_id": "d9c0e641f8ceb61e5d6e416bfc6492-C001-47", "intents": ["@UNSURE@"], "paper_id": "ABC_d9c0e641f8ceb61e5d6e416bfc6492_18", "text": "This raises the question of whether this phenomenon actually happened in the study by Nilsson et al. (2007) ."}
{"sent_id": "f2db88c0d4e0ec4c34fc295a5d59ba-C001-15", "intents": ["@UNSURE@"], "paper_id": "ABC_f2db88c0d4e0ec4c34fc295a5d59ba_18", "text": "Here, we focus on the latent variable approach of Petrov et al. (2006) , where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars."}
{"sent_id": "f2db88c0d4e0ec4c34fc295a5d59ba-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_f2db88c0d4e0ec4c34fc295a5d59ba_18", "text": "Computing the joint likelihood of the observed parse trees T and sentences w requires summing over all derivations t over split subcategories: Matsuzaki et al. (2005) derive an EM algorithm for maximizing the joint likelihood, and Petrov et al. (2006) extend this algorithm to use a split&merge procedure to adaptively determine the optimal number of subcategories for each observed category."}
{"sent_id": "f2db88c0d4e0ec4c34fc295a5d59ba-C001-191", "intents": ["@BACK@"], "paper_id": "ABC_f2db88c0d4e0ec4c34fc295a5d59ba_18", "text": "It is also interesting to note that the best results in Zhang et al. (2009) are achieved by combining kbest lists from a latent variable grammar of Petrov et al. (2006) with the self-trained reranking parser of McClosky et al. (2006) ."}
{"sent_id": "e7b1c00e747f5bfbb96499d7223496-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_e7b1c00e747f5bfbb96499d7223496_18", "text": "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; Lazaridou et al., 2015; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts."}
{"sent_id": "e7b1c00e747f5bfbb96499d7223496-C001-15", "intents": ["@EXT@"], "paper_id": "ABC_e7b1c00e747f5bfbb96499d7223496_18", "text": "In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a; b) , , and Lazaridou et al. (2015) ."}
{"sent_id": "e7b1c00e747f5bfbb96499d7223496-C001-77", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_e7b1c00e747f5bfbb96499d7223496_18", "text": "To construct the vectors for the visual representations, we follow a similar experimental set-up as that used by Lazaridou et al. (2015) ."}
{"sent_id": "e7b1c00e747f5bfbb96499d7223496-C001-89", "intents": ["@USE@"], "paper_id": "ABC_e7b1c00e747f5bfbb96499d7223496_18", "text": "Using the results published by Lazaridou et al. (2015) and a target word embedding of 300, we compare our results to their MMSKIP-GRAM-A and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations Table 1 : Spearman correlation between the generated multimodal similarities and the benchmark human judgments."}
{"sent_id": "9d1699d4ca3b4026ed5aab125a737d-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_9d1699d4ca3b4026ed5aab125a737d_18", "text": "Bigrams. Gillick and Favre (2009) proposed to use bigrams as concepts, and to weight their contribution to the objective function in Equation (1) by the frequency with which they occur in the document."}
{"sent_id": "9d1699d4ca3b4026ed5aab125a737d-C001-29", "intents": ["@UNSURE@"], "paper_id": "ABC_9d1699d4ca3b4026ed5aab125a737d_18", "text": "We consider three new types of concepts, all suggested, but subsequently rejected by Gillick and Favre (2009 Semantic frames."}
{"sent_id": "9d1699d4ca3b4026ed5aab125a737d-C001-92", "intents": ["@UNSURE@"], "paper_id": "ABC_9d1699d4ca3b4026ed5aab125a737d_18", "text": "Hence, Gillick and Favre (2009) were right in their assumption that syntactic and semantic concepts would not lead to performance improvements, when restricting ourselves to this dataset."}
{"sent_id": "9d1699d4ca3b4026ed5aab125a737d-C001-55", "intents": ["@USE@"], "paper_id": "ABC_9d1699d4ca3b4026ed5aab125a737d_18", "text": "Our baseline is the bigram-based extraction summarization system of Gillick and Favre (2009) , icsisumm 7 ."}
{"sent_id": "9d1699d4ca3b4026ed5aab125a737d-C001-90", "intents": ["@DIF@"], "paper_id": "ABC_9d1699d4ca3b4026ed5aab125a737d_18", "text": "We first note that our runs of the current distribution of icsisumm yield significantly worse ROUGE-2 results than reported in (Gillick and Favre, 2009 ) (see Table 1 , BIGRAMS): 0.081 compared to 0.110 respectively."}
{"sent_id": "67b6d87aa2a943a854251fada6e183-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_67b6d87aa2a943a854251fada6e183_18", "text": "Given that most tweets are from urban areas, Han et al. (2012) consider a citybased class division, and explore different feature selection methods to extract \"location indicative words\", which they show to improve prediction accuracy."}
{"sent_id": "43622e43d6ef5291b64320d2d68b95-C001-79", "intents": ["@USE@"], "paper_id": "ABC_43622e43d6ef5291b64320d2d68b95_18", "text": "We conducted experiments with the Transformer model (Vaswani et al., 2017) on English⇒German (En⇒De), Chinese⇒English (Zh⇒En) and Japanese⇒English (Ja⇒En) translation tasks."}
{"sent_id": "b49e6f8181d51a998c6c27a830b98e-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_b49e6f8181d51a998c6c27a830b98e_18", "text": "Several studies have been conducted in order to measure compositionality for compounds in different languages (von der Heide and Borgwaldt, 2009; Reddy et al., 2011; Schulte im Walde et al., 2016b) ."}
{"sent_id": "b49e6f8181d51a998c6c27a830b98e-C001-70", "intents": ["@SIM@"], "paper_id": "ABC_b49e6f8181d51a998c6c27a830b98e_18", "text": "Like Reddy et al. (2011) and Schulte im Walde et al. (2013), we opt for Spearman's ρ."}
{"sent_id": "b49e6f8181d51a998c6c27a830b98e-C001-85", "intents": ["@DIF@"], "paper_id": "ABC_b49e6f8181d51a998c6c27a830b98e_18", "text": "As can also be seen from Table 2 , our correlation values are considerably lower than that of Reddy et al. (2011) , but on par with a replication study by Schulte im Walde et al. (2016a) for compound-mean."}
{"sent_id": "878c6cf1c47c86f36a7ff3f04e2998-C001-61", "intents": ["@DIF@"], "paper_id": "ABC_878c6cf1c47c86f36a7ff3f04e2998_18", "text": "Instead of selecting the sentences by recomputing sentence-level TF-IDF features between claim and document text as in Thorne et al. (2018) , we propose a neural ranker using decomposable attention (DA) model (Parikh et al., 2016) to perform evidence selection."}
{"sent_id": "878c6cf1c47c86f36a7ff3f04e2998-C001-83", "intents": ["@SIM@"], "paper_id": "ABC_878c6cf1c47c86f36a7ff3f04e2998_18", "text": "To overcome this issue, same as (Thorne et al., 2018) , the most probable NEI evidence are simulated by sampling sentences from the nearest page to the claim using the document retrieval module."}
{"sent_id": "3bc48bea420e4977027832240450ec-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_3bc48bea420e4977027832240450ec_18", "text": "In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (Mitchell et al., 2013; Zhang et al., 2016; Liu and Zhang, 2017; Wang et al., 2017) but other papers have not released code (Wang et al., 2016; Tay et al., 2017) ."}
{"sent_id": "3bc48bea420e4977027832240450ec-C001-95", "intents": ["@UNSURE@"], "paper_id": "ABC_3bc48bea420e4977027832240450ec_18", "text": "From the datasets we have used, we have only had issue with parsing Wang et al. (2017) where the annotations for the first set of the data contains the target span but the second set does not."}
{"sent_id": "3bc48bea420e4977027832240450ec-C001-99", "intents": ["@UNSURE@"], "paper_id": "ABC_3bc48bea420e4977027832240450ec_18", "text": "The only dataset that has a small difference between the number of unique sentiments per sentence is the Wang et al. (2017)"}
{"sent_id": "3bc48bea420e4977027832240450ec-C001-138", "intents": ["@UNSURE@"], "paper_id": "ABC_3bc48bea420e4977027832240450ec_18", "text": "However, the embeddings were released through Wang et al. (2017) code base 9 following requesting of the code from ."}
{"sent_id": "3bc48bea420e4977027832240450ec-C001-119", "intents": ["@USE@"], "paper_id": "ABC_3bc48bea420e4977027832240450ec_18", "text": "We therefore took the approach of Wang et al. (2017) and found all of the features for each appearance and performed median pooling over features."}
{"sent_id": "af6c68ef5f80eac2274bf33a894d1f-C001-67", "intents": ["@USE@"], "paper_id": "ABC_af6c68ef5f80eac2274bf33a894d1f_18", "text": "We follow the methodology of Mikolov et al. (2013b) in limiting analogy questions to the 100 most frequent verbs or nouns."}
{"sent_id": "af6c68ef5f80eac2274bf33a894d1f-C001-21", "intents": ["@DIF@"], "paper_id": "ABC_af6c68ef5f80eac2274bf33a894d1f_18", "text": "Mikolov et al. (2013b) obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed."}
{"sent_id": "af6c68ef5f80eac2274bf33a894d1f-C001-52", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_af6c68ef5f80eac2274bf33a894d1f_18", "text": "Our verbal and adjectival vectors obtain slightly lower accuracies than the RNN trained vectors of Mikolov et al. (2013b) , but they are not far off."}
{"sent_id": "af6c68ef5f80eac2274bf33a894d1f-C001-30", "intents": ["@UNSURE@"], "paper_id": "ABC_af6c68ef5f80eac2274bf33a894d1f_18", "text": "The test set of Mikolov et al. (2013b) is publicly available 3 ."}
{"sent_id": "3356313ee5cdf186816cd6fecfce84-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_3356313ee5cdf186816cd6fecfce84_18", "text": "In recent years, end-to-end architectures gained traction that directly classify keyword posterior probabilites based on the previously extracted features, e.g., [1, 2, 3, 4, 5] ."}
{"sent_id": "3356313ee5cdf186816cd6fecfce84-C001-30", "intents": ["@SIM@"], "paper_id": "ABC_3356313ee5cdf186816cd6fecfce84_18", "text": "[3] , while keeping the number of parameters comparable to [2] ."}
{"sent_id": "08d3f7a0938ab85d9a251b6a2364ed-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_08d3f7a0938ab85d9a251b6a2364ed_18", "text": "We now introduce the widely-used Attardi (2006) system, which includes transitions that create arcs between non-consecutive subtrees, thus allowing it to produce some non-projective trees."}
{"sent_id": "08d3f7a0938ab85d9a251b6a2364ed-C001-15", "intents": ["@UNSURE@"], "paper_id": "ABC_08d3f7a0938ab85d9a251b6a2364ed_18", "text": "In particular, we present three novel variants of the degree-2 Attardi parser, summarized in Fig. 1 (our technique can also be applied to generalized Attardi (2006) systems; see §3.2)."}
{"sent_id": "123d8e8ddef15fed120908c5c20656-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_123d8e8ddef15fed120908c5c20656_18", "text": "While most of the work in this direction has been devoted to learning the acoustic model directly from sequences of phonemes or characters without intermediate alignment step or phone-state/senome induction, the other end of the pipeline model -namely, learning directly from the waveform rather than from speech features such as mel-filterbanks or MFCC -has recently received attention [1, 2, 3, 4, 5, 6, 7, 8] , but the performances on the master task of speech recognition still seem to be lagging behind those of models trained on speech features [9, 10] ."}
{"sent_id": "123d8e8ddef15fed120908c5c20656-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_123d8e8ddef15fed120908c5c20656_18", "text": "In contrast, Zeghidour et al. [8] use 40 complex-valued filters with a square modulus operator as non-linearity."}
{"sent_id": "123d8e8ddef15fed120908c5c20656-C001-25", "intents": ["@DIF@"], "paper_id": "ABC_123d8e8ddef15fed120908c5c20656_18", "text": "3. For scattering-based trainable filterbanks, keeping the lowpass filter fixed during training allows to efficiently learn the filters from a random initialization, whereas the results of [8] with random initialization of both the filters and the lowpass filter showed poor performances compared to a suitable initialization;"}
{"sent_id": "123d8e8ddef15fed120908c5c20656-C001-124", "intents": ["@DIF@"], "paper_id": "ABC_123d8e8ddef15fed120908c5c20656_18", "text": "As described in Section 2.2, we evaluate the integration of instance normalization after the log-compression in the trainable filterbanks, which was not used in previous work [3, 4, 7, 8] but is used in our baseline."}
{"sent_id": "0f66e9a5c51cff004d97e4aaddf4d0-C001-49", "intents": ["@USE@"], "paper_id": "ABC_0f66e9a5c51cff004d97e4aaddf4d0_18", "text": "The original model of (Conneau et al., 2017) was trained on the SNLI dataset in a supervised fashion -given pairs of sentences s 1 and s 2 , denote their representation bys 1 and s 2 ."}
{"sent_id": "0f66e9a5c51cff004d97e4aaddf4d0-C001-40", "intents": ["@EXT@"], "paper_id": "ABC_0f66e9a5c51cff004d97e4aaddf4d0_18", "text": "To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by (Conneau et al., 2017) ."}
{"sent_id": "0f66e9a5c51cff004d97e4aaddf4d0-C001-44", "intents": ["@EXT@"], "paper_id": "ABC_0f66e9a5c51cff004d97e4aaddf4d0_18", "text": "Our approach builds upon the previous work of (Conneau et al., 2017) ."}
{"sent_id": "0f66e9a5c51cff004d97e4aaddf4d0-C001-41", "intents": ["@DIF@"], "paper_id": "ABC_0f66e9a5c51cff004d97e4aaddf4d0_18", "text": "We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of (Conneau et al., 2017) ."}
{"sent_id": "759c1c892361f62ad8f2c46e569e8a-C001-121", "intents": ["@USE@"], "paper_id": "ABC_759c1c892361f62ad8f2c46e569e8a_18", "text": "Following the PyDial benchmarking process, we leave all hyperparameters constant across all environments and dialog domains (Casanueva et al., 2017) , thus also evaluating the generalization capabilities of the agents."}
{"sent_id": "7293ab5db16d3fe1fee48d45154697-C001-100", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_7293ab5db16d3fe1fee48d45154697_19", "text": "The results obtained confirm the conclusion by Esplà-Gomis et al. (2015) that combining the baseline features with those obtained from external sources of bilingual information provide a noticeable improvement, in this case, not only for word-level MTQE, but also for phrase-level MTQE."}
{"sent_id": "7293ab5db16d3fe1fee48d45154697-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_7293ab5db16d3fe1fee48d45154697_19", "text": "The method used to produce the word-level MTQE submissions is the same than that used by the UAlacant team in the last edition of the shared task of MTQE at WMT 2015 (Esplà-Gomis et al., 2015) , which uses binary classification based on a collection of information."}
{"sent_id": "1c0d971cf771f351b51661950f4b14-C001-12", "intents": ["@SIM@"], "paper_id": "ABC_1c0d971cf771f351b51661950f4b14_19", "text": "Contributions In this work, we alleviate the requirements: (1) We present the first model that is able to induce bilingual word embeddings from non-parallel data without any other readily available translation resources such as pre-given bilingual lexicons; (2) We demonstrate the utility of BWEs induced by this simple yet effective model in the BLI task from comparable Wikipedia data on benchmarking datasets for three language pairs (Vulić and Moens, 2013b )."}
{"sent_id": "1c0d971cf771f351b51661950f4b14-C001-68", "intents": ["@SIM@"], "paper_id": "ABC_1c0d971cf771f351b51661950f4b14_19", "text": "Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vulić and Moens, 2013a; Vulić and Moens, 2013b) ."}
{"sent_id": "f5d1c0d3ac45ea4949f7d01d1704f6-C001-2", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_f5d1c0d3ac45ea4949f7d01d1704f6_19", "text": "In this work, we propose a semisupervised extension to a well-known supervised domain adaptation approach (EA) (Daumé III, 2007) ."}
{"sent_id": "f5d1c0d3ac45ea4949f7d01d1704f6-C001-54", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_f5d1c0d3ac45ea4949f7d01d1704f6_19", "text": "As mentioned earlier, this work considers linear hypotheses only and the the proposed techniques can be extended (Daumé III, 2007) to non-linear hypotheses."}
{"sent_id": "f5d1c0d3ac45ea4949f7d01d1704f6-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_f5d1c0d3ac45ea4949f7d01d1704f6_19", "text": "A domain adaptation approach for sequential labeling tasks in NLP was proposed in (Daumé III, 2007) ."}
{"sent_id": "f5d1c0d3ac45ea4949f7d01d1704f6-C001-151", "intents": ["@BACK@"], "paper_id": "ABC_f5d1c0d3ac45ea4949f7d01d1704f6_19", "text": "Feature sharing algorithms are effective for domain adaptation because they are simple, easy to implement as a preprocessing step and outperform many existing state-of-the-art techniques (shown previously for domain adaptation (Daumé III, 2007) )."}
{"sent_id": "f5d1c0d3ac45ea4949f7d01d1704f6-C001-109", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f5d1c0d3ac45ea4949f7d01d1704f6_19", "text": "We follow the same experimental setup used in (Daumé III, 2007) and perform two sequence labelling tasks (a) named-entity-recognition (NER), and (b) part-of-speech-tagging (POS )on the following datasets:"}
{"sent_id": "c2bfe3534597a8f192ec846619f6b1-C001-123", "intents": ["@SIM@"], "paper_id": "ABC_c2bfe3534597a8f192ec846619f6b1_19", "text": "As in (Platt et al., 2010) and (Mimno et al., 2009 ) the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs."}
{"sent_id": "c2bfe3534597a8f192ec846619f6b1-C001-141", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c2bfe3534597a8f192ec846619f6b1_19", "text": "This same approach was used by (Platt et al., 2010) to show the absolute performance comparison."}
{"sent_id": "faeac0a0e3c0cad79d39dea04ec59a-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_faeac0a0e3c0cad79d39dea04ec59a_19", "text": "The lack of consensus has resulted in contradictory annotation guidelines -some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and Davidson et al. (2017) ."}
{"sent_id": "faeac0a0e3c0cad79d39dea04ec59a-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_faeac0a0e3c0cad79d39dea04ec59a_19", "text": "Previous work has identified instances of hate speech that are both directed and generalized (Burnap and Williams, 2015; Waseem and Hovy, 2016; Davidson et al., 2017) , although Nobata et al. (2016) come closest to making a distinction between directed and generalized hate."}
{"sent_id": "faeac0a0e3c0cad79d39dea04ec59a-C001-92", "intents": ["@FUT@"], "paper_id": "ABC_faeac0a0e3c0cad79d39dea04ec59a_19", "text": "Hence, dictionary-based approaches may be well suited to identify this type of abuse (Warner and Hirschberg, 2012; Nobata et al., 2016) , although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways (Kwok and Wang, 2013; Davidson et al., 2017) ."}
{"sent_id": "a6954db741df61f014cc622c5b8263-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_a6954db741df61f014cc622c5b8263_19", "text": "Charniak (2000) shows the value his parser gains from parentannotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and Collins (1999) uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG, such as differentiating \"base NPs\" from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those where there is an overt subject NP."}
{"sent_id": "a6954db741df61f014cc622c5b8263-C001-79", "intents": ["@BACK@"], "paper_id": "ABC_a6954db741df61f014cc622c5b8263_19", "text": "One successful method of combating sparsity is to markovize the rules (Collins, 1999) ."}
{"sent_id": "a6954db741df61f014cc622c5b8263-C001-191", "intents": ["@USE@"], "paper_id": "ABC_a6954db741df61f014cc622c5b8263_19", "text": "Following Collins (1999) , the annotation GAPPED-S marks S nodes which have an empty subject (i.e., raising and control constructions)."}
{"sent_id": "bb133ba3dfe483412672b44b777c4a-C001-81", "intents": ["@BACK@"], "paper_id": "ABC_bb133ba3dfe483412672b44b777c4a_19", "text": "In contrast to all previous works Wu et al., 2019) which show that deep Transformers with the computation order as in Vaswani et al. (2017) have difficulty in convergence."}
{"sent_id": "bb133ba3dfe483412672b44b777c4a-C001-33", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_bb133ba3dfe483412672b44b777c4a_19", "text": "We used the same setting as the Transformer base (Vaswani et al., 2017) except the number of warm-up steps was set to 8k."}
{"sent_id": "bb133ba3dfe483412672b44b777c4a-C001-83", "intents": ["@DIF@"], "paper_id": "ABC_bb133ba3dfe483412672b44b777c4a_19", "text": "In this paper, we first investigate convergence differences between the published Transformer (Vaswani et al., 2017) and the official implementation of the Transformer , and compare the differences of computation orders between them."}
{"sent_id": "bb133ba3dfe483412672b44b777c4a-C001-37", "intents": ["@SIM@"], "paper_id": "ABC_bb133ba3dfe483412672b44b777c4a_19", "text": "We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU with the averaged model of the last 5 checkpoints saved with an interval of 1,500 training steps (Vaswani et al., 2017) ."}
{"sent_id": "6e92b1fa4f3b78a099cb222b3eb9a9-C001-102", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6e92b1fa4f3b78a099cb222b3eb9a9_19", "text": "Training and test data used in this work is the same as used in [2] ."}
{"sent_id": "6e92b1fa4f3b78a099cb222b3eb9a9-C001-31", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_6e92b1fa4f3b78a099cb222b3eb9a9_19", "text": "This model combination approach has been shown to give performace improvements on the DID task [2] ."}
{"sent_id": "866ae880aa0de1e60d306eac2e66fc-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_866ae880aa0de1e60d306eac2e66fc_19", "text": "Fukui et al. [6] propose multimodal compact bilinear pooling (MCB) to efficiently implement an outer product operator that combines visual and textual representations."}
{"sent_id": "866ae880aa0de1e60d306eac2e66fc-C001-141", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_866ae880aa0de1e60d306eac2e66fc_19", "text": "We build the attention supervision on top of the opensourced implementation of MCB [6] and MFB [25] ."}
{"sent_id": "866ae880aa0de1e60d306eac2e66fc-C001-178", "intents": ["@SIM@"], "paper_id": "ABC_866ae880aa0de1e60d306eac2e66fc_19", "text": "Accuracy/% VQA-HAT VQA-X VQA-2.0 Human [5] 0.623 -80.62 PJ-X [17] 0.396 0.342 -MCB [6] 0 authors also collect 1374 × 3 = 4122 HAT maps for VQA-1.0 validation sets, where each of the 1374 (I, Q, A) were labeled by three different annotators, so one can compare the level of agreement among labels."}
{"sent_id": "2407cfa8572ccbab7f9a081f45a4ad-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_2407cfa8572ccbab7f9a081f45a4ad_19", "text": "Recent work by Khapra et al. (2009) has shown that it is possible to project the parameters learnt from the annotation work of one language to another language provided aligned Wordnets for two languages are available."}
{"sent_id": "2407cfa8572ccbab7f9a081f45a4ad-C001-93", "intents": ["@BACK@"], "paper_id": "ABC_2407cfa8572ccbab7f9a081f45a4ad_19", "text": "The model proposed by Khapra et al. (2009 ) is a deterministic model where the expected count for (Sense S, Marathi Word W ), i.e., the number of times the word W appears in sense S is approximated by the count for the corresponding cross linked Hindi word."}
{"sent_id": "6678c19792be8d9ad66cf923d00c23-C001-14", "intents": ["@DIF@"], "paper_id": "ABC_6678c19792be8d9ad66cf923d00c23_19", "text": "Unlike the previous study (Kogan et al., 2009) , in which a regression model is employed to predict stock return volatilities via text information, our work utilizes learning-to-rank methods to model the ranking of relative risk levels directly."}
{"sent_id": "6678c19792be8d9ad66cf923d00c23-C001-67", "intents": ["@SIM@"], "paper_id": "ABC_6678c19792be8d9ad66cf923d00c23_19", "text": "For regression, linear kernel is adopted with ε = 0.1 and the trade-off C is set to the default choice of SVM light , which are the similar settings of (Kogan et al., 2009 )."}
{"sent_id": "be77eed8430b6492c81ae6535f1dd5-C001-15", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_be77eed8430b6492c81ae6535f1dd5_19", "text": "Our algorithm builds up on the span-based parser (Cross and Huang, 2016) ; it employs the strong generalization power of bi-directional LSTMs, and parses efficiently and robustly with an extremely simple span-based feature set that does not use any tree structure information."}
{"sent_id": "be77eed8430b6492c81ae6535f1dd5-C001-68", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_be77eed8430b6492c81ae6535f1dd5_19", "text": "But different from Cross and Huang (2016) , after a structural action, we choose to keep the last branching point k, i.e., i Some text and the symbol or scaled k j (mostly for combine, but also trivially for shift)."}
{"sent_id": "be77eed8430b6492c81ae6535f1dd5-C001-71", "intents": ["@DIF@"], "paper_id": "ABC_be77eed8430b6492c81ae6535f1dd5_19", "text": "This midpoint k disappears after a label action; therefore we can use the shape of the last span on the stack (whether it contains the split point, i.e., i xt and the symbol or scaled k j or i Some text and the symbol or scaled j ) to determine the parity of the step and thus no longer need to carry the step z in the state as in Cross and Huang (2016) ."}
{"sent_id": "be77eed8430b6492c81ae6535f1dd5-C001-88", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_be77eed8430b6492c81ae6535f1dd5_19", "text": "For most of the hyperparameters we settle with the same values suggested by Cross and Huang (2016) ."}
{"sent_id": "983ef31a44646d8e6276ee1933e41d-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_983ef31a44646d8e6276ee1933e41d_19", "text": "Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and * Both authors contributed equally."}
{"sent_id": "983ef31a44646d8e6276ee1933e41d-C001-50", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_983ef31a44646d8e6276ee1933e41d_19", "text": "Our extraction heuristic is imperfect, so we manually assess corpus quality using the same categorization schema proposed by Aharoni and Goldberg (2018) ; see Table 1 for examples of correct, unsupported and missing sentences in splits extracted from Wikipedia."}
{"sent_id": "983ef31a44646d8e6276ee1933e41d-C001-75", "intents": ["@SIM@"], "paper_id": "ABC_983ef31a44646d8e6276ee1933e41d_19", "text": "Text-to-text training instances are defined as all the unique pairs of (C, S), where C is a complex sentence and S is its simplification into multiple simple sentences Aharoni and Goldberg, 2018) ."}
{"sent_id": "983ef31a44646d8e6276ee1933e41d-C001-90", "intents": ["@DIF@"], "paper_id": "ABC_983ef31a44646d8e6276ee1933e41d_19", "text": "5 Past work on WebSplit Aharoni and Goldberg, 2018) reported macro-averaged sentence-level BLEU, calculated without smoothing precision values of zero."}
{"sent_id": "983ef31a44646d8e6276ee1933e41d-C001-105", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_983ef31a44646d8e6276ee1933e41d_19", "text": "Our best performance in BLEU is again obtained by combining the proposed WikiSplit dataset with the downsampled WebSplit, yielding Aharoni and Goldberg (2018) , while the other outputs are from our models trained on the corresponding data."}
{"sent_id": "ebd4488438579946c23904cc0f5932-C001-46", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_ebd4488438579946c23904cc0f5932_19", "text": "Due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in Gildea and Jurafsky (2000) ."}
{"sent_id": "ebd4488438579946c23904cc0f5932-C001-59", "intents": ["@DIF@"], "paper_id": "ABC_ebd4488438579946c23904cc0f5932_19", "text": "2 Gildea and Jurafsky (2000) use 36995 training, 4000 development, and 3865 test sentences."}
{"sent_id": "ebd4488438579946c23904cc0f5932-C001-96", "intents": ["@DIF@"], "paper_id": "ABC_ebd4488438579946c23904cc0f5932_19", "text": "Following Gildea and Jurafsky (2000) , automatic extraction of grammatical information here is limited to the governing category of a Noun Phrase."}
{"sent_id": "79ff6e23cc951aa18ae53763e9c982-C001-84", "intents": ["@BACK@"], "paper_id": "ABC_79ff6e23cc951aa18ae53763e9c982_19", "text": "Webber et al. (2003) observe that identification of the correct antecedent of a definite description such as the tower or this tower in (12a) or a discourse adverbial such as otherwise in (12b) may require reference to abstract discourse objects such as the result of stacking blocks (to form a tower) or the state of not wanting an apple as the logical antecedent of a definite description or of a discourse adverbial."}
{"sent_id": "79ff6e23cc951aa18ae53763e9c982-C001-18", "intents": ["@SIM@"], "paper_id": "ABC_79ff6e23cc951aa18ae53763e9c982_19", "text": "It is the latter group, namely discourse adverbials, that, according to Webber et al. (2003) , should be considered as anaphors in very much the same way as other anaphoric expressions such as definite descriptions and pronouns."}
{"sent_id": "90522b5ac99d1657bf9af9d165c36e-C001-74", "intents": ["@BACK@"], "paper_id": "ABC_90522b5ac99d1657bf9af9d165c36e_19", "text": "For instance, the largest corpus in Antoniak and Mimno (2018) contains 15M tokens, whereas the corpus used by Hellrich and Hahn (2017) and the largest corpus from Wendlandt et al. (2018) each contain about 60M tokens."}
{"sent_id": "90522b5ac99d1657bf9af9d165c36e-C001-135", "intents": ["@BACK@"], "paper_id": "ABC_90522b5ac99d1657bf9af9d165c36e_19", "text": "GLOVE's high stability in other studies (Antoniak and Mimno, 2018; Wendlandt et al., 2018) seems to be counterbalanced by its low accuracy and also appears to be limited to training on small corpora."}
{"sent_id": "2e7df0912d9aac8bf97f4061de613f-C001-12", "intents": ["@SIM@"], "paper_id": "ABC_2e7df0912d9aac8bf97f4061de613f_19", "text": "In our previous work [7] , we proposed a fully neural network based system, using a stack of fully connected and recurrent (LSTM, Long ShortTerm Memory) [10, 11] neural network layers."}
{"sent_id": "2e7df0912d9aac8bf97f4061de613f-C001-29", "intents": ["@SIM@"], "paper_id": "ABC_2e7df0912d9aac8bf97f4061de613f_19", "text": "Since then, several end-to-end trainable systems were presented [7, 8, 9] ."}
{"sent_id": "2e7df0912d9aac8bf97f4061de613f-C001-46", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_2e7df0912d9aac8bf97f4061de613f_19", "text": "We converted the \"normal\" quality videos (360 × 288 pixels) to greyscale and extracted 40×40 pixel windows containing the mouth area, as described in [7] ."}
{"sent_id": "2e7df0912d9aac8bf97f4061de613f-C001-57", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_2e7df0912d9aac8bf97f4061de613f_19", "text": "The system is based on the lipreading setup from [7] , reimplemented in Tensorflow [36] ."}
{"sent_id": "46b9079fb1dd6b4626f20819ccfa07-C001-6", "intents": ["@DIF@"], "paper_id": "ABC_46b9079fb1dd6b4626f20819ccfa07_19", "text": "The syntactic features provide an additional 0.3% reduction in test-set error rate beyond the model of (Roark et al., 2004a; Roark et al., 2004b ) (significant at p < 0.001), which makes use of a discriminatively trained n-gram model, giving a total reduction of 1.2% over the baseline Switchboard system."}
{"sent_id": "46b9079fb1dd6b4626f20819ccfa07-C001-17", "intents": ["@EXT@"], "paper_id": "ABC_46b9079fb1dd6b4626f20819ccfa07_19", "text": "We build on the work in Roark et al. (2004a; 2004b) , which was summarized and extended in Roark et al. (2005) ."}
{"sent_id": "46b9079fb1dd6b4626f20819ccfa07-C001-157", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_46b9079fb1dd6b4626f20819ccfa07_19", "text": "The experimental set-up we use is very similar to that of Roark et al. (2004a; 2004b) , and the extensions to that work in Roark et al. (2005) ."}
{"sent_id": "46b9079fb1dd6b4626f20819ccfa07-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_46b9079fb1dd6b4626f20819ccfa07_19", "text": "A number of results-e.g., in Sha and Pereira (2003) and Roark et al. (2004b) -suggest that the GCLM approach leads to slightly higher accuracy than the perceptron training method."}
{"sent_id": "b6efc2f5239a0c5d9210d7da8466ab-C001-72", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_b6efc2f5239a0c5d9210d7da8466ab_19", "text": "5 The Post et al. (2012) also induce translations for source language words which are low frequency in the training data and supplement our SMT models with top-k translations, not just the highest ranked."}
{"sent_id": "b6efc2f5239a0c5d9210d7da8466ab-C001-192", "intents": ["@SIM@"], "paper_id": "ABC_b6efc2f5239a0c5d9210d7da8466ab_19", "text": "As Post et al. (2012) showed, it is reasonable to assume a small parallel corpus for training an SMT model even in a low resource setting."}
{"sent_id": "320a5c79d9884e652c42f85847172b-C001-41", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_320a5c79d9884e652c42f85847172b_20", "text": "For our proposed CLIR models, we investigate cross-lingual embedding spaces produced with state-of-the-art representative methods requiring different amount and type of bilingual supervision: 1) document-aligned comparable data [21] , 2) word translation pairs [19] ; and 3) no bilingual data at all [3] ."}
{"sent_id": "0cc576e90c5ee2af043e09234792f5-C001-55", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_0cc576e90c5ee2af043e09234792f5_20", "text": "Dataset: The analyses were conducted on the First Certificate in English (FCE) ESOL examination scripts described in Yannakoudakis et al. (2011 Yannakoudakis et al. ( , 2012 ."}
{"sent_id": "0cc576e90c5ee2af043e09234792f5-C001-57", "intents": ["@SIM@"], "paper_id": "ABC_0cc576e90c5ee2af043e09234792f5_20", "text": "As in Yannakoudakis et al. (2011) , the 1141 texts from the year 2000 were used for training, while the 97 texts from the year 2001 were used for testing."}
{"sent_id": "0cc576e90c5ee2af043e09234792f5-C001-66", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_0cc576e90c5ee2af043e09234792f5_20", "text": "These features were extracted as described in Yannakoudakis et al. (2011) ; the only difference is that they used the RASP tagger and not the CLAWS tagger."}
{"sent_id": "d66ca5ff22e508da239fc7fdf5ac29-C001-119", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_d66ca5ff22e508da239fc7fdf5ac29_20", "text": "These required less sophisticated text processing and had previously been used in several studies with success (Beinborn et al., 2012; Dell'Orletta et al., 2011; François and Fairon, 2012; Heimann Mühlenbock, 2013; Vajjala and Meurers, 2012) ."}
{"sent_id": "d66ca5ff22e508da239fc7fdf5ac29-C001-126", "intents": ["@SIM@"], "paper_id": "ABC_d66ca5ff22e508da239fc7fdf5ac29_20", "text": "Subordinates (11) were detected on the basis of the \"UA\" (subordinate clause minus subordinating conjunction) dependency relation tag (Heimann Mühlenbock, 2013) ."}
{"sent_id": "d66ca5ff22e508da239fc7fdf5ac29-C001-132", "intents": ["@BACK@"], "paper_id": "ABC_d66ca5ff22e508da239fc7fdf5ac29_20", "text": "The NN/VB ratio feature, which has a higher value in written text, can also indicate a more complex sentence (Biber et al., 2004; Heimann Mühlenbock, 2013) ."}
{"sent_id": "d66ca5ff22e508da239fc7fdf5ac29-C001-147", "intents": ["@DIF@"], "paper_id": "ABC_d66ca5ff22e508da239fc7fdf5ac29_20", "text": "Previous classification results for a similar task obtained an average of 77.25% of precision for the classification of easy-to-read texts within an L1 Swedish text-level readability study (Heimann Mühlenbock, 2013) ."}
{"sent_id": "b49807b058e5e1e50eae524e592401-C001-33", "intents": ["@DIF@"], "paper_id": "ABC_b49807b058e5e1e50eae524e592401_20", "text": "As shown in Figure 1 , featurizer ϕ in our model contains three encoders which encode entity e and its context x into feature vectors, and we consider both sentence-level context x s and document-level context x d in contrast to prior work which only takes sentence-level context (Gillick et al., 2014; Shimaoka et al., 2017) ."}
{"sent_id": "b49807b058e5e1e50eae524e592401-C001-43", "intents": ["@DIF@"], "paper_id": "ABC_b49807b058e5e1e50eae524e592401_20", "text": "The dot-product attention differs from the self attention (Shimaoka et al., 2017) which only considers the context."}
{"sent_id": "b49807b058e5e1e50eae524e592401-C001-49", "intents": ["@DIF@"], "paper_id": "ABC_b49807b058e5e1e50eae524e592401_20", "text": "In prior work, a fixed threshold (r t = 0.5) is used for classification of all types (Ling and Weld, 2012; Shimaoka et al., 2017) ."}
{"sent_id": "b49807b058e5e1e50eae524e592401-C001-79", "intents": ["@DIF@"], "paper_id": "ABC_b49807b058e5e1e50eae524e592401_20", "text": "Ma et al. (2016) 49.30 68.23 61.27 AFET (Ren et al., 2016a) 55.10 71.10 64.70 FNET (Abhishek et al., 2017) 52.20 68.50 63.30 NEURAL (Shimaoka et al., 2017) This indicates the benefits of our proposed model architecture for learning fine-grained entity typing, which is discussed in detail in Section 3.4; and (3) BINARY and KWASIBIE were trained on a different dataset, so their results are not directly comparable."}
{"sent_id": "b49807b058e5e1e50eae524e592401-C001-100", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_b49807b058e5e1e50eae524e592401_20", "text": "Through experiments, we observe no improvement by encoding type hierarchical information (Shimaoka et al., 2017) ."}
{"sent_id": "ca98f16fa3a118f83b16586bba04c8-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_ca98f16fa3a118f83b16586bba04c8_20", "text": "A severe problem is that the Seq2Seq model tends to generate short and meaningless replies, e.g., \"I don't know\" [2] and \"Me too\" [3] ."}
{"sent_id": "ca98f16fa3a118f83b16586bba04c8-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_ca98f16fa3a118f83b16586bba04c8_20", "text": "Although it is tempting to think of Seq2Seq's performance in this way [3] , barely a practical approach exists to verify the conjecture in the dialog setting alone."}
{"sent_id": "ca98f16fa3a118f83b16586bba04c8-C001-134", "intents": ["@SIM@"], "paper_id": "ABC_ca98f16fa3a118f83b16586bba04c8_20", "text": "Our findings also explain why referring to additional information-including dialog context [19] , keywords [3] and knowledge bases [20]-helps dialog systems: the number of plausible target sentences decreases if the generation is conditioned on more information; this intuition is helpful for future development of text-based response generation in Seq2Seq dialog systems."}
{"sent_id": "649eff228a47b484d01872a980e58f-C001-102", "intents": ["@BACK@"], "paper_id": "ABC_649eff228a47b484d01872a980e58f_20", "text": "Convolution recurrent neural network [7] is a hybrid of CNN and RNN, which takes advantages of both."}
{"sent_id": "649eff228a47b484d01872a980e58f-C001-36", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_649eff228a47b484d01872a980e58f_20", "text": "• We first train the popular KWS neural net models from the literature [5, 6, 7, 8] on Google speech commands dataset [9] and compare them in terms of accuracy, memory footprint and number of operations per inference."}
{"sent_id": "649eff228a47b484d01872a980e58f-C001-124", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_649eff228a47b484d01872a980e58f_20", "text": "Table 2 summarizes the accuracy, memory requirement and operations per inference for the network architectures for KWS from literature [5, 6, 7, 8] trained on Google speech commands dataset [9] ."}
{"sent_id": "48e3715c55fcc188367dcfdc26c05f-C001-119", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_48e3715c55fcc188367dcfdc26c05f_20", "text": "We first test our models against the disambiguation task for transitive sentences described in Grefenstette and Sadrzadeh (2011a) ."}
{"sent_id": "48e3715c55fcc188367dcfdc26c05f-C001-129", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_48e3715c55fcc188367dcfdc26c05f_20", "text": "The most successful S = N model for this task is the copyobject model, which is performing really close to the original relational model of Grefenstette and Sadrzadeh (2011a) , with the difference to be statistically insignificant."}
{"sent_id": "c4e0e12362bd7d505f6887abad78d4-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_c4e0e12362bd7d505f6887abad78d4_20", "text": "While all the prior KWS neural networks are trained with cross entropy loss function, a max-pooling based loss function for training KWS model with long short-term memory (LSTM) is proposed in [8] , which achieves better accuracy than the DNNs and LSTMs trained with cross entropy loss."}
{"sent_id": "c4e0e12362bd7d505f6887abad78d4-C001-165", "intents": ["@SIM@"], "paper_id": "ABC_c4e0e12362bd7d505f6887abad78d4_20", "text": "The LSTM model mentioned in the table includes peephole connections and output projection layer similar to that in [8] , whereas basic LSTM model does not include those."}
{"sent_id": "44916cd85311c78666839a3376ccc6-C001-124", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_44916cd85311c78666839a3376ccc6_20", "text": "Baseline re-implementation The baseline model from Ajjour et al. (2017) uses a total of three Bi-LSTMs (two of them fully connected) to assign labels to tokens (see Figure 1a) ."}
{"sent_id": "44916cd85311c78666839a3376ccc6-C001-139", "intents": ["@SIM@"], "paper_id": "ABC_44916cd85311c78666839a3376ccc6_20", "text": "According to Ajjour et al. (2017) , the latter Bi-LSTM is used to correct the errors of the first one."}
{"sent_id": "44916cd85311c78666839a3376ccc6-C001-81", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_44916cd85311c78666839a3376ccc6_20", "text": "The architectures proposed in this section build on Ajjour et al. (2017) , omitting the second Bi-LSTM, which was used to process features other than word embeddings (see section 3.1)."}
{"sent_id": "3bbc588f06e326e1d75985fe253a5f-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_3bbc588f06e326e1d75985fe253a5f_20", "text": "In fact, Lample et al. (2018) , Artetxe et al. (2018b) and Marie and Fujita (2018) use the News Crawl of source and target language as training data."}
{"sent_id": "bb2609c568540390a560757dd40b32-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_bb2609c568540390a560757dd40b32_20", "text": "Note that pointwise method was also used in the original baselines [1] ."}
{"sent_id": "bb2609c568540390a560757dd40b32-C001-126", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_bb2609c568540390a560757dd40b32_20", "text": "Baselines from [1] Our Table 1 : Results of our experiments compared to the results reported in [1] ."}
{"sent_id": "bb2609c568540390a560757dd40b32-C001-107", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_bb2609c568540390a560757dd40b32_20", "text": "To match the original setup of [1] we use the same training data 3 ."}
{"sent_id": "bb2609c568540390a560757dd40b32-C001-171", "intents": ["@DIF@", "@EXT@"], "paper_id": "ABC_bb2609c568540390a560757dd40b32_20", "text": "In this work we achieved a new state-of-the-art results on the next utterance ranking problem recently introduced in [1] ."}
{"sent_id": "831342435ca0a4695e2a7f149891e4-C001-52", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_831342435ca0a4695e2a7f149891e4_20", "text": "Following recent speech translation [21] and recognition [28] models, the encoder is composed of a stack of 8 bidirectional LSTM layers."}
{"sent_id": "831342435ca0a4695e2a7f149891e4-C001-83", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_831342435ca0a4695e2a7f149891e4_20", "text": "This proprietary dataset described in [21] was obtained by crowdsourcing humans to read the both sides of a conversational Spanish-English MT dataset."}
{"sent_id": "831342435ca0a4695e2a7f149891e4-C001-116", "intents": ["@SIM@"], "paper_id": "ABC_831342435ca0a4695e2a7f149891e4_20", "text": "Finally, as in [21] , we find that pretraining the bottom 6 encoder layers on an ST task improves BLEU scores by over 5 points."}
{"sent_id": "831342435ca0a4695e2a7f149891e4-C001-161", "intents": ["@FUT@"], "paper_id": "ABC_831342435ca0a4695e2a7f149891e4_20", "text": "Other future work includes utilizing weakly supervision to scale up training with synthetic data [21] or multitask learning [19, 20] , and transferring prosody and other acoustic factors from the source speech to the translated speech following [45] [46] [47] ."}
{"sent_id": "4f5a25d7a961e7e61c2caef81418e0-C001-106", "intents": ["@DIF@"], "paper_id": "ABC_4f5a25d7a961e7e61c2caef81418e0_20", "text": "Table 1 shows that both our RAS-Elman and RAS-LSTM models achieve lower perplexity than ABS as well as other models reported in Rush et al. (2015) ."}
{"sent_id": "4f5a25d7a961e7e61c2caef81418e0-C001-86", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4f5a25d7a961e7e61c2caef81418e0_20", "text": "For Gigaword we report results on the same randomly held-out test set of 2000 sentence-summary pairs as (Rush et al., 2015) ."}
{"sent_id": "4f5a25d7a961e7e61c2caef81418e0-C001-104", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4f5a25d7a961e7e61c2caef81418e0_20", "text": "As baseline we use the state-of-the-art attention-based system (ABS) of Rush et al. (2015) which relies on a feed-forward network decoder."}
{"sent_id": "4f5a25d7a961e7e61c2caef81418e0-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_4f5a25d7a961e7e61c2caef81418e0_20", "text": "Very recently Rush et al. (2015) proposed a neural attention model for this problem using a new data set for training and showing state-of-the-art performance on the DUC tasks."}
{"sent_id": "73eaa7d5a54b2d60bd8128e0270683-C001-74", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_73eaa7d5a54b2d60bd8128e0270683_20", "text": "We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (Ratinov and Roth, 2009; Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016) ."}
{"sent_id": "73eaa7d5a54b2d60bd8128e0270683-C001-76", "intents": ["@SIM@"], "paper_id": "ABC_73eaa7d5a54b2d60bd8128e0270683_20", "text": "Following the work of Ma and Hovy (2016) , we initialise word embeddings with GloVe (Pennington et al., 2014 ) (300-dimensional, trained on a 6B-token corpus)."}
{"sent_id": "4e7ee576b07a8a21a42472bf921291-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_4e7ee576b07a8a21a42472bf921291_20", "text": "SIGIR '18, July [8] [9] [10] [11] [12] 2018 word embeddings tailored for search tasks and kernels that group matches into bins of different quality."}
{"sent_id": "4e7ee576b07a8a21a42472bf921291-C001-49", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4e7ee576b07a8a21a42472bf921291_20", "text": "Our experiments followed the original K-NRM work [10] and used its open-source implementation 1 ."}
{"sent_id": "4e7ee576b07a8a21a42472bf921291-C001-62", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_4e7ee576b07a8a21a42472bf921291_20", "text": "Model Configuration: We adopted the same default hyperparameter configuration and 11 Gaussian kernels as in prior work [10] ."}
{"sent_id": "4e7ee576b07a8a21a42472bf921291-C001-80", "intents": ["@DIF@"], "paper_id": "ABC_4e7ee576b07a8a21a42472bf921291_20", "text": "Table 1 also shows results reported by Xiong et al [10] ."}
{"sent_id": "7cb7cfed8b7e7bf2f0a810e02e6cbc-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_7cb7cfed8b7e7bf2f0a810e02e6cbc_20", "text": "We first briefly describe the AMR-based summarization method of Liu et al. (2015) and then our guided NLG approach."}
{"sent_id": "7cb7cfed8b7e7bf2f0a810e02e6cbc-C001-100", "intents": ["@SIM@"], "paper_id": "ABC_7cb7cfed8b7e7bf2f0a810e02e6cbc_20", "text": "This phenomenon was also observed in Liu et al. (2015) 's experiment where the summary graphs extracted from automatic parses had higher accuracy than those extracted from manual parses."}
{"sent_id": "9c8c5da4cdd13efb187690e7d3aa20-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_9c8c5da4cdd13efb187690e7d3aa20_20", "text": "However, relevance scores for captions of the same image score only 3.38 out of 5 under human evaluation (in contrast, the score is 4.82 for QUORA) (Gupta et al., 2018) , due to the fact that different captions for the same image often vary in the semantic information conveyed."}
{"sent_id": "9c8c5da4cdd13efb187690e7d3aa20-C001-80", "intents": ["@FUT@"], "paper_id": "ABC_9c8c5da4cdd13efb187690e7d3aa20_20", "text": "Consequently, although the exact test sets used by (Gupta et al., 2018) and (Li et al., 2018) are not available, it is logical to assume that parroting performance would still exceed or be on par with the state-of-the-art on those test sets."}
{"sent_id": "e9779b09826d709f8851550d958df7-C001-137", "intents": ["@BACK@"], "paper_id": "ABC_e9779b09826d709f8851550d958df7_20", "text": "In [8] , a parallel corpus was needed as the loss functions adopted try to minimise either the distance between captions in two languages or the distance between captions in two languages and the associated image as pivot."}
{"sent_id": "e9779b09826d709f8851550d958df7-C001-144", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_e9779b09826d709f8851550d958df7_20", "text": "We evaluated our approach on 1k captions of our test corpus to be comparable with [8] ."}
{"sent_id": "e9779b09826d709f8851550d958df7-C001-148", "intents": ["@SIM@"], "paper_id": "ABC_e9779b09826d709f8851550d958df7_20", "text": "For comparison, we report [8] 's results on English to Hindi (HI) and Hindi to English speech-to-speech retrieval."}
{"sent_id": "d0007c7f1f9ecfbdd7b6ad7c59cc92-C001-98", "intents": ["@DIF@"], "paper_id": "ABC_d0007c7f1f9ecfbdd7b6ad7c59cc92_20", "text": "As we can see from table 2, generally the social media datasets (Twitter and YouTube) contain more targets per sentence with the exception of Dong et al. (2014) and Mitchell et al. (2013) ."}
{"sent_id": "d0007c7f1f9ecfbdd7b6ad7c59cc92-C001-166", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_d0007c7f1f9ecfbdd7b6ad7c59cc92_20", "text": "The experiments are performed on the Dong et al. (2014) dataset where we train and test on the specified splits."}
{"sent_id": "bbd8c1007b573758fc78a6d16e2b77-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_bbd8c1007b573758fc78a6d16e2b77_20", "text": "This distinguishes DOP1 from most other statistical parsing models that identify exactly one derivation for each parse tree and thus compute the probability of a tree by only one product of probabilities --see Collins (1997 Collins ( , 1999 , Charniak (1997 Charniak ( , 2000 and Eisner (1997 (Dempster et al. 1977) , but this resulted in a decrease in parse accuracy on the ATIS and OVIS corpora (Bod 2000a) , although it slightly improved the word error rate for OVIS word-graphs."}
{"sent_id": "bbd8c1007b573758fc78a6d16e2b77-C001-64", "intents": ["@SIM@"], "paper_id": "ABC_bbd8c1007b573758fc78a6d16e2b77_20", "text": "For our base line parse accuracy, we used the now standard division of the WSJ (see Collins 1997 Collins , 1999 Charniak 1997 Charniak , 2000 Ratnaparkhi 1999 ) with sections 2 through 21 for training (approx. 40,000 sentences) and section 23 for testing (2416 sentences ≤ 100 words); section 22 was used as development set."}
{"sent_id": "1ffadfc2d4961beeb1621502298a70-C001-16", "intents": ["@SIM@"], "paper_id": "ABC_1ffadfc2d4961beeb1621502298a70_21", "text": "Therefore, in conjunction with this task, we present the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019) ."}
{"sent_id": "1ffadfc2d4961beeb1621502298a70-C001-45", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_1ffadfc2d4961beeb1621502298a70_21", "text": "OLID was annotated using a hierarchical three-level annotation model introduced in Zampieri et al. (2019) ."}
{"sent_id": "1ffadfc2d4961beeb1621502298a70-C001-82", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_1ffadfc2d4961beeb1621502298a70_21", "text": "A detailed description of the data collection process and annotation is presented in Zampieri et al. (2019) ."}
{"sent_id": "2357152e66ad3ae1c23738ac95f971-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_2357152e66ad3ae1c23738ac95f971_21", "text": "Algorithm 1 is the beam search algorithm (Nuhn et al., 2013 (Nuhn et al., , 2014 Hs."}
{"sent_id": "2357152e66ad3ae1c23738ac95f971-C001-120", "intents": ["@BACK@"], "paper_id": "ABC_2357152e66ad3ae1c23738ac95f971_21", "text": "Nuhn et al. (2014) present various improvements to the beam search algorithm in Nuhn et al. (2013) including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols."}
{"sent_id": "2357152e66ad3ae1c23738ac95f971-C001-93", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_2357152e66ad3ae1c23738ac95f971_21", "text": "In this experiment we use a synthetic 1:1 letter substitution cipher dataset following Ravi and Knight (2008) , Nuhn et al. (2013) and Hauer et al. (2014) ."}
{"sent_id": "c91781e76a8d7d6de7d7bc4407e799-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_c91781e76a8d7d6de7d7bc4407e799_21", "text": "In addition, NCRF++ integrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many recent work (Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016) ."}
{"sent_id": "c91781e76a8d7d6de7d7bc4407e799-C001-109", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_c91781e76a8d7d6de7d7bc4407e799_21", "text": "For POS tagging, we use the same data and split with Ma and Hovy (2016) ."}
{"sent_id": "ee8163c5a76ed9f929a960b3086356-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_ee8163c5a76ed9f929a960b3086356_21", "text": "We then use this noisy seed lexicon to train context vectors via neural network (Mikolov et al., 2013b) , inducing a cross-lingual transformation that approximates semantic similarity."}
{"sent_id": "ee8163c5a76ed9f929a960b3086356-C001-22", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_ee8163c5a76ed9f929a960b3086356_21", "text": "The results demonstrate a substantial error reduction with respect to a word-vector-based method of Mikolov et al. (2013b) , when using the same word vectors on six source-target pairs."}
{"sent_id": "ee8163c5a76ed9f929a960b3086356-C001-43", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_ee8163c5a76ed9f929a960b3086356_21", "text": "We adapt the approach of Mikolov et al. (2013b) for learning a linear transformation between the source and target vector spaces to enable it to function given only a small, noisy seed."}
{"sent_id": "ee8163c5a76ed9f929a960b3086356-C001-109", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_ee8163c5a76ed9f929a960b3086356_21", "text": "The results in Table 2 show that the method of Mikolov et al. (2013b) (MIK13-Auto) , represented by the first translation matrix derived on our automatically extracted the seed lexicon, performs well below the edit distance baseline."}
{"sent_id": "ee8163c5a76ed9f929a960b3086356-C001-46", "intents": ["@DIF@"], "paper_id": "ABC_ee8163c5a76ed9f929a960b3086356_21", "text": "While Mikolov et al. (2013b) derive the translation matrix using five thousand translation pairs obtained via Google Translate,"}
{"sent_id": "ee8163c5a76ed9f929a960b3086356-C001-62", "intents": ["@DIF@"], "paper_id": "ABC_ee8163c5a76ed9f929a960b3086356_21", "text": "Unlike Mikolov et al. (2013b) , our algorithm iteratively expands the lexicon, which gradually increases the accuracy of the translation matrices."}
{"sent_id": "a82bdc55c15bb2bcee77c57641b1b5-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_a82bdc55c15bb2bcee77c57641b1b5_21", "text": "This could be useful for: (1) creating automatic summaries of each position on an issue (SparckJones, 1999); (2) gaining a deeper understanding of what makes an argument persuasive (Marwell and Schmitt, 1967) ; and (3) identifying the linguistic reflexes of perlocutionary acts such as persuasion and disagreement (Walker, 1996; Greene and Resnik, 2009; Somasundaran and Wiebe, 2010; Marcu, 2000) ."}
{"sent_id": "a82bdc55c15bb2bcee77c57641b1b5-C001-199", "intents": ["@BACK@"], "paper_id": "ABC_a82bdc55c15bb2bcee77c57641b1b5_21", "text": "The best performance of (Somasundaran and Wiebe, 2010) below the majority class baseline for all of the features without context."}
{"sent_id": "a82bdc55c15bb2bcee77c57641b1b5-C001-102", "intents": ["@SIM@"], "paper_id": "ABC_a82bdc55c15bb2bcee77c57641b1b5_21", "text": "Each of our 12 topics consists of more than one debate: each debate was mapped by hand to the topic and topic-siding (as in (Somasundaran and Wiebe, 2010))."}
{"sent_id": "3bb6243de9f77fc6ebf2dc24de7faa-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_3bb6243de9f77fc6ebf2dc24de7faa_21", "text": "There are various works Cheng et al., 2018; Conneau et al., 2017; Fu et al., 2018; Hill et al., 2016; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Kusner et al., 2015; Le and Mikolov, 2014; Clinchant and Perronnin, 2013; Shen et al., 2018; Torki, 2018; Zhao et al., 2015; Zhou et al., 2018 ) that propose to build effective sentence-level or document-level representations based on word embeddings."}
{"sent_id": "fe8d369d4a6f940a1eb25aa7c9b4fe-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_fe8d369d4a6f940a1eb25aa7c9b4fe_21", "text": "It is in general to normalize the model score by translation length (say length normalization) to eliminate this system bias (Wu et al., 2016) ."}
{"sent_id": "fe8d369d4a6f940a1eb25aa7c9b4fe-C001-81", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_fe8d369d4a6f940a1eb25aa7c9b4fe_21", "text": "We used grid search to tune all hyperparameters on the development set as Wu et al. (2016) ."}
{"sent_id": "fe8d369d4a6f940a1eb25aa7c9b4fe-C001-122", "intents": ["@DIF@"], "paper_id": "ABC_fe8d369d4a6f940a1eb25aa7c9b4fe_21", "text": "Another difference lies in that our coverage model is applied to every beam search step, while Wu et al. (2016) 's model affects only a small number of translation outputs."}
{"sent_id": "f32bbd580d93f77ef764c5341b93db-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_f32bbd580d93f77ef764c5341b93db_21", "text": "With many recent results [9, 10, 11, 12, 13] approaching the stateof-the-art, end-to-end deep learning has definitely been a very important direction for speech recognition."}
{"sent_id": "f32bbd580d93f77ef764c5341b93db-C001-118", "intents": ["@BACK@"], "paper_id": "ABC_f32bbd580d93f77ef764c5341b93db_21", "text": "The results are listed in Table 1 , where \"Baseline\" refers to the plain end-toend speech recognition framework as described in Sec. 2.3, \"+LM\" refers to the shallow fusion decoding with a separately trained RNN language model (RNN-LM) [13, 20] and \"+AT\" refers to the adversarial training proposed here."}
{"sent_id": "f32bbd580d93f77ef764c5341b93db-C001-71", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f32bbd580d93f77ef764c5341b93db_21", "text": "Any network architecture for end-toend speech recognition can be used here, while Fig. 3 gives the one used in this work, following the previous work [13] of integrating attentioned Seq2seq with CTC."}
{"sent_id": "f32bbd580d93f77ef764c5341b93db-C001-102", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_f32bbd580d93f77ef764c5341b93db_21", "text": "We followed the previous work [13, 21] to use 80-dimensional log Mel-filter bank and 3-dimensional pitch features as the acoustic features."}
{"sent_id": "5a000efaa052588f6cfbb69f8ced2d-C001-5", "intents": ["@DIF@"], "paper_id": "ABC_5a000efaa052588f6cfbb69f8ced2d_21", "text": "We show that the proposed method outperforms the baseline of Richardson et al. (2013) , and despite its relative simplicity, is comparable to recent work using machine learning."}
{"sent_id": "5a000efaa052588f6cfbb69f8ced2d-C001-91", "intents": ["@DIF@"], "paper_id": "ABC_5a000efaa052588f6cfbb69f8ced2d_21", "text": "If we linearly combine the RTE scores used in the MSR baseline with our method, we achieve 5 and 2.5 accuracy points higher than the best results achieved by Richardson et al. (2013) ."}
{"sent_id": "5a000efaa052588f6cfbb69f8ced2d-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_5a000efaa052588f6cfbb69f8ced2d_21", "text": "Richardson et al. (2013) also showed how the creation of stories and questions can be crowdsourced efficiently, constructing two datasets for the task, namely MC160 and MC500."}
{"sent_id": "5e85f66e9971497e5e21af6893418d-C001-52", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_5e85f66e9971497e5e21af6893418d_21", "text": "For the former we use cosine similarity and for the latter we use the metric of Vendrov et al. (2016) which is useful for learning embeddings that maintain an order, e.g., dog and cat are more closer to pet than animal while being distinct."}
{"sent_id": "c82c31a3e7b229b5aed8faeff21efa-C001-100", "intents": ["@BACK@"], "paper_id": "ABC_c82c31a3e7b229b5aed8faeff21efa_21", "text": "However, Vu et al. (2016a) has shown that the extended context helps."}
{"sent_id": "c82c31a3e7b229b5aed8faeff21efa-C001-37", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_c82c31a3e7b229b5aed8faeff21efa_21", "text": "We adopt the bi-directional recurrent neural network architecture with ranking loss, proposed by Vu et al. (2016a) ."}
{"sent_id": "c82c31a3e7b229b5aed8faeff21efa-C001-52", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_c82c31a3e7b229b5aed8faeff21efa_21", "text": "Following Vu et al. (2016a) , we set γ = 2, m + = 2.5 and m − = 0.5."}
{"sent_id": "c82c31a3e7b229b5aed8faeff21efa-C001-58", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_c82c31a3e7b229b5aed8faeff21efa_21", "text": "In our analysis and interpretation of recurrent neural networks, we use the trained C-BRNN ( Figure 1 ) (Vu et al., 2016a) model."}
{"sent_id": "10de18ba49c0da530b15ff2d14f343-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_10de18ba49c0da530b15ff2d14f343_21", "text": "Aspect and/or opinion terms extraction research has been conducted by Wang et al. [2] and Xu et al. [3] that outperformed the best systems in the aspect-based sentiment analysis task on the International Workshop on Semantic Evaluation (SemEval) for aspect and opinion terms extraction."}
{"sent_id": "10de18ba49c0da530b15ff2d14f343-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_10de18ba49c0da530b15ff2d14f343_21", "text": "Wang et al. [2] and Xu et al. [3] approaches have not been applied for Indonesian reviews."}
{"sent_id": "10de18ba49c0da530b15ff2d14f343-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_10de18ba49c0da530b15ff2d14f343_21", "text": "The experiment conducted in [3] demonstrated that double embedding mechanism achieved better performance for aspect terms extraction compared to the use of general embeddings or domain embeddings alone."}
{"sent_id": "10de18ba49c0da530b15ff2d14f343-C001-23", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_10de18ba49c0da530b15ff2d14f343_21", "text": "This paper aims to perform aspect and opinion terms extraction in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism [3] ."}
{"sent_id": "10de18ba49c0da530b15ff2d14f343-C001-83", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_10de18ba49c0da530b15ff2d14f343_21", "text": "For the general embeddings and domain embeddings, we use the same dimension and number of iterations as in [3] ."}
{"sent_id": "14fcaa3645771e9ca183558eb2e9a1-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_14fcaa3645771e9ca183558eb2e9a1_21", "text": "Alternatively, multilingual versions have been tested in transfer learning scenarios for other languages, where they have not been compared to monolingual versions (Devlin et al., 2019) ."}
{"sent_id": "14fcaa3645771e9ca183558eb2e9a1-C001-88", "intents": ["@BACK@"], "paper_id": "ABC_14fcaa3645771e9ca183558eb2e9a1_21", "text": "Flair (embeddings and system) have been successfully applied to sequence labeling tasks obtaining state-of-the-art results for a number of English Named Entity Recognition (NER) and Part-of-Speech tagging benchmarks (Akbik et al., 2018) , outperforming other well-known approaches such as BERT and ELMO (Devlin et al., 2019; Peters et al., 2018) ."}
{"sent_id": "14fcaa3645771e9ca183558eb2e9a1-C001-112", "intents": ["@USE@"], "paper_id": "ABC_14fcaa3645771e9ca183558eb2e9a1_21", "text": "We have trained a BERT (Devlin et al., 2019) model for Basque Language using the BMC corpus motivated by the rather low representation this language has in the original multilingual BERT model."}
{"sent_id": "14fcaa3645771e9ca183558eb2e9a1-C001-132", "intents": ["@SIM@"], "paper_id": "ABC_14fcaa3645771e9ca183558eb2e9a1_21", "text": "Pre-training procedure Similar to (Devlin et al., 2019) we use Adam with learning rate of 1e − 4, β 1 = 0.9, β 2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10, 0000 steps, and linear decay of the learning rate."}
{"sent_id": "6388fd7167389982be2f01fbe594cd-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_6388fd7167389982be2f01fbe594cd_21", "text": "Because Bicknell and Levy's (2010) model implements the main proposal for why word length effects should arise, i.e., visual acuity limitations, the fact that the model does not reproduce humanlike word length effects suggests that our understanding of the causes of word length effects may be incomplete."}
{"sent_id": "6388fd7167389982be2f01fbe594cd-C001-72", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_6388fd7167389982be2f01fbe594cd_21", "text": "Following Bicknell and Levy (2010), we use very simple probabilistic models of language knowledge: word n-gram models (Jurafsky & Martin, 2009) , which encode the probability of each word conditional on the n − 1 previous words."}
{"sent_id": "6388fd7167389982be2f01fbe594cd-C001-125", "intents": ["@SIM@"], "paper_id": "ABC_6388fd7167389982be2f01fbe594cd_21", "text": "We compare three levels of length uncertainty: δ ∈ {0, .05, .1}. The first of these (δ = 0) corresponds to Bicknell and Levy's (2010) model, which has no uncertainty about word length."}
{"sent_id": "c796e11db9203d35c1fad61d1329ef-C001-115", "intents": ["@BACK@"], "paper_id": "ABC_c796e11db9203d35c1fad61d1329ef_21", "text": "Finally, a parameter in SVD determines the asymmetry of factorization, which was simulated with 0, 0.5 and 1 eig (for more details refer to Levy et al., 2015) ."}
{"sent_id": "c796e11db9203d35c1fad61d1329ef-C001-141", "intents": ["@DIF@"], "paper_id": "ABC_c796e11db9203d35c1fad61d1329ef_21", "text": "SVD also benefits from a w+c equivalent setting proposed by Levy & Goldberg (2015) in performing the syntagmatic task, however the enhancement is tightly bounded for this model."}
{"sent_id": "92d9291093f4ff9f10cca1b8ad2a27-C001-129", "intents": ["@BACK@"], "paper_id": "ABC_92d9291093f4ff9f10cca1b8ad2a27_21", "text": "Hier-Res-BiLSTM (Yu et al., 2017) uses hierarchical residual connections to ease the training procedure of BiL-STM."}
{"sent_id": "92d9291093f4ff9f10cca1b8ad2a27-C001-131", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_92d9291093f4ff9f10cca1b8ad2a27_21", "text": "The model is reimplemented for SimpleQuestions by Yu et al. (2017) ."}
{"sent_id": "65f6a6fce98c511473a3ab144a73e4-C001-2", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_65f6a6fce98c511473a3ab144a73e4_21", "text": "Our submission to the W-NUT Named Entity Recognition in Twitter task closely follows the approach detailed by Cherry and Guo (2015) , who use a discriminative, semi-Markov tagger, augmented with multiple word representations."}
{"sent_id": "65f6a6fce98c511473a3ab144a73e4-C001-41", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_65f6a6fce98c511473a3ab144a73e4_21", "text": "We first summarize the approach of Cherry and Guo (2015) , which we build upon for our system."}
{"sent_id": "65f6a6fce98c511473a3ab144a73e4-C001-139", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_65f6a6fce98c511473a3ab144a73e4_21", "text": "C&G 2015 adds Brown clusters and word embeddings to create a complete re-implementation of Cherry and Guo (2015) ."}
{"sent_id": "7e73137c97a84fe7fa4941ecd06a91-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_7e73137c97a84fe7fa4941ecd06a91_21", "text": "For example, the most commonly-used answer type hierarchy (ATH), the University of Illinois (UIUC) answer type hierarchy created by (Li and Roth, 2002) , includes only a total of 50 unique expected answer types (generally referred to as \"fine\" answer types), organized into 6 different categories (referred to as \"coarse\" answer types)."}
{"sent_id": "7e73137c97a84fe7fa4941ecd06a91-C001-126", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_7e73137c97a84fe7fa4941ecd06a91_21", "text": "In a departure from previous machine-learning based approaches (Li and Roth, 2002; Krishnan et al., 2005) , we used a maximum entropy classifier to learn our ATH."}
{"sent_id": "7e73137c97a84fe7fa4941ecd06a91-C001-167", "intents": ["@SIM@"], "paper_id": "ABC_7e73137c97a84fe7fa4941ecd06a91_21", "text": "In a departure from previous work in answer type detection (Krishnan et al., 2005; Li and Roth, 2002) , we have demonstrated how a large, multi-tiered answer type hierarchy can be created which incorporates many of the entity types included in LCC's wide coverage named entity recognition system, CICEROLITE; this hierarchy was then used in order to create a new corpus of more than 10,000 questions which could be used to train an ATD system."}
{"sent_id": "fa33495582abd0c6efe8f599c73d0e-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_fa33495582abd0c6efe8f599c73d0e_21", "text": "In that time, researchers have explored architectures ranging from convolutional neural networks (Kalchbrenner and Blunsom, 2013) to recurrent neural networks (Chung et al., 2014) to attentional models (Bahdanau et al., 2015; Luong et al., 2015) and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs."}
{"sent_id": "fa33495582abd0c6efe8f599c73d0e-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_fa33495582abd0c6efe8f599c73d0e_21", "text": "NMT models first achieved state-of-the-art performance on the WMT English→German news-domain task in 2015 (Luong et al., 2015) and subsequent improvements have been reported since then (Sennrich et al., 2015a; Li and Jurafsky, 2016) ."}
{"sent_id": "fa33495582abd0c6efe8f599c73d0e-C001-27", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_fa33495582abd0c6efe8f599c73d0e_21", "text": "The model identified as metamind-single is based on the attention-based encoder-decoder framework described in Luong (2015) , using the attention mechanism referred to as \"Global attention (dot)."}
{"sent_id": "fa33495582abd0c6efe8f599c73d0e-C001-61", "intents": ["@DIF@"], "paper_id": "ABC_fa33495582abd0c6efe8f599c73d0e_21", "text": "The Y-LSTM model underperformed relative to the model based on Luong (2015) , but provided a small additional boost to the ensemble."}
{"sent_id": "13fe4afa75c5a02727cb8ce3a73297-C001-67", "intents": ["@USE@"], "paper_id": "ABC_13fe4afa75c5a02727cb8ce3a73297_22", "text": "Following Fan et al. (2018) , we count a random story sample as correct when it ranks the true prompt with the lowest perplexity."}
{"sent_id": "13fe4afa75c5a02727cb8ce3a73297-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_13fe4afa75c5a02727cb8ce3a73297_22", "text": "WritingPrompts (Fan et al., 2018 ) is a dataset of prompts and short stories crawled from Reddit."}
{"sent_id": "13fe4afa75c5a02727cb8ce3a73297-C001-65", "intents": ["@BACK@"], "paper_id": "ABC_13fe4afa75c5a02727cb8ce3a73297_22", "text": "Prompt ranking (Fan et al., 2018) assesses how well a model matches a story to its given prompt."}
{"sent_id": "13fe4afa75c5a02727cb8ce3a73297-C001-80", "intents": ["@UNSURE@"], "paper_id": "ABC_13fe4afa75c5a02727cb8ce3a73297_22", "text": "Pre-training: We compare our models with the attention-based Fusion Model (Fan et al., 2018) , which has been designed for and trained on WritingPrompts."}
{"sent_id": "2ef456a3f6b043350121c4c5cfd404-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_2ef456a3f6b043350121c4c5cfd404_22", "text": "Hence, an adaptive IS may use a large number of samples to solve this problem whereas NCE is more stable and requires a fixed small number of noise samples (e.g., 100) to achieve a good performance [13, 16] ."}
{"sent_id": "2ef456a3f6b043350121c4c5cfd404-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_2ef456a3f6b043350121c4c5cfd404_22", "text": "To alleviate this problem, noise samples can be shared across the batch [16] ."}
{"sent_id": "2ef456a3f6b043350121c4c5cfd404-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_2ef456a3f6b043350121c4c5cfd404_22", "text": "This can be done by simply drawing an additional K samples form the noise distribution pn, and share them across the batch as it was done in [16] ."}
{"sent_id": "d13502d44435988822e59bcf66b635-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_d13502d44435988822e59bcf66b635_22", "text": "Because LSH alone performed ineffectively, Petrovic et al. (2010) additionally compared each incoming tweet with the k most recent tweets."}
{"sent_id": "d13502d44435988822e59bcf66b635-C001-102", "intents": ["@BACK@"], "paper_id": "ABC_d13502d44435988822e59bcf66b635_22", "text": "LSH-FSD is a highly-scalable system by Petrovic et al. (2010) ."}
{"sent_id": "d13502d44435988822e59bcf66b635-C001-131", "intents": ["@DIF@"], "paper_id": "ABC_d13502d44435988822e59bcf66b635_22", "text": "Although Petrovic et al. (2010) designed their system (LSH-FSD) to operate in constant space, we found that the memory requirement gradually increases with the number of documents processed, as seen in Figure 3 ."}
{"sent_id": "375b9c865d9f1b559387aa01a20a78-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_375b9c865d9f1b559387aa01a20a78_22", "text": "In recent years, an increasing number of studies have investigated character-level models with subwords in both unsupervised (Bojanowski et al., 2017; Pagliardini et al., 2018) and supervised learning (Zhang et al., 2015; Sennrich et al., 2016; Wieting et al., 2016; Lee et al., 2017) ."}
{"sent_id": "375b9c865d9f1b559387aa01a20a78-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_375b9c865d9f1b559387aa01a20a78_22", "text": "In addition, OOV (or unseen) words can be composed from sub-words, which are present at training (Bojanowski et al., 2017) ."}
{"sent_id": "375b9c865d9f1b559387aa01a20a78-C001-68", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_375b9c865d9f1b559387aa01a20a78_22", "text": "Most of the settings are the same as that of Bojanowski et al. (2017) ."}
{"sent_id": "87a190b1df5a7a941ba7b9a98064a3-C001-34", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_87a190b1df5a7a941ba7b9a98064a3_22", "text": "We employ the same convolution tree kernel used by Collins and Duffy (2001) , Moschitti (2004) and Zhang et al. (2006) ."}
{"sent_id": "87a190b1df5a7a941ba7b9a98064a3-C001-47", "intents": ["@SIM@"], "paper_id": "ABC_87a190b1df5a7a941ba7b9a98064a3_22", "text": "This case is also explored by Zhang et al. (2006) , and we include it here just for the purpose of comparison."}
{"sent_id": "87a190b1df5a7a941ba7b9a98064a3-C001-42", "intents": ["@USE@"], "paper_id": "ABC_87a190b1df5a7a941ba7b9a98064a3_22", "text": "(1) Compressed Path-enclosed Tree (CPT, T1 in Fig.1 ): Originated from PT in Zhang et al. (2006) , we further make two kinds of compression."}
{"sent_id": "be39cfec0479ace0a7e08508239cb0-C001-106", "intents": ["@BACK@"], "paper_id": "ABC_be39cfec0479ace0a7e08508239cb0_22", "text": "Text embeddings and probe tasks A variety of methods exist for obtaining fixed-length dense vector representations of words (e.g., Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018) , sentences (e.g., Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018) , and larger bodies of text (e.g., Le and Mikolov, 2014; Dai et al., 2015; Iyyer et al., 2015; Li et al., 2015; Chen, 2017; Zhang et al., 2017) To analyze word and sentence embeddings, recent work has studied classification tasks that probe them for various linguistic properties (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017a,b; Conneau et al., 2018; Tenney et al., 2019) ."}
{"sent_id": "be39cfec0479ace0a7e08508239cb0-C001-90", "intents": ["@UNSURE@"], "paper_id": "ABC_be39cfec0479ace0a7e08508239cb0_22", "text": "For each task, we compare against the original CNN-R model in (Zhang et al., 2017) ."}
{"sent_id": "be39cfec0479ace0a7e08508239cb0-C001-28", "intents": ["@DIF@"], "paper_id": "ABC_be39cfec0479ace0a7e08508239cb0_22", "text": "In this section, we first fully specify our probe task before comparing the model of Zhang et al. (2017) to a simple bag-of-words model."}
{"sent_id": "45551e674210bb9bbb56c8778d2f8c-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_45551e674210bb9bbb56c8778d2f8c_22", "text": "To model homophily, recent research in abusive language detection on Twitter (Mishra et al., 2018a) incorporates embeddings for authors (i.e., users who have composed tweets) that encode the structure of their surrounding communities."}
{"sent_id": "45551e674210bb9bbb56c8778d2f8c-C001-27", "intents": ["@USE@"], "paper_id": "ABC_45551e674210bb9bbb56c8778d2f8c_22", "text": "Qian et al. (2018) Following previous work (Mishra et al., 2018a) , we experiment with a subset of the Twitter dataset compiled by Waseem and Hovy (2016"}
{"sent_id": "45551e674210bb9bbb56c8778d2f8c-C001-32", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_45551e674210bb9bbb56c8778d2f8c_22", "text": "We create two different graphs: the first one is identical to the community graph of Mishra et al. (2018a) (referred to as the community graph)."}
{"sent_id": "ebb79e6e223d4747987aa4abfd1a58-C001-69", "intents": ["@EXT@"], "paper_id": "ABC_ebb79e6e223d4747987aa4abfd1a58_22", "text": "The implementation proposed by Artetxe et al. (2018b) 7 was modified to conduct the experiments."}
{"sent_id": "3ced64da2c64b0963c4c3d88fd60e0-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_3ced64da2c64b0963c4c3d88fd60e0_22", "text": "Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004) ."}
{"sent_id": "3ced64da2c64b0963c4c3d88fd60e0-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_3ced64da2c64b0963c4c3d88fd60e0_22", "text": "This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004) , or as a single joint process of finding a morpheme/POS string from unsegmented text (Kudo et al., 2004; Nakagawa, 2004; Kruengkrai et al., 2009) ."}
{"sent_id": "ce86cf36ee3b359c34b68e5d82b563-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_ce86cf36ee3b359c34b68e5d82b563_22", "text": "Two ten-letter strings have anywhere from 26,797 to 8,079,453 different alignments depending on exactly what alignments are considered distinct (Covington 1996, Covington and Canfield 1996) ."}
{"sent_id": "ce86cf36ee3b359c34b68e5d82b563-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_ce86cf36ee3b359c34b68e5d82b563_22", "text": "The phonetic similarity criterion used by Covington (1996) is shown in Table 1 ."}
{"sent_id": "ce86cf36ee3b359c34b68e5d82b563-C001-66", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_ce86cf36ee3b359c34b68e5d82b563_22", "text": "Accordingly, when computing badness I count each skip only once (assessing it 50 points), then ignore skips when comparing the segments against each other. I have not implemented the rule from Covington (1996) that gives a reduced penalty for adjacent skips in the same string to reflect the fact that affixes tend to be contiguous."}
{"sent_id": "ce86cf36ee3b359c34b68e5d82b563-C001-71", "intents": ["@USE@"], "paper_id": "ABC_ce86cf36ee3b359c34b68e5d82b563_22", "text": "Accordingly, I follow Covington (1996) in recasting the problem as a tree search."}
{"sent_id": "ce86cf36ee3b359c34b68e5d82b563-C001-87", "intents": ["@USE@"], "paper_id": "ABC_ce86cf36ee3b359c34b68e5d82b563_22", "text": "Following Covington (1996) , I implemented a very simple pruning strategy."}
{"sent_id": "6cd4235e66a6e6e9768250c3db7fc6-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_6cd4235e66a6e6e9768250c3db7fc6_22", "text": "However, no TempEval-2 participants addressed Chinese and only Angeli and Uszkoreit (2013) report evaluation results on this corpus."}
{"sent_id": "6cd4235e66a6e6e9768250c3db7fc6-C001-103", "intents": ["@UNSURE@"], "paper_id": "ABC_6cd4235e66a6e6e9768250c3db7fc6_22", "text": "In addition, we compare our results for the normalization sub-task to Angeli and Uszkoreit (2013) ."}
{"sent_id": "6cd4235e66a6e6e9768250c3db7fc6-C001-106", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_6cd4235e66a6e6e9768250c3db7fc6_22", "text": "Corpus: We use three versions of the TempEval-2 training and test sets: (i) the original versions, (ii) the improved versions described in Section 3.3, and (iii) the cleaned versions also used by Angeli and Uszkoreit (2013) in which temporal expressions without value information are removed."}
{"sent_id": "6cd4235e66a6e6e9768250c3db7fc6-C001-127", "intents": ["@DIF@"], "paper_id": "ABC_6cd4235e66a6e6e9768250c3db7fc6_22", "text": "Table 4 shows the comparison between our approach and the one by Angeli and Uszkoreit (2013) ."}
{"sent_id": "022049c0e75a490978b2c49da41deb-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_022049c0e75a490978b2c49da41deb_22", "text": "Recent NLP work on semantic idiomaticity has focused on the task of \"compositionality prediction\", in the form of a regression task whereby a given MWE is mapped onto a continuous-valued compositionality score, either for the MWE as a whole or for each of its component words (Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi et al., 2014b) ."}
{"sent_id": "022049c0e75a490978b2c49da41deb-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_022049c0e75a490978b2c49da41deb_22", "text": "There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014; Salehi et al., 2014b; Schneider et al., 2014) ."}
{"sent_id": "022049c0e75a490978b2c49da41deb-C001-75", "intents": ["@SIM@"], "paper_id": "ABC_022049c0e75a490978b2c49da41deb_22", "text": "Note that for EVPC, we don't use the vector for the particle, in keeping with Salehi et al. (2014b) ; as such, there are no results for comp 2 ."}
{"sent_id": "742d9ca22bf801b0ade5fd1671473c-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_742d9ca22bf801b0ade5fd1671473c_22", "text": "To train these fully data driven models, large-scale datasets for both English and Chinese were recently introduced (Wang et al., 2017; Koncel-Kedziorski et al., 2016) ."}
{"sent_id": "742d9ca22bf801b0ade5fd1671473c-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_742d9ca22bf801b0ade5fd1671473c_22", "text": "In response to the success of representation learning elsewhere in NLP, sequence to sequence (seq2seq) models have been applied to algebra problem solving (Wang et al., 2017) ."}
{"sent_id": "742d9ca22bf801b0ade5fd1671473c-C001-78", "intents": ["@USE@"], "paper_id": "ABC_742d9ca22bf801b0ade5fd1671473c_22", "text": "Datasets For comparison, we report solution accuracy on the Chinese language Math23K dataset (Wang et al., 2017) , and the English language DRAW (Upadhyay and Chang, 2015) and MAWPS (Koncel-Kedziorski et al., 2016) datasets."}
{"sent_id": "a5f00f524fdf18e62a4e98a92a2d82-C001-17", "intents": ["@USE@"], "paper_id": "ABC_a5f00f524fdf18e62a4e98a92a2d82_22", "text": "We therefore explore an alternative approach to Arabic SA on social media, using off-the-shelf Machine Translation systems to translate Arabic tweets into English and then use a state-of-the-art sentiment classifier (Socher et al., 2013) to assign sentiment labels."}
{"sent_id": "a5f00f524fdf18e62a4e98a92a2d82-C001-132", "intents": ["@USE@"], "paper_id": "ABC_a5f00f524fdf18e62a4e98a92a2d82_22", "text": "We then use the Stanford Sentiment Classifier (Socher et al., 2013) to automatically assign sentiment labels (positive, negative) to translated tweets."}
{"sent_id": "26658b95c9bac96f1206da96b95921-C001-5", "intents": ["@EXT@"], "paper_id": "ABC_26658b95c9bac96f1206da96b95921_22", "text": "In this paper, we introduce graded matrix grammars of natural language, a variant of the matrix grammars proposed by Rudolph and Giesbrecht (2010) , and show a close correspondence between this matrix-space model and weighted finite automata."}
{"sent_id": "26658b95c9bac96f1206da96b95921-C001-29", "intents": ["@EXT@"], "paper_id": "ABC_26658b95c9bac96f1206da96b95921_22", "text": "In this paper, we are concerned with Graded Matrix Grammars, a variant of the Matrix Grammars of Rudolph and Giesbrecht (2010) , where instead of the \"yes or no\" decision, if a sequence is part of a language, a real-valued score is assigned."}
{"sent_id": "26658b95c9bac96f1206da96b95921-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_26658b95c9bac96f1206da96b95921_22", "text": "To overcome the limitations of VSMs, Rudolph and Giesbrecht (2010) proposed Compositional Matrix-Space Models (CMSM) as a recent alternative model to work with distributional approaches."}
{"sent_id": "26658b95c9bac96f1206da96b95921-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_26658b95c9bac96f1206da96b95921_22", "text": "Rudolph and Giesbrecht (2010) showed theoretically that by employing matrices instead of vectors, CMSMs subsume a wide range of linguistic models such as statistical models (vector-space models and word space models)."}
{"sent_id": "237ac6f9b635e56119be956d7521e1-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_237ac6f9b635e56119be956d7521e1_22", "text": "Meanwhile, word representation models based on subword units, such as characters or word segments, have been shown to perform well in many NLP tasks such as POS tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015) , language modeling (Ling et al., 2015; Kim et al., 2016; Vania and Lopez, 2017) , machine translation (Vylomova et al., 2016; Lee et al., 2016; Sennrich et al., 2016) , dependency parsing (Ballesteros et al., 2015) , and sequence labeling (Rei et al., 2016; Lample et al., 2016) ."}
{"sent_id": "cd56849805cdb43bba567f74b31b87-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_cd56849805cdb43bba567f74b31b87_22", "text": "This availability of annotated data in English has translated into the development of a plethora of models, including encoder-decoders (Dong and Lapata, 2016; Jia and Liang, 2016) as well as tree or graph-structured decoders Lapata, 2016, 2018; Liu et al., 2018; Yin and Neubig, 2017) ."}
{"sent_id": "cd56849805cdb43bba567f74b31b87-C001-58", "intents": ["@BACK@"], "paper_id": "ABC_cd56849805cdb43bba567f74b31b87_22", "text": "We use the PMB v. In order to be used as input to the parser, Liu et al. (2018) first convert the DRS into treebased representations, which are subsequently linearized into PTB-style bracketed sequences."}
{"sent_id": "cd56849805cdb43bba567f74b31b87-C001-84", "intents": ["@BACK@"], "paper_id": "ABC_cd56849805cdb43bba567f74b31b87_22", "text": "It is worth reminding that unlike other work on the PMB (e.g. van Noord et al., 2018), Liu et al. (2018) does not deal with presupposition."}
{"sent_id": "cd56849805cdb43bba567f74b31b87-C001-32", "intents": ["@EXT@"], "paper_id": "ABC_cd56849805cdb43bba567f74b31b87_22", "text": "In this section, we describe the modifications to the coarse-to-fine encoder-decoder architecture of Liu et al. (2018) ; for more detail, we refer the reader to the original paper."}
{"sent_id": "cd10d509dacd8f55993396258eb92a-C001-84", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_cd10d509dacd8f55993396258eb92a_22", "text": "We use context-independent phonemes as in [16] ."}
{"sent_id": "a0c0076fa8c3be914d93ec1d66d0c1-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_a0c0076fa8c3be914d93ec1d66d0c1_22", "text": "More recently, Kisselew et al. (2015) put the task of modeling derivation into the perspective of zero-shot-learning: instead of using cosine similarities they predicted the derived term by learning a mapping function between the base term and the derived term."}
{"sent_id": "a0c0076fa8c3be914d93ec1d66d0c1-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_a0c0076fa8c3be914d93ec1d66d0c1_22", "text": "The experiments by Kisselew et al. (2015) were performed over six derivational patterns for German (cf."}
{"sent_id": "a0c0076fa8c3be914d93ec1d66d0c1-C001-33", "intents": ["@SIM@"], "paper_id": "ABC_a0c0076fa8c3be914d93ec1d66d0c1_22", "text": "We created a new collection of German particle verb derivations 1 relying on the same resource as Kisselew et al. (2015) , the semiautomatic derivational lexicon for German DErivBase (Zeller et al., 2013) ."}
{"sent_id": "c022b7cf4568e26c7408a835eaafb7-C001-5", "intents": ["@EXT@"], "paper_id": "ABC_c022b7cf4568e26c7408a835eaafb7_22", "text": "Parses are weighted by their probabilities and combined using an adapted version of Sagae and Lavie (2006) ."}
{"sent_id": "c022b7cf4568e26c7408a835eaafb7-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_c022b7cf4568e26c7408a835eaafb7_22", "text": "Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman anď Zabokrtskỳ, 2005; Sagae and Lavie, 2006; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015) ."}
{"sent_id": "c022b7cf4568e26c7408a835eaafb7-C001-30", "intents": ["@USE@"], "paper_id": "ABC_c022b7cf4568e26c7408a835eaafb7_22", "text": "Our extension takes the n-best trees from a parser as if they are 1-best parses from n parsers, then follows Sagae and Lavie (2006) ."}
{"sent_id": "c022b7cf4568e26c7408a835eaafb7-C001-112", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_c022b7cf4568e26c7408a835eaafb7_22", "text": "We explored two ways to apply fusion when starting from constituency parses: (1) fuse constituents and then convert them to dependencies and (2) convert to dependencies then fuse the dependencies as in Sagae and Lavie (2006) ."}
{"sent_id": "0143619c1c54129702aafb585463d2-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_0143619c1c54129702aafb585463d2_23", "text": "While useful, citation texts might lack the appropriate context from the reference article [4, 5, 18] ."}
{"sent_id": "0143619c1c54129702aafb585463d2-C001-101", "intents": ["@BACK@"], "paper_id": "ABC_0143619c1c54129702aafb585463d2_23", "text": "The best baseline performance is the query reformulation (QR) method by [4] which improves over other baselines."}
{"sent_id": "0143619c1c54129702aafb585463d2-C001-149", "intents": ["@SIM@"], "paper_id": "ABC_0143619c1c54129702aafb585463d2_23", "text": "The most relevant prior work to ours is [4] where the authors approached the problem using a vector space model similarity ranking and query reformulations."}
{"sent_id": "b1c06a67b03d81b249b320413a6e7e-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_b1c06a67b03d81b249b320413a6e7e_23", "text": "As a result various knowledge bases have been produced like TopicSignatures (Agirre and Lacalle, 2004) , ConceptNet (Liu and Singh, 2004) , Yago (Suchanek et al., 2007) , NELL (Carlson et al., 2009) and ReVerb (Fader et al., 2011) ."}
{"sent_id": "b1c06a67b03d81b249b320413a6e7e-C001-116", "intents": ["@BACK@"], "paper_id": "ABC_b1c06a67b03d81b249b320413a6e7e_23", "text": "According to (Fader et al., 2011) ReVerb outperforms TextRunner (Banko et al., 2007) and the open Wikipedia extractor WOE (Wu and Weld, 2010) in terms of the quantity and quality of the learned relations."}
{"sent_id": "b1c06a67b03d81b249b320413a6e7e-C001-122", "intents": ["@SIM@"], "paper_id": "ABC_b1c06a67b03d81b249b320413a6e7e_23", "text": "We have evaluated the accuracy of our approach using human based evaluation and have compared results against the ReVerb (Fader et al., 2011) system and existing knowledge bases like NELL (Carlson et al., 2009) , Yago (Suchanek et al., 2007) and ConceptNet (Liu and Singh, 2004) ."}
{"sent_id": "a334cda78f8ba6dea709809f0999b6-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_a334cda78f8ba6dea709809f0999b6_23", "text": "Several methods have been proposed to test for the presence of gender bias in word embeddings; an example being the Word Embedding Association Test (WEAT) (Caliskan et al., 2017) ."}
{"sent_id": "a334cda78f8ba6dea709809f0999b6-C001-69", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_a334cda78f8ba6dea709809f0999b6_23", "text": "We largely follow the WEAT Hypothesis testing protocol introduced by Caliskan et al. (2017) ."}
{"sent_id": "cc992a7a918858f9e04b9bb5c15c3f-C001-6", "intents": ["@BACK@"], "paper_id": "ABC_cc992a7a918858f9e04b9bb5c15c3f_23", "text": "We present positive results from a fully automated judge for semantic similarity based on Referential Translation Machines (Biçici and Way, 2014b) in two semantic similarity tasks at SemEval-2015, Semantic Evaluation Exercises -International Workshop on Semantic Evaluation (Nakov et al., 2015)."}
{"sent_id": "cc992a7a918858f9e04b9bb5c15c3f-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_cc992a7a918858f9e04b9bb5c15c3f_23", "text": "RTMs achieve (i) top performance when predicting the quality of translations (Biçici, 2013; Biçici and Way, 2014a) ; (ii) top performance when predicting monolingual cross-level semantic similarity; (iii) second performance when predicting paraphrase and semantic similarity in Twitter (iv) good performance when judging the semantic similarity of sentences; (iv) good performance when evaluating the semantic relatedness of sentences and their entailment (Biçici and Way, 2014b) ."}
{"sent_id": "cc992a7a918858f9e04b9bb5c15c3f-C001-56", "intents": ["@BACK@"], "paper_id": "ABC_cc992a7a918858f9e04b9bb5c15c3f_23", "text": "More details about the optimization processes are in (Biçici and Way, 2014b; Biçici et al., 2014) ."}
{"sent_id": "cc992a7a918858f9e04b9bb5c15c3f-C001-95", "intents": ["@BACK@"], "paper_id": "ABC_cc992a7a918858f9e04b9bb5c15c3f_23", "text": "In Table 8 , we list the RAE, MAER, and MRAER obtained for different tasks and subtasks, also listing RTM results from SemEval-2013 , from SemEval-2014 (Biçici and Way, 2014b) , and and from quality estimation task (QET) (Biçici and Way, 2014a ) of machine translation (Bojar et al., 2014) ."}
{"sent_id": "86af8f2cc08b00821a7a83abdfd964-C001-46", "intents": ["@DIF@"], "paper_id": "ABC_86af8f2cc08b00821a7a83abdfd964_23", "text": "Learning from multiple languages have been shown to be of benefit both in unsupervised learning of syntax and part-of-speech (Snyder et al., 2009; BergKirkpatrick and Klein, 2010) and in transfer learning of dependency syntax (Cohen et al., 2011; 256 cross-lingual word clusters and the same feature templates as Täckström et al. (2012) , with the exception that the transition factors are not conditioned on the input."}
{"sent_id": "86af8f2cc08b00821a7a83abdfd964-C001-29", "intents": ["@BACK@"], "paper_id": "ABC_86af8f2cc08b00821a7a83abdfd964_23", "text": "This idea is at the heart of both direct transfer methods Täckström et al., 2012) and of annotation projection methods (Yarowsky et al., 2001; Diab and Resnik, 2002; Hwa et al., 2005) ."}
{"sent_id": "86af8f2cc08b00821a7a83abdfd964-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_86af8f2cc08b00821a7a83abdfd964_23", "text": "Recently, Täckström et al. (2012) developed an algorithm for inducing cross-lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems."}
{"sent_id": "86af8f2cc08b00821a7a83abdfd964-C001-19", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_86af8f2cc08b00821a7a83abdfd964_23", "text": "In this study, we turn to direct transfer methods Täckström et al., 2012) as a way to combat the need for annotated resources in all languages."}
{"sent_id": "918caabbc0bcad04cd07761b29e767-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_918caabbc0bcad04cd07761b29e767_23", "text": "Zhang and Lee (2003) used the same taxonomy as Li and Roth (2002) , as well as the same training and testing data."}
{"sent_id": "918caabbc0bcad04cd07761b29e767-C001-96", "intents": ["@BACK@"], "paper_id": "ABC_918caabbc0bcad04cd07761b29e767_23", "text": "The results in this paper indicate that some of the results found in previous work (Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003) on question classification might be incorrect due to an unbiased training and test corpus."}
{"sent_id": "918caabbc0bcad04cd07761b29e767-C001-56", "intents": ["@SIM@", "@USE@"], "paper_id": "ABC_918caabbc0bcad04cd07761b29e767_23", "text": "The corpora used is both the corpus constructed and tagged by Li and Roth (2002) , as well as a newly tagged corpus extracted from the AnswerBus logs."}
{"sent_id": "fb1061d28dbf80858c1a630621a975-C001-115", "intents": ["@BACK@"], "paper_id": "ABC_fb1061d28dbf80858c1a630621a975_23", "text": "Baseline: (1) CISIR-SHCNN (Jiang et al., 2018) : A recently proposed model based on CNN and ranking message pairs."}
{"sent_id": "fb1061d28dbf80858c1a630621a975-C001-30", "intents": ["@USE@"], "paper_id": "ABC_fb1061d28dbf80858c1a630621a975_23", "text": "The contribution of this work is two-fold: 1) We propose context-aware deep learning models for thread detection and it advances the state-ofthe-art; 2) Based on the dataset in (Jiang et al., 2018) , we develop and release a more realistic multi-party multi-thread conversation dataset for future research."}
{"sent_id": "fb1061d28dbf80858c1a630621a975-C001-95", "intents": ["@USE@", "@SIM@"], "paper_id": "ABC_fb1061d28dbf80858c1a630621a975_23", "text": "We strictly follow (Jiang et al., 2018) to construct our data."}
{"sent_id": "fb1061d28dbf80858c1a630621a975-C001-44", "intents": ["@SIM@"], "paper_id": "ABC_fb1061d28dbf80858c1a630621a975_23", "text": "These two features are also used in (Jiang et al., 2018) , and another baseline model GTM uses only these features (Elsner and Charniak, 2008) ."}
{"sent_id": "520588fbf0643725153b07a09430d1-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_520588fbf0643725153b07a09430d1_23", "text": "Recent researches on sequence-to-sequence attention-based models try to remove this dependency on the pronunciation lexicon [8, 9, 10] ."}
{"sent_id": "520588fbf0643725153b07a09430d1-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_520588fbf0643725153b07a09430d1_23", "text": "We generate more training data by linearly scaling the audio lengths by factors of 0.9 and 1.1 [22] , since it is always beneficial for training the ASR Transformer [9] ."}
{"sent_id": "29294f2ed3cc2772ca57fd4294274c-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_29294f2ed3cc2772ca57fd4294274c_23", "text": "Leuski et al. (2006) developed algorithms for training such characters using linked questions and responses in the form of unstructured natural language text."}
{"sent_id": "29294f2ed3cc2772ca57fd4294274c-C001-62", "intents": ["@DIF@"], "paper_id": "ABC_29294f2ed3cc2772ca57fd4294274c_23", "text": "We did not implement the parameter learning of Leuski et al. (2006) ; instead we use a constant smoothing parameter λ π = λ φ = 0.1."}
{"sent_id": "29294f2ed3cc2772ca57fd4294274c-C001-110", "intents": ["@DIF@"], "paper_id": "ABC_29294f2ed3cc2772ca57fd4294274c_23", "text": "In our experiments the LM approach consistently outperforms the CLM approach, contra Leuski et al. (2006) ."}
{"sent_id": "bce5c3bf551a8aa211dfd962cde7a8-C001-133", "intents": ["@BACK@"], "paper_id": "ABC_bce5c3bf551a8aa211dfd962cde7a8_23", "text": "The conclusion of Zhang et al. (2008) has been reconfirmed on multiple languages for which we handbuilt HPSG grammars exist, even where grammatical coverage is low."}
{"sent_id": "bce5c3bf551a8aa211dfd962cde7a8-C001-73", "intents": ["@SIM@"], "paper_id": "ABC_bce5c3bf551a8aa211dfd962cde7a8_23", "text": "Deep Semantic Features Similar to Zhang et al. (2008) , we extract a set of features from the semantic outputs (MRS) of the HPSG parses."}
{"sent_id": "bce5c3bf551a8aa211dfd962cde7a8-C001-116", "intents": ["@SIM@"], "paper_id": "ABC_bce5c3bf551a8aa211dfd962cde7a8_23", "text": "Confirming the observation of Zhang et al. (2008) , the gain with HPSG features is more significant on outdomain tests, this time on German as well."}
{"sent_id": "bce5c3bf551a8aa211dfd962cde7a8-C001-33", "intents": ["@DIF@"], "paper_id": "ABC_bce5c3bf551a8aa211dfd962cde7a8_23", "text": "While Zhang et al. (2008) only used seman- tic features from HPSG parsing in the SRL task, we added extra syntactic features from deep parsing to help both tasks."}
{"sent_id": "836992d035c4be0c8eacdd419f151e-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_836992d035c4be0c8eacdd419f151e_23", "text": "Eshel et al. (2017) , allowing the model to weight the importance of the outputs of the GRU at each time step."}
{"sent_id": "836992d035c4be0c8eacdd419f151e-C001-55", "intents": ["@DIF@"], "paper_id": "ABC_836992d035c4be0c8eacdd419f151e_23", "text": "It also outperforms the roughly similar model of Eshel et al. (2017) on the test set: this gain is due to a combination of factors including the improved training procedure and some small modeling changes."}
{"sent_id": "4b65a59fc2331b9771ea09a12f32de-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_4b65a59fc2331b9771ea09a12f32de_23", "text": "Recently, Locascio et al. (2016) designed the Deep-Regex model based on the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014) using minimal domain knowledge during the learning phase while still accurately predicting regular expressions from NLs."}
{"sent_id": "4b65a59fc2331b9771ea09a12f32de-C001-89", "intents": ["@BACK@"], "paper_id": "ABC_4b65a59fc2331b9771ea09a12f32de_23", "text": "On the other hand, NL-RX-Synth is data generated automatically and NL-RX-Turk is made from ordinary people by paraphrasing NL descriptions in NL-RX-Synth using Mechanical Turk (Locascio et al., 2016) ."}
{"sent_id": "4b65a59fc2331b9771ea09a12f32de-C001-114", "intents": ["@BACK@"], "paper_id": "ABC_4b65a59fc2331b9771ea09a12f32de_23", "text": "Specifically, there are some ambiguities since Locascio et al. (2016) tried to obtain data from machine-generated sentences."}
{"sent_id": "dd603c79f87e98d23f6f8e13028ae9-C001-44", "intents": ["@SIM@"], "paper_id": "ABC_dd603c79f87e98d23f6f8e13028ae9_23", "text": "Other model parameters reflect the implementation outlined in Michel and Neubig (2018) ."}
{"sent_id": "dd603c79f87e98d23f6f8e13028ae9-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_dd603c79f87e98d23f6f8e13028ae9_23", "text": "In this work we present two primary methods of synthesizing natural noise in accordance with the types of noise identified in prior work (Eisenstein, 2013; Michel and Neubig, 2018) as naturally occurring in internet and social media based text."}
{"sent_id": "874a8d4f847aff2895deb7c7560c56-C001-178", "intents": ["@BACK@"], "paper_id": "ABC_874a8d4f847aff2895deb7c7560c56_23", "text": "[9] prepared their data by stemming and then creating unigram, bigram and trigram features, each of which was weighted by its TF-IDF metric."}
{"sent_id": "874a8d4f847aff2895deb7c7560c56-C001-336", "intents": ["@BACK@"], "paper_id": "ABC_874a8d4f847aff2895deb7c7560c56_23", "text": "For Religion data set, this data set collected after Woolwich event, which contains language about Muslim and African man (religion and race), according to [9] Twitter users use this type of language in their everyday communications which could be contained in the benign tweets."}
{"sent_id": "b3f7051cbba3344f0aec0f2e80d5e0-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_b3f7051cbba3344f0aec0f2e80d5e0_23", "text": "In other cases, up to 6 different system SAs were selected (DeVault et al., 2011) ."}
{"sent_id": "b3f7051cbba3344f0aec0f2e80d5e0-C001-65", "intents": ["@BACK@"], "paper_id": "ABC_b3f7051cbba3344f0aec0f2e80d5e0_23", "text": "(We do not expect that an automatic system would outperform a human referee.) This score is .79; see DeVault et al. (2011) for discussion."}
{"sent_id": "b5149b6136c8baaed8356b562d3f96-C001-3", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_b5149b6136c8baaed8356b562d3f96_23", "text": "While Ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed (Moore, 2014) makes tagging as fast as with Ratnaparkhi's tag dictionary, but with no decrease in accuracy."}
{"sent_id": "b5149b6136c8baaed8356b562d3f96-C001-48", "intents": ["@BACK@"], "paper_id": "ABC_b5149b6136c8baaed8356b562d3f96_23", "text": "We previously presented (Moore, 2014) a tag dictionary constructed by using the annotated training set to compute a smoothed probability estimate for any possible tag given any possible word, and for each word in the training set, including in the dictionary the tags having an estimated probability greater than a fixed threshold T ."}
{"sent_id": "b5149b6136c8baaed8356b562d3f96-C001-99", "intents": ["@BACK@"], "paper_id": "ABC_b5149b6136c8baaed8356b562d3f96_23", "text": "The tagger we used is based on the fastest of the methods described in our previous work (Moore, 2014, Section 3.1) ."}
{"sent_id": "b5149b6136c8baaed8356b562d3f96-C001-122", "intents": ["@BACK@"], "paper_id": "ABC_b5149b6136c8baaed8356b562d3f96_23", "text": "For comparison, we tested our previous tagger and the fast version (english-left3words-distsim) of the Stanford tagger (Toutanova et al., 2003; Manning, 2011) recommended for practical use on the Stanford tagger website, which we found to be by far the fastest of the six publicly available taggers tested in our previous work (Moore, 2014) ."}
{"sent_id": "b5149b6136c8baaed8356b562d3f96-C001-131", "intents": ["@DIF@"], "paper_id": "ABC_b5149b6136c8baaed8356b562d3f96_23", "text": "It reduces the mean number of possible tags per token by 57% and increases the number of unambiguous tokens by by 47%, compared to the previous state of the art (Moore, 2014) for a tag dictionary that does not degrade tagging accuracy."}
{"sent_id": "09dfa2f17283fe6b3fc28383f36732-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_09dfa2f17283fe6b3fc28383f36732_23", "text": "In [10] , a joint model is proposed to learn robot affordances (i. e., relationships between actions, objects and resulting effects) together with word meanings."}
{"sent_id": "09dfa2f17283fe6b3fc28383f36732-C001-80", "intents": ["@BACK@"], "paper_id": "ABC_09dfa2f17283fe6b3fc28383f36732_23", "text": "In the experimental section, we will show that what the robot has learned subjectively or alone (by self-exploration, knowing the action identity as a prior [10] ), can subsequently be used when observing a new agent (human), provided that the actions can be estimated with Gesture HMMs as in [4] ."}
{"sent_id": "4cf805818bed233fabb81f5f64f4cc-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_4cf805818bed233fabb81f5f64f4cc_23", "text": "For example, Bohnet and Nivre (2012) had to carefully discretize the real-valued POS tag score in order to combine it with the other discrete binary features in their system."}
{"sent_id": "4cf805818bed233fabb81f5f64f4cc-C001-64", "intents": ["@USE@"], "paper_id": "ABC_4cf805818bed233fabb81f5f64f4cc_23", "text": "The integrated arc-standard transition system of Bohnet and Nivre (2012) allows the parser to participate in tagging decisions, rather than being forced to treat the tagger's tags as given, as in the arc-standard system."}
{"sent_id": "4cf805818bed233fabb81f5f64f4cc-C001-98", "intents": ["@DIF@"], "paper_id": "ABC_4cf805818bed233fabb81f5f64f4cc_23", "text": "Additionally, our pipelined neural network parser always outperforms its linear counterpart, an in-house reimplementation of the system of Zhang and Nivre (2011) , as well as the more recent and highly accurate parsers of Zhang and McDonald (2014) and Lei et al. (2014 again outperforms its linear counterpart (Bohnet and Nivre, 2012) , however, in some cases the addition of graph-based and cluster features (Bohnet and Nivre, 2012 )+G+C can lead to even better results."}
{"sent_id": "1bcd442a685e5fb2d0f3f44d3c66c3-C001-51", "intents": ["@BACK@"], "paper_id": "ABC_1bcd442a685e5fb2d0f3f44d3c66c3_23", "text": "In all other cases and intermediate timesteps, the payoff is 0. Because of the high dimensional search space introduced due to brushstrokes, we use Proximal Policy Optimization (PPO) (Schulman et al. 2017) for optimizing the weights of sender and receiver agents."}
{"sent_id": "982991efdb6b14f187702e0a577bac-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_982991efdb6b14f187702e0a577bac_23", "text": "Recent work has applied neural networks (Mehri and Carenini, 2017; Guo et al. (2017) 1,500 1 48 hr 5 n/a 2 Table 1 : Annotated disentanglement dataset comparison."}
{"sent_id": "982991efdb6b14f187702e0a577bac-C001-67", "intents": ["@BACK@"], "paper_id": "ABC_982991efdb6b14f187702e0a577bac_23", "text": "Studies that do consider graphs for disentanglement have used small datasets (Dulceanu, 2016; Mehri and Carenini, 2017) that are not always released (Wang et al., 2008; Guo et al., 2017) ."}
{"sent_id": "982991efdb6b14f187702e0a577bac-C001-135", "intents": ["@DIF@"], "paper_id": "ABC_982991efdb6b14f187702e0a577bac_23", "text": "Interestingly, while κ was higher for us than Mehri and Carenini (2017) , our scores for conversations are lower."}
{"sent_id": "982991efdb6b14f187702e0a577bac-C001-153", "intents": ["@SIM@"], "paper_id": "ABC_982991efdb6b14f187702e0a577bac_23", "text": "For Channel Two we also compare to Wang and Oard (2009) and Mehri and Carenini (2017) , but their code was unavailable, preventing evaluation on our data."}
{"sent_id": "c0cac496ec0abdfd3f6bd9914f4cc4-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_c0cac496ec0abdfd3f6bd9914f4cc4_24", "text": "While WordNet Affect, EmoLex and AffectNet include terms with emotion labels, Affect database (Neviarouskaya et al., 2007) and DepecheMood (Staiano and Guerini, 2014) include words that have emotion scores instead, which can be useful for compositional computations of emotion scores."}
{"sent_id": "c0cac496ec0abdfd3f6bd9914f4cc4-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_c0cac496ec0abdfd3f6bd9914f4cc4_24", "text": "Staiano and Guerini (2014) utilized news articles from rappler.com."}
{"sent_id": "c0cac496ec0abdfd3f6bd9914f4cc4-C001-95", "intents": ["@SIM@"], "paper_id": "ABC_c0cac496ec0abdfd3f6bd9914f4cc4_24", "text": "Disgust was not aligned with any emotion in EmoWordNet and hence was discarded as also assumed in (Staiano and Guerini, 2014) ."}
{"sent_id": "6891aebc7bb1152884d2236a893b55-C001-76", "intents": ["@DIF@"], "paper_id": "ABC_6891aebc7bb1152884d2236a893b55_24", "text": "Our results conform to the results of (Grishina and Stede, 2017) : For both languages, pronouns exhibit the highest projection quality, while common and proper nouns are projected slightly less accurately, which is probably due to the fact that pronouns typically consist of single tokens and are better aligned than multi-token common and proper names."}
{"sent_id": "939274ae40a68acc322b34d8f91f7e-C001-137", "intents": ["@BACK@"], "paper_id": "ABC_939274ae40a68acc322b34d8f91f7e_24", "text": "Verga et al. (2018) considered multi-instance learning for document-level RE."}
{"sent_id": "dd875dd5c0f2558bb173f31bbdea00-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_dd875dd5c0f2558bb173f31bbdea00_24", "text": "According to Aristotle, there are four basic factors or causes by which the nature of an object can be described (cf. (Kronlid, 2003) ): the material cause, i.e. the material an object is made of the agentive cause, i.e. the source of movement, creation or change the formal cause, i.e. its form or type the final cause, i.e. its purpose, intention or aim In his Generative Lexicon (GL) framework (Pustejovsky, 1991) reused Aristotle's basic factors for the description of the meaning of lexical elements."}
{"sent_id": "dd875dd5c0f2558bb173f31bbdea00-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_dd875dd5c0f2558bb173f31bbdea00_24", "text": "Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in (Pustejovsky, 1991) however seem to have a more restricted interpretation."}
{"sent_id": "642aa9fe999d0b2b3793cb1603c04c-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_642aa9fe999d0b2b3793cb1603c04c_24", "text": "Recently, the mechanism of self-attention [22, 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time."}
{"sent_id": "642aa9fe999d0b2b3793cb1603c04c-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_642aa9fe999d0b2b3793cb1603c04c_24", "text": "The second sublayer is a position-wise feed-forward network [22] FFN(H) = ReLU(HW1 + b1)W2 + b2 where parameters"}
{"sent_id": "642aa9fe999d0b2b3793cb1603c04c-C001-82", "intents": ["@BACK@"], "paper_id": "ABC_642aa9fe999d0b2b3793cb1603c04c_24", "text": "Self-attention is inherently content-based [22] , and so one often encodes position into the post-embedding vectors."}
{"sent_id": "2fbf5397a8219923d1d9bc0464cb59-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_2fbf5397a8219923d1d9bc0464cb59_24", "text": "As for tree kernel-based methods, Yang et al (2006) captured syntactic structured information for pronoun resolution by using the convolution tree kernel (Collins and Duffy 2001) to measure the common sub-trees enumerated from the parse trees and achieved quite success on the ACE 2003 corpus."}
{"sent_id": "2fbf5397a8219923d1d9bc0464cb59-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_2fbf5397a8219923d1d9bc0464cb59_24", "text": "Figure 2 shows the three tree span schemes explored in Yang et al (2006) : MinExpansion (only including the shortest path connecting the anaphor and the antecedent candidate), Simple-Expansion (containing not only all the nodes in Min-Expansion but also the first level children of these nodes) and Full-Expansion (covering the sub-tree between the anaphor and the candidate), such as the sub-trees inside the dash circles of Figures 2(a) , 2(b) and 2(c) respectively."}
{"sent_id": "2fbf5397a8219923d1d9bc0464cb59-C001-63", "intents": ["@SIM@"], "paper_id": "ABC_2fbf5397a8219923d1d9bc0464cb59_24", "text": "To deal with the cases that an anaphor and an antecedent candidate do not occur in the same sentence, we construct a pseudo parse tree for an entire text by attaching the parse trees of all its sentences to an upper \"S \" node, similar to Yang et al (2006) ."}
{"sent_id": "f3f61d50929f862e263e3f658852bc-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_f3f61d50929f862e263e3f658852bc_24", "text": "Conv A is more convincing than B. Table 2 : The 17+1 practical reason labels given in the corpus of Habernal and Gurevych (2016a) ."}
{"sent_id": "f3f61d50929f862e263e3f658852bc-C001-77", "intents": ["@BACK@"], "paper_id": "ABC_f3f61d50929f862e263e3f658852bc_24", "text": "Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study (Habernal and Gurevych, 2016a) , these reasons were used to derive a hierarchical annotation scheme."}
{"sent_id": "f3f61d50929f862e263e3f658852bc-C001-148", "intents": ["@BACK@"], "paper_id": "ABC_f3f61d50929f862e263e3f658852bc_24", "text": "Regarding simplification, the most common practical reasons of Habernal and Gurevych (2016a) imply what to focus on."}
{"sent_id": "497b717bc4ff6b9e2160ee823f6b42-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_497b717bc4ff6b9e2160ee823f6b42_24", "text": "Latest results are based on neural network experience, but are far more simple: various versions of Word2Vec, Skip-gram and CBOW models [8] , which currently show the State-of-the-Art results and have proven success with morphologically complex languages like Russian [1] , [10] ."}
{"sent_id": "497b717bc4ff6b9e2160ee823f6b42-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_497b717bc4ff6b9e2160ee823f6b42_24", "text": "Traditional corpora for the word semantic similarity task are News, Wikipedia, electronic libraries and others (e.g. RUSSE workshop [10] )."}
{"sent_id": "497b717bc4ff6b9e2160ee823f6b42-C001-57", "intents": ["@BACK@"], "paper_id": "ABC_497b717bc4ff6b9e2160ee823f6b42_24", "text": "The RUSSE contest was followed by paper from its organizers [10] and several participators [1] , [5] , thus filling the gap in word semantic similarity task for Russian language."}
{"sent_id": "242aacd35fb92d836fea9eb33961a3-C001-97", "intents": ["@BACK@"], "paper_id": "ABC_242aacd35fb92d836fea9eb33961a3_24", "text": "FAIRSEQ supports language modeling with gated convolutional models and Transformer models (Vaswani et al., 2017) ."}
{"sent_id": "242aacd35fb92d836fea9eb33961a3-C001-94", "intents": ["@DIF@"], "paper_id": "ABC_242aacd35fb92d836fea9eb33961a3_24", "text": "We reported improved BLEU scores over Vaswani et al. (2017) by training with a bigger batch size and an increased learning rate ."}
{"sent_id": "817576dbe36f79ac3e0031211f400d-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_817576dbe36f79ac3e0031211f400d_24", "text": "BERT (Devlin et al., 2019) improved over previous transformer models and recurrent networks by allowing the system to learn from input text in a bidirectional way, rather than only from left-to-right or the other way around."}
{"sent_id": "817576dbe36f79ac3e0031211f400d-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_817576dbe36f79ac3e0031211f400d_24", "text": "This cornerstone was used for BERT, a transformer model that obtained stateof-the-art results for eleven natural language processing tasks, such as question answering and natural language inference (Devlin et al., 2019) ."}
{"sent_id": "817576dbe36f79ac3e0031211f400d-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_817576dbe36f79ac3e0031211f400d_24", "text": "This architecture has been shown to be a general language model that could be fine-tuned with little data in a relatively efficient way for a very distinct range of tasks and still outperform previous architectures (Devlin et al., 2019) ."}
{"sent_id": "817576dbe36f79ac3e0031211f400d-C001-121", "intents": ["@BACK@"], "paper_id": "ABC_817576dbe36f79ac3e0031211f400d_24", "text": "ZeroR (majority class) 66.70 mBERT (Devlin et al., 2019) 90.21 BERTje (de Vries et al., 2019) 94.94 RobBERT (ours) 98.03"}
{"sent_id": "652534f801dbff0c009c4a39fdef4d-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_652534f801dbff0c009c4a39fdef4d_24", "text": "A variant of the basic ASR QE task is to consider it as a QE-based ranking problem (Jalalvand et al., 2015b) , in which each utterance is captured by multiple microphones or transcribed by multiple ASR systems."}
{"sent_id": "652534f801dbff0c009c4a39fdef4d-C001-116", "intents": ["@BACK@"], "paper_id": "ABC_652534f801dbff0c009c4a39fdef4d_24", "text": "The features and algorithms contained in TranscRater have been successfully used in previous works C. de Souza et al., 2015; Jalalvand et al., 2015b; Jalalvand et al., 2015a) ."}
{"sent_id": "db42e01dbc86b77335a0e488ff85e2-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_db42e01dbc86b77335a0e488ff85e2_24", "text": "Google's Vaswani et al. [3] proposed the reduction in the sequential steps seen in CNNs and RNNs."}
{"sent_id": "db42e01dbc86b77335a0e488ff85e2-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_db42e01dbc86b77335a0e488ff85e2_24", "text": "A motivation for creating the Transformer model was the sluggish training and generation times of other common sequence-to-sequence models such as RNNs and CNNs [3] ."}
{"sent_id": "db42e01dbc86b77335a0e488ff85e2-C001-45", "intents": ["@SIM@"], "paper_id": "ABC_db42e01dbc86b77335a0e488ff85e2_24", "text": "The Transformer architectures proposed by Vaswani et al. [3] , seen in Figure 1 , inspires this paper's work."}
{"sent_id": "0763666190b6b4be1bcf494d7c6fe2-C001-117", "intents": ["@SIM@"], "paper_id": "ABC_0763666190b6b4be1bcf494d7c6fe2_24", "text": "The other results respectively correspond to the Split Merge approach presented in (Lang and Lapata, 2011a ) (Split Merge), the Graph Partitioning algorithm (Graph Part.) presented in (Lang and Lapata, 2011b) , and two Bayesian approaches presented in (Titov and Klementiev, 2012) , which achieve the best current unsupervised SRL results."}
{"sent_id": "0763666190b6b4be1bcf494d7c6fe2-C001-116", "intents": ["@DIF@"], "paper_id": "ABC_0763666190b6b4be1bcf494d7c6fe2_24", "text": "We can first note that, despite our efforts to reproduce the same baseline, there is still a difference between our baseline (Synt.Func.) and the baseline reported in (Lang and Lapata, 2011a)"}
{"sent_id": "8bd97eb118175c9fd2147b6456421c-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_8bd97eb118175c9fd2147b6456421c_24", "text": "Recent advances have shown that E2E models can outperform the state-of-the-art conventional system when trained on thousands of hours of data [5, 6] ."}
{"sent_id": "8bd97eb118175c9fd2147b6456421c-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_8bd97eb118175c9fd2147b6456421c_24", "text": "Further improvements such as biasing before beam pruning, and wordpiece-based biasing have been proposed to achieve state-of-the-art biasing results [12, 6, 13] ."}
{"sent_id": "537ec54aac2c3e3c62070468dcd8a3-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_537ec54aac2c3e3c62070468dcd8a3_24", "text": "One is a phrase-based translation in which a phrasal unit is employed for translation (Koehn et al., 2003) ."}
{"sent_id": "537ec54aac2c3e3c62070468dcd8a3-C001-55", "intents": ["@BACK@"], "paper_id": "ABC_537ec54aac2c3e3c62070468dcd8a3_24", "text": "For details, please refer to Koehn et al. (2003) ."}
{"sent_id": "3e0704e0928f2df8b7c2ffa9863a55-C001-72", "intents": ["@SIM@"], "paper_id": "ABC_3e0704e0928f2df8b7c2ffa9863a55_24", "text": "This is consistent with Shoemark et al. (2017) , who found more Scots usage among proindependence users (d = 0.00555 for pro/anti tweets, d = 0.00709 for all tweets)."}
{"sent_id": "3e0704e0928f2df8b7c2ffa9863a55-C001-43", "intents": ["@BACK@"], "paper_id": "ABC_3e0704e0928f2df8b7c2ffa9863a55_24", "text": "They are shown with their frequencies (including variants) in Table 1 (cf. the 47 hashtags and similar frequency distribution in Table 1 of Shoemark et al. (2017) )."}
{"sent_id": "3e0704e0928f2df8b7c2ffa9863a55-C001-96", "intents": ["@DIF@"], "paper_id": "ABC_3e0704e0928f2df8b7c2ffa9863a55_24", "text": "Table 7 in Shoemark et al. (2017) ; d u = −0.0015 for all controls)."}
{"sent_id": "805935a672f5d706bd878a73fa8171-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_805935a672f5d706bd878a73fa8171_24", "text": "Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004 ) on semantically-enriched input, where content words had been substituted with their semantic classes."}
{"sent_id": "805935a672f5d706bd878a73fa8171-C001-127", "intents": ["@BACK@"], "paper_id": "ABC_805935a672f5d706bd878a73fa8171_24", "text": "Agirre et al. (2008) used a simple method of substituting wordforms with semantic information, which only allowed using a single semantic feature."}
{"sent_id": "f881f6c65301fdfe2fffe7a18e05c4-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_f881f6c65301fdfe2fffe7a18e05c4_25", "text": "Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.ASES, CLUE WORDS, DISCOURSE MAI:tKERS~ arid DISCOURSE PARTICLES [3, 4, 14, 17, 19] ."}
{"sent_id": "f881f6c65301fdfe2fffe7a18e05c4-C001-37", "intents": ["@BACK@"], "paper_id": "ABC_f881f6c65301fdfe2fffe7a18e05c4_25", "text": "Once a cue phrase has been identified, however, it is not always clear whether to interpret it as a discourse marker or not [6, 4, 8, 18] ."}
{"sent_id": "c7821d22613ad91f77ea454d50a5ce-C001-96", "intents": ["@BACK@"], "paper_id": "ABC_c7821d22613ad91f77ea454d50a5ce_25", "text": "The (Du et al., 2017) , and SQuAD 81K is the setting of (Zhao et al., 2018) ."}
{"sent_id": "09493a62815b4b826248d6d9be47cb-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_09493a62815b4b826248d6d9be47cb_25", "text": "Limsopatham and Collier (2016) utilized convolutional neural networks (CNNs) for phrase normalization in user reviews, while Tutubalina et al. (2018) , Han et al. (2017) , and Belousov et al. (2017) applied recurrent neural networks (RNNs) to UGTs, achieving similar results."}
{"sent_id": "e99baf9c4b8650f29f410501c5165b-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_e99baf9c4b8650f29f410501c5165b_25", "text": "Some recent research of image captioning take inspiration from neural machine translation systems (NMT) [15] [16] [17] [18] that successfully use sequence-to-sequence learning for translation."}
{"sent_id": "e99baf9c4b8650f29f410501c5165b-C001-80", "intents": ["@SIM@"], "paper_id": "ABC_e99baf9c4b8650f29f410501c5165b_25", "text": "In our work, we model the distribution p(w i t |X i , w i 1:t−1 ; θ) with a LSTM cell wrapped with Luong-style attention mechanism [18] ."}
{"sent_id": "d9aa77a03ff98cae29701eddb414d3-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_d9aa77a03ff98cae29701eddb414d3_25", "text": "Currently, state of the art uses lightweight neural networks [1, 2, 3, 4] , which can perform inference in real-time even on low-end devices [4, 5] ."}
{"sent_id": "d9aa77a03ff98cae29701eddb414d3-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_d9aa77a03ff98cae29701eddb414d3_25", "text": "KWS is the task of detecting a spoken phrase in audio, applicable to simple command recognition [3, 10] and wake-word detection [2, 1] ."}
{"sent_id": "e0e21b4e473ad6fde28378b2dc4f34-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_e0e21b4e473ad6fde28378b2dc4f34_25", "text": "The algorithm of Sokolov et al. (2013) combines batch boosting with bagging over a number of independently drawn bootstrap data samples from R. In each step, the single word pair feature is selected that provides the largest decrease of L exp ."}
{"sent_id": "e0e21b4e473ad6fde28378b2dc4f34-C001-77", "intents": ["@USE@"], "paper_id": "ABC_e0e21b4e473ad6fde28378b2dc4f34_25", "text": "We use BoostCLIR 1 , a Japanese-English (JP-EN) corpus of patent abstracts from the MAREC and NTCIR data (Sokolov et al., 2013) ."}
{"sent_id": "e0e21b4e473ad6fde28378b2dc4f34-C001-113", "intents": ["@USE@"], "paper_id": "ABC_e0e21b4e473ad6fde28378b2dc4f34_25", "text": "PSQ on patents reuses settings found by Sokolov et al. (2013) ; settings for Wikipedia were adjusted on its dev set (n=1000, λ=0.4, L=0, C=1)."}
{"sent_id": "820fa732cc4cedf2d5d94b2afb90fc-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_820fa732cc4cedf2d5d94b2afb90fc_25", "text": "The Transformer neural sequence model [Vaswani et al., 2017] has emerged as a popular alternative to recurrent sequence models."}
{"sent_id": "820fa732cc4cedf2d5d94b2afb90fc-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_820fa732cc4cedf2d5d94b2afb90fc_25", "text": "The \"Transformer\" seuqence-to-sequence model [Vaswani et al., 2017] uses h different attention layers (heads) in parallel, which the authors refer to as \"Multi-head attention\"."}
{"sent_id": "820fa732cc4cedf2d5d94b2afb90fc-C001-62", "intents": ["@SIM@"], "paper_id": "ABC_820fa732cc4cedf2d5d94b2afb90fc_25", "text": "An example is a self-attention layer in an autoregressive language model such as Transformer [Vaswani et al., 2017] ."}
{"sent_id": "d4563562cd0dfd8ef6cdb57117fb22-C001-3", "intents": ["@USE@"], "paper_id": "ABC_d4563562cd0dfd8ef6cdb57117fb22_25", "text": "In this paper, we employ a variety of these methods to learn Schank and Abelson's canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called \"Dinners from Hell.\" Our models learn narrative chains, script-like structures that we evaluate with the \"narrative cloze\" task (Chambers and Jurafsky, 2008) ."}
{"sent_id": "d4563562cd0dfd8ef6cdb57117fb22-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_d4563562cd0dfd8ef6cdb57117fb22_25", "text": "e n , at insertion point k. The original model, proposed by Chambers and Jurafsky (2008) , predicts the event that maximizes unordered pmi,"}
{"sent_id": "0706cab049274ffc82c5e2ef6f7b99-C001-52", "intents": ["@BACK@"], "paper_id": "ABC_0706cab049274ffc82c5e2ef6f7b99_25", "text": "Di Eugenio and Glass (2004) identify three general classes of agreement statistics and suggest that all three should be used in conjunction in order to accurately evaluate coding schemes."}
{"sent_id": "0706cab049274ffc82c5e2ef6f7b99-C001-64", "intents": ["@BACK@"], "paper_id": "ABC_0706cab049274ffc82c5e2ef6f7b99_25", "text": "For example, Table 1 shows an example taken from Di Eugenio and Glass (2004) showing the classification of the utterance Okay as an acceptance or acknowledgment."}
{"sent_id": "0706cab049274ffc82c5e2ef6f7b99-C001-116", "intents": ["@BACK@"], "paper_id": "ABC_0706cab049274ffc82c5e2ef6f7b99_25", "text": "The prevalent use of this criterion despite repeated advice that it is unlikely to be suitable for all studies (Carletta 1996; Di Eugenio and Glass 2004; Krippendorff 2004a ) is probably due to a desire for a simple system that can be easily applied to a scheme."}
{"sent_id": "0706cab049274ffc82c5e2ef6f7b99-C001-70", "intents": ["@DIF@"], "paper_id": "ABC_0706cab049274ffc82c5e2ef6f7b99_25", "text": "Di Eugenio and Glass (2004) perceive this as an \"unpleasant behavior\" of chancecorrected tests, one that prevents us from concluding that the example given in Table 1 shows satisfactory levels of agreement."}
{"sent_id": "7b5ca6526f460139f273484bd276bc-C001-27", "intents": ["@USE@"], "paper_id": "ABC_7b5ca6526f460139f273484bd276bc_25", "text": "The experiments on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al. 2016) , which contains a variety of human-generated factoid and non-factoid questions, have shown the effectiveness of above three contributions."}
{"sent_id": "7b5ca6526f460139f273484bd276bc-C001-55", "intents": ["@USE@"], "paper_id": "ABC_7b5ca6526f460139f273484bd276bc_25", "text": "Answer Chunking To reduce the errors generated by the rule-based chunker in (Rajpurkar et al. 2016) , first, we capture the part-of-speech (POS) pattern of all answer subsequences in the training dataset to form a POS pattern trie tree, and then apply the answer POS patterns to passage P i to acquire a collection of all subsequences (chunk candidates) C i whose POS patterns can be matched to the POS pattern trie."}
{"sent_id": "7b5ca6526f460139f273484bd276bc-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_7b5ca6526f460139f273484bd276bc_25", "text": "Both models improved significantly over the method proposed by Rajpurkar et al. (2016) ."}
{"sent_id": "7b5ca6526f460139f273484bd276bc-C001-138", "intents": ["@DIF@"], "paper_id": "ABC_7b5ca6526f460139f273484bd276bc_25", "text": "As the first row of Table 3 shows, our baseline system improves 10% (EM) over Rajpurkar et al. (2016) (Table 2 , row 1), the feature-based ranking system."}
{"sent_id": "7b5ca6526f460139f273484bd276bc-C001-52", "intents": ["@SIM@"], "paper_id": "ABC_7b5ca6526f460139f273484bd276bc_25", "text": "In order to make the cloze-style RC system to make chunk-level decision, we use the RC model to generate features for chunks, which are further used in a feature-based ranker like in (Rajpurkar et al. 2016) ."}
{"sent_id": "e48a1eac39987cb2f504b66d135572-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_e48a1eac39987cb2f504b66d135572_25", "text": "Centralized fine-tuning of the i2b2 NER tasks plateaued after 4 epochs, with the learning rate set at 2e − 5 and a batch size of 32 (Alsentzer et al., 2019) ."}
{"sent_id": "e48a1eac39987cb2f504b66d135572-C001-43", "intents": ["@SIM@"], "paper_id": "ABC_e48a1eac39987cb2f504b66d135572_25", "text": "We included only discharge summaries in our study as previous studies have shown that performance of a model trained on only discharge summaries in this corpus is only marginally worse than model trained on all notes types (Alsentzer et al., 2019) ."}
{"sent_id": "60c1245eff625441383913f947a8b1-C001-153", "intents": ["@BACK@"], "paper_id": "ABC_60c1245eff625441383913f947a8b1_25", "text": "The Waseem and Hovy (2016) classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class."}
{"sent_id": "d44648766e68cb914c5489e385f42e-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_d44648766e68cb914c5489e385f42e_25", "text": "They locate the source and target fragments independently, making the extracted fragments unreliable (Munteanu and Marcu, 2006) ."}
{"sent_id": "d44648766e68cb914c5489e385f42e-C001-79", "intents": ["@BACK@"], "paper_id": "ABC_d44648766e68cb914c5489e385f42e_25", "text": "Munteanu and Marcu (2006) show that the LLR lexicon performs better than the IBM Model 1 lexicon for parallel fragment extraction."}
{"sent_id": "d44648766e68cb914c5489e385f42e-C001-140", "intents": ["@BACK@"], "paper_id": "ABC_d44648766e68cb914c5489e385f42e_25", "text": "We can see that the average size of fragments extracted by (Munteanu and Marcu, 2006 ) is unusually long, which is also reported in (Quirk et al., 2007) ."}
{"sent_id": "d44648766e68cb914c5489e385f42e-C001-108", "intents": ["@SIM@"], "paper_id": "ABC_d44648766e68cb914c5489e385f42e_25", "text": "In our experiments, we compared our proposed fragment extraction method with (Munteanu and Marcu, 2006) ."}
{"sent_id": "3188ee1583a9c711cf147fc596768d-C001-3", "intents": ["@BACK@"], "paper_id": "ABC_3188ee1583a9c711cf147fc596768d_25", "text": "The techniques examined are Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Self-training (Abney, 2007; McClosky et al., 2006) ."}
{"sent_id": "3188ee1583a9c711cf147fc596768d-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_3188ee1583a9c711cf147fc596768d_25", "text": "Structural Correspondence Learning (Blitzer et al., 2006) exploits unlabeled data from both source and target domain to find correspondences among features from different domains."}
{"sent_id": "3188ee1583a9c711cf147fc596768d-C001-46", "intents": ["@BACK@"], "paper_id": "ABC_3188ee1583a9c711cf147fc596768d_25", "text": "So far, pivot features on the word level were used (Blitzer et al., 2006; ."}
{"sent_id": "3188ee1583a9c711cf147fc596768d-C001-14", "intents": ["@SIM@"], "paper_id": "ABC_3188ee1583a9c711cf147fc596768d_25", "text": "We examine Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for this task, and compare it to several variants of Self-training (Abney, 2007; McClosky et al., 2006) ."}
{"sent_id": "3188ee1583a9c711cf147fc596768d-C001-124", "intents": ["@SIM@"], "paper_id": "ABC_3188ee1583a9c711cf147fc596768d_25", "text": "The paper compares Structural Correspondence Learning (Blitzer et al., 2006) with (various instances of) self-training (Abney, 2007; McClosky et al., 2006) for the adaptation of a parse selection model to Wikipedia domains."}
{"sent_id": "d987872352e4602fd48936cf2fdab8-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_d987872352e4602fd48936cf2fdab8_25", "text": "1 It is becoming more challenging since recent conversational interaction systems such as Amazon Alexa, Google Assistant, and Microsoft Cortana support more than thousands of domains developed by external developers [4, 3, 5] ."}
{"sent_id": "d987872352e4602fd48936cf2fdab8-C001-26", "intents": ["@DIF@"], "paper_id": "ABC_d987872352e4602fd48936cf2fdab8_25", "text": "Evaluating on an annotated dataset from the user logs of a large-scale conversation interaction system, we show that the proposed approach significantly improves the domain classification especially when hypothesis reranking is used [14, 5] ."}
{"sent_id": "d987872352e4602fd48936cf2fdab8-C001-88", "intents": ["@SIM@"], "paper_id": "ABC_d987872352e4602fd48936cf2fdab8_25", "text": "We set k=3 in our experiments following [5] ."}
{"sent_id": "d987872352e4602fd48936cf2fdab8-C001-105", "intents": ["@USE@"], "paper_id": "ABC_d987872352e4602fd48936cf2fdab8_25", "text": "We utilize utterances with explicit invocation patterns from an intelligent conversational system for the model training similarly to [5] and [18] ."}
{"sent_id": "22dc2a38e29a1f5ac55c9ac220782b-C001-16", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_22dc2a38e29a1f5ac55c9ac220782b_25", "text": "Recently, Vaswani et al. (2017) proposed the Transformer architecture for machine translation."}
{"sent_id": "22dc2a38e29a1f5ac55c9ac220782b-C001-45", "intents": ["@BACK@"], "paper_id": "ABC_22dc2a38e29a1f5ac55c9ac220782b_25", "text": "In the case of self-attention, Q, K and V are linear projections of X. Thus, we define the dot-product (Vaswani et al., 2017) ."}
{"sent_id": "22dc2a38e29a1f5ac55c9ac220782b-C001-52", "intents": ["@SIM@"], "paper_id": "ABC_22dc2a38e29a1f5ac55c9ac220782b_25", "text": "We use the positional encoding vectors that were defined by Vaswani et al. (2017) as follows."}
{"sent_id": "22dc2a38e29a1f5ac55c9ac220782b-C001-41", "intents": ["@DIF@"], "paper_id": "ABC_22dc2a38e29a1f5ac55c9ac220782b_25", "text": "One key difference between our approach and Vaswani et al. (2017) 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification."}
{"sent_id": "1527ce2786adfe0decf8c926a3d846-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_1527ce2786adfe0decf8c926a3d846_25", "text": "Further, some approaches work by detecting all pairs of words in a corpus and filtering to isolate synonym or hypernym-relationship pairs using WordNet (Biran et al., 2011) ."}
{"sent_id": "1527ce2786adfe0decf8c926a3d846-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_1527ce2786adfe0decf8c926a3d846_25", "text": "In prior context-aware simplification systems, the decision of whether to apply a simplification rule in an input sentence is complex, involving several similarity operations on word co-occurrence matrices (Biran et al., 2011) or using embeddings to incorporate co-occurrence context for pairs generated using other means (Paetzold and Specia, 2015) ."}
{"sent_id": "1527ce2786adfe0decf8c926a3d846-C001-59", "intents": ["@DIF@"], "paper_id": "ABC_1527ce2786adfe0decf8c926a3d846_25", "text": "While this simplicity filter has been shown to work well in general corpora (Biran et al., 2011) , it is sensitive to very small differences in the frequencies with which both words appear in the corpora."}
{"sent_id": "05d1ecc230c7907d9a14d3351070c3-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_05d1ecc230c7907d9a14d3351070c3_25", "text": "However, the most successful ones are BERT [2] and Open-GPT [3] ."}
{"sent_id": "05d1ecc230c7907d9a14d3351070c3-C001-78", "intents": ["@BACK@"], "paper_id": "ABC_05d1ecc230c7907d9a14d3351070c3_25", "text": "In the paradigm proposed in the original work by Devlin et al. [2] , the author directly trained BERT along with with a light-weighted task-specific head."}
{"sent_id": "13d3d973a4be832f66b049b364fea5-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_13d3d973a4be832f66b049b364fea5_25", "text": "Several neural architectures have been employed including variants of Long Short-Term Memory (LSTM) (Alikaniotis et al., 2016; Taghipour and Ng, 2016) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016) ."}
{"sent_id": "13d3d973a4be832f66b049b364fea5-C001-70", "intents": ["@SIM@"], "paper_id": "ABC_13d3d973a4be832f66b049b364fea5_25", "text": "We compare our pre-training models to the SSWE developed by Alikaniotis et al. (2016) ."}
{"sent_id": "13d3d973a4be832f66b049b364fea5-C001-66", "intents": ["@USE@"], "paper_id": "ABC_13d3d973a4be832f66b049b364fea5_25", "text": "In this section, we describe three different neural networks to pre-train word representations: the model implemented by Alikaniotis et al. (2016) and the two error-oriented models we propose in this work."}
{"sent_id": "70d41cad40091bcc30a1fd544c277d-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_70d41cad40091bcc30a1fd544c277d_26", "text": "Bidirectional Encoder Representations from Transformers (BERT; Devlin et al., 2018) currently represents state of the art, vastly outperforming previous models, such as the Generative Pretrained Transformer (GPT; Radford et al.) and Embeddings from Language Models (ELMo; Peters et al., 2018) ."}
{"sent_id": "70d41cad40091bcc30a1fd544c277d-C001-68", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_70d41cad40091bcc30a1fd544c277d_26", "text": "Originally, Devlin et al. (2018) find that fine-tuning for three or four epochs works well for both small and large datasets alike."}
{"sent_id": "70d41cad40091bcc30a1fd544c277d-C001-43", "intents": ["@USE@", "@EXT@"], "paper_id": "ABC_70d41cad40091bcc30a1fd544c277d_26", "text": "We begin with the pre-trained BERT base and BERT large models, which respectively represent the normal and large model variants (Devlin et al., 2018) ."}
{"sent_id": "c067711a58722737ef8b7ea987bcf3-C001-44", "intents": ["@USE@"], "paper_id": "ABC_c067711a58722737ef8b7ea987bcf3_26", "text": "We employ a recurrent sequence-to-sequence neural network with attention (Bahdanau et al., 2014) and two bi-LSTM layers, similar to the one used by Van Noord, Abzianidze, Toral, and Bos (2018) ."}
{"sent_id": "c067711a58722737ef8b7ea987bcf3-C001-60", "intents": ["@USE@"], "paper_id": "ABC_c067711a58722737ef8b7ea987bcf3_26", "text": "The produced DRSs go through a strict syntactic and semantic validation process, as described in Van Noord, Abzianidze, Toral, and Bos (2018) ."}
{"sent_id": "c067711a58722737ef8b7ea987bcf3-C001-80", "intents": ["@DIF@"], "paper_id": "ABC_c067711a58722737ef8b7ea987bcf3_26", "text": "When compared to Van Noord, Abzianidze, Toral, and Bos (2018) , retrained with the same data used in our systems, the largest improvement (3.6 and 3.5 for dev and test) comes from switching framework and changing certain parameters such as the optimizer and learning rate."}
{"sent_id": "ab5788da3f24e01b0ec40fba0bdbec-C001-79", "intents": ["@BACK@"], "paper_id": "ABC_ab5788da3f24e01b0ec40fba0bdbec_26", "text": "Iterative back-translation was also explored by Marie and Fujita (2018) and Artetxe et al. (2019) in the context of unsupervised machine translation, relying on an unsupervised SMT system (Lample et al., 2018b; Artetxe et al., 2018a) for warmup."}
{"sent_id": "ab5788da3f24e01b0ec40fba0bdbec-C001-24", "intents": ["@USE@"], "paper_id": "ABC_ab5788da3f24e01b0ec40fba0bdbec_26", "text": "We next describe the iterative back-translation implementation used in our experiments, which was proposed by Artetxe et al. (2019) ."}
{"sent_id": "ab5788da3f24e01b0ec40fba0bdbec-C001-31", "intents": ["@USE@"], "paper_id": "ABC_ab5788da3f24e01b0ec40fba0bdbec_26", "text": "Following Artetxe et al. (2019) , we set N = 1, 000, 000 and a = 30, and perform a total of 60 such iterations."}
{"sent_id": "d92e92b9a375914f3dd74868f463fc-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_d92e92b9a375914f3dd74868f463fc_26", "text": "Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (Schwenk, 2018) , on translation quality (Lu et al., 2018) , on speed comparison (Britz et al., 2017) , or only exploring a bilingual scenario (Cífka and Bojar, 2018) ."}
{"sent_id": "d92e92b9a375914f3dd74868f463fc-C001-20", "intents": ["@DIF@"], "paper_id": "ABC_d92e92b9a375914f3dd74868f463fc_26", "text": "In contrast to previous work (Cífka and Bojar, 2018) , we demonstrate that there is a correlation between translation performance and trainable downstream tasks when adjusting the size of the intermediate layer."}
{"sent_id": "d92e92b9a375914f3dd74868f463fc-C001-57", "intents": ["@USE@"], "paper_id": "ABC_d92e92b9a375914f3dd74868f463fc_26", "text": "Results with † taken from Cífka and Bojar (2018)."}
{"sent_id": "d92e92b9a375914f3dd74868f463fc-C001-69", "intents": ["@SIM@"], "paper_id": "ABC_d92e92b9a375914f3dd74868f463fc_26", "text": "This is in line with the findings of Cífka and Bojar (2018) and could also be expected as the model is more strongly pushed into a dense semantic abstraction that is beneficial for measuring similarities without further training."}
{"sent_id": "3d84cf97f48dad66b4d8de0baf79b1-C001-36", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_3d84cf97f48dad66b4d8de0baf79b1_26", "text": "We make primary comparison with the work of Fagarasan et al. (2015) , although their approach differs from ours in that they map from an embedding space onto the feature space, while we learn a mapping from the feature domain onto the embedding space."}
{"sent_id": "3d84cf97f48dad66b4d8de0baf79b1-C001-57", "intents": ["@USE@"], "paper_id": "ABC_3d84cf97f48dad66b4d8de0baf79b1_26", "text": "In implementing PLSR, we set the intermediate dimension size to 50, following Fagarasan et al. (2015) ."}
{"sent_id": "3d84cf97f48dad66b4d8de0baf79b1-C001-88", "intents": ["@SIM@"], "paper_id": "ABC_3d84cf97f48dad66b4d8de0baf79b1_26", "text": "We report results over both 50 (as in Fagarasan et al. (2015) ) and 120 dimensions for a range of values of N (Table 1) ."}
{"sent_id": "3d84cf97f48dad66b4d8de0baf79b1-C001-118", "intents": ["@BACK@"], "paper_id": "ABC_3d84cf97f48dad66b4d8de0baf79b1_26", "text": "As property norms do not represent an exhaustive listing of property knowledge, this is not surprising, and predicted properties not in the norms are not necessarily errors (Devereux et al., 2009; Fagarasan et al., 2015) ."}
{"sent_id": "c437e447603ecdbe4053651169770a-C001-4", "intents": ["@BACK@"], "paper_id": "ABC_c437e447603ecdbe4053651169770a_26", "text": "Preliminary experiments using a similar collection used in Potthast et al. (2018) show that neural-based classification methods reach state-of-the art results."}
{"sent_id": "c437e447603ecdbe4053651169770a-C001-67", "intents": ["@BACK@"], "paper_id": "ABC_c437e447603ecdbe4053651169770a_26", "text": "Two state-of-the-art models (SpaCy and Kim (2014)) outperform the approach presented in Potthast et al. (2018) , showing that stylometric features are probably not necessary for the task."}
{"sent_id": "c437e447603ecdbe4053651169770a-C001-53", "intents": ["@USE@"], "paper_id": "ABC_c437e447603ecdbe4053651169770a_26", "text": "Experiments were performed using two collections, the ACL2018 collection (Potthast et al., 2018) and the SemEval19 collection ."}
{"sent_id": "d9877fc29c2e4f20805076392a70d0-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_d9877fc29c2e4f20805076392a70d0_26", "text": "These two lines of research converge in prior work to show, e.g., the increasing association of the lexical item 'gay' with the meaning dimension of homosexuality (Kim et al., 2014; Kulkarni et al., 2015) ."}
{"sent_id": "d9877fc29c2e4f20805076392a70d0-C001-66", "intents": ["@USE@"], "paper_id": "ABC_d9877fc29c2e4f20805076392a70d0_26", "text": "Following Kulkarni et al. (2015) , we trained our models on all 5-grams occurring during five consecutive years for the two time spans, 5 1900-1904 and 2005-2009 ; the number of 5-grams 6 for each time span is listed in Table 1 ."}
{"sent_id": "cee22bd0384d3d3fd4e45833341e77-C001-19", "intents": ["@USE@"], "paper_id": "ABC_cee22bd0384d3d3fd4e45833341e77_26", "text": "Our paper expands the existing FOIL dataset (Shekhar et al., 2017) ."}
{"sent_id": "cee22bd0384d3d3fd4e45833341e77-C001-30", "intents": ["@USE@"], "paper_id": "ABC_cee22bd0384d3d3fd4e45833341e77_26", "text": "We follow the methodology highlighted in Shekhar et al. (2017) , which consists of replacing a single word in a human-generated caption with a 'foil' item, making the caption unsuitable to describe the original image."}
{"sent_id": "cee22bd0384d3d3fd4e45833341e77-C001-87", "intents": ["@USE@"], "paper_id": "ABC_cee22bd0384d3d3fd4e45833341e77_26", "text": "The foil captions are generated by replacing nouns are directly extracted from the FOIL dataset by Shekhar et al. (2017) ."}
{"sent_id": "74db2b52e81969742f8f7e5681bd2b-C001-82", "intents": ["@BACK@"], "paper_id": "ABC_74db2b52e81969742f8f7e5681bd2b_26", "text": "While the novel attention mechanism, eschewing recurrence, is famous for modeling global dependencies and considered faster than recurrent layers (Vaswani et al., 2017) , recent work points out that it may tend to overlook neighboring information (Yang et al., 2019a; Xu et al., 2019) ."}
{"sent_id": "74db2b52e81969742f8f7e5681bd2b-C001-11", "intents": ["@EXT@"], "paper_id": "ABC_74db2b52e81969742f8f7e5681bd2b_26", "text": "We customize the transformer (Vaswani et al., 2017) featured by non-local operations (Wang et al., 2018) with two * The work was done when Yaoyiran was working at Living Analytics Research Centre, Singapore Management University who is now a PhD student at University of Cambridge."}
{"sent_id": "74db2b52e81969742f8f7e5681bd2b-C001-40", "intents": ["@EXT@", "@USE@"], "paper_id": "ABC_74db2b52e81969742f8f7e5681bd2b_26", "text": "Different from previously known coattention mechanisms such as (Xiong et al., 2017; Lu et al., 2016a) , our co-attention is built through connecting two multiplicative attention modules (Vaswani et al., 2017 ) each containing three gates, i.e., Value, Key and Query."}
{"sent_id": "74db2b52e81969742f8f7e5681bd2b-C001-38", "intents": ["@USE@"], "paper_id": "ABC_74db2b52e81969742f8f7e5681bd2b_26", "text": "Based on the transformer model (Vaswani et al., 2017) , we design a novel co-attention mechanism."}
{"sent_id": "013f0e54384a8a4662a746eb4c30d9-C001-14", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_013f0e54384a8a4662a746eb4c30d9_26", "text": "Recently, Akbik et al. (2018) proposed a character-level contextualized embeddings ap- context."}
{"sent_id": "013f0e54384a8a4662a746eb4c30d9-C001-37", "intents": ["@USE@"], "paper_id": "ABC_013f0e54384a8a4662a746eb4c30d9_26", "text": "It requires an embed() function that produces a contextualized embedding for a given word in a sentence context (see Akbik et al. (2018) )."}
{"sent_id": "013f0e54384a8a4662a746eb4c30d9-C001-83", "intents": ["@USE@"], "paper_id": "ABC_013f0e54384a8a4662a746eb4c30d9_26", "text": "For our experiments, we follow the training and evaluation procedure outlined in Akbik et al. (2018) and follow most hyperparameter suggestions as given by the in-depth study presented in Reimers and Gurevych (2017) ."}
{"sent_id": "d52dfb30158deae64a1c3d787d9b95-C001-65", "intents": ["@USE@"], "paper_id": "ABC_d52dfb30158deae64a1c3d787d9b95_26", "text": "We used the following probabilistic framework to this end [5] :"}
{"sent_id": "d52dfb30158deae64a1c3d787d9b95-C001-81", "intents": ["@USE@"], "paper_id": "ABC_d52dfb30158deae64a1c3d787d9b95_26", "text": "We removed Persian stop words from the queries and documents [4, 5] ."}
{"sent_id": "d52dfb30158deae64a1c3d787d9b95-C001-95", "intents": ["@USE@"], "paper_id": "ABC_d52dfb30158deae64a1c3d787d9b95_26", "text": "Iterative translation disambiguation (ITD) [11] , joint cross-lingual topical relevance model (JCLTRLM) [7] , top-ranked translation (TOP-1), and the bi-gram coherence translation method (BiCTM), introduced in [5] (assume |c i | = 0), are the baselines without any morphological processing units."}
{"sent_id": "d52dfb30158deae64a1c3d787d9b95-C001-85", "intents": ["@BACK@"], "paper_id": "ABC_d52dfb30158deae64a1c3d787d9b95_26", "text": "Dadashkarimi et al., demonstrated that Google has better coverage compared to other English-Persian dictionaries [5] ."}
{"sent_id": "eebf1edb6dbd3e58a904eff309f548-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_eebf1edb6dbd3e58a904eff309f548_26", "text": "A major distinction between the work of Berant et al. (2013) and Yao and Van Durme (2014) is the ability of the former to represent, and compose, aggregation operators (such as argmax, or count), as well as integrate disparate pieces of information."}
{"sent_id": "eebf1edb6dbd3e58a904eff309f548-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_eebf1edb6dbd3e58a904eff309f548_26", "text": "SEMPRE 3 is an open-source system for training semantic parsers, that has been utilized to train a semantic parser against Freebase by Berant et al. (2013) ."}
{"sent_id": "499580e888a1598681a8d877b07866-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_499580e888a1598681a8d877b07866_26", "text": "Kim and Lee (2016) apply neural machine translation (NMT) models, based on recurrent neural network, to sentence-level QE."}
{"sent_id": "499580e888a1598681a8d877b07866-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_499580e888a1598681a8d877b07866_26", "text": "Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors (Kim and Lee, 2016) sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs."}
{"sent_id": "499580e888a1598681a8d877b07866-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_499580e888a1598681a8d877b07866_26", "text": "where f is the activation function of RNN (Kim and Lee, 2016) ."}
{"sent_id": "35ef3eba487c3cd97d32210670678a-C001-18", "intents": ["@SIM@", "@DIF@", "@EXT@"], "paper_id": "ABC_35ef3eba487c3cd97d32210670678a_26", "text": "The results we get are in line with Glockner et al. (2018) , showing that the generalization capability of the individual NLI systems is very limited, but, what is more, they further show the only system that was less prone to breaking in Glockner et al. (2018) , breaks in the experiments we have conducted as well."}
{"sent_id": "0a55859a36d0887ba4febc98762715-C001-3", "intents": ["@USE@"], "paper_id": "ABC_0a55859a36d0887ba4febc98762715_26", "text": "This paper proposes a new scalable and accurate neural dialogue state tracking model, based on the recently proposed Global-Local Self-Attention encoder (GLAD) model by Zhong et al. (2018) which uses global modules to share parameters between estimators for different types (called slots) of dialogue states, and uses local modules to learn slot-specific features."}
{"sent_id": "0a55859a36d0887ba4febc98762715-C001-38", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_0a55859a36d0887ba4febc98762715_26", "text": "First, section 2.1 explains the recently proposed GLAD encoder (Zhong et al., 2018) architecture, followed by our proposed encoder in section 2.2."}
{"sent_id": "0a55859a36d0887ba4febc98762715-C001-52", "intents": ["@USE@"], "paper_id": "ABC_0a55859a36d0887ba4febc98762715_26", "text": "Here, we employ the similar approach of learning slot-specific temporal and context representation of user utterance and system actions, as proposed in GLAD (Zhong et al., 2018) ."}
{"sent_id": "58b423c4ea2a3530d0c469fc0f5528-C001-18", "intents": ["@USE@"], "paper_id": "ABC_58b423c4ea2a3530d0c469fc0f5528_26", "text": "Given the same parallel corpus used in Kushman and Barzilay (2013) , we use an LSTM-based sequence to sequence neural network to perform the mapping."}
{"sent_id": "58b423c4ea2a3530d0c469fc0f5528-C001-77", "intents": ["@USE@"], "paper_id": "ABC_58b423c4ea2a3530d0c469fc0f5528_26", "text": "In addition, we also evaluate our model on the dataset used by Kushman and Barzilay (2013) (KB13), although it contains far fewer data points (824)."}
{"sent_id": "28900a293048fdb0c40dc540985cf1-C001-25", "intents": ["@BACK@"], "paper_id": "ABC_28900a293048fdb0c40dc540985cf1_26", "text": "However, Kann et al. (2017) show that transferring morphological knowledge from Spanish to Portuguese, two languages with similar morphology and 89% lexical similarity, works well and, more surprisingly, even supposedly very different languages like Arabic and Spanish can benefit from each other."}
{"sent_id": "28900a293048fdb0c40dc540985cf1-C001-9", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_28900a293048fdb0c40dc540985cf1_26", "text": "Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016 (Cotterell et al., , 2017 Kann and Schütze, 2016) , the task of generating inflected forms of a lemma's paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields."}
{"sent_id": "28900a293048fdb0c40dc540985cf1-C001-102", "intents": ["@BACK@"], "paper_id": "ABC_28900a293048fdb0c40dc540985cf1_26", "text": "This explains the unexpected result that l-emb performs best for Arabic (200) and Portuguese (200): both source languages potentially confuse the language model; in Portuguese we contribute this to a big overlap of lemmata in the two languages with Portuguese often inflecting in a different way (Kann et al., 2017) ."}
{"sent_id": "28900a293048fdb0c40dc540985cf1-C001-51", "intents": ["@USE@"], "paper_id": "ABC_28900a293048fdb0c40dc540985cf1_26", "text": "We use the Romance and Arabic language data from Kann et al. (2017) ."}
{"sent_id": "ce8997b630e9544b0f5812be319a59-C001-144", "intents": ["@BACK@"], "paper_id": "ABC_ce8997b630e9544b0f5812be319a59_26", "text": "More recently, neural models for poetry generation have been proposed (Zhang and Lapata, 2014; Ghazvininejad et al., 2016 Ghazvininejad et al., , 2017 Hopkins and Kiela, 2017; Lau et al., 2018; Liu et al., 2019) ."}
{"sent_id": "ce8997b630e9544b0f5812be319a59-C001-48", "intents": ["@USE@"], "paper_id": "ABC_ce8997b630e9544b0f5812be319a59_26", "text": "We initialize the word embeddings in the generator with pre-trained word embeddings (Lau et al., 2018) trained on a separate non-sonnet corpus."}
{"sent_id": "ce8997b630e9544b0f5812be319a59-C001-90", "intents": ["@USE@"], "paper_id": "ABC_ce8997b630e9544b0f5812be319a59_26", "text": "Fol-lowing prior work (Lau et al., 2018) , words are sampled with a temperature value between 0.6 and 0.8."}
{"sent_id": "cb81d56412d1e800074777687fb45a-C001-23", "intents": ["@USE@"], "paper_id": "ABC_cb81d56412d1e800074777687fb45a_26", "text": "Using the model of Yu et al. (2018a) , we compare several key model configurations."}
{"sent_id": "cb81d56412d1e800074777687fb45a-C001-115", "intents": ["@DIF@"], "paper_id": "ABC_cb81d56412d1e800074777687fb45a_26", "text": "Figure 2 shows F1 scores of several typical components, including 4 Note that the results are lower than those reported by Yu et al. (2018a) under their split due to different training/test splits."}
{"sent_id": "9227b5afd1ef18ecf83400dc402459-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_9227b5afd1ef18ecf83400dc402459_27", "text": "Results presented by Tseng (2001) show that speakers of Mandarin adopt some 30 words for building core structures of utterances in conversation, independently of individual speakers."}
{"sent_id": "9227b5afd1ef18ecf83400dc402459-C001-33", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_9227b5afd1ef18ecf83400dc402459_27", "text": "In Tseng (2001) , each subject used on average 1.6 discourse particles per turn."}
{"sent_id": "9227b5afd1ef18ecf83400dc402459-C001-42", "intents": ["@BACK@"], "paper_id": "ABC_9227b5afd1ef18ecf83400dc402459_27", "text": "In Mandarin conversation, there are words preferably used in turn-initial position (Tseng 2001 , Chui 2000 ."}
{"sent_id": "6f4dc72277119f0df3d4a7155c61fc-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_6f4dc72277119f0df3d4a7155c61fc_27", "text": "In many practical problems, RNNs can be compressed orders of times with only slight quality drop or even with quality improvement [2, 15, 20] ."}
{"sent_id": "6f4dc72277119f0df3d4a7155c61fc-C001-22", "intents": ["@USE@"], "paper_id": "ABC_6f4dc72277119f0df3d4a7155c61fc_27", "text": "Following [2, 14] , we sparsify individual weights of the RNN."}
{"sent_id": "5a039a2af7e07cffff76d3470f32f1-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_5a039a2af7e07cffff76d3470f32f1_27", "text": "There is a wealth of research on evaluating unsupervised word embeddings, which can be can be broadly divided into intrinsic and extrinsic evalu- (Mikolov et al., 2013; Gao et al., 2014; Schnabel et al., 2015) ."}
{"sent_id": "5a039a2af7e07cffff76d3470f32f1-C001-55", "intents": ["@USE@"], "paper_id": "ABC_5a039a2af7e07cffff76d3470f32f1_27", "text": "Once tuples have been generated, they can be used as word analogy questions to evaluate different word embeddings as defined by Mikolov et al. (Mikolov et al., 2013) ."}
{"sent_id": "5a039a2af7e07cffff76d3470f32f1-C001-61", "intents": ["@SIM@"], "paper_id": "ABC_5a039a2af7e07cffff76d3470f32f1_27", "text": "This provides a more accurate representation of a relation as mentioned in (Mikolov et al., 2013) ."}
{"sent_id": "167511f278a8596aed0124c3a4242b-C001-10", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_167511f278a8596aed0124c3a4242b_27", "text": "Current Simultaneous Neural Machine Translation (SNMT) systems (Satija and Pineau, 2016; Cho and Esipova, 2016; Gu et al., 2017) use an AGENT to control an incremental encoder-decoder (or sequence to sequence) NMT model."}
{"sent_id": "167511f278a8596aed0124c3a4242b-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_167511f278a8596aed0124c3a4242b_27", "text": "An agent-based framework whose actions decide whether to translate or wait for more input is a natural way to extend neural MT to simultaneous neural MT and has been explored in (Satija and Pineau, 2016; Gu et al., 2017) which contains two main components: The ENVIRONMENT which receives the input words X = {x 1 , . . . , x N } from the source language and incrementally generates translated words W = {w 1 , . . . , w M } in the target language; And the AGENT which decides an action for each time step, a t ."}
{"sent_id": "167511f278a8596aed0124c3a4242b-C001-102", "intents": ["@EXT@"], "paper_id": "ABC_167511f278a8596aed0124c3a4242b_27", "text": "We modified the SNMT trainable agent in (Gu et al., 2017) and added a new non-trivial PREDICT action to the agent."}
{"sent_id": "167511f278a8596aed0124c3a4242b-C001-41", "intents": ["@USE@"], "paper_id": "ABC_167511f278a8596aed0124c3a4242b_27", "text": "The agent in the greedy decoding framework (Gu et al., 2017) was trained using reinforcement learning with the policy gradient algorithm (Williams, 1992) , which observes the current state of the ENVIRONMENT at time step t as o t where o t = [c t ; s t ; w m ]."}
{"sent_id": "167511f278a8596aed0124c3a4242b-C001-63", "intents": ["@USE@"], "paper_id": "ABC_167511f278a8596aed0124c3a4242b_27", "text": "Reinforcement Learning is used to train the AGENT using a policy gradient algorithm (Gu et al., 2017; Williams, 1992) which searches for the maximum in"}
{"sent_id": "167511f278a8596aed0124c3a4242b-C001-70", "intents": ["@USE@"], "paper_id": "ABC_167511f278a8596aed0124c3a4242b_27", "text": "All sentences have been tokenized and the words are segmented using byte pair encoding (BPE) (Sennrich et al., 2016 Model Configuration For a fair comparison, we follow the settings that worked the best for the greedy decoding model in (Gu et al., 2017) and set the target delay d ⇤ for the AGENT to 0.7."}
{"sent_id": "e92c6b44f4482ca868221bff551d67-C001-27", "intents": ["@EXT@"], "paper_id": "ABC_e92c6b44f4482ca868221bff551d67_27", "text": "Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art results in function labelling, by suitably extending a Simple Synchrony Network (SSN) parser (Henderson, 2003) into a single integrated system."}
{"sent_id": "e92c6b44f4482ca868221bff551d67-C001-53", "intents": ["@BACK@"], "paper_id": "ABC_e92c6b44f4482ca868221bff551d67_27", "text": "SSN parsers, on the other hand, do not state any explicit independence assumptions: they induce a finite history representation of an unbounded sequence of moves, so that the representation of a move i − 1 is included in the inputs to the represention of the next move i, as explained in more detail in (Henderson, 2003) ."}
{"sent_id": "d8e3cdea7f61152ed37395c5f9393e-C001-30", "intents": ["@USE@"], "paper_id": "ABC_d8e3cdea7f61152ed37395c5f9393e_27", "text": "We also test the PMI methodology (Newman et al., 2010) and make the same observation."}
{"sent_id": "d8e3cdea7f61152ed37395c5f9393e-C001-45", "intents": ["@USE@"], "paper_id": "ABC_d8e3cdea7f61152ed37395c5f9393e_27", "text": "3 To collect the coherence judgements, we used Amazon Mechanical Turk and asked Turkers to rate topics in terms of coherence using a 3-point ordinal scale, where 1 indicates incoherent and 3 very coherent (Newman et al., 2010) ."}
{"sent_id": "fca75d394e9f7007e1f674c7b99794-C001-80", "intents": ["@USE@"], "paper_id": "ABC_fca75d394e9f7007e1f674c7b99794_27", "text": "More specifically, T Dif f unw (unweighted team difference) converts the team difference of Litman et al. (2016) to deal with multiple feature categories."}
{"sent_id": "fca75d394e9f7007e1f674c7b99794-C001-75", "intents": ["@SIM@"], "paper_id": "ABC_fca75d394e9f7007e1f674c7b99794_27", "text": "Recently, Litman et al. (2016) proposed a method to compute multi-party entrainment on acoustic-prosodic features based on the same Teams Corpus as used here."}
{"sent_id": "f3c2c538019b1d9daa8e6c932d9826-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_f3c2c538019b1d9daa8e6c932d9826_27", "text": "In recent works, short phrases [11, 4] , images [3] or summaries [19] have been used as alternatives."}
{"sent_id": "f3c2c538019b1d9daa8e6c932d9826-C001-58", "intents": ["@USE@"], "paper_id": "ABC_f3c2c538019b1d9daa8e6c932d9826_27", "text": "We evaluate our model on the publicly available data set provided by [3] ."}
{"sent_id": "f3c2c538019b1d9daa8e6c932d9826-C001-70", "intents": ["@USE@"], "paper_id": "ABC_f3c2c538019b1d9daa8e6c932d9826_27", "text": "Our evaluation follows prior work [11, 3] using two metrics."}
{"sent_id": "f3c2c538019b1d9daa8e6c932d9826-C001-82", "intents": ["@USE@"], "paper_id": "ABC_f3c2c538019b1d9daa8e6c932d9826_27", "text": "We compare our approach to the state-of-the-art method that uses Personalized PageRank [3] to re-rank image candidates (Local PPR) and an adapted version that computes the PageRank scores of all the available images in the test set (Global PPR)."}
{"sent_id": "27ee0fbed3a88854ebe945dfffefd8-C001-105", "intents": ["@USE@"], "paper_id": "ABC_27ee0fbed3a88854ebe945dfffefd8_27", "text": "Experiments were carried out by the method proposed in [35] ."}
{"sent_id": "27ee0fbed3a88854ebe945dfffefd8-C001-125", "intents": ["@USE@"], "paper_id": "ABC_27ee0fbed3a88854ebe945dfffefd8_27", "text": "If there was an overlap, a relaxed type F1-score (Type.F1) was calculated [35] ."}
{"sent_id": "27ee0fbed3a88854ebe945dfffefd8-C001-139", "intents": ["@USE@"], "paper_id": "ABC_27ee0fbed3a88854ebe945dfffefd8_27", "text": "Table 9 : Evaluation results for all TIMEX3 classes (total) for 9 word embeddings models (3 best models from each embeddings group: EE, EP, EC from Table 8 ) using the following measures from [35] : strict precision, strict recall, strict F1-score, relaxed precision, relaxed recall, relaxed F1-score, type F1-score."}
{"sent_id": "74420437db295ca874d5c946891f69-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_74420437db295ca874d5c946891f69_27", "text": "Recently, Tsvetkov and Wintner (2014) proposed an approach for the detection of MWE candidates that combines a number of statistical and linguistic features."}
{"sent_id": "74420437db295ca874d5c946891f69-C001-20", "intents": ["@EXT@"], "paper_id": "ABC_74420437db295ca874d5c946891f69_27", "text": "The starting point of our work is the model of Tsvetkov and Wintner (2014) , which we extend with a number of features, including language-specific ones that account for the relatively free word order."}
{"sent_id": "74420437db295ca874d5c946891f69-C001-26", "intents": ["@DIF@"], "paper_id": "ABC_74420437db295ca874d5c946891f69_27", "text": "Unlike Tsvetkov and Wintner (2014) , who only consider bigrams, we consider MWEs of up to five words in length."}
{"sent_id": "91723cf7f22ba6405c85a929ac2d8e-C001-31", "intents": ["@USE@"], "paper_id": "ABC_91723cf7f22ba6405c85a929ac2d8e_27", "text": "Following Hermann et al. (2015) , our model estimates the conditional probability p(e|D, q), where q is a query and D is a document."}
{"sent_id": "91723cf7f22ba6405c85a929ac2d8e-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_91723cf7f22ba6405c85a929ac2d8e_27", "text": "These datasets have enabled training and testing of complicated neural network models of hypothesized machine readers (Hermann et al., 2015; Hill et al., 2015) ."}
{"sent_id": "91723cf7f22ba6405c85a929ac2d8e-C001-92", "intents": ["@DIF@"], "paper_id": "ABC_91723cf7f22ba6405c85a929ac2d8e_27", "text": "Finally, we note that our model, full DER Network, shows the best results compared to several previous reader models (Hermann et al., 2015; Hill et al., 2015) , endorsing our approach as promising."}
{"sent_id": "3881903212a2d0fea039c8967ab553-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_3881903212a2d0fea039c8967ab553_27", "text": "Prabhakaran et al. (2013a) introduced the notion of power in the domain of presidential debates, and Prabhakaran et al. (2013b) followed it up with an automatic power ranker system based on interactions within the debates."}
{"sent_id": "3881903212a2d0fea039c8967ab553-C001-35", "intents": ["@DIF@"], "paper_id": "ABC_3881903212a2d0fea039c8967ab553_27", "text": "As an additional contribution of this paper, we demonstrate the utility of our topic shift features extracted using both types of SITS-based analyses in improving the performance of the automatic power ranker system presented in (Prabhakaran et al., 2013b) ."}
{"sent_id": "3881903212a2d0fea039c8967ab553-C001-101", "intents": ["@USE@"], "paper_id": "ABC_3881903212a2d0fea039c8967ab553_27", "text": "As we do in (Prabhakaran et al., 2013b) , we here report Kendall's Tau and Normalized Discounted Cumulative Gain values (NDCG and NDCG@3) on 5-fold cross validation (at the debate level)."}
{"sent_id": "0753a2be70f9844d353ec54c04fd53-C001-47", "intents": ["@USE@"], "paper_id": "ABC_0753a2be70f9844d353ec54c04fd53_27", "text": "Following the experimental settings and evaluation metrics in Bai and Zhao (2018) , we use two most-used splitting methods of PDTB data, denoted as PDTB-Lin (Lin et al., 2009) , which uses sections 2-21, 22, 23 as training, validation and test sets, and PDTB-Ji (Ji and Eisenstein, 2015) , which uses 2-20, 0-1, 21-22 as training, validation and test sets and report the overall accuracy score."}
{"sent_id": "3e5c070a6966361b54f069248438ec-C001-36", "intents": ["@BACK@"], "paper_id": "ABC_3e5c070a6966361b54f069248438ec_27", "text": "Chen and Manning (2014) developed a neural network architecture for dependency parsing."}
{"sent_id": "3e5c070a6966361b54f069248438ec-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_3e5c070a6966361b54f069248438ec_27", "text": "Chen and Manning (2014) 's parser used a feed forward neural network."}
{"sent_id": "3e5c070a6966361b54f069248438ec-C001-16", "intents": ["@EXT@"], "paper_id": "ABC_3e5c070a6966361b54f069248438ec_27", "text": "We first adapt Chen and Manning (2014) 's shift-reduce dependency parser for CCG parsing."}
{"sent_id": "3e5c070a6966361b54f069248438ec-C001-67", "intents": ["@USE@"], "paper_id": "ABC_3e5c070a6966361b54f069248438ec_27", "text": "We use the training settings of Chen and Manning (2014) for our parser."}
{"sent_id": "3e5c070a6966361b54f069248438ec-C001-80", "intents": ["@USE@"], "paper_id": "ABC_3e5c070a6966361b54f069248438ec_27", "text": "Unlike 's neural network architecture, which consists of two hidden layers with 2048 hidden units each, we use the Chen and Manning (2014) style architecture described in the previous sections."}
{"sent_id": "3e5c070a6966361b54f069248438ec-C001-83", "intents": ["@DIF@"], "paper_id": "ABC_3e5c070a6966361b54f069248438ec_27", "text": "Our neural network parser differs from Chen and Manning (2014) in a number of respects."}
{"sent_id": "98d8ea63896cc80f0989130e7cbbf1-C001-21", "intents": ["@DIF@"], "paper_id": "ABC_98d8ea63896cc80f0989130e7cbbf1_27", "text": "Additionally, in cases where authors are rhetorically similar, our framework outperforms techniques outlined by Akiva (2013) and Aldebei (2015) , which both rely on word occurrences as features."}
{"sent_id": "684349c06be7ff51c0944b25be10ce-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_684349c06be7ff51c0944b25be10ce_27", "text": "Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature (Elson et al., 2010; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015) ."}
{"sent_id": "684349c06be7ff51c0944b25be10ce-C001-107", "intents": ["@USE@"], "paper_id": "ABC_684349c06be7ff51c0944b25be10ce_27", "text": "(For comparison, BookNLP, the next best system, extracted 69 and 72 characters for Pride and Prejudice and The Moonstone, respectively, and within 1.2, on average, on the Sherlock Holmes set.) Experiment 2: Literary Theories Elson et al. (2010) analyze 60 novels to computationally test literary theories for novels in urban and rural settings (Williams, 1975; Moretti, 1999) ."}
{"sent_id": "2e967f8560ffdb216135ae387776eb-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_2e967f8560ffdb216135ae387776eb_27", "text": "For German readability assessment, however, little progress has been made in recent years, despite a series of promising results published around the turn of the decade (Vor der Brück et al., 2008; Hancke et al., 2012) ."}
{"sent_id": "2e967f8560ffdb216135ae387776eb-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_2e967f8560ffdb216135ae387776eb_27", "text": "There was a series of articles on this issue from the late 2000s to the early 2010s that demonstrated the benefits of broad linguistic modeling, in particular the use of morphological complexity measures for languages with rich morphological systems like German (Vor der Brück et al., 2008; Hancke et al., 2012) , but also Russian (Reynolds, 2016) or French (François and Fairon, 2012) ."}
{"sent_id": "2e967f8560ffdb216135ae387776eb-C001-183", "intents": ["@EXT@"], "paper_id": "ABC_2e967f8560ffdb216135ae387776eb_27", "text": "The approach presented thus extends the state-of-the-art in Hancke et al. (2012) in terms of the breadth of features integrated and the accuracy and generalizability of the model -and provides two new data sources for this line of research."}
{"sent_id": "99b26d9151c7c0a1df1df1300fc764-C001-144", "intents": ["@EXT@"], "paper_id": "ABC_99b26d9151c7c0a1df1df1300fc764_27", "text": "Speech2Vec, which integrates a RNN Encoder-Decoder framework with skipgrams or cbow for training, extends the textbased Word2Vec [1] model to learn word embeddings directly from speech."}
{"sent_id": "99b26d9151c7c0a1df1df1300fc764-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_99b26d9151c7c0a1df1df1300fc764_27", "text": "Natural language processing (NLP) techniques such as Word2Vec [1, 2] and GloVe [3] transform words into fixed dimensional vectors, or word embeddings."}
{"sent_id": "99b26d9151c7c0a1df1df1300fc764-C001-94", "intents": ["@USE@"], "paper_id": "ABC_99b26d9151c7c0a1df1df1300fc764_27", "text": "The intrinsic method directly tests for semantic or syntactic relationships between words, and includes the tasks of word similarity and word analogy [1] ."}
{"sent_id": "bdeffcf02a86d06f57dbfae979b098-C001-25", "intents": ["@USE@"], "paper_id": "ABC_bdeffcf02a86d06f57dbfae979b098_27", "text": "We begin in §2 by extending the online variational Bayes approach of Hoffman et al. (2010) to polylingual topic models (Mimno et al., 2009) ."}
{"sent_id": "bdeffcf02a86d06f57dbfae979b098-C001-131", "intents": ["@USE@"], "paper_id": "ABC_bdeffcf02a86d06f57dbfae979b098_27", "text": "For all four topic models, we use the same settings for PLTM (hyperparameter values and number of Gibbs sampling iterations) as in (Mimno et al., 2009) 2 ."}
{"sent_id": "5c5abc2773143af41d49087e17310e-C001-19", "intents": ["@EXT@"], "paper_id": "ABC_5c5abc2773143af41d49087e17310e_27", "text": "We re-implement the HIERSUM system from Haghighi and Vanderwende (2009) , and show that using our objective dramatically improves the content of extracted summaries."}
{"sent_id": "5c5abc2773143af41d49087e17310e-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_5c5abc2773143af41d49087e17310e_27", "text": "This idea was first presented by Daumé and Marcu (2006) for their BAYESUM system for query-focused summarization, and later adapted for non-query summarization in the TOPICSUM system by Haghighi and Vanderwende (2009) ."}
{"sent_id": "5c5abc2773143af41d49087e17310e-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_5c5abc2773143af41d49087e17310e_27", "text": "HIERSUM (Haghighi & Vanderwende, 2009 ) adds more structure to TOPICSUM by further splitting the content distribution into multiple sub-topics."}
{"sent_id": "fe443d5e13b525cbdfa58dafb83162-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_fe443d5e13b525cbdfa58dafb83162_28", "text": "Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classification problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation."}
{"sent_id": "fe443d5e13b525cbdfa58dafb83162-C001-21", "intents": ["@DIF@"], "paper_id": "ABC_fe443d5e13b525cbdfa58dafb83162_28", "text": "The method is language-independent and performs very well on both languages we tested it on: for English, it outperforms the best published method we are aware of (Schmid, 2006) , and for Chinese, it outperforms the method of Yang and Xue (2010) ."}
{"sent_id": "fe443d5e13b525cbdfa58dafb83162-C001-83", "intents": ["@DIF@"], "paper_id": "ABC_fe443d5e13b525cbdfa58dafb83162_28", "text": "Our system outperformed that of Yang and Xue (2010) especially on *pro*, used for dropped arguments, and *T*, used for relative clauses and topicalization."}
{"sent_id": "fe443d5e13b525cbdfa58dafb83162-C001-78", "intents": ["@UNSURE@"], "paper_id": "ABC_fe443d5e13b525cbdfa58dafb83162_28", "text": "For comparability with previous work (Yang and Xue, 2010) , we trained the parser on sections 0081-0900, used sections 0041-0080 for development, and sections 0001-0040 and 0901-0931 for testing."}
{"sent_id": "622bd6f16d55ab5853389286cdda56-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_622bd6f16d55ab5853389286cdda56_28", "text": "Existing works on the analysis of writing revisions (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013; Zhang and Litman, 2015) typically compare two versions of a text to extract revisions, then classify the purpose of each revision in isolation."}
{"sent_id": "622bd6f16d55ab5853389286cdda56-C001-54", "intents": ["@BACK@"], "paper_id": "ABC_622bd6f16d55ab5853389286cdda56_28", "text": "Our previous work (Zhang and Litman, 2015) used three types of features primarily from prior work (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013) for argumentative revision classification."}
{"sent_id": "622bd6f16d55ab5853389286cdda56-C001-40", "intents": ["@EXT@"], "paper_id": "ABC_622bd6f16d55ab5853389286cdda56_28", "text": "To label our data, we adapt the schema defined in (Zhang and Litman, 2015) as it can be reliably annotated and is argument- (Faigley and Witte, 1981) ."}
{"sent_id": "622bd6f16d55ab5853389286cdda56-C001-45", "intents": ["@USE@"], "paper_id": "ABC_622bd6f16d55ab5853389286cdda56_28", "text": "Corpus A was collected in our earlier pa-per (Zhang and Litman, 2015) , although the original annotations were modified as described above."}
{"sent_id": "318487ac270ca272ec11a3de6c0685-C001-81", "intents": ["@UNSURE@"], "paper_id": "ABC_318487ac270ca272ec11a3de6c0685_28", "text": "To our knowledge, the current state-of-theart is a supervised system that combines several machine translation metrics (Madnani et al., 2012 ), but we also compare with state-of-the-art unsupervised matrix factorization work (Guo and Diab, 2012) ."}
{"sent_id": "49b42346795d541dbcac9e2b9ad00a-C001-30", "intents": ["@BACK@"], "paper_id": "ABC_49b42346795d541dbcac9e2b9ad00a_28", "text": "This model was further improved by (Makarov et al., 2017) , whose system was the winner of Sigmorphon 2017 evaluation campaign (Cotterell et al., 2017) ."}
{"sent_id": "49b42346795d541dbcac9e2b9ad00a-C001-39", "intents": ["@BACK@"], "paper_id": "ABC_49b42346795d541dbcac9e2b9ad00a_28", "text": "However, this decomposition is already realised in model of (Makarov et al., 2017) since the grammatical features are treated as a list of atomic elements, not as entire label."}
{"sent_id": "49b42346795d541dbcac9e2b9ad00a-C001-47", "intents": ["@DIF@"], "paper_id": "ABC_49b42346795d541dbcac9e2b9ad00a_28", "text": "We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of (Makarov et al., 2017) for most of the languages."}
{"sent_id": "49b42346795d541dbcac9e2b9ad00a-C001-70", "intents": ["@USE@"], "paper_id": "ABC_49b42346795d541dbcac9e2b9ad00a_28", "text": "We also use the copy gate from (Makarov et al., 2017) : since the neural network copies the vast majority of its symbols, the output distribution p i is obtained as a weighted sum of singleton distribution which outputs current input symbol and the preliminary distribution p i specified above."}
{"sent_id": "afee292717afe1b0dcd77e155a5121-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_afee292717afe1b0dcd77e155a5121_28", "text": "Dakka and Cucerzan (2008) expanded their set of 800 hand-labelled articles using a semisupervised approach, extracting training samples from Wikipedia \"List\" pages -pages that group other articles by type."}
{"sent_id": "afee292717afe1b0dcd77e155a5121-C001-100", "intents": ["@UNSURE@"], "paper_id": "ABC_afee292717afe1b0dcd77e155a5121_28", "text": "Table 3 is a comparison of precision, recall and F -scores between our baseline and final systems, and the systems produced by Nothman et al. (2009) and Dakka and Cucerzan (2008) ."}
{"sent_id": "c594df62c01bef2ffb1a7ee9c5ea28-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_c594df62c01bef2ffb1a7ee9c5ea28_28", "text": "However, tensor training can be expensive and in practice, for some tasks, the verb can be approximated as a matrix [7, 14] ."}
{"sent_id": "c594df62c01bef2ffb1a7ee9c5ea28-C001-40", "intents": ["@USE@"], "paper_id": "ABC_c594df62c01bef2ffb1a7ee9c5ea28_28", "text": "Following [7] , we generate a K × K matrix for each verb as the average of outer products of subject and verb vectors from the positively labelled subset of the training data:"}
{"sent_id": "4ad830d8377d2584798a30bed65254-C001-63", "intents": ["@BACK@"], "paper_id": "ABC_4ad830d8377d2584798a30bed65254_28", "text": "Debiaswe is a postprocess method for debiasing previously generated embeddings (Bolukbasi et al., 2016) ."}
{"sent_id": "4ad830d8377d2584798a30bed65254-C001-66", "intents": ["@BACK@"], "paper_id": "ABC_4ad830d8377d2584798a30bed65254_28", "text": "Debiaswe (Bolukbasi et al., 2016 ) is a postprocess method for debiasing word embeddings."}
{"sent_id": "4ad830d8377d2584798a30bed65254-C001-176", "intents": ["@USE@"], "paper_id": "ABC_4ad830d8377d2584798a30bed65254_28", "text": "Then, we debiased the embeddings using Debiaswe (Bolukbasi et al., 2016) and also trained its gender neutral version with GN-GloVe (Zhao et al., 2018b) ."}
{"sent_id": "188f10a5b78a5e691e10d180dfde6f-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_188f10a5b78a5e691e10d180dfde6f_28", "text": "A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, including clustering based word representation learning methods (Huang and Yates, 2009; Candito et al., 2011) , word embedding based representation learning methods (Turian et al., 2010; Hovy et al., 2015) and some other representation learning methods (Blitzer et al., 2006) ."}
{"sent_id": "188f10a5b78a5e691e10d180dfde6f-C001-50", "intents": ["@BACK@"], "paper_id": "ABC_188f10a5b78a5e691e10d180dfde6f_28", "text": "Previous works have demonstrated the usefulness of discrete hidden states induced from a HMM on addressing feature sparsity in domain adaptation (Huang and Yates, 2009 )."}
{"sent_id": "9666fd26c7e9a02505ff26a687076d-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_9666fd26c7e9a02505ff26a687076d_28", "text": "Dyer et al. (2013) present a simple reparameterization of IBM Model 2 that is very fast to train, and achieves results similar to IBM Model 4."}
{"sent_id": "9666fd26c7e9a02505ff26a687076d-C001-31", "intents": ["@EXT@"], "paper_id": "ABC_9666fd26c7e9a02505ff26a687076d_28", "text": "We make use of a modified version of Model 2, from Dyer et al. (2013) , which has an alignment model that is parameterised in its original form solely on the variable λ."}
{"sent_id": "9666fd26c7e9a02505ff26a687076d-C001-63", "intents": ["@DIF@"], "paper_id": "ABC_9666fd26c7e9a02505ff26a687076d_28", "text": "This is not done in the current work however, so timing results will not be directly comparable to those found in (Dyer et al., 2013) ."}
{"sent_id": "9666fd26c7e9a02505ff26a687076d-C001-70", "intents": ["@UNSURE@"], "paper_id": "ABC_9666fd26c7e9a02505ff26a687076d_28", "text": "The models that are compared are the original reparameterization of Model 2, a version where λ is split around the diagonal (split), one where pos tags are used, but λ is not split around the diagonal (pos), one where an offset is used, but parameters aren't split about the diagonal (offset), one that's split about the diagonal and uses pos tags used as in (Dyer et al., 2013) , with stepsize for updates to λ and γ during gradient ascent is 1000, and that for ω is 0.03, decaying after every gradient descent step by 0.9, using 8 steps every iteration."}
{"sent_id": "0d1fb27d847ca44af36862cf78744e-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_0d1fb27d847ca44af36862cf78744e_28", "text": "In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003) ."}
{"sent_id": "0d1fb27d847ca44af36862cf78744e-C001-23", "intents": ["@BACK@"], "paper_id": "ABC_0d1fb27d847ca44af36862cf78744e_28", "text": "First, the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al., 1998) and encoding information about these lifts in arc labels."}
{"sent_id": "0d1fb27d847ca44af36862cf78744e-C001-27", "intents": ["@UNSURE@"], "paper_id": "ABC_0d1fb27d847ca44af36862cf78744e_28", "text": "We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998) ."}
{"sent_id": "fe30705e03f0475f9ab9d044a3c9ca-C001-110", "intents": ["@BACK@"], "paper_id": "ABC_fe30705e03f0475f9ab9d044a3c9ca_28", "text": "2 Building a gold standard thesaurus following Curran (2002) needs access to all the used thesauri."}
{"sent_id": "fe30705e03f0475f9ab9d044a3c9ca-C001-49", "intents": ["@DIF@"], "paper_id": "ABC_fe30705e03f0475f9ab9d044a3c9ca_28", "text": "As an example the dependency relation (nsub;gave 2 ;I 1 ) could be transferred to  and . This representation scheme is more generic then the schemes introduced in (Lin, 1998; Curran, 2002) , as it allows to characterise pairs by several holes, which could be used to learn analogies, cf."}
{"sent_id": "fe30705e03f0475f9ab9d044a3c9ca-C001-99", "intents": ["@DIF@"], "paper_id": "ABC_fe30705e03f0475f9ab9d044a3c9ca_28", "text": "We could not confirm that his measure outperforms Lin's measure as stated in (Curran, 2002) 1 ."}
{"sent_id": "fe30705e03f0475f9ab9d044a3c9ca-C001-81", "intents": ["@USE@"], "paper_id": "ABC_fe30705e03f0475f9ab9d044a3c9ca_28", "text": "We create a gold standard, by extracting reasonable entries of these 2000 nouns using Roget's 1911 thesaurus, Moby Thesaurus, Merriam Webster's Thesaurus, the Big Huge Thesaurus and the OpenOffice Thesaurus and employ the inverse ranking measure (Curran, 2002) to evaluate the DTs."}
{"sent_id": "3351b13fc0c9d4d2de16d897c78aee-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_3351b13fc0c9d4d2de16d897c78aee_28", "text": "In recent years, methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification (Popescu and Grozea, 2012) and sentiment analysis (Giménez-Pérez et al., 2017; to native language identification (Popescu and Ionescu, 2013; Ionescu et al., 2014 Ionescu et al., , 2016 , dialect identification (Ionescu and Popescu, 2016b; Ionescu and Butnaru, 2017; and automatic essay scoring (Cozma et al., 2018) ."}
{"sent_id": "3351b13fc0c9d4d2de16d897c78aee-C001-102", "intents": ["@DIF@"], "paper_id": "ABC_3351b13fc0c9d4d2de16d897c78aee_28", "text": "The marker * indicates that the performance is significantly better than (Ionescu and Butnaru, 2017) according to a paired McNemar's test performed at a significance level of 0.01."}
{"sent_id": "3351b13fc0c9d4d2de16d897c78aee-C001-92", "intents": ["@USE@"], "paper_id": "ABC_3351b13fc0c9d4d2de16d897c78aee_28", "text": "We choose as baseline the approach of Ionescu and Butnaru (2017) , which is based on string kernels and multiple kernel learning."}
{"sent_id": "5e6d5bb4fb5be2b18ce3256302bf28-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_5e6d5bb4fb5be2b18ce3256302bf28_28", "text": "Generation of referring expression (GRE) is an important task in the field of Natural Language Generation (NLG) systems (Reiter and Dale, 1995) ."}
{"sent_id": "a6f32017135e984fbe59f2171c50f4-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_a6f32017135e984fbe59f2171c50f4_28", "text": "Neural machine translation (NMT) (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017) trains an encoder-decoder network on sentence pairs to maximize the likelihood of predicting a target-language sentence given the corresponding source-language sentence, without considering the document context."}
{"sent_id": "a6f32017135e984fbe59f2171c50f4-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_a6f32017135e984fbe59f2171c50f4_28", "text": "where f s is a linear transformation, q s is the query for the attention function, FFN is a position-wise feed-forward layer (Vaswani et al., 2017 )."}
{"sent_id": "a6f32017135e984fbe59f2171c50f4-C001-22", "intents": ["@USE@"], "paper_id": "ABC_a6f32017135e984fbe59f2171c50f4_28", "text": "Furthermore, we extend the original HAN with a multi-head attention (Vaswani et al., 2017) to capture different types of discourse phenomena."}
{"sent_id": "a6f32017135e984fbe59f2171c50f4-C001-40", "intents": ["@USE@"], "paper_id": "ABC_a6f32017135e984fbe59f2171c50f4_28", "text": "We used the MultiHead attention function proposed by (Vaswani et al., 2017) to capture different types of relations among words."}
{"sent_id": "d3f5f9b1ef8bda3d33c563d252d58a-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_d3f5f9b1ef8bda3d33c563d252d58a_28", "text": "A high accuracy was reported on a word translation task, where a word projected to the vector space of the target language is expected to be as close as possible to its translation (Mikolov et al., 2013b) ."}
{"sent_id": "2db9f6c8540d8d2b7a5946c3c132e9-C001-34", "intents": ["@BACK@"], "paper_id": "ABC_2db9f6c8540d8d2b7a5946c3c132e9_28", "text": "Most work is centered around word2vec (e.g., Kim et al. (2014) ; Kulkarni et al. (2015) ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., Jo (2016) using GloVe (Pennington et al., 2014) and Hamilton et al. (2016) using SVD PPMI ."}
{"sent_id": "2db9f6c8540d8d2b7a5946c3c132e9-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_2db9f6c8540d8d2b7a5946c3c132e9_28", "text": "Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016) and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space."}
{"sent_id": "2db9f6c8540d8d2b7a5946c3c132e9-C001-70", "intents": ["@DIF@"], "paper_id": "ABC_2db9f6c8540d8d2b7a5946c3c132e9_28", "text": "We follow Kim et al. (2014) in choosing such a visualization, while we refrain from using the two-dimensional projection used in other studies (Kulkarni et al., 2015; Hamilton et al., 2016) ."}
{"sent_id": "2db9f6c8540d8d2b7a5946c3c132e9-C001-91", "intents": ["@UNSURE@"], "paper_id": "ABC_2db9f6c8540d8d2b7a5946c3c132e9_28", "text": "In contrast to other corpus exploration tools, JESEME is based on cutting-edge word embedding technology (Levy et al., 2015; Hamilton et al., 2016; Hahn, 2016a, 2017) and provides access to five popular corpora for the English and German language."}
{"sent_id": "9a52e0ea1f12e3455fca48ac8f8936-C001-36", "intents": ["@USE@"], "paper_id": "ABC_9a52e0ea1f12e3455fca48ac8f8936_28", "text": "Following a meta-learning approach, we apply a genetic algorithm and a sequential search algorithm, described in the next section, initialized using the best configuration reported in [4] to search the space around optimal hyper parameters for the AWD-LSTM model."}
{"sent_id": "9a52e0ea1f12e3455fca48ac8f8936-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_9a52e0ea1f12e3455fca48ac8f8936_28", "text": "We begin our work by establishing what the baseline and current state of the art model is for a language modeling task [4] ."}
{"sent_id": "fe0f9312caccf41def06e4311d15fb-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_fe0f9312caccf41def06e4311d15fb_28", "text": "Given a text corpus, they extract structured representations (i.e. graphs), for example chains (Chambers & Jurafsky, 2008) or more gen- eral directed acyclic graphs (Regneri et al., 2010) ."}
{"sent_id": "fe0f9312caccf41def06e4311d15fb-C001-20", "intents": ["@BACK@"], "paper_id": "ABC_fe0f9312caccf41def06e4311d15fb_28", "text": "Similarly, an approach which treats the whole predicate-argument structure as an atomic unit (Regneri et al., 2010) will probably fail as well, as such a sparse model is unlikely to be effectively learnable even from large amounts of data."}
{"sent_id": "fe0f9312caccf41def06e4311d15fb-C001-72", "intents": ["@BACK@"], "paper_id": "ABC_fe0f9312caccf41def06e4311d15fb_28", "text": "The methods are evaluated on human annotated scenariospecific tests: the goal is to classify event pairs as appearing in a given stereotypical order or not (Regneri et al., 2010 )."}
{"sent_id": "fe0f9312caccf41def06e4311d15fb-C001-61", "intents": ["@USE@"], "paper_id": "ABC_fe0f9312caccf41def06e4311d15fb_28", "text": "We evaluate our approach on crowdsourced data collected for script induction by Regneri et al. (2010) , though, in principle, the method is applicable in arguably more general setting of Chambers & Jurafsky (2008) ."}
{"sent_id": "fe0f9312caccf41def06e4311d15fb-C001-70", "intents": ["@DIF@"], "paper_id": "ABC_fe0f9312caccf41def06e4311d15fb_28", "text": "Unlike our work, Regneri et al. (2010) relies on WordNet to provide extra signal when using the Multiple Sequence Alignment (MSA) algorithm."}
{"sent_id": "7d89a96743d9db5667d90cbd3ebd30-C001-94", "intents": ["@UNSURE@"], "paper_id": "ABC_7d89a96743d9db5667d90cbd3ebd30_28", "text": "The most competitive method is the method based on reweighed bag-of-words (Wang & Manning, 2012) 2 ."}
{"sent_id": "591e2873606d6171e48fd34a731fc7-C001-19", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_591e2873606d6171e48fd34a731fc7_29", "text": "1.) Geodesic grids are the most straightforward, but do not \"lead to a natural representation of the administrative, population-based or language boundaries in the region\" (Han et al., 2012) ."}
{"sent_id": "591e2873606d6171e48fd34a731fc7-C001-77", "intents": ["@USE@"], "paper_id": "ABC_591e2873606d6171e48fd34a731fc7_29", "text": "The algorithm converges when the final number of points cannot be further reduced, since they all are farther apart from each other than the maximum distance d. After assigning each instance its new coordinates, we follow Han et al. (2012 Han et al. ( , 2014 in using the GeoNames data set to associate clusters with cities, by substituting the instance coordinates with those of the closest town center."}
{"sent_id": "2bb115f1c3e753e9dc66735887a52d-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_2bb115f1c3e753e9dc66735887a52d_29", "text": "Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b) , the type of classifier (Denis and Baldridge, 2007) , and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b) ."}
{"sent_id": "2bb115f1c3e753e9dc66735887a52d-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_2bb115f1c3e753e9dc66735887a52d_29", "text": "More recently, Denis and Baldridge (2007) utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers."}
{"sent_id": "2bb115f1c3e753e9dc66735887a52d-C001-27", "intents": ["@USE@"], "paper_id": "ABC_2bb115f1c3e753e9dc66735887a52d_29", "text": "When describing our model, we build upon the notation used by Denis and Baldridge (2007) ."}
{"sent_id": "2bb115f1c3e753e9dc66735887a52d-C001-45", "intents": ["@USE@"], "paper_id": "ABC_2bb115f1c3e753e9dc66735887a52d_29", "text": "Our D&B-STYLE baseline used the same test time method as Denis and Baldridge (2007) , however at training time we created data for all mention pairs."}
{"sent_id": "18ef4e4fafdf62839d6797d62eb76b-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_18ef4e4fafdf62839d6797d62eb76b_29", "text": "Previous works (Rubin et al., 2016; Rashkin et al., 2017) rely on various linguistic and handcrafted semantic features for differentiating between news articles."}
{"sent_id": "18ef4e4fafdf62839d6797d62eb76b-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_18ef4e4fafdf62839d6797d62eb76b_29", "text": "Rubin et al. (2016) defines news satire as a genre of satire that mimics the format and style of journalistic reporting."}
{"sent_id": "18ef4e4fafdf62839d6797d62eb76b-C001-42", "intents": ["@USE@"], "paper_id": "ABC_18ef4e4fafdf62839d6797d62eb76b_29", "text": "We use SLN: Satirical and Legitimate News Database (Rubin et al., 2016) , RPN: Random Political News Dataset (Horne and Adali, 2017) and LUN: Labeled Unreliable News Dataset Rashkin et al. (2017) for our experiments."}
{"sent_id": "18ef4e4fafdf62839d6797d62eb76b-C001-32", "intents": ["@DIF@"], "paper_id": "ABC_18ef4e4fafdf62839d6797d62eb76b_29", "text": "Rubin et al. (2016) 's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire."}
{"sent_id": "18ef4e4fafdf62839d6797d62eb76b-C001-106", "intents": ["@DIF@"], "paper_id": "ABC_18ef4e4fafdf62839d6797d62eb76b_29", "text": "Given that we use SLN as an out of domain test set (just one overlapping source, no overlap in articles), whereas the SoTA paper (Rubin et al., 2016) reports a 10fold cross validation number on SLN."}
{"sent_id": "e2705ae777acffc894f7aa18d42771-C001-31", "intents": ["@USE@"], "paper_id": "ABC_e2705ae777acffc894f7aa18d42771_29", "text": "We follow the transfer learning approach proposed by Zoph et al. (2016) ."}
{"sent_id": "e2705ae777acffc894f7aa18d42771-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_e2705ae777acffc894f7aa18d42771_29", "text": "For example, Zoph et al. (2016) train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair."}
{"sent_id": "eab79e8aa2cbe6f3aeef0018129208-C001-44", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_eab79e8aa2cbe6f3aeef0018129208_29", "text": "(Le and Zuidema, 2014) where r = 0 if y is the first dependent of h; oth-"}
{"sent_id": "17d44521cfdd351d29b4e5f80d41cd-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_17d44521cfdd351d29b4e5f80d41cd_29", "text": "Transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008; Huang and Sagae, 2010 ) utilize a deterministic shift-reduce process for making structural predictions."}
{"sent_id": "17d44521cfdd351d29b4e5f80d41cd-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_17d44521cfdd351d29b4e5f80d41cd_29", "text": "In the aspect of decoding, beam-search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2009 ) and partial dynamic-programming (Huang and Sagae, 2010) have been applied to improve upon greedy one-best search, and positive results were reported."}
{"sent_id": "17d44521cfdd351d29b4e5f80d41cd-C001-81", "intents": ["@USE@"], "paper_id": "ABC_17d44521cfdd351d29b4e5f80d41cd_29", "text": "2 Following Huang and Sagae (2010), we assign POS-tags to the training data using ten-way jackknifing."}
{"sent_id": "17d44521cfdd351d29b4e5f80d41cd-C001-102", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_17d44521cfdd351d29b4e5f80d41cd_29", "text": "Table 5 shows the results of our final parser, the pure transition-based parser of Zhang and Clark (2008) , and the parser of Huang and Sagae (2010) on Chinese."}
{"sent_id": "1c86f563ababf5ec3c67cbf259252b-C001-37", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_1c86f563ababf5ec3c67cbf259252b_29", "text": "Our approach is similar to (Chen and Bansal, 2018) , except that they use parallel data to train their extractors and abstractors."}
{"sent_id": "1c86f563ababf5ec3c67cbf259252b-C001-67", "intents": ["@USE@"], "paper_id": "ABC_1c86f563ababf5ec3c67cbf259252b_29", "text": "We follow the preprocessing pipeline of (Chen and Bansal, 2018) , splitting the dataset into 287k/11k/11k pairs for training/validation/testing."}
{"sent_id": "d0fa481abaf6d1b5529e40ff73f00a-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_d0fa481abaf6d1b5529e40ff73f00a_29", "text": "Neural models have consistently shown top performance in shared evaluation tasks (Bojar et al., 2016; Cettolo et al., 2016) and are becoming the technology of choice for commercial MT service providers (Wu et al., 2016; Crego et al., 2016) ."}
{"sent_id": "d0fa481abaf6d1b5529e40ff73f00a-C001-31", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_d0fa481abaf6d1b5529e40ff73f00a_29", "text": "Translation models are trained until perplexity convergence on held-out data using the Adam algorithm with a maximum step size of 0.0002 (Kingma and Ba, 2015; Wu et al., 2016) ."}
{"sent_id": "7176d3dd72e781dca42f8c146d062d-C001-46", "intents": ["@DIF@", "@SIM@"], "paper_id": "ABC_7176d3dd72e781dca42f8c146d062d_29", "text": "Our technique is similar to that of (Munteanu and Marcu, 2005) but we bypass the need of the bilingual dictionary by using proper SMT translations and instead of a maximum entropy classifier we use simple measures like the word error rate (WER) and the translation edit rate (TER) to decide whether sentences are parallel or not."}
{"sent_id": "7176d3dd72e781dca42f8c146d062d-C001-203", "intents": ["@DIF@"], "paper_id": "ABC_7176d3dd72e781dca42f8c146d062d_29", "text": "Contrary to the previous approaches as in (Munteanu and Marcu, 2005) which used small amounts of in-domain parallel corpus as an initial resource, our system exploits the target language side of the comparable corpus to attain the same goal, thus the comparable corpus itself helps to better extract possible parallel sentences."}
{"sent_id": "7176d3dd72e781dca42f8c146d062d-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_7176d3dd72e781dca42f8c146d062d_29", "text": "The ease of availability of these comparable corpora and the potential for parallel corpus as well as dictionary creation has sparked an interest in trying to make maximum use of these comparable resources, some of these works include dictionary learning and identifying word translations (Rapp, 1995) , named entity recognition (Sproat et al., 2006) , word sense disambiguation (Kaji, 2003) , improving SMT performance using extracted parallel sentences (Munteanu and Marcu, 2005) , (Rauf and Schwenk, 2009 )."}
{"sent_id": "7176d3dd72e781dca42f8c146d062d-C001-54", "intents": ["@USE@"], "paper_id": "ABC_7176d3dd72e781dca42f8c146d062d_29", "text": "We also perform a comparison of the data extracted by our approach and that by (Munteanu and Marcu, 2005) and report the results in Section 5.3."}
{"sent_id": "0ae49d1618e18eb794666543d924ed-C001-91", "intents": ["@USE@"], "paper_id": "ABC_0ae49d1618e18eb794666543d924ed_29", "text": "We compare with the state-of-theart character-level neural NER system of (Lample et al., 2016) , which inherently encodes comparable information to CLMs, as a way to investigate how much of that system's performance can be attributed directly to name-internal structure."}
{"sent_id": "0ae49d1618e18eb794666543d924ed-C001-102", "intents": ["@DIF@", "@MOT@"], "paper_id": "ABC_0ae49d1618e18eb794666543d924ed_29", "text": "While the end-to-end model developed by (Lample et al., 2016) clearly includes information comparable to that in the CLM, it requires a fully annotated NER corpus, takes significant time and computational resources to train, and is non-trivial to integrate into a new NER system."}
{"sent_id": "b8d0e66901698d201b9fb1f362b8c6-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_b8d0e66901698d201b9fb1f362b8c6_29", "text": "Recently, neural approaches have reached very competitive accuracy levels, improving over the state of the art in a number of settings (Plank et al., 2016) ."}
{"sent_id": "b8d0e66901698d201b9fb1f362b8c6-C001-28", "intents": ["@BACK@"], "paper_id": "ABC_b8d0e66901698d201b9fb1f362b8c6_29", "text": "As shown by Plank et al. (2016) , state-of-the-art performance can be achieved using a bi-LSTM architecture fed with word representations."}
{"sent_id": "b8d0e66901698d201b9fb1f362b8c6-C001-61", "intents": ["@USE@"], "paper_id": "ABC_b8d0e66901698d201b9fb1f362b8c6_29", "text": "Pre-computed embeddings Whenever available and following Plank et al. (2016) , we performed experiments using Polyglot pre-computed embeddings (Al-Rfou et al., 2013) ."}
{"sent_id": "b8d0e66901698d201b9fb1f362b8c6-C001-74", "intents": ["@USE@"], "paper_id": "ABC_b8d0e66901698d201b9fb1f362b8c6_29", "text": "We use as a baseline the state-of-the-art bi-LSTM PoS tagger bilty, a freely available 6 and \"significantly refactored version of the code originally used\" by Plank et al. (2016) ."}
{"sent_id": "603f49fc6ecf90da67a9a55986f217-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_603f49fc6ecf90da67a9a55986f217_29", "text": "More recently, Xu et al. (2014) combined the training objective of SKIP-GRAM (Mikolov et al., 2013a) with the training objective of (Bordes et al., 2013) to incorporate lexical 1 There exists work on relation extraction and knowledgebase completion that combines structured relation triplets and logical rules with unstructured text using various forms of latent variable models (Riedel et al., 2013; Chang et al., 2014; Toutanova et al., 2015; Rocktäschel et al., 2015) ."}
{"sent_id": "603f49fc6ecf90da67a9a55986f217-C001-107", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_603f49fc6ecf90da67a9a55986f217_29", "text": "We compare the proposed RELSUB model with two methods: (i) CBOW (Mikolov et al., 2013a) , and (ii) RELCONST which is based on constant translation model for relations which was originally proposed in (Bordes et al., 2013) for embedding knowledge-bases and was recently used by Table 2 : Google analogy data: Accuracy on word analogy task (Xu et al., 2014) for learning word embeddings."}
{"sent_id": "4d8ae52583d41b4124800c419963df-C001-68", "intents": ["@EXT@"], "paper_id": "ABC_4d8ae52583d41b4124800c419963df_29", "text": "In order to alleviate this problem, we follow Zwarts and Johnson (2011) by training LMs on different corpora, but we apply state-ofthe-art recurrent neural network (RNN) language models."}
{"sent_id": "4d8ae52583d41b4124800c419963df-C001-85", "intents": ["@EXT@"], "paper_id": "ABC_4d8ae52583d41b4124800c419963df_29", "text": "We use the feature set introduced by Zwarts and Johnson (2011) , but instead of n-gram scores, we apply the LSTM language model probabilities."}
{"sent_id": "4d8ae52583d41b4124800c419963df-C001-88", "intents": ["@USE@"], "paper_id": "ABC_4d8ae52583d41b4124800c419963df_29", "text": "Our reranker optimizes the expected f-score approximation described in Zwarts and Johnson (2011) with L2 regularisation."}
{"sent_id": "4d8ae52583d41b4124800c419963df-C001-100", "intents": ["@USE@", "@UNSURE@"], "paper_id": "ABC_4d8ae52583d41b4124800c419963df_29", "text": "This is because the fluent sentence itself is part of the language model (Zwarts and Johnson, 2011) ."}
{"sent_id": "685b0b0da37b81765bb78f0f87505b-C001-26", "intents": ["@BACK@"], "paper_id": "ABC_685b0b0da37b81765bb78f0f87505b_29", "text": "Several advances have been made in this direction using machine translation inspired encoder-aligner-decoder models, convolution-based encoders, switching pointer and copy mechanisms, and hierarchical attention models (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017) ."}
{"sent_id": "09f627b9a70966dc7b63316c56a2a0-C001-3", "intents": ["@DIF@"], "paper_id": "ABC_09f627b9a70966dc7b63316c56a2a0_29", "text": "Our NCE-trained language models achieve significantly lower perplexity on the One Billion Word Benchmark language modeling challenge, and contain one sixth of the parameters in the best single model in Chelba et al. (2013) ."}
{"sent_id": "09f627b9a70966dc7b63316c56a2a0-C001-111", "intents": ["@DIF@"], "paper_id": "ABC_09f627b9a70966dc7b63316c56a2a0_29", "text": "Our perplexity results are shown in Table 1 , where we get significantly lower perplexities than the best single model from Chelba et al. (2013) , while having almost 6 times fewer parameters."}
{"sent_id": "09f627b9a70966dc7b63316c56a2a0-C001-83", "intents": ["@USE@"], "paper_id": "ABC_09f627b9a70966dc7b63316c56a2a0_29", "text": "We conducted two series of experiments to validate the efficiency of our approach and the quality of the models we learned using it: An intrinsic study of language model perplexity using the standard One Billion Word benchmark (Chelba et al., 2013) and an extrinsic end-to-end statistical machine translation task that uses an LSTM as one of several feature functions in re-ranking."}
{"sent_id": "68d41bee7361b6680103c9951a6570-C001-38", "intents": ["@USE@"], "paper_id": "ABC_68d41bee7361b6680103c9951a6570_29", "text": "The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12, 2] ."}
{"sent_id": "68d41bee7361b6680103c9951a6570-C001-42", "intents": ["@USE@"], "paper_id": "ABC_68d41bee7361b6680103c9951a6570_29", "text": "Batch normalization [13] , is employed between each layer, but not between individual timesteps [2] ."}
{"sent_id": "40d370558d873499e493a83f106f17-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_40d370558d873499e493a83f106f17_29", "text": "Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 2002; Ferret, 2010) , identifying and demoting bad neighbors (Ferret, 2013) , or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013) ."}
{"sent_id": "40d370558d873499e493a83f106f17-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_40d370558d873499e493a83f106f17_29", "text": "The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin's (1998) , cosine, Jensen-Shannon divergence, Dice or Jaccard."}
{"sent_id": "40d370558d873499e493a83f106f17-C001-28", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_40d370558d873499e493a83f106f17_29", "text": "Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 1998; Curran and Moens, 2002) ."}
{"sent_id": "40d370558d873499e493a83f106f17-C001-41", "intents": ["@USE@"], "paper_id": "ABC_40d370558d873499e493a83f106f17_29", "text": "The thesauri were constructed using Lin's (1998) method."}
{"sent_id": "3d99ad1ba1696c8ef743f233530601-C001-32", "intents": ["@BACK@"], "paper_id": "ABC_3d99ad1ba1696c8ef743f233530601_29", "text": "2 Related Work 2.1 Supervised Discourse Parsing Soricut and Marcu (2003) use two probabilistic models to perform a sentence-level analysis, one for segmentation and other to identify the relations and build the rhetorical structure."}
{"sent_id": "3d99ad1ba1696c8ef743f233530601-C001-78", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_3d99ad1ba1696c8ef743f233530601_29", "text": "Syntactic information is crucial in SPADE (Soricut and Marcu, 2003) and for Portuguese the parser most similar to that used by Soricut and Marcu is the LX-parser (Stanford parser trained to Portuguese (Silva et al., 2010) )."}
{"sent_id": "3d99ad1ba1696c8ef743f233530601-C001-66", "intents": ["@USE@"], "paper_id": "ABC_3d99ad1ba1696c8ef743f233530601_29", "text": "With the aim of surpassing the limitation of labeled RST in Portuguese to develop a good DP, we employ SSNEL in the task by adapting the work of Soricut and Marcu (2003) and Hernault et al. (2010) ."}
{"sent_id": "84ae490de92cb9d064993be751b3e0-C001-129", "intents": ["@BACK@"], "paper_id": "ABC_84ae490de92cb9d064993be751b3e0_29", "text": "The ARPA WSJ corpus [19] was designed to provide general-purpose speech data with large vocabularies."}
{"sent_id": "84ae490de92cb9d064993be751b3e0-C001-132", "intents": ["@USE@"], "paper_id": "ABC_84ae490de92cb9d064993be751b3e0_29", "text": "The 20k open test is also referred to as a 64k test since all of the words in these sentences occur in the 63,495 most frequent words in the normalized WSJ text material [ 19] ."}
{"sent_id": "84ae490de92cb9d064993be751b3e0-C001-171", "intents": ["@USE@"], "paper_id": "ABC_84ae490de92cb9d064993be751b3e0_29", "text": "For WSJ, paragraphs were selected ensuring not more than one word was out of the 5.6k most frequent words [ 19] , and these additional words were then included as part of the vocabulary."}
{"sent_id": "1deb67be8226867fe6b9514cdecdec-C001-51", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_1deb67be8226867fe6b9514cdecdec_29", "text": "(Henderson, 2003) exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step."}
{"sent_id": "1deb67be8226867fe6b9514cdecdec-C001-93", "intents": ["@BACK@"], "paper_id": "ABC_1deb67be8226867fe6b9514cdecdec_29", "text": "According to the original SSN model in (Henderson, 2003) , only the information carried over by the leftmost child and the most recent child of a constituent directly flows to that constituent."}
{"sent_id": "b335178d833e26190b7056469d3fa7-C001-185", "intents": ["@BACK@"], "paper_id": "ABC_b335178d833e26190b7056469d3fa7_30", "text": "Regarding massively multilingual models, Neubig and Hu (2018) explored methods for rapid adaptation of NMT to new languages by training multilingual models on the 59-language TED Talks corpus and fine-tuning them using data from the new languages."}
{"sent_id": "b335178d833e26190b7056469d3fa7-C001-52", "intents": ["@USE@"], "paper_id": "ABC_b335178d833e26190b7056469d3fa7_30", "text": "We also compare our massively multilingual models to bilingual baselines and to two recently published results on this dataset (Neubig and Hu (2018) ; Wang et al. (2019) )."}
{"sent_id": "b335178d833e26190b7056469d3fa7-C001-73", "intents": ["@DIF@"], "paper_id": "ABC_b335178d833e26190b7056469d3fa7_30", "text": "We first note that our many-to-many model outperforms all other models when translating into English, with 1.82 BLEU improvement (when av-eraged across the four language pairs) over the best fine-tuned many-to-one models of Neubig and Hu (2018) and 2.44 BLEU improvement over our many-to-one model when averaged across the four low-resource language pairs (Table 1) ."}
{"sent_id": "b335178d833e26190b7056469d3fa7-C001-81", "intents": ["@DIF@"], "paper_id": "ABC_b335178d833e26190b7056469d3fa7_30", "text": "We also note that our many-to-one model is on average 0.75 BLEU behind the best many-to-one models in Neubig and Hu (2018) ."}
{"sent_id": "35522a080b41f716723d2a619f59c4-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_35522a080b41f716723d2a619f59c4_30", "text": "To reduce this kind of error introduced by the translator, Wan in (Wan, 2009 ) applied a co-training scheme."}
{"sent_id": "35522a080b41f716723d2a619f59c4-C001-98", "intents": ["@USE@"], "paper_id": "ABC_35522a080b41f716723d2a619f59c4_30", "text": "For comparsion, we use the same data set in (Wan, 2009) :"}
{"sent_id": "35522a080b41f716723d2a619f59c4-C001-108", "intents": ["@USE@"], "paper_id": "ABC_35522a080b41f716723d2a619f59c4_30", "text": "Also following the setting in (Wan, 2009) , we only use the Chinese unlabeled data and English training sets for our SCL training procedures."}
{"sent_id": "35522a080b41f716723d2a619f59c4-C001-117", "intents": ["@USE@"], "paper_id": "ABC_35522a080b41f716723d2a619f59c4_30", "text": "We compare our procedure with the co-training scheme reported in (Wan, 2009) :"}
{"sent_id": "fe3e71020dfb32927f5c348a6fdcfc-C001-26", "intents": ["@USE@"], "paper_id": "ABC_fe3e71020dfb32927f5c348a6fdcfc_30", "text": "In the following we evaluate different aspects of an objective function obtained from Wizard-of-Oz (WOZ) data (Rieser and Lemon, 2008b) ."}
{"sent_id": "fe3e71020dfb32927f5c348a6fdcfc-C001-59", "intents": ["@USE@"], "paper_id": "ABC_fe3e71020dfb32927f5c348a6fdcfc_30", "text": "The PARADISE regression model is constructed from 3 different corpora: the SAMMIE WOZ experiment (Rieser et al., 2005) , and the iTalk system used for the user tests (Rieser and Lemon, 2008b) running the supervised baseline policy and the RL-based policy."}
{"sent_id": "fe3e71020dfb32927f5c348a6fdcfc-C001-98", "intents": ["@BACK@"], "paper_id": "ABC_fe3e71020dfb32927f5c348a6fdcfc_30", "text": "In previous work we showed that the RL-based policy significantly outperforms the supervised policy in terms of improved user ratings and dialogue performance measures (Rieser and Lemon, 2008b) ."}
{"sent_id": "56d1812bec8abbdb31a2346d96e5ca-C001-32", "intents": ["@USE@"], "paper_id": "ABC_56d1812bec8abbdb31a2346d96e5ca_30", "text": "Experiments demonstrate the effectiveness of our approach on translation from Japanese and Uyghur to English, with a simple, linguistically motivated method of head finalization (HF; Isozaki et al. (2010b) ) as our reordering method."}
{"sent_id": "56d1812bec8abbdb31a2346d96e5ca-C001-41", "intents": ["@USE@"], "paper_id": "ABC_56d1812bec8abbdb31a2346d96e5ca_30", "text": "In experiments we use Isozaki et al. (2010b) 's method of reordering SVO languages (e.g. English) into the order of SOV languages (e.g. Japanese) by simply (1) applying a syntactic parser to English (Tsuruoka et al., 2004) , (2) identifying the head constituent of each phrase and moving it to the end of the phrase, and (3) inserting special tokens after subjects and objects of predicates to mimic Japanese case markers."}
{"sent_id": "bd3663405d2d68f943acc73720b42d-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_bd3663405d2d68f943acc73720b42d_30", "text": "In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (Devlin and Tait, 1998) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts (Horn et al., 2014; Paetzold and Specia, 2017) , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glavaš andŠtajner, 2015; Kim et al., 2016; Specia, 2016a, 2017) ."}
{"sent_id": "bd3663405d2d68f943acc73720b42d-C001-62", "intents": ["@USE@"], "paper_id": "ABC_bd3663405d2d68f943acc73720b42d_30", "text": "Following previous works that used supervised machine learning for ranking in lexical simplification (Horn et al., 2014; Paetzold and Specia, 2017) , we train the DSSM using the LexMTurk dataset (Horn et al., 2014) , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity (Paetzold and Specia, 2017) ."}
{"sent_id": "80e3aec943c37927050f97459360b4-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_80e3aec943c37927050f97459360b4_30", "text": "In the deep speech [19] [20] and EESEN [21] [22] work, the end-to-end speech recognition system was explored to directly predict characters instead of phonemes, hence removing the need of using lexicons and decision trees which are the building blocks in [17] [18] ."}
{"sent_id": "80e3aec943c37927050f97459360b4-C001-37", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_80e3aec943c37927050f97459360b4_30", "text": "In [17] [23] [24] , only frequent words in the training set are used as the targets and the remaining words are just tagged as the OOV."}
{"sent_id": "80e3aec943c37927050f97459360b4-C001-62", "intents": ["@BACK@"], "paper_id": "ABC_80e3aec943c37927050f97459360b4_30", "text": "The CTC criterion [15] was introduced to map the speech input frames into an output label sequence [16] [17] [18] ."}
{"sent_id": "80e3aec943c37927050f97459360b4-C001-132", "intents": ["@USE@"], "paper_id": "ABC_80e3aec943c37927050f97459360b4_30", "text": "Eight frames of 80-dim log Mel-filter-bank features are stacked together as the input, and the time step shift is three frames as in [17] ."}
{"sent_id": "fb75198b7c9e569932dfd486ba6c0a-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_fb75198b7c9e569932dfd486ba6c0a_30", "text": "Current research on applying WSD to specific domains has been evaluated on three available lexicalsample datasets (Ng and Lee, 1996; Weeber et al., 2001; Koeling et al., 2005) ."}
{"sent_id": "fb75198b7c9e569932dfd486ba6c0a-C001-69", "intents": ["@BACK@"], "paper_id": "ABC_fb75198b7c9e569932dfd486ba6c0a_30", "text": "In (Agirre and Lopez de Lacalle, 2008) , the authors also show that state-of-the-art WSD systems are not able to adapt to the domains in the context of the Koeling et al. (2005) dataset."}
{"sent_id": "fb75198b7c9e569932dfd486ba6c0a-C001-89", "intents": ["@BACK@"], "paper_id": "ABC_fb75198b7c9e569932dfd486ba6c0a_30", "text": "In ) the authors report successful adaptation on the (Koeling et al., 2005 ) dataset on supervised setting."}
{"sent_id": "fb75198b7c9e569932dfd486ba6c0a-C001-105", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_fb75198b7c9e569932dfd486ba6c0a_30", "text": "When a general corpus is used, the most predominant sense in general is obtained, and when a domain-specific corpus is used, the most predominant sense for that corpus is obtained (Koeling et al., 2005) ."}
{"sent_id": "fb75198b7c9e569932dfd486ba6c0a-C001-101", "intents": ["@USE@"], "paper_id": "ABC_fb75198b7c9e569932dfd486ba6c0a_30", "text": "The predominant sense acquisition method was succesfully applied to specific domains in (Koeling et al., 2005) ."}
{"sent_id": "332e252e09d28763deb1ded2171c90-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_332e252e09d28763deb1ded2171c90_30", "text": "This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in Wu (1997) , but the specific bracketing of the parse tree provided."}
{"sent_id": "332e252e09d28763deb1ded2171c90-C001-27", "intents": ["@BACK@"], "paper_id": "ABC_332e252e09d28763deb1ded2171c90_30", "text": "The Inversion Transduction Grammar of Wu (1997) can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context-free grammar productions."}
{"sent_id": "332e252e09d28763deb1ded2171c90-C001-101", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_332e252e09d28763deb1ded2171c90_30", "text": "\"Inversion Transduction Grammar\" (ITG) is the model of Wu (1997) , \"Tree-to-String\" is the model of Yamada and Knight (2001) , and \"Tree-to-String, Clone\" allows the node cloning operation described above."}
{"sent_id": "332e252e09d28763deb1ded2171c90-C001-34", "intents": ["@USE@"], "paper_id": "ABC_332e252e09d28763deb1ded2171c90_30", "text": "In our experiments we use a grammar with a start symbol S, a single preterminal C, and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment (ignoring insertions and deletions) (Wu, 1997; Zens and Ney, 2003) ."}
{"sent_id": "4a7fecf3b80c274739e9c83be9a36b-C001-74", "intents": ["@USE@"], "paper_id": "ABC_4a7fecf3b80c274739e9c83be9a36b_30", "text": "We perform experiments on the ISNotes corpus (Markert et al., 2012) , which contains 10,980 mentions annotated for information status in 50 news texts."}
{"sent_id": "4a7fecf3b80c274739e9c83be9a36b-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_4a7fecf3b80c274739e9c83be9a36b_30", "text": "For instance, according to Markert et al. (2012) , old mentions 1 refer to entities that have been referred to previously; mediated men-tions have not been mentioned before but are accessible to the hearer by reference to another old mention or to prior world knowledge; and new mentions refer to entities that are introduced to the discourse for the first time and are not known to the hearer before."}
{"sent_id": "4a7fecf3b80c274739e9c83be9a36b-C001-47", "intents": ["@BACK@"], "paper_id": "ABC_4a7fecf3b80c274739e9c83be9a36b_30", "text": "The IS scheme proposed by Markert et al. (2012) adopts three major IS categories (old, new and mediated) from Nissim et al. (2004) and distinguishes six subcategories for mediated."}
{"sent_id": "89b2b492b4319636ff2f28a4ba0d95-C001-93", "intents": ["@USE@"], "paper_id": "ABC_89b2b492b4319636ff2f28a4ba0d95_30", "text": "We characterize the errors of ZPar and add it to the error comparison between MaltParser and MSTParser (McDonald and Nivre, 2007) ."}
{"sent_id": "89b2b492b4319636ff2f28a4ba0d95-C001-95", "intents": ["@USE@"], "paper_id": "ABC_89b2b492b4319636ff2f28a4ba0d95_30", "text": "For each parser, we conjoin the outputs for all 13 languages in the same way as McDonald and Nivre (2007) , and calculate error distributions over the aggregated output."}
{"sent_id": "370da04cbb2a6ab807428f7e058110-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_370da04cbb2a6ab807428f7e058110_30", "text": "The overall structure and dynamics of networks representing texts have been modeled to describe their mechanism of growth and attachment [21, 22] , while nuances in the topology of real networks were exploited in practical problems, including natural language processing [23, 24, 25] ."}
{"sent_id": "370da04cbb2a6ab807428f7e058110-C001-56", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_370da04cbb2a6ab807428f7e058110_30", "text": "Moreover, this measurement has been proven useful for analyzing text styles [25] ."}
{"sent_id": "370da04cbb2a6ab807428f7e058110-C001-142", "intents": ["@USE@"], "paper_id": "ABC_370da04cbb2a6ab807428f7e058110_30", "text": "The first moments µ 1 represent the static metrics previously studied (see e.g. [25, 51] ) and define a subset of 12 attributes."}
{"sent_id": "460a83a07ca3aa4d56deabad4f9831-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_460a83a07ca3aa4d56deabad4f9831_30", "text": "A more direct dataset was recently released (Huang et al., 2016) , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk."}
{"sent_id": "460a83a07ca3aa4d56deabad4f9831-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_460a83a07ca3aa4d56deabad4f9831_30", "text": "Previous works include storyline graph modeling Xing, 2014), unsupervised mining (Sigurdsson et al., 2016) , blog-photo alignment , and language retelling (Huang et al., 2016; Park and Kim, 2015) ."}
{"sent_id": "460a83a07ca3aa4d56deabad4f9831-C001-53", "intents": ["@USE@"], "paper_id": "ABC_460a83a07ca3aa4d56deabad4f9831_30", "text": "Following (Huang et al., 2016) , we choose a Gated Recurrent Unit (GRU) as the RNN unit to encode the photo sequence."}
{"sent_id": "460a83a07ca3aa4d56deabad4f9831-C001-82", "intents": ["@USE@"], "paper_id": "ABC_460a83a07ca3aa4d56deabad4f9831_30", "text": "We use the Visual Storytelling Dataset (Huang et al., 2016) , consisting of 10,000 albums with 200,000 photos."}
{"sent_id": "616e8732490f0fa87d35998f769196-C001-68", "intents": ["@BACK@"], "paper_id": "ABC_616e8732490f0fa87d35998f769196_30", "text": "There are a few applications, including flexible composition and scrambling in free-word order languages that benefit from TAG-based grammars that drop the simultaneity requirement (Chiang and Scheffler, 2008; Rambow, 1994) ."}
{"sent_id": "616e8732490f0fa87d35998f769196-C001-109", "intents": ["@USE@"], "paper_id": "ABC_616e8732490f0fa87d35998f769196_30", "text": "Borrowing directly from Chiang and Scheffler (2008) , Figure 7 gives two examples."}
{"sent_id": "616e8732490f0fa87d35998f769196-C001-110", "intents": ["@DIF@"], "paper_id": "ABC_616e8732490f0fa87d35998f769196_30", "text": "Parsing for delayed TL-MCTAG is not discussed by Chiang and Scheffler (2008) but can be accomplished using a similar CKY-style strategy as in the two parsers above."}
{"sent_id": "dcfd8cb0179ab156a6ffcab3358a45-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_dcfd8cb0179ab156a6ffcab3358a45_30", "text": "AI has moved on from naming the entities in the image (Mei et al. 2008; Wang et al. 2009) , to describing the image with a natural sentence (Vinyals et al. 2015; Xu et al. 2015; Karpathy and Li 2015) and then to answering specific questions about the image with the advent of visual question answering (VQA) task (Antol et al. 2015) ."}
{"sent_id": "dcfd8cb0179ab156a6ffcab3358a45-C001-25", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_dcfd8cb0179ab156a6ffcab3358a45_30", "text": "A number of datasets on visual question answering have been introduced in recent years (Malinowski and Fritz 2014; Ren, Kiros, and Zemel 2015) , among which (Antol et al. 2015) in particular has gained the most attention and helped popularize the task."}
{"sent_id": "b4d7e9b7942698ef0678d3b4a0ad7d-C001-9", "intents": ["@BACK@"], "paper_id": "ABC_b4d7e9b7942698ef0678d3b4a0ad7d_30", "text": "Researchers have proposed systems to detect social power relations between participants of organizational email threads (Bramsen et al., 2011; Gilbert, 2012; , online forums (Danescu-NiculescuMizil et al., 2012; Biran et al., 2012; DanescuNiculescu-Mizil et al., 2013) , chats (Strzalkowski et al., 2012) , and off-line interactions such as presidential debates Nguyen et al., 2013) ."}
{"sent_id": "280affafa32147a63e7eeda8d5f763-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_280affafa32147a63e7eeda8d5f763_30", "text": "Recent approaches to this task have been based on slot-filling (Yang et al., 2011; Elliott and Keller, 2013) , combining web-scale ngrams , syntactic tree substitution (Mitchell et al., 2012) , and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013) ."}
{"sent_id": "280affafa32147a63e7eeda8d5f763-C001-106", "intents": ["@USE@"], "paper_id": "ABC_280affafa32147a63e7eeda8d5f763_30", "text": "There are no fluency judgements available for Flickr8K, but Elliott and Keller (2013) report grammaticality judgements for their data, which are comparable to fluency ratings."}
{"sent_id": "3e86788379f2c0074ff16687d68fc9-C001-24", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_3e86788379f2c0074ff16687d68fc9_30", "text": "Chunking in Korean texts with only simple heuristic rules obtained through observation on the text shows a good performance similar to other machine learning methods [6] ."}
{"sent_id": "3e86788379f2c0074ff16687d68fc9-C001-73", "intents": ["@BACK@"], "paper_id": "ABC_3e86788379f2c0074ff16687d68fc9_30", "text": "For example, when the part-of-speech of current word is one of determiner, pronoun and noun, the following seven rules for NP chunking in Table 1 can find most NP chunks in text, with about 89% accuracy [6] ."}
{"sent_id": "3e86788379f2c0074ff16687d68fc9-C001-83", "intents": ["@BACK@"], "paper_id": "ABC_3e86788379f2c0074ff16687d68fc9_30", "text": "In Korean, there are four basic phrases: noun phrase (NP), verb phrase (VP), adverb phrase (ADVP), and independent phrase (IP) [6] ."}
{"sent_id": "3e86788379f2c0074ff16687d68fc9-C001-118", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_3e86788379f2c0074ff16687d68fc9_30", "text": "Park et al. reported the performance of various chunking methods [6] ."}
{"sent_id": "3e86788379f2c0074ff16687d68fc9-C001-25", "intents": ["@USE@"], "paper_id": "ABC_3e86788379f2c0074ff16687d68fc9_30", "text": "Park et al. proposed a hybrid of rule-based and machine learning method to handle exceptional cases of the rules, to improve the performance of chunking in Korean texts [5, 6] ."}
{"sent_id": "a800862f17f7a8c13ed13fc6e9433f-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_a800862f17f7a8c13ed13fc6e9433f_30", "text": "Nevertheless, the best performing systems follow the traditional hybrid approach [3] , outperforming attention based encoder-decoder models [4, 5, 6, 7] , and when less training data is used, the gap between \"end-to-end\" and hybrid models is more prominent [4, 8] ."}
{"sent_id": "a800862f17f7a8c13ed13fc6e9433f-C001-153", "intents": ["@USE@"], "paper_id": "ABC_a800862f17f7a8c13ed13fc6e9433f_30", "text": "For comparison with results in the literature we refer to the Switchboard-300 results in [4, 8, 52, 53] and the Switchboard-2000 results in [51, 52, 54, 55, 56, 57] ."}
{"sent_id": "a800862f17f7a8c13ed13fc6e9433f-C001-154", "intents": ["@DIF@"], "paper_id": "ABC_a800862f17f7a8c13ed13fc6e9433f_30", "text": "Our 300hour model not only outperforms the previous best attention based encoder-decoder model [4] by a large margin, it also surpasses the best hybrid systems with multiple LMs [8] ."}
{"sent_id": "472b7c9f53b3130d7e5bba772e5b88-C001-19", "intents": ["@BACK@"], "paper_id": "ABC_472b7c9f53b3130d7e5bba772e5b88_30", "text": "Unsupervised speech representation learning [2, 3, 4, 5, 6, 7, 8, 9, 10] is effective in extracting high-level properties from speech."}
{"sent_id": "472b7c9f53b3130d7e5bba772e5b88-C001-33", "intents": ["@DIF@"], "paper_id": "ABC_472b7c9f53b3130d7e5bba772e5b88_30", "text": "Moreover, as previous approaches restrict the power of the pre-trained models to representation extraction only [5, 6, 7, 8] , the proposed method is robust and can be fine-tuned easily on downstream tasks."}
{"sent_id": "472b7c9f53b3130d7e5bba772e5b88-C001-94", "intents": ["@USE@"], "paper_id": "ABC_472b7c9f53b3130d7e5bba772e5b88_30", "text": "Following previous works [2, 3, 4, 5, 6, 7, 8] , we evaluate different features and representations on downstream tasks, including: phoneme classification, speaker recognition, and sentiment classification on spoken content."}
{"sent_id": "4590b1a4a0566915a6f2d6439a4e8a-C001-11", "intents": ["@USE@"], "paper_id": "ABC_4590b1a4a0566915a6f2d6439a4e8a_30", "text": "Our interest here is the ability of capturing hierarchical structure without being equipped with explicit structural representations (Bowman et al., 2015b; Tran et al., 2016; Linzen et al., 2016) ."}
{"sent_id": "4590b1a4a0566915a6f2d6439a4e8a-C001-78", "intents": ["@USE@"], "paper_id": "ABC_4590b1a4a0566915a6f2d6439a4e8a_30", "text": "We follow the general architecture proposed in (Bowman et al., 2015b) : Premise and hypothesis sentences are encoded by fixed-size vectors."}
{"sent_id": "abfc6373c577980154dbb93190d69b-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_abfc6373c577980154dbb93190d69b_31", "text": "In particular, there has been growing interest in both multilingual POS induction ) and cross-lingual POS induction via treebank projection (Yarowsky and Ngai, 2001; Xi and Hwa, 2005; Das and Petrov, 2011) ."}
{"sent_id": "abfc6373c577980154dbb93190d69b-C001-93", "intents": ["@USE@"], "paper_id": "ABC_abfc6373c577980154dbb93190d69b_31", "text": "In our experiments, we did not make use of refined categories, as the POS tags induced by Das and Petrov (2011) were all coarse."}
{"sent_id": "1265a336e56a4535f0a904ca89b220-C001-40", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_1265a336e56a4535f0a904ca89b220_31", "text": "We nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples (Hellrich and Hahn, 2016a) ."}
{"sent_id": "1265a336e56a4535f0a904ca89b220-C001-45", "intents": ["@BACK@", "@USE@", "@UNSURE@"], "paper_id": "ABC_1265a336e56a4535f0a904ca89b220_31", "text": "The Google Books Ngram corpus (GBN; Michel et al. (2011 ), Lin et al. (2012 ) is used in most of the studies we already mentioned, including our current study and its predecessor (Hellrich and Hahn, 2016a) ."}
{"sent_id": "48def208400142f043a07be5d83713-C001-30", "intents": ["@BACK@", "@MOT@", "@DIF@"], "paper_id": "ABC_48def208400142f043a07be5d83713_31", "text": "We improve our RTM models (Biçici and Way, 2014 ):"}
{"sent_id": "48def208400142f043a07be5d83713-C001-77", "intents": ["@USE@"], "paper_id": "ABC_48def208400142f043a07be5d83713_31", "text": "We develop individual RTM models for each subtask and use GLMd model (Biçici, 2013; Biçici and Way, 2014) , for predicting the quality at the word-level."}
{"sent_id": "365171603fb13c6534ef4abab092d6-C001-22", "intents": ["@BACK@"], "paper_id": "ABC_365171603fb13c6534ef4abab092d6_31", "text": "More recent methods have focused on representation learning by using sparse coding (Cha et al., 2015) or neural networks (Liu and Inkpen, 2015) , utilising both text and network information (Rahimi et al., 2015a) ."}
{"sent_id": "365171603fb13c6534ef4abab092d6-C001-78", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_365171603fb13c6534ef4abab092d6_31", "text": "We also incorporated the MLP predictions into a network-based model based on the method of Rahimi et al. (2015a) , and improved upon their work."}
{"sent_id": "a7e49bec53a2bfd7795b9c770f5d0c-C001-17", "intents": ["@EXT@"], "paper_id": "ABC_a7e49bec53a2bfd7795b9c770f5d0c_31", "text": "Moreover, we provide two ways to train existing algorithms (Mikolov et al., 2013a; Mikolov et al., 2013b ) when the memory is limited during training and show that, here, too, an order of magnitude saving in memory is possible without degrading performance."}
{"sent_id": "a7e49bec53a2bfd7795b9c770f5d0c-C001-67", "intents": ["@USE@"], "paper_id": "ABC_a7e49bec53a2bfd7795b9c770f5d0c_31", "text": "We train the word embedding algorithms, word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b) , based on the Oct. 2013 Wikipedia dump."}
{"sent_id": "021e5dbe22bf0f4ebda4d37040d0a6-C001-19", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_021e5dbe22bf0f4ebda4d37040d0a6_31", "text": "In the cross-lingual study of McDonald et al. (2011) , where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source."}
{"sent_id": "021e5dbe22bf0f4ebda4d37040d0a6-C001-28", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_021e5dbe22bf0f4ebda4d37040d0a6_31", "text": "First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011) ."}
{"sent_id": "021e5dbe22bf0f4ebda4d37040d0a6-C001-102", "intents": ["@BACK@"], "paper_id": "ABC_021e5dbe22bf0f4ebda4d37040d0a6_31", "text": "With respect to evaluation, it is interesting to compare the absolute numbers to those reported in McDonald et al. (2011)"}
{"sent_id": "062e9348de5fda68e61fff3ca4f186-C001-84", "intents": ["@BACK@", "@MOT@", "@USE@"], "paper_id": "ABC_062e9348de5fda68e61fff3ca4f186_31", "text": "In experiments, Barzilay and Lapata (2008) assume that articles taken from Encyclopedia Britannica are more difficult to read (less coherent) than the corresponding articles from Encyclopedia Britannica Elementary, its version for children."}
{"sent_id": "062e9348de5fda68e61fff3ca4f186-C001-74", "intents": ["@USE@"], "paper_id": "ABC_062e9348de5fda68e61fff3ca4f186_31", "text": "We follow Barzilay and Lapata (2008) for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003)."}
{"sent_id": "062e9348de5fda68e61fff3ca4f186-C001-75", "intents": ["@USE@"], "paper_id": "ABC_062e9348de5fda68e61fff3ca4f186_31", "text": "Human coherence scores are associated with each pair of summarized documents (Barzilay and Lapata, 2008) ."}
{"sent_id": "42ca932eaa96c174cdfb815bee82cb-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_42ca932eaa96c174cdfb815bee82cb_31", "text": "In the context of an initial verb followed by two arguments (e.g., subject and object), the principle predicts that the heads of the verbal arguments are placed first with respect to their dependents, e.g., articles or adjectives follow the head noun, whereas, for a final verb, the prediction is that the heads of the arguments are placed last with respect to their dependents, e.g. articles or adjectives precede the head noun (Ferrer-i-Cancho, 2015) ."}
{"sent_id": "42ca932eaa96c174cdfb815bee82cb-C001-12", "intents": ["@BACK@"], "paper_id": "ABC_42ca932eaa96c174cdfb815bee82cb_31", "text": "1 If there is one head, the principle predicts a rather symmetric head placement but various heads can lead to an anti-symmetric placement (Ferrer-i-Cancho, 2015 , 2008 ."}
{"sent_id": "42ca932eaa96c174cdfb815bee82cb-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_42ca932eaa96c174cdfb815bee82cb_31", "text": "2 The monotonic dependency between cognitive cost and distance Ferrer-i-Cancho (2015) assumes that the cognitive cost of a dependency is a strictly monotonic function of its length."}
{"sent_id": "42ca932eaa96c174cdfb815bee82cb-C001-60", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_42ca932eaa96c174cdfb815bee82cb_31", "text": "3 The unit of measurement of dependency length A limitation of Ferrer-i-Cancho (2015) is that dependency length is measured in words."}
{"sent_id": "c608567abe72c75bbbc8eb917ab5d3-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_c608567abe72c75bbbc8eb917ab5d3_31", "text": "Neural network methods have been exploited to overcome this feature design issue and usually involve RNNs and CNNs (Miwa and Bansal, 2016; Zheng et al., 2017) ."}
{"sent_id": "c608567abe72c75bbbc8eb917ab5d3-C001-102", "intents": ["@DIF@"], "paper_id": "ABC_c608567abe72c75bbbc8eb917ab5d3_31", "text": "Compared to Miwa and Bansal (2016) , who rely on NLP tools, the baseline performs within a reasonable margin (less than 1%) on the joint task."}
{"sent_id": "307c18e2928c4a45f574a9c3a36b76-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_307c18e2928c4a45f574a9c3a36b76_31", "text": "While early work (Bramsen et al., 2011; Gilbert, 2012) focused on surface level lexical features aggregated at corpus level, more recent work has looked into the thread structure of emails as well (Prabhakaran and Rambow, 2014) ."}
{"sent_id": "307c18e2928c4a45f574a9c3a36b76-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_307c18e2928c4a45f574a9c3a36b76_31", "text": "However, both (Bramsen et al., 2011; Gilbert, 2012) and (Prabhakaran and Rambow, 2014 ) group all messages sent by an individual to another individual (at the corpus-level and at the thread-level, respectively) and rely on word-ngram * Authors (listed in alphabetical order) contributed equally."}
{"sent_id": "a869bebe1744e3a7c71cb0f6fed12c-C001-13", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_a869bebe1744e3a7c71cb0f6fed12c_31", "text": "However, annotating for more complex representations of affective states-such as Basic Emotions (Ekman, 1992) or ValenceArousal-Dominance (Bradley and Lang, 1994 )-seems to be significantly harder in terms of both time consumption and inter-annotator agreement (IAA) (Strapparava and Mihalcea, 2007) ."}
{"sent_id": "a869bebe1744e3a7c71cb0f6fed12c-C001-82", "intents": ["@USE@"], "paper_id": "ABC_a869bebe1744e3a7c71cb0f6fed12c_31", "text": "Table 5 provides the performance of the winning system of the original shared task (WIN-NER; Chaumartin (2007) ), the IAA as reported by the organizers (Strapparava and Mihalcea, 2007) , the performance by Beck (2017) , the highest one reported for this data set so far (BECK), as well as the results of our GRU from the 10×10-CV."}
{"sent_id": "a869bebe1744e3a7c71cb0f6fed12c-C001-99", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_a869bebe1744e3a7c71cb0f6fed12c_31", "text": "Our proposed GRU model even established a novel state-of-the-art result on the SemEval 2007 test set (Strapparava and Mihalcea, 2007) outperforming human reliability."}
{"sent_id": "43422cf92d8cbb280c0b4c590632f1-C001-33", "intents": ["@BACK@", "@FUT@"], "paper_id": "ABC_43422cf92d8cbb280c0b4c590632f1_31", "text": "In order to mitigate such an issue, [15] proposed a method that learns multi-codebook."}
{"sent_id": "43422cf92d8cbb280c0b4c590632f1-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_43422cf92d8cbb280c0b4c590632f1_31", "text": "Presenting a different use for the previous work done by the field of codebooks compression based source coding, known as product quantization (PQ) [21] and additive quantization [22] , they show that by minimizing the squared distance between both distributions (baseline and composed embeddings), and using a direct learning approach for the codes in an end-to-end neural network, with a Gumbel-softmax layer [23] to encourage the discreteness [15] , it is possible to construct the word embeddings radically reducing the number of parameters without hurting performance."}
{"sent_id": "43422cf92d8cbb280c0b4c590632f1-C001-53", "intents": ["@USE@"], "paper_id": "ABC_43422cf92d8cbb280c0b4c590632f1_31", "text": "For the sake of simplicity, we adopted the same architecture as [15] , as shown in Fig. 2 ."}
{"sent_id": "43422cf92d8cbb280c0b4c590632f1-C001-80", "intents": ["@EXT@"], "paper_id": "ABC_43422cf92d8cbb280c0b4c590632f1_31", "text": "Our framework is an extension of [15] work to deal with latent representations of contextual pretrained LM."}
{"sent_id": "61f88b86c451fb6a5e5893c8c42a24-C001-21", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_61f88b86c451fb6a5e5893c8c42a24_31", "text": "This observation can be exploited for the tasks of wish detection (Ramanand et al., 2010) , and suggestion extraction (Brun, 2013) ."}
{"sent_id": "ee66681690f2c92fe705a09bf7015d-C001-55", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_ee66681690f2c92fe705a09bf7015d_31", "text": "Given a source sequence f computing the best scoring target sequence e = arg max e ′ s(e ′ , f ) among all possible sequences E * requires a beam search procedure (Freitag and Khadivi, 2007) ."}
{"sent_id": "ee66681690f2c92fe705a09bf7015d-C001-77", "intents": ["@USE@"], "paper_id": "ABC_ee66681690f2c92fe705a09bf7015d_31", "text": "We use the same training/development/testing (8084/1000/1000) set as the one used in a previous benchmark study (Freitag and Khadivi, 2007) ."}
{"sent_id": "5b98a80237182b2d506ea4c9d71aa1-C001-21", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5b98a80237182b2d506ea4c9d71aa1_31", "text": "In order to establish the long dependencies easily and overcome the disadvantage of the approximate inference, Krishnan and Manning (2006) propose a two-stage approach using Conditional Random Fields (CRFs) with extract inference."}
{"sent_id": "5b98a80237182b2d506ea4c9d71aa1-C001-86", "intents": ["@BACK@"], "paper_id": "ABC_5b98a80237182b2d506ea4c9d71aa1_31", "text": "These non-local features are applied in English NER in one-step approach (Krishnan and Manning, 2006; Wong and Ng, 2007) , they employ these features to improve entity consistence among their different occurrences."}
{"sent_id": "5b98a80237182b2d506ea4c9d71aa1-C001-26", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_5b98a80237182b2d506ea4c9d71aa1_31", "text": "Similar to Krishnan and Manning (2006) , we also encode non-local information with features and apply the simple two-stage architecture."}
{"sent_id": "5b98a80237182b2d506ea4c9d71aa1-C001-96", "intents": ["@UNSURE@"], "paper_id": "ABC_5b98a80237182b2d506ea4c9d71aa1_31", "text": "Different from (Krishnan and Manning, 2006; Wong and Ng, 2007) , they only assign the majority type information, like Maj-Loc, to each token in matched candidates, boundary information like B, I and E is ignored, it is acceptable because they utilize these features only for English corpora, and the boundary information can be captured by the capitalization characteristics."}
{"sent_id": "4a63ef4085639a66d1c7f6344f7548-C001-35", "intents": ["@BACK@"], "paper_id": "ABC_4a63ef4085639a66d1c7f6344f7548_31", "text": "Liu et al. (2007) differentiated the rules in their tree-to-string model which integrated with forest 1 -to-string into fully lexicalized rules, non-lexicalized rules and partial lexicalized rules according to the lexicalization levels."}
{"sent_id": "45d804ec30d20bd7e484c3bbd8399f-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_45d804ec30d20bd7e484c3bbd8399f_31", "text": "Additionally, researchers have investigated the syntactic knowledge that BERT learns by analyzing the contextualized embeddings (Warstadt et al., 2019a) and attention heads of BERT (Clark et al., 2019) ."}
{"sent_id": "4c8e83eb213879e68285e9cd09be47-C001-20", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_4c8e83eb213879e68285e9cd09be47_31", "text": "The soft attention model performs surprisingly poorly on seen words, so that its overall performance is worse than the naïve baseline and several earlier models (Pettersson et al., 2014) ."}
{"sent_id": "4c8e83eb213879e68285e9cd09be47-C001-86", "intents": ["@USE@"], "paper_id": "ABC_4c8e83eb213879e68285e9cd09be47_31", "text": "Upper half: results on (A)ll tokens reported by Pettersson et al. (2014) for a hybrid model (apply memorization baseline to seen tokens and an edit-distance-based model to unseen tokens) and two SMT models (which align character unigrams and bigrams, respectively)."}
{"sent_id": "6bf17a793eaee0593596df0c2249b5-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_6bf17a793eaee0593596df0c2249b5_32", "text": "Multi-hop QA requires finding multiple supporting evidence, and reasoning over them in order to answer a question (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018) ."}
{"sent_id": "6bf17a793eaee0593596df0c2249b5-C001-33", "intents": ["@USE@"], "paper_id": "ABC_6bf17a793eaee0593596df0c2249b5_32", "text": "When the retrieved paragraphs are supplied to the baseline QA model introduced in Yang et al. (2018) , it improved the QA performance on the hidden test set by 10.59 F1 points."}
{"sent_id": "6bf17a793eaee0593596df0c2249b5-C001-102", "intents": ["@USE@"], "paper_id": "ABC_6bf17a793eaee0593596df0c2249b5_32", "text": "Baseline Reader (Yang et al., 2018) Table 2 shows the performance on the QA task."}
{"sent_id": "6bf17a793eaee0593596df0c2249b5-C001-98", "intents": ["@UNSURE@"], "paper_id": "ABC_6bf17a793eaee0593596df0c2249b5_32", "text": "This evaluation is a bit tricky to do in HOTPOTQA, since the evaluataion set only contains questions from 'hard' subset (Yang et al., 2018) ."}
{"sent_id": "8905d5936a5b2a839bfd56783ff55d-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_8905d5936a5b2a839bfd56783ff55d_32", "text": "Consider for example a training instance for the restaurant domain consisting of a meaning representation (MR) from the End-to-End (E2E) Generation Challenge 1 and a sample output from one of our neural generation models in Figure 1 [4, 5] ."}
{"sent_id": "8905d5936a5b2a839bfd56783ff55d-C001-45", "intents": ["@USE@"], "paper_id": "ABC_8905d5936a5b2a839bfd56783ff55d_32", "text": "Our NNLG model uses a single token to represent personality encoding, following the use of single language labels used in machine translation and other work on neural generation [10, 5] ."}
{"sent_id": "8905d5936a5b2a839bfd56783ff55d-C001-54", "intents": ["@DIF@"], "paper_id": "ABC_8905d5936a5b2a839bfd56783ff55d_32", "text": "Our model differs from the TO-KEN model used in our previous work [5] because it is trained on unsorted inputs to allow us to add multiple CONVERT tags to the MR at generation time."}
{"sent_id": "10acbeba830b2f8b3feb30de542c56-C001-2", "intents": ["@BACK@"], "paper_id": "ABC_10acbeba830b2f8b3feb30de542c56_32", "text": "The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location."}
{"sent_id": "10acbeba830b2f8b3feb30de542c56-C001-7", "intents": ["@USE@"], "paper_id": "ABC_10acbeba830b2f8b3feb30de542c56_32", "text": "We compare our model results to those given in Chen et al. (2019) and show that the panoramas we have added to StreetLearn fully support both Touchdown tasks and can be used effectively for further research and comparison."}
{"sent_id": "10acbeba830b2f8b3feb30de542c56-C001-76", "intents": ["@USE@"], "paper_id": "ABC_10acbeba830b2f8b3feb30de542c56_32", "text": "Following Chen et al. (2019) , we report mean distance error and accuracy with different thresholds (40px, 80px, and 120px), which measures the proportion of evaluation items where the pixel chosen by the model is within the specified pixel distance."}
{"sent_id": "55e429045af4434f9cb27ae8c6db66-C001-11", "intents": ["@BACK@"], "paper_id": "ABC_55e429045af4434f9cb27ae8c6db66_32", "text": "Traditional approaches treat AES as a classification (Larkey, 1998; Rudner and Liang, 2002) , regression (Attali and Burstein, 2004; Phandi et al., 2015) , or ranking classification problem (Yannakoudakis et al., 2011; Chen and He, 2013) , addressing AES by supervised learning."}
{"sent_id": "55e429045af4434f9cb27ae8c6db66-C001-54", "intents": ["@USE@"], "paper_id": "ABC_55e429045af4434f9cb27ae8c6db66_32", "text": "ML-ρ (Phandi et al., 2015) was proposed to address this issue."}
{"sent_id": "06276db79ed5aa04bb24a31c10d3a9-C001-16", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_06276db79ed5aa04bb24a31c10d3a9_32", "text": "Various methods have been proposed to automatically parse AMRs, ranging from syntax-based approaches (e.g. Flanigan et al. (2014) ; Wang et al. (2015) ; Pust et al. (2015) ; Damonte et al. (2017) ) to the more recent neural approaches (Peng et al. (2017) ; Buys and Blunsom (2017) ; Konstas et al. (2017) ; Foland and Martin (2017); van Noord and Bos (2017) )."}
{"sent_id": "06276db79ed5aa04bb24a31c10d3a9-C001-24", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_06276db79ed5aa04bb24a31c10d3a9_32", "text": "These methods are applied on the model that reported the best results in the literature, the character-level neural semantic parsing method of van Noord and Bos (2017) ."}
{"sent_id": "06276db79ed5aa04bb24a31c10d3a9-C001-55", "intents": ["@USE@"], "paper_id": "ABC_06276db79ed5aa04bb24a31c10d3a9_32", "text": "This approach was applied by van Noord and Bos (2017) and Foland and Martin (2017) ."}
{"sent_id": "06276db79ed5aa04bb24a31c10d3a9-C001-101", "intents": ["@USE@"], "paper_id": "ABC_06276db79ed5aa04bb24a31c10d3a9_32", "text": "We test the impact of the different methods on two of our earlier models, described in van Noord and Bos (2017) ."}
{"sent_id": "672d4299e60752e866293d72f97905-C001-14", "intents": ["@BACK@"], "paper_id": "ABC_672d4299e60752e866293d72f97905_32", "text": "Psycholinguistic properties have been used in various approaches, such as for Lexical Simplification [12] , for Text Simplification at the sentence level, with the aim of reducing the difficulty of informative text for language learners [18] , to predict the reading times (RTs) of each word in a sentence to assess sentence complexity [14] and also to create robust text level readability models [17] , which is also one of the purposes of this paper."}
{"sent_id": "672d4299e60752e866293d72f97905-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_672d4299e60752e866293d72f97905_32", "text": "[12] automatically estimate missing psycholinguistic properties in the MRC Database through a bootstrapping algorithm for regression."}
{"sent_id": "672d4299e60752e866293d72f97905-C001-36", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_672d4299e60752e866293d72f97905_32", "text": "The fact that the methods developed by [4] and [12] are based on a large, scarce lexical resources as WordNet, led us to raise the question \"Could we have a similar performance with a simpler set of features which are easily obtainable for most languages?\"."}
{"sent_id": "672d4299e60752e866293d72f97905-C001-38", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_672d4299e60752e866293d72f97905_32", "text": "One critical difference between the strategy of [12] and ours is that they concatenate all features to train a regressor, while we take a different approach."}
{"sent_id": "6da7dcbcb7f52f31ec23c8131d438d-C001-18", "intents": ["@BACK@"], "paper_id": "ABC_6da7dcbcb7f52f31ec23c8131d438d_32", "text": "This is facilitated by recent advances in learning joint multilingual representations (Lample and Conneau, 2019; Artetxe and Schwenk, 2018; Devlin et al., 2018) ."}
{"sent_id": "6da7dcbcb7f52f31ec23c8131d438d-C001-19", "intents": ["@EXT@"], "paper_id": "ABC_6da7dcbcb7f52f31ec23c8131d438d_32", "text": "In our work, we propose a self-learning framework to incorporate the predictions of the multilingual BERT model (Devlin et al., 2018) on non-English data into an English training procedure."}
{"sent_id": "6da7dcbcb7f52f31ec23c8131d438d-C001-60", "intents": ["@USE@"], "paper_id": "ABC_6da7dcbcb7f52f31ec23c8131d438d_32", "text": "For the encoder, we invoke the multilingual BERT model (Devlin et al., 2018) , which supports 104 languages 1 ."}
{"sent_id": "43d670e583caab9b38ddce999b8872-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_43d670e583caab9b38ddce999b8872_32", "text": "First introduced for evaluating user simulations by Schatzmann et al. (2005) , such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015; Dodge et al., 2015) ."}
{"sent_id": "43d670e583caab9b38ddce999b8872-C001-21", "intents": ["@BACK@"], "paper_id": "ABC_43d670e583caab9b38ddce999b8872_32", "text": "With the huge size of current dialogue datasets that contain millions of utterances (Lowe et al., 2015a; Banchs, 2012; Ritter et al., 2010) and the increasing amount of natural language data, it is conceivable that retrieval-based systems will be able to have engaging conversations with humans."}
{"sent_id": "43d670e583caab9b38ddce999b8872-C001-26", "intents": ["@USE@"], "paper_id": "ABC_43d670e583caab9b38ddce999b8872_32", "text": "We performed a user study on three different datasets: the SubTle Corpus of movie dialogues (Banchs, 2012) , the Twitter Corpus (Ritter et al., 2010) , and the Ubuntu Dialogue Corpus (Lowe et al., 2015a) ."}
{"sent_id": "43d670e583caab9b38ddce999b8872-C001-85", "intents": ["@USE@"], "paper_id": "ABC_43d670e583caab9b38ddce999b8872_32", "text": "We also presents results on the same task for a state-of-the-art artificial neural network (ANN) dialogue model (see (Lowe et al., 2015a) for implementation details)."}
{"sent_id": "3ea1f4acd7e2812e68eca54600fc5c-C001-10", "intents": ["@BACK@"], "paper_id": "ABC_3ea1f4acd7e2812e68eca54600fc5c_32", "text": "One promising approach is based on exact search and structural learning (McDonald et al., 2005; McDonald and Pereira, 2006) ."}
{"sent_id": "3ea1f4acd7e2812e68eca54600fc5c-C001-15", "intents": ["@UNSURE@"], "paper_id": "ABC_3ea1f4acd7e2812e68eca54600fc5c_32", "text": "In order to preserve the strength of McDonald et al. (2005) 's approach in terms of unlabelled attachment score, we add feature vectors for generalizations over deprels."}
{"sent_id": "4924d721b50abe1c8d883a7efd0205-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_4924d721b50abe1c8d883a7efd0205_32", "text": "When UKB was released, the papers specified the optimal parameters for WSD Agirre et al., 2014) , as well as other key issues like the underlying knowledge-base version, specific set of relations to be used, and method to pre-process the input text."}
{"sent_id": "4924d721b50abe1c8d883a7efd0205-C001-81", "intents": ["@DIF@"], "paper_id": "ABC_4924d721b50abe1c8d883a7efd0205_32", "text": "In addition to UKB, the table also reports the 5 Note that the UKB results for S2, S3 and S07 (62.6, 63.0 and 48.6 respectively) are different from those in (Agirre et al., 2014) , which is to be expected, as the new datasets have been converted to WordNet 3.0 (we confirmed experimentally that this is the sole difference between the two experiments)."}
{"sent_id": "91685660d3d689c50e7436be46f37e-C001-4", "intents": ["@USE@"], "paper_id": "ABC_91685660d3d689c50e7436be46f37e_32", "text": "Using a multilayer convolutional encoder-decoder neural network GEC approach (Chollampatt and Ng, 2018), we evaluate the contribution of Wikipedia edits and find that carefully selected Wikipedia edits increase performance by over 5%."}
{"sent_id": "91685660d3d689c50e7436be46f37e-C001-37", "intents": ["@USE@"], "paper_id": "ABC_91685660d3d689c50e7436be46f37e_32", "text": "As suggested by Chollampatt and Ng (2018) , we encode the Wikipedia article text using the BPE model and learn fastText embeddings (Bojanowski et al., 2017) with 500 dimensions."}
{"sent_id": "91685660d3d689c50e7436be46f37e-C001-91", "intents": ["@USE@"], "paper_id": "ABC_91685660d3d689c50e7436be46f37e_32", "text": "We evaluate our method using the multilayer convolutional encoder-decoder neural network GEC approach from Chollampatt and Ng (2018) and find that augmenting a small gold German GEC corpus with one million filtered Wikipedia edits improves the performance from 39.22 to 44.47 F 0.5 and additional language model reranking increases performance to 45.22."}
{"sent_id": "91685660d3d689c50e7436be46f37e-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_91685660d3d689c50e7436be46f37e_32", "text": "On the basis of these resources along with advances in machine translation, the current state-of-the-art English GEC systems use ensembles of neural MT models (Chollampatt and Ng, 2018) and hybrid systems with both statistical and neural MT models (Grundkiewicz and Junczys-Dowmunt, 2018) ."}
{"sent_id": "91685660d3d689c50e7436be46f37e-C001-79", "intents": ["@UNSURE@"], "paper_id": "ABC_91685660d3d689c50e7436be46f37e_32", "text": "In contrast to the results for English in Chollampatt and Ng (2018) In order to explore the possibility of developing GEC systems for languages with fewer resources, we trained models solely on Wikipedia edits, which leads to a huge drop in performance (45.22 vs. 24.37 F 0.5 )."}
{"sent_id": "0fd87fbdbe64e7d002ca31783448fb-C001-41", "intents": ["@USE@"], "paper_id": "ABC_0fd87fbdbe64e7d002ca31783448fb_32", "text": "We adopted open-source software jieba 2 for Chinese and Stanford parser [Toutanova and Manning, 2000] for English POS tagging."}
{"sent_id": "81dd7a27479f0cec3a01337c57ca95-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_81dd7a27479f0cec3a01337c57ca95_32", "text": "However, almost all existing approaches require either parallel data, hand-crafted rules, or extra syntactic information such as dependency labels or part-of-speech tag features trees (McDonald, 2006; Filippova and Strube, 2008; Zhao et al., 2018) ."}
{"sent_id": "81dd7a27479f0cec3a01337c57ca95-C001-67", "intents": ["@USE@"], "paper_id": "ABC_81dd7a27479f0cec3a01337c57ca95_32", "text": "Following (Zhao et al., 2018) , we employed Readability and Informativeness as criteria on a five-point Likert scale."}
{"sent_id": "81dd7a27479f0cec3a01337c57ca95-C001-71", "intents": ["@USE@"], "paper_id": "ABC_81dd7a27479f0cec3a01337c57ca95_32", "text": "Since each of the 200 sentences from Giga test set has two references (by Annotator 1 and 2, respectively), we report two F1's following Zhao et al. (2018) ."}
{"sent_id": "1f463f2f87bc2d572299d96481084f-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_1f463f2f87bc2d572299d96481084f_32", "text": "Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004) , to assess for a pair of systems how likely a difference in BLEU scores occurred by chance."}
{"sent_id": "1f463f2f87bc2d572299d96481084f-C001-45", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_1f463f2f87bc2d572299d96481084f_32", "text": "A bootstrap pseudo-sample consists of the translations by the two systems (X b , Y b ) of a bootstrapped test set (Koehn, 2004) , constructed by sampling with replacement from the original test set translations."}
{"sent_id": "b31acd3535cd740e609d45986fbf33-C001-8", "intents": ["@BACK@"], "paper_id": "ABC_b31acd3535cd740e609d45986fbf33_32", "text": "Pre-trained language models (LMs) such as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) , OpenAI GPT (Radford et al., 2018) , Flair (Akbik et al., 2018) and Bert (Devlin et al., 2018) have shown great improvements in NLP tasks ranging from sentiment analysis to named entity recognition to question answering."}
{"sent_id": "b31acd3535cd740e609d45986fbf33-C001-87", "intents": ["@EXT@"], "paper_id": "ABC_b31acd3535cd740e609d45986fbf33_32", "text": "We derive pre-trained character-level contextual embeddings from Flair (Akbik et al., 2018) , a wordlevel embedding model, inject these into a state-ofthe-art time normalization system, and achieve major performance improvements: 51% error reduction in news and 33% in clinical notes."}
{"sent_id": "46cc0df5c6ed25f735cc0afd301ec8-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_46cc0df5c6ed25f735cc0afd301ec8_32", "text": "(1) an extraction-based method (Lei et al., 2016) , which first extracts evidences from the original text and then makes a prediction solely based on the extracted evidences; (2) an attentionbased method (Lin et al., 2017; Mullenbach et al., 2018) , which leverages the self-attention mechaMedical Report: The patient was admitted to the Neurological Intensive Care Unit for close observation."}
{"sent_id": "46cc0df5c6ed25f735cc0afd301ec8-C001-37", "intents": ["@USE@"], "paper_id": "ABC_46cc0df5c6ed25f735cc0afd301ec8_32", "text": "Our work leverages the attention-based selfexplaining method (Lin et al., 2017) , as shown in Figure 1 ."}
{"sent_id": "8a8670fd7cfb8db9ddd3f546ce4534-C001-17", "intents": ["@MOT@", "@BACK@"], "paper_id": "ABC_8a8670fd7cfb8db9ddd3f546ce4534_32", "text": "However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (Véronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b) ."}
{"sent_id": "8a8670fd7cfb8db9ddd3f546ce4534-C001-37", "intents": ["@UNSURE@"], "paper_id": "ABC_8a8670fd7cfb8db9ddd3f546ce4534_32", "text": "We consider three methodologies for gathering sense labels: (1) the methodology of Erk et al. (2009) for gathering weighted labels, (2) a multistage strategy that uses both binary and Likert ratings, and (3) MaxDiff, a paired choice format."}
{"sent_id": "8a8670fd7cfb8db9ddd3f546ce4534-C001-39", "intents": ["@USE@"], "paper_id": "ABC_8a8670fd7cfb8db9ddd3f546ce4534_32", "text": "We adopt the annotation guidelines of Erk et al. (2009) which used a five-point scale, ranging from 1 to 5, indicating the sense does not apply or that it matches the contextual usage exactly, respectively."}
{"sent_id": "8a8670fd7cfb8db9ddd3f546ce4534-C001-66", "intents": ["@USE@"], "paper_id": "ABC_8a8670fd7cfb8db9ddd3f546ce4534_32", "text": "For the reference sense labeling, we use a subset of the GWS dataset of Erk et al. (2009) , where three annotators rated 50 instances each for eight words."}
{"sent_id": "d0c12613f09b36e071b9a842a4d844-C001-5", "intents": ["@BACK@"], "paper_id": "ABC_d0c12613f09b36e071b9a842a4d844_32", "text": "This paper presents some experiments carried out based on two syntactic tree alignment algorithms presented in [Lavie et al. 2008] and [Tinsley et al. 2007 ]."}
{"sent_id": "5c63296c36cbd95e07f05f2563a2a1-C001-17", "intents": ["@BACK@"], "paper_id": "ABC_5c63296c36cbd95e07f05f2563a2a1_32", "text": "Recently, Convolutional Neural Network (CNN) was introduced into many models for extracting sub-word information from a word (Santos and Guimaraes, 2015; Ma and Hovy, 2016) ."}
{"sent_id": "5c63296c36cbd95e07f05f2563a2a1-C001-41", "intents": ["@BACK@"], "paper_id": "ABC_5c63296c36cbd95e07f05f2563a2a1_32", "text": "Above all, BLSTM-CNNs-CRF (Ma and Hovy, 2016 ) achieved state-of-theart performance on the standard English corpus: CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) ."}
{"sent_id": "5c63296c36cbd95e07f05f2563a2a1-C001-93", "intents": ["@UNSURE@"], "paper_id": "ABC_5c63296c36cbd95e07f05f2563a2a1_32", "text": "This enables us to utilize pre-training of word embeddings with the effectiveness shown in English (Ma and Hovy, 2016) ."}
{"sent_id": "0fed8b9e785426880fa8e5641116a4-C001-24", "intents": ["@BACK@"], "paper_id": "ABC_0fed8b9e785426880fa8e5641116a4_33", "text": "Our original AMBER paper (Chen and Kuhn, 2011) describes the ten penalties used at that time; two of these penalties, the normalized Spearman's correlation penalty and the normalized Kendall's correlation penalty, model word reordering."}
{"sent_id": "0fed8b9e785426880fa8e5641116a4-C001-76", "intents": ["@UNSURE@"], "paper_id": "ABC_0fed8b9e785426880fa8e5641116a4_33", "text": "In what follows, \"AMBER1\" will denote a variant of AMBER as described in (Chen and Kuhn, 2011) ."}
{"sent_id": "0fed8b9e785426880fa8e5641116a4-C001-109", "intents": ["@EXT@"], "paper_id": "ABC_0fed8b9e785426880fa8e5641116a4_33", "text": "We have made two changes to AMBER, a metric described in (Chen and Kuhn, 2011) ."}
{"sent_id": "c54a1aba5845a52f468cde916c970b-C001-44", "intents": ["@BACK@"], "paper_id": "ABC_c54a1aba5845a52f468cde916c970b_33", "text": "Other cited work using the DSTC2 dataset (Bordes and Weston, 2016; Liu and Perez, 2016; Seo et al., 2016) implement similar mechanisms whereby they expand the feature representations of candidate system responses based on whether there is lexical entity class matching with provided dialogue context."}
{"sent_id": "c54a1aba5845a52f468cde916c970b-C001-52", "intents": ["@USE@"], "paper_id": "ABC_c54a1aba5845a52f468cde916c970b_33", "text": "While the goal of the original challenge was building a system for inferring dialogue state, for our study, we use the version of the data from Bordes and Weston (2016) , which ignores the dialogue state annotations, using only the raw text of the dialogues."}
{"sent_id": "c54a1aba5845a52f468cde916c970b-C001-71", "intents": ["@USE@"], "paper_id": "ABC_c54a1aba5845a52f468cde916c970b_33", "text": "We employ several metrics for assessing specific aspects of our model, drawn from previous work: • Per-Response Accuracy: Bordes and Weston (2016) report a per-turn response accuracy, which tests their model's ability to select the system response at a certain timestep."}
{"sent_id": "45c4e9bc90d28cd1a5a61393625062-C001-75", "intents": ["@USE@"], "paper_id": "ABC_45c4e9bc90d28cd1a5a61393625062_33", "text": "As mentioned before, we also used the method described in (Li and Gaussier, 2010) on the same data, producing resulting corpora P 1 (with P 1 T ) and P 2 (with P 2 T ) from P 0 ."}
{"sent_id": "45c4e9bc90d28cd1a5a61393625062-C001-100", "intents": ["@USE@"], "paper_id": "ABC_45c4e9bc90d28cd1a5a61393625062_33", "text": "In a first series of experiments, bilingual lexicons were extracted from the corpora obtained by our approach (P 1 and P 2 ), the corpora obtained by the approach described in (Li and Gaussier, 2010 ) (P 1 and P 2 ) and the original corpus P 0 , with the fixed N value set to 20."}
{"sent_id": "d61f75366022f043d4c3a005b5a73d-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_d61f75366022f043d4c3a005b5a73d_33", "text": "The works differ in the way they acquire morphological knowledge (from using linguistically derived morphological analyzers on one end, to approximating morphology using substrings while relying on the concatenative nature of morphology, on the other) and in the model form (cDSMs (Lazaridou et al., 2013) , RNN (Luong et al., 2013) , LBL (Botha and Blunsom, 2014) , CBOW (Qiu et al., 2014) , SkipGram (Soricut and Och, 2015; Bojanowski et al., 2016) , GGM (Cotterell et al., 2016) )."}
{"sent_id": "d61f75366022f043d4c3a005b5a73d-C001-86", "intents": ["@UNSURE@"], "paper_id": "ABC_d61f75366022f043d4c3a005b5a73d_33", "text": "We compare the different models on the different measures, and also compare to the state-of-the-art n-gram based fastText model of Bojanowski et al (2016) that does not require morphological analysis."}
{"sent_id": "b208c7180bc3f973b8616937b2801c-C001-33", "intents": ["@BACK@"], "paper_id": "ABC_b208c7180bc3f973b8616937b2801c_33", "text": "Krause et al. (2016) introduced the first large-scale paragraph captioning dataset, a subset of the Visual Genome dataset, along with a number of models for paragraph captioning."}
{"sent_id": "b208c7180bc3f973b8616937b2801c-C001-38", "intents": ["@BACK@"], "paper_id": "ABC_b208c7180bc3f973b8616937b2801c_33", "text": "The paragraph captioning models proposed by Krause et al. (2016) included template-based (nonneural) approaches and two encoder-decoder models."}
{"sent_id": "b208c7180bc3f973b8616937b2801c-C001-86", "intents": ["@USE@"], "paper_id": "ABC_b208c7180bc3f973b8616937b2801c_33", "text": "Evaluation is done on the Visual Genome dataset with the splits provided by Krause et al. (2016) ."}
{"sent_id": "201aa2a740b5d45f273ee298595f5a-C001-31", "intents": ["@BACK@"], "paper_id": "ABC_201aa2a740b5d45f273ee298595f5a_33", "text": "Moreover, as discussed in previous studies by Naumann et al. (2018) and Pollock (2018) , mid-range concreteness scores indicate words that are difficult to categorise unambiguously regarding their concreteness."}
{"sent_id": "4a19f17c00e904a595c1703ab9318d-C001-15", "intents": ["@BACK@"], "paper_id": "ABC_4a19f17c00e904a595c1703ab9318d_33", "text": "Furthermore, as the newly proposed architecture called the Transformer network (Vaswani et al., 2017) has shown to be effective in various sequence-to-sequence problems, various multi-source adaptations of the Transformer (Junczys-Dowmunt and Grundkiewicz, 2018; Shin and Lee, 2018; Tebbifakhr et al., 2018) have been applied to APE."}
{"sent_id": "4a19f17c00e904a595c1703ab9318d-C001-70", "intents": ["@USE@"], "paper_id": "ABC_4a19f17c00e904a595c1703ab9318d_33", "text": "We trained our model for ~14K update steps with the Adam optimizer (Kingma and Ba, 2014), warm up learning rates (Vaswani et al., 2017 ) with a size of 12,000, and batch size of approximately 17,000 tokens for each triplet."}
{"sent_id": "cbed76ac8086637fe1d2e30f39c585-C001-13", "intents": ["@BACK@"], "paper_id": "ABC_cbed76ac8086637fe1d2e30f39c585_33", "text": "The generalization of SMT-based GEC systems has been shown to improve further by adding neural network models (Chollampatt et al., 2016b) ."}
{"sent_id": "cbed76ac8086637fe1d2e30f39c585-C001-40", "intents": ["@BACK@"], "paper_id": "ABC_cbed76ac8086637fe1d2e30f39c585_33", "text": "Additionally, we use neural network joint models (Devlin et al., 2014) introduced in (Chollampatt et al., 2016b) and a character-level SMT component."}
{"sent_id": "cbe9e36f371c072432ca25800c96d3-C001-16", "intents": ["@BACK@"], "paper_id": "ABC_cbe9e36f371c072432ca25800c96d3_33", "text": "Furthermore, this learning can take place jointly with ASR [15] , or separately with some tasks that have aligned objectives [16, 17, 18] ."}
{"sent_id": "cbe9e36f371c072432ca25800c96d3-C001-115", "intents": ["@BACK@"], "paper_id": "ABC_cbe9e36f371c072432ca25800c96d3_33", "text": "FHVAE learns to encode sequence-level and segment-level information into separate latent variables without supervision by optimizing an evidence lower bound derived from a factorized graphical model, and has been shown effective for extracting domain invariant ASR features [17] ."}
{"sent_id": "0a538968f0cd121a1ef63b58a0c9f7-C001-83", "intents": ["@UNSURE@"], "paper_id": "ABC_0a538968f0cd121a1ef63b58a0c9f7_33", "text": "Hence, we use a context-based learning approach and regard the 73.9% accuracy (Kalchbrenner and Blunsom, 2013) on the SwDA corpus as a current state of the art for this task."}
{"sent_id": "52b9efca757fe5a376ca6b548d77ce-C001-61", "intents": ["@BACK@"], "paper_id": "ABC_52b9efca757fe5a376ca6b548d77ce_33", "text": "The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jebara et al., 2004) which is frequently used in verb clustering experiments (Korhonen et al., 2003; Sun and Korhonen, 2009) ."}
{"sent_id": "52b9efca757fe5a376ca6b548d77ce-C001-66", "intents": ["@USE@"], "paper_id": "ABC_52b9efca757fe5a376ca6b548d77ce_33", "text": "The clustering results are evaluated using FMeasure as in Sun and Korhonen (2009) which provides the harmonic mean of precision (P ) and recall (R) P is calculated using modified purity -a global measure which evaluates the mean precision of clusters."}
{"sent_id": "52b9efca757fe5a376ca6b548d77ce-C001-98", "intents": ["@USE@"], "paper_id": "ABC_52b9efca757fe5a376ca6b548d77ce_33", "text": "Unlike these previous works, we will use selectional preferences to generalize the argument heads but will do so using preferences from distributional data (Sun and Korhonen, 2009 ) rather than WordNet, and use all argument head data in all frames."}
{"sent_id": "cb57b8886be9ea4f0c50fd2c3a178a-C001-95", "intents": ["@USE@"], "paper_id": "ABC_cb57b8886be9ea4f0c50fd2c3a178a_33", "text": "5 Following prior work [6] , documents are truncated to 800 tokens."}
{"sent_id": "8b2bb6753dc72a048ec28958e943fb-C001-161", "intents": ["@UNSURE@"], "paper_id": "ABC_8b2bb6753dc72a048ec28958e943fb_33", "text": "The results of the multilayer perceptron developed in (Sak, Güngör, and Saraçlar 2007) and the decision list learning algorithm developed in (Yüret and Türe 2006) are presented in lines 1 and 2 respectively."}
{"sent_id": "41d56f3f962fa3b0e0563544a6de40-C001-26", "intents": ["@USE@"], "paper_id": "ABC_41d56f3f962fa3b0e0563544a6de40_33", "text": "The rules are obtained from a word and phrase aligned bitext using the rule extraction algorithm in (Watanabe et al., 2006) ."}
{"sent_id": "8622616ffd4db96058a9b8aff54212-C001-17", "intents": ["@USE@"], "paper_id": "ABC_8622616ffd4db96058a9b8aff54212_33", "text": "The keyword extraction discussed in this paper is based on work presented in Hulth (2003a) and Hulth (2003b) ."}
{"sent_id": "8622616ffd4db96058a9b8aff54212-C001-77", "intents": ["@UNSURE@"], "paper_id": "ABC_8622616ffd4db96058a9b8aff54212_33", "text": "In the experiments discussed so far, the weights given to the positive examples are those resulting in the best performance for each individual classifier (as described in Hulth (2003a) )."}
{"sent_id": "4be5a47b5fd900c3578330b352b24c-C001-68", "intents": ["@USE@"], "paper_id": "ABC_4be5a47b5fd900c3578330b352b24c_33", "text": "We include POS tags and the top 500 n-gram features (Agarwal et al., 2009) ."}
{"sent_id": "e9f7d339ccda101000b53d89da4e49-C001-41", "intents": ["@USE@"], "paper_id": "ABC_e9f7d339ccda101000b53d89da4e49_33", "text": "We adopt the method proposed by Flanigan et al. (2014) as our baseline, which is a two-step pipeline method of concept identification step and (Flanigan et al., 2014) for a retired plant worker."}
{"sent_id": "e9f7d339ccda101000b53d89da4e49-C001-82", "intents": ["@USE@"], "paper_id": "ABC_e9f7d339ccda101000b53d89da4e49_33", "text": "We use the implementation 2 of (Flanigan et al., 2014) as our baseline."}
{"sent_id": "7a1a1593a9480b6ee246ff4248668e-C001-47", "intents": ["@USE@"], "paper_id": "ABC_7a1a1593a9480b6ee246ff4248668e_34", "text": "The context vector c t = α t,i h i is a weighted combination of encoder hidden states h i , where the attention weights are learned through the bilinear attention mechanism proposed in Luong et al. (2015) ."}
{"sent_id": "7a1a1593a9480b6ee246ff4248668e-C001-53", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_7a1a1593a9480b6ee246ff4248668e_34", "text": "Multi-task learning helps in sharing knowledge between related tasks across domains (Luong et al., 2015) ."}
{"sent_id": "6c872be6b2fbe83890e28ddc1098a3-C001-24", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_6c872be6b2fbe83890e28ddc1098a3_34", "text": "IRT was previously used to build a new test set for the NLI task (Lalor et al., 2016) and show that model performance is dependent on test set difficulty."}
{"sent_id": "6c872be6b2fbe83890e28ddc1098a3-C001-37", "intents": ["@USE@"], "paper_id": "ABC_6c872be6b2fbe83890e28ddc1098a3_34", "text": "To model item difficulty we use the Three Parameter Logistic (3PL) model from IRT (Baker, 2001; Baker and Kim, 2004; Lalor et al., 2016) ."}
{"sent_id": "6c872be6b2fbe83890e28ddc1098a3-C001-48", "intents": ["@USE@"], "paper_id": "ABC_6c872be6b2fbe83890e28ddc1098a3_34", "text": "To estimate item difficulties for NLI, we used the pre-trained IRT models of Lalor et al. (2016) and extracted the difficulty item parameters."}
{"sent_id": "c67297dc1c4376dc715bf5c1c9132f-C001-82", "intents": ["@USE@"], "paper_id": "ABC_c67297dc1c4376dc715bf5c1c9132f_34", "text": "Sense-based KNN Adapted from ELMo (Peters et al., 2018) with k = 1, words that have the same senses are clustered together, and the average of that cluster is used as the sense vector, which is then fitted using a one KNN classifier."}
{"sent_id": "2e636754342e9bb857068922519dbc-C001-58", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_2e636754342e9bb857068922519dbc_34", "text": "The idea behind this model is to pre-train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer (Vaswani et al., 2017; Devlin et al., 2018) ."}
{"sent_id": "2e636754342e9bb857068922519dbc-C001-63", "intents": ["@USE@"], "paper_id": "ABC_2e636754342e9bb857068922519dbc_34", "text": "The first version is based on the original BERT release (Devlin et al., 2018) ."}
{"sent_id": "c6ae69051a6d9111dea1a6e8405ac9-C001-109", "intents": ["@USE@"], "paper_id": "ABC_c6ae69051a6d9111dea1a6e8405ac9_34", "text": "Additionally, we also compare oracle-BLEU re-estimation to forced decoding with leave-oneout (Wuebker et al., 2010) by evaluating both on a concatenation of 5 test sets (MT03, MT05-MT09)."}
{"sent_id": "033ce75c882764e08fb3871656a8d1-C001-78", "intents": ["@USE@"], "paper_id": "ABC_033ce75c882764e08fb3871656a8d1_34", "text": "For preprocessing we used the same processes as described in Zilio et al. (2011) ."}
{"sent_id": "033ce75c882764e08fb3871656a8d1-C001-107", "intents": ["@USE@"], "paper_id": "ABC_033ce75c882764e08fb3871656a8d1_34", "text": "We tested several algorithms offered by Weka as well as the training options suggested by Zilio et al. (2011) ."}
{"sent_id": "6869f08e826aa434471c51c010ef28-C001-83", "intents": ["@UNSURE@"], "paper_id": "ABC_6869f08e826aa434471c51c010ef28_34", "text": "However, previous experiments showed that the extra improvements brought in this way is almost negligible, probably because the M × N different pattern sequences based on the M × N different pattern sets can be considered as a huge lattice including many one-best paths which will be jointly considered here [19] ."}
{"sent_id": "b9e9f358ace19da43bfe9e5bc380c5-C001-13", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_b9e9f358ace19da43bfe9e5bc380c5_34", "text": "Yu et al. (2018b) 's dataset is exclusive for English questions."}
{"sent_id": "b9e9f358ace19da43bfe9e5bc380c5-C001-81", "intents": ["@USE@"], "paper_id": "ABC_b9e9f358ace19da43bfe9e5bc380c5_34", "text": "We follow Yu et al. (2018b) , evaluating the results using two major 2 https://nlp.stanford.edu/projects/glove/ 3 https://ai.tencent.com/ailab/nlp/embedding.html types of metrics."}
{"sent_id": "8b5e14bdf3f415725333de672be114-C001-118", "intents": ["@USE@"], "paper_id": "ABC_8b5e14bdf3f415725333de672be114_34", "text": "As mentioned above, the second aim of this study is to see how much benefit we can get from incorporating high-level structural features, such as those used in (Feng et al., 2010) (described in Section 4.2), with the features in our previous study."}
{"sent_id": "d5d81a4c7759f9a4ab81195819c6d9-C001-64", "intents": ["@USE@"], "paper_id": "ABC_d5d81a4c7759f9a4ab81195819c6d9_34", "text": "We adopt the big transformer configuration following Vaswani et al. (2017) , with the dimension of word embeddings, hidden states and non-linear layer set as 1024, 1024 and 4096 respectively."}
{"sent_id": "aa87225d7d326adfb4a8b2702b8f25-C001-53", "intents": ["@USE@"], "paper_id": "ABC_aa87225d7d326adfb4a8b2702b8f25_34", "text": "For comparison, we trained a monolingual skipgram model [13] and its Glove variant [15] for the same number of epochs on the English half of the bilingual corpus."}
{"sent_id": "aa87225d7d326adfb4a8b2702b8f25-C001-80", "intents": ["@USE@"], "paper_id": "ABC_aa87225d7d326adfb4a8b2702b8f25_34", "text": "We evaluated the embeddings on this task using the same vector-algebra method as [13] ."}
{"sent_id": "0f0e13e275c4bc4021b1b0d26f3e0c-C001-74", "intents": ["@USE@"], "paper_id": "ABC_0f0e13e275c4bc4021b1b0d26f3e0c_34", "text": "Data set: We use the data set from (Fader et al., 2011) which consists of 500 sentences sampled from the Web using Yahoo's random link service."}
{"sent_id": "5b9a6590d2e7c49f9a9788abe6dc1b-C001-30", "intents": ["@USE@"], "paper_id": "ABC_5b9a6590d2e7c49f9a9788abe6dc1b_34", "text": "We refer to Dyer et al. (2016) for a complete description of these actions, and the constraints on them necessary to ensure valid parse trees."}
{"sent_id": "ada92083e8c012c328d5b6172b76ad-C001-130", "intents": ["@USE@"], "paper_id": "ABC_ada92083e8c012c328d5b6172b76ad_34", "text": "According to the above findings, we design a simple system by mapping SRL outputs into ORL directly (Kim and Hovy, 2006; Ruppenhofer et al., 2008) ."}
{"sent_id": "eeada4aedbb43b575365a15d75f2ac-C001-19", "intents": ["@USE@"], "paper_id": "ABC_eeada4aedbb43b575365a15d75f2ac_34", "text": "In Section 3, I examine the size of the training corpus on the accuracy of WSD, using a corpus of 192,800 occurrences of 191 words hand tagged with WORDNET senses (Ng and Lee, 1996) ."}
{"sent_id": "eeada4aedbb43b575365a15d75f2ac-C001-62", "intents": ["@USE@"], "paper_id": "ABC_eeada4aedbb43b575365a15d75f2ac_34", "text": "To investigate the effect of the number of training examples on WSD accuracy, I ran the exemplarbased WSD algorithm L~.XAS on varying number of training examples to obtain learning curves for the 191 words (details of LEXAS are described in (Ng and Lee, 1996) )."}
{"sent_id": "eeada4aedbb43b575365a15d75f2ac-C001-33", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_eeada4aedbb43b575365a15d75f2ac_34", "text": "When the task is to resolve word senses to the fine-grain distinction of WORD-NET senses, the accuracy figures achieved are generally not very high (Miller et al., 1994; Ng and Lee, 1996) ."}
{"sent_id": "eeada4aedbb43b575365a15d75f2ac-C001-38", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_eeada4aedbb43b575365a15d75f2ac_34", "text": "This is in contrast to disambiguating word senses to the refined senses of WoRDNET, where for instance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in (Ng and Lee, 1996) ."}
{"sent_id": "8e59c2c48e27b2abd5f63d6b4ce23d-C001-82", "intents": ["@USE@"], "paper_id": "ABC_8e59c2c48e27b2abd5f63d6b4ce23d_34", "text": "We evaluate decoding architectures using different levels of granularity in the vocabulary units and the attention mechanism, including the standard decoding architecture implemented either with subword (Sennrich et al., 2016) or fully character-level (Cherry et al., 2018) units, which constitute the baseline approaches, and the hierarchical decoding architecture, by implementing all in Pytorch (Paszke et al., 2017) within the OpenNMT-py framework (Klein et al., 2017) ."}
{"sent_id": "d1bff202991116a6a957aa61c05770-C001-56", "intents": ["@USE@"], "paper_id": "ABC_d1bff202991116a6a957aa61c05770_35", "text": "Our method operates on a collection of pre-trained word representations and is then trained on the concatenation of SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) datasets as in Conneau et al. (2017) ."}
{"sent_id": "d1bff202991116a6a957aa61c05770-C001-112", "intents": ["@USE@"], "paper_id": "ABC_d1bff202991116a6a957aa61c05770_35", "text": "We use 4096-dimensional embeddings as in Conneau et al. (2017) ."}
{"sent_id": "cc3d38692097020ee7f4f17cf9247d-C001-29", "intents": ["@USE@"], "paper_id": "ABC_cc3d38692097020ee7f4f17cf9247d_35", "text": "In this study, we provide a systematic comparison and analysis of three such kernels -subsequence kernel (Bunescu and Mooney, 2005b) , dependency tree kernel (Culotta and Sorensen, 2004) and dependency path kernel (Bunescu and Mooney, 2005a) ."}
{"sent_id": "a789aea59eebfefb990dfc6367d323-C001-37", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_a789aea59eebfefb990dfc6367d323_35", "text": "Previous work only retrieves time-rich sentences that include both the query and some TEs McClosky and Manning, 2012; Garrido et al., 2012) ."}
{"sent_id": "289ea9be270f68e23ca1809f997be9-C001-35", "intents": ["@USE@"], "paper_id": "ABC_289ea9be270f68e23ca1809f997be9_35", "text": "In this contribution, we begin by reproducing and validating the [11] proposed models and their results on the three datasets, Formspring [12] , Twitter [13] and Wikipedia [14] , which have been used by the authors."}
{"sent_id": "289ea9be270f68e23ca1809f997be9-C001-170", "intents": ["@USE@"], "paper_id": "ABC_289ea9be270f68e23ca1809f997be9_35", "text": "In this study, we successfully reproduced the reference literature [11] for detection of cyberbullying incidents in social media platforms using DNN based models."}
{"sent_id": "b6afd492c60af7ab1ba0be3cd654b2-C001-26", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_b6afd492c60af7ab1ba0be3cd654b2_35", "text": "Both modes offer their advantages: The twostep mode simplifies generation by abstracting away from complex surface syntax and morphology, which can be handled by a handcrafted, domain-independent module to ensure grammatical correctness at all times (Dušek and Jurčíček, 2015) , and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline (Konstas and Lapata, 2013) ."}
{"sent_id": "b6afd492c60af7ab1ba0be3cd654b2-C001-100", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_b6afd492c60af7ab1ba0be3cd654b2_35", "text": "Works using the pipeline approach in SDS tend to focus on sentence planning, improving a handcrafted generator (Walker et al., 2001; Stent et al., 2004; Paiva and Evans, 2005) or using perceptron-guided A* search (Dušek and Jurčíček, 2015) ."}
{"sent_id": "c7e304499654516cce43c550256eae-C001-14", "intents": ["@BACK@", "@DIF@", "@MOT@"], "paper_id": "ABC_c7e304499654516cce43c550256eae_35", "text": "For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language Zhang et al., 2016; Fang and Cohn, 2016) ."}
{"sent_id": "3e344c590b4d5270a29054ac15efa5-C001-102", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_3e344c590b4d5270a29054ac15efa5_35", "text": "Character-level CNN can sufficiently replace words for classifications [2] ."}
{"sent_id": "3ebfa05038431571701a7199163832-C001-49", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_3ebfa05038431571701a7199163832_35", "text": "where (ϕ n ) n=1...N are Gabor wavelets defined in [22] such that |φ n | 2 ≈ |ψ n | 2 . [22] shows that this computation can be implemented as neural network layers, referred as TimeDomain filterbanks (TD-filterbanks)."}
{"sent_id": "3ebfa05038431571701a7199163832-C001-36", "intents": ["@USE@"], "paper_id": "ABC_3ebfa05038431571701a7199163832_35", "text": "Our experiments show that by training a PCEN block on top of mel-filterbanks or replacing them by learnable time-domain filterbanks from [22] , we get a gain in accuracy around 10% in absolute when training an identical neural network for dysarthria detection."}
{"sent_id": "3ebfa05038431571701a7199163832-C001-53", "intents": ["@USE@"], "paper_id": "ABC_3ebfa05038431571701a7199163832_35", "text": "Following [22] , the first 1D convolution filters are initialized with Gabor wavelets, to replicate mel-filterbanks, and are then learnt at the same time as the rest of the model."}
{"sent_id": "30718e751f18432c2478442530267e-C001-3", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_30718e751f18432c2478442530267e_35", "text": "According to Jia and Liang (2017) , the single BiDAF system (Seo et al., 2016) only achieves an F1 score of 4.8 on the ADDANY adversarial dataset."}
{"sent_id": "30718e751f18432c2478442530267e-C001-101", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_30718e751f18432c2478442530267e_35", "text": "However, following the idea of adversarial examples in image recognition (Goodfellow et al., 2014; Kurakin et al., 2016; Papernot et al., 2016) , Jia and Liang (2017) point out the unreliability of existing question answering models in the presence of adversarial sentences."}
{"sent_id": "30718e751f18432c2478442530267e-C001-64", "intents": ["@USE@"], "paper_id": "ABC_30718e751f18432c2478442530267e_35", "text": "Our test set is Jia and Liang (2017)'s ADDANY adversarial dataset."}
{"sent_id": "05eecafea7684dc8de13c29a76b767-C001-32", "intents": ["@USE@"], "paper_id": "ABC_05eecafea7684dc8de13c29a76b767_35", "text": "We use three existing Twitter user geolocation datasets: (1) GEOTEXT (Eisenstein et al., 2010) , (2) TWITTER-US (Roller et al., 2012) , and (3) TWITTER-WORLD (Han et al., 2012) ."}
{"sent_id": "b7e0879c4cac85054870146e61aa6f-C001-64", "intents": ["@USE@"], "paper_id": "ABC_b7e0879c4cac85054870146e61aa6f_35", "text": "Following Das et al. [6] , we report the agent's top-1 accuracy on the test set when spawned 10, 20 and 50 steps away from the goal, denoted as T 10 , T 20 and T 50 respectively."}
{"sent_id": "b7e0879c4cac85054870146e61aa6f-C001-79", "intents": ["@USE@"], "paper_id": "ABC_b7e0879c4cac85054870146e61aa6f_35", "text": "Specifically we train the VQA model described in [6] on the last 5 frames of oracle navigation for 50 epochs with ADAM and a learning rate of 3e − 4 using batch size 20."}
{"sent_id": "0751f2ced4f7ced37cf206fea051fa-C001-14", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_0751f2ced4f7ced37cf206fea051fa_35", "text": "In (Sánchez and Benedí, 2006b ), SITGs were used for obtaining word phrases, reporting preliminary results on the EuroParl corpus."}
{"sent_id": "0751f2ced4f7ced37cf206fea051fa-C001-71", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_0751f2ced4f7ced37cf206fea051fa_35", "text": "Comparatively, the best result that (Sánchez and Benedí, 2006b) reported in the Spanish-English task was a BLEU score of 23.0, which they obtained by combining segments extracted from both the bracketed and the non-bracketed corpus."}
{"sent_id": "0751f2ced4f7ced37cf206fea051fa-C001-50", "intents": ["@USE@"], "paper_id": "ABC_0751f2ced4f7ced37cf206fea051fa_35", "text": "(Sánchez and Benedí, 2006b ) Translation results of this setup can be seen in Table 1 ."}
{"sent_id": "2b7ba7f7aa2a03ad0de84e007c1f64-C001-24", "intents": ["@USE@"], "paper_id": "ABC_2b7ba7f7aa2a03ad0de84e007c1f64_35", "text": "We provide a sample negotiation from the test set (He et al., 2018 ) along with our model predictions in Table 1 ."}
{"sent_id": "5eb321d3c63642a4b148e1276eab20-C001-10", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_5eb321d3c63642a4b148e1276eab20_35", "text": "Recent years witness the boost of neural models in this task, e.g., (Shimaoka et al. 2016) employs an attention based LSTM to attain sentence representations and achieves state-of-the-art performance."}
{"sent_id": "5eb321d3c63642a4b148e1276eab20-C001-56", "intents": ["@USE@"], "paper_id": "ABC_5eb321d3c63642a4b148e1276eab20_35", "text": "We employ Strict Accuracy (Acc), Loose Macro F1 (Ma-F1), and Loose Micro F1 (Mi-F1) as evaluation measures following (Shimaoka et al. 2016 )."}
{"sent_id": "5eb321d3c63642a4b148e1276eab20-C001-58", "intents": ["@USE@"], "paper_id": "ABC_5eb321d3c63642a4b148e1276eab20_35", "text": "The baselines are chosen from two aspects: (1) Predicting types in a unified process using raw noisy data, i.e., TLSTM (Shimaoka et al. 2016) , and other methods shown in Table2."}
{"sent_id": "0c2f7cea9f27b4799736fbcba48192-C001-57", "intents": ["@USE@"], "paper_id": "ABC_0c2f7cea9f27b4799736fbcba48192_36", "text": "In order to put our proposed labelwise attention mechanism in context, we compare its performance with a set of baselines: (1 (Felbo et al., 2017) ."}
{"sent_id": "b2a6ec11403fe73b9bae7742c1c5a2-C001-64", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_b2a6ec11403fe73b9bae7742c1c5a2_36", "text": "A corpus study in [12] showed that content changes are correlated with argumentative writing improvement, reaffirming the statement of [4] ."}
{"sent_id": "b2a6ec11403fe73b9bae7742c1c5a2-C001-26", "intents": ["@USE@"], "paper_id": "ABC_b2a6ec11403fe73b9bae7742c1c5a2_36", "text": "Our work takes advantage of several corpora of multiple drafts of argumentative essays written by both high-school and college students [12, 11] , where all data has been annotated for revision using the framework of [12] ."}
{"sent_id": "196e7ca5ccd6754ac986137ec55cd3-C001-40", "intents": ["@USE@", "@DIF@", "@EXT@"], "paper_id": "ABC_196e7ca5ccd6754ac986137ec55cd3_36", "text": "Following previous work (Yu et al., 2016) , here we use a positive confidence threshold, which determines when the agent believes its own predictions."}
{"sent_id": "a67f40381e82afaf249f097a208555-C001-79", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_a67f40381e82afaf249f097a208555_36", "text": "The directional similarity model (Zhila et al. (2013) ) explores the difference of two relationships in multiple topicality dimensions in the vector space."}
{"sent_id": "a67f40381e82afaf249f097a208555-C001-88", "intents": ["@USE@"], "paper_id": "ABC_a67f40381e82afaf249f097a208555_36", "text": "Based on the directional similarity model (Zhila et al., 2013) , we define the domain similarity of two pairs of words as follows:"}
{"sent_id": "07cee5aa02b518d48e41b1d6010c2f-C001-43", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_07cee5aa02b518d48e41b1d6010c2f_36", "text": "Following Huang (2008) , this algorithm traverses a parse forest in a bottom-up manner."}
{"sent_id": "ff5122ce817d506fbcb269b7ae41fe-C001-26", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_ff5122ce817d506fbcb269b7ae41fe_36", "text": "Ziering and Van der Plas (2014) propose an approach that refrains from using any human annotation."}
{"sent_id": "ff5122ce817d506fbcb269b7ae41fe-C001-68", "intents": ["@USE@"], "paper_id": "ABC_ff5122ce817d506fbcb269b7ae41fe_36", "text": "We use the Europarl 2 compound database 3 developed by Ziering and Van der Plas (2014) ."}
{"sent_id": "ff5122ce817d506fbcb269b7ae41fe-C001-86", "intents": ["@USE@"], "paper_id": "ABC_ff5122ce817d506fbcb269b7ae41fe_36", "text": "We created a back-off model for the bracketing system of Ziering and Van der Plas (2014) and for AWDB that falls back to using χ 2 if no bracketing structure can be derived (system → χ 2 )."}
{"sent_id": "8ace0627a085efd0cf0ccc211c556f-C001-47", "intents": ["@USE@"], "paper_id": "ABC_8ace0627a085efd0cf0ccc211c556f_36", "text": "We use XLNet [11] to attempt to capture long range language dependencies."}
{"sent_id": "8ace0627a085efd0cf0ccc211c556f-C001-185", "intents": ["@USE@"], "paper_id": "ABC_8ace0627a085efd0cf0ccc211c556f_36", "text": "Given the size of the model, and the fact that it was pre-trained on 512 TPUs [11] , we expect that training for 20 epochs on TED-LIUM's text is not enough to overcome the differences between written text and conversational speech."}
{"sent_id": "8e0dcaec15a3b9c4947946a4e885c8-C001-34", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_8e0dcaec15a3b9c4947946a4e885c8_36", "text": "Before describing the loss functions used, we explain the aforementioned variation on the autoencoding method and how it slightly differs from 1TON/1TON + (Yin and Schütze, 2015) and standard AEs (Bao and Bollegala, 2018) presented in previous work."}
{"sent_id": "8e0dcaec15a3b9c4947946a4e885c8-C001-67", "intents": ["@USE@"], "paper_id": "ABC_8e0dcaec15a3b9c4947946a4e885c8_36", "text": "Table 1 shows the scaled Spearman correlation test scores, where (1) shows the original single embeddings, (2) results for standard metaembedding approaches that either apply a single mathematical operation or employ a linear projection as an encoding, (3) presents the results using autoencoder schemes by (Bao and Bollegala, 2018 ) that we have used to test the various losses, (4) introduces TAE without concatenating the target Y embedding post-training with MSE loss and (5) shows the results of concatenating Y with the lower-dimensional (200-dimensions) vector that encodes all embeddings apart from the target vector."}
{"sent_id": "291a6ac3f0c2d27ca69ee8f5f266f5-C001-25", "intents": ["@USE@"], "paper_id": "ABC_291a6ac3f0c2d27ca69ee8f5f266f5_36", "text": "Using the FST notation of Eisner (1997a) , the implementation for this constraint would be the following FST:"}
{"sent_id": "291a6ac3f0c2d27ca69ee8f5f266f5-C001-6", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_291a6ac3f0c2d27ca69ee8f5f266f5_36", "text": "Primitive Optimality Theory (OTP) (Eisner, 1997a) , and extensions to it (e.g., Albro (1998) ), can be useful as a formal system in which phonological analyses can be implemented and evaluated."}
{"sent_id": "0ab60c5c9ace058a5fbe3bc2643cba-C001-66", "intents": ["@USE@"], "paper_id": "ABC_0ab60c5c9ace058a5fbe3bc2643cba_36", "text": "tecture as Zhu et al. (2010) and Nisioi et al. (2017) : 2 layers of stacked unidirectional LSTMs with bi-linear global attention as proposed by Luong et al. (2015) , with hidden states of 512 dimensions."}
{"sent_id": "97fd0f1ce3d4f510c1566d642e9d2c-C001-27", "intents": ["@USE@"], "paper_id": "ABC_97fd0f1ce3d4f510c1566d642e9d2c_37", "text": "Our NMT model consists of an encoder and a decoder, each of which is a Recurrent Neural Network (RNN) as described in (Luong et al., 2015) ."}
{"sent_id": "97fd0f1ce3d4f510c1566d642e9d2c-C001-36", "intents": ["@USE@"], "paper_id": "ABC_97fd0f1ce3d4f510c1566d642e9d2c_37", "text": "Our NMT model uses a bi-directional RNN as an encoder and a unidirectional RNN as a decoder with global attention (Luong et al., 2015) ."}
{"sent_id": "97fd0f1ce3d4f510c1566d642e9d2c-C001-77", "intents": ["@USE@"], "paper_id": "ABC_97fd0f1ce3d4f510c1566d642e9d2c_37", "text": "The structure of our NMT model is same as in Luong et al. (2015) , an RNN based encoder-decoder model with Global Attention mechanism."}
{"sent_id": "877a0b5b5d25b3849ca44ed42b8d6d-C001-39", "intents": ["@USE@"], "paper_id": "ABC_877a0b5b5d25b3849ca44ed42b8d6d_37", "text": "For the page limit and the fair comparison, we only adopt the conventional features as in (Zhang et al., 2008) in our current implementation."}
{"sent_id": "9340338e7cf8ff8de4db84b462dfe5-C001-22", "intents": ["@USE@"], "paper_id": "ABC_9340338e7cf8ff8de4db84b462dfe5_37", "text": "To validate the extensibility of the system, we complete an additional evaluation of the system using claims taken from Vlachos and Riedel (2015) ."}
{"sent_id": "9340338e7cf8ff8de4db84b462dfe5-C001-94", "intents": ["@USE@"], "paper_id": "ABC_9340338e7cf8ff8de4db84b462dfe5_37", "text": "Although the range of claims is limited, the system is a fieldtested prototype and has been evaluated on a published data set (Vlachos and Riedel, 2015) and on real-world claims presented as part of the HeroX fact checking challenge."}
{"sent_id": "24b38363d53468175e0274ac0b4fd3-C001-61", "intents": ["@USE@"], "paper_id": "ABC_24b38363d53468175e0274ac0b4fd3_37", "text": "We also employed the word embeddings encoding sentiment information generated through the unified models in (Tang et al., 2014b) ."}
{"sent_id": "6bb7d5f16861470214626c1cc497bb-C001-15", "intents": ["@USE@"], "paper_id": "ABC_6bb7d5f16861470214626c1cc497bb_37", "text": "In this paper, we use it to evaluate two state-of-the-art (SoA) models of speaker commitment: Stanovsky et al. (2017) and ."}
{"sent_id": "6bb7d5f16861470214626c1cc497bb-C001-34", "intents": ["@USE@"], "paper_id": "ABC_6bb7d5f16861470214626c1cc497bb_37", "text": "We evaluate the performance of two speaker commitment models on the CommitmentBank: a rulebased model (Stanovsky et al., 2017 ) and a neuralbased one ."}
{"sent_id": "db1fd6f10a3ee22e22093d50395217-C001-77", "intents": ["@DIF@", "@USE@"], "paper_id": "ABC_db1fd6f10a3ee22e22093d50395217_37", "text": "However, we compare our results with (Kim et al., 2011) and (Verbeke et al., 2012) using the microaveraged F-scores as in Table 3 ."}
{"sent_id": "6fd0c2fbbe0c7fb669f2618f4d01f7-C001-37", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_6fd0c2fbbe0c7fb669f2618f4d01f7_37", "text": "About 74% of Wikipedia articles fall under the category of named entity classes [4] , therefore, we consider Wikipedia links among articles to construct the Hi-En-WP NER annotated corpus."}
{"sent_id": "782517ae7688cf18b4bca37a8087dd-C001-64", "intents": ["@USE@"], "paper_id": "ABC_782517ae7688cf18b4bca37a8087dd_37", "text": "Following Rao et al. (2018) , we evaluate our models in a reranking task, where the inputs are up to the top 1000 tweets retrieved from the classical query likelihood (QL) language model (Ponte and Croft, 1998) ."}
{"sent_id": "782517ae7688cf18b4bca37a8087dd-C001-78", "intents": ["@USE@"], "paper_id": "ABC_782517ae7688cf18b4bca37a8087dd_37", "text": "Results from 5 -8 are adopted from Rao et al. (2018) ."}
{"sent_id": "28805bfa8f8b847110664d7e05b1b3-C001-25", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_28805bfa8f8b847110664d7e05b1b3_37", "text": "PCW have been successfully applied to the extraction of named entity coordinate terms, including city and person names, from graphs representing newswire text (Minkov and Cohen, 2008) , where the specialized measures learned outperformed the state-ofthe-art dependency vectors method (Padó and Lapata, 2007) for small-and medium-sized corpora."}
{"sent_id": "28805bfa8f8b847110664d7e05b1b3-C001-24", "intents": ["@USE@"], "paper_id": "ABC_28805bfa8f8b847110664d7e05b1b3_37", "text": "To this end, we consider a path constrained graph walk (PCW) algorithm, which allows one to learn meaningful paths given a small number of labeled examples and incorporates this information in assessing node relatedness in the graph (Minkov and Cohen, 2008) ."}
{"sent_id": "f326a3e2a5e349ce84b0a759f8e0b2-C001-84", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_f326a3e2a5e349ce84b0a759f8e0b2_37", "text": "Stoia (2007) observed that IGs use move instructions to focus the IF's attention on a particular area."}
{"sent_id": "193d388c3f4c346cb62711f3f04c0f-C001-10", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_193d388c3f4c346cb62711f3f04c0f_37", "text": "State-of-the-art deep neural networks leverage task-specific architectures to develop hierarchical representations of their input, with each layer building a refined abstraction of the layer that came before it (Conneau et al., 2016) ."}
{"sent_id": "193d388c3f4c346cb62711f3f04c0f-C001-93", "intents": ["@USE@"], "paper_id": "ABC_193d388c3f4c346cb62711f3f04c0f_37", "text": "That is, we replace Equation 5 with v i = s (Conneau et al., 2016)"}
{"sent_id": "5f2f4087b80aa8dc3a5ccdb686983d-C001-25", "intents": ["@USE@"], "paper_id": "ABC_5f2f4087b80aa8dc3a5ccdb686983d_37", "text": "For our experiments we use the dataset described in (DeVault et al., 2011b) ."}
{"sent_id": "9af4a895dd4b45bb3827c74bdc7f05-C001-58", "intents": ["@USE@"], "paper_id": "ABC_9af4a895dd4b45bb3827c74bdc7f05_37", "text": "Collocational Features: The global statistical features in Somasundaran et al. (2015) and were used: the mean, the median, the maximum and the minimum of the ASs, and the proportion of bigrams that are present in the learner text but absent from the reference corpus."}
{"sent_id": "9af4a895dd4b45bb3827c74bdc7f05-C001-83", "intents": ["@USE@"], "paper_id": "ABC_9af4a895dd4b45bb3827c74bdc7f05_37", "text": "To determine if the automatic procedure for discretizing the ASs is at least as effective as the bin boundaries manually set by Somasundaran et al. (2015) , I used them instead of the automatic bins for the model with eight bins based on MI."}
{"sent_id": "dcfad33f4322738906e2fdffe2e721-C001-10", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_dcfad33f4322738906e2fdffe2e721_37", "text": "There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , Church (1993) , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , WarwickArmstrong and Russell (1990) , Wu (to appear)."}
{"sent_id": "dcfad33f4322738906e2fdffe2e721-C001-80", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_dcfad33f4322738906e2fdffe2e721_37", "text": "Currently, word_align depends on charalign (Church, 1993) to generate a starting point, which limits its applicability to European languages since char_align was designed for language pairs that share a common alphabet."}
{"sent_id": "dcfad33f4322738906e2fdffe2e721-C001-65", "intents": ["@USE@"], "paper_id": "ABC_dcfad33f4322738906e2fdffe2e721_37", "text": "This algorithm was applied to a fragment of the Canadian Hansards that has been used in a number of other studies: Church (1993) and Simard et al (1992) ."}
{"sent_id": "2d7e98487698b0b6ae85f052402f7c-C001-59", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_2d7e98487698b0b6ae85f052402f7c_38", "text": "for more than a decade using a standard Hidden Markov Model (HMM) with language features such as words and n-grams (Stolcke et al., 2000) ."}
{"sent_id": "73d4518e44f14a725a28e86de96963-C001-86", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_73d4518e44f14a725a28e86de96963_38", "text": "In computational linguistics, only few research works have attempted to address such a difficult task (Wang, 2013; Barbieri et al., 2014; Sulis et al., 2016; Van Hee et al., 2016) ."}
{"sent_id": "c5e1debe3fcab509737e092505a29e-C001-58", "intents": ["@USE@"], "paper_id": "ABC_c5e1debe3fcab509737e092505a29e_38", "text": "To handle the structural divergence for English-Hindi SMT, we exploited source side preordering (Patel et al., 2013; Ramanathan et al., 2008) ."}
{"sent_id": "c5e1debe3fcab509737e092505a29e-C001-64", "intents": ["@USE@"], "paper_id": "ABC_c5e1debe3fcab509737e092505a29e_38", "text": "Stemming was done using lightweight stemmer (Ramanathan and Rao, 2003) for Hindi."}
{"sent_id": "fd7bae08fd3e69744a3980daa1a649-C001-10", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_fd7bae08fd3e69744a3980daa1a649_38", "text": "Extending the word2vec framework (Mikolov et al., 2013b) , paragraph vectors are typically presented as neural language models, and compute a single vector representation for each paragraph."}
{"sent_id": "ad9b663ac88667c1b88767ca4b2f8f-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_ad9b663ac88667c1b88767ca4b2f8f_38", "text": "That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder (Shen et al., 2008) ."}
{"sent_id": "ad9b663ac88667c1b88767ca4b2f8f-C001-35", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_ad9b663ac88667c1b88767ca4b2f8f_38", "text": "Similar approach has been originally used by Shen et al. (2008) ."}
{"sent_id": "ad9b663ac88667c1b88767ca4b2f8f-C001-22", "intents": ["@USE@"], "paper_id": "ABC_ad9b663ac88667c1b88767ca4b2f8f_38", "text": "For extracting string-to-dependency transfer rules, we use well-formed dependency structures, either fixed or floating, as defined in (Shen et al., 2008) ."}
{"sent_id": "ad9b663ac88667c1b88767ca4b2f8f-C001-67", "intents": ["@USE@"], "paper_id": "ABC_ad9b663ac88667c1b88767ca4b2f8f_38", "text": "We re-implemented the string-to-dependency decoder described in (Shen et al., 2008 (Pauls and Klein, 2011) , was employed to train (1) a five-gram LM on the Xinhua portion of LDC English Gigaword corpus v3 (LDC2007T07) and (2) a tri-gram dependency LM on the English dependency structures of the training data."}
{"sent_id": "a5d7f5c5fed218149818463427d6a1-C001-25", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_a5d7f5c5fed218149818463427d6a1_38", "text": "For word representations, Bolukbasi et al. (2016) and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women."}
{"sent_id": "c9d9997b61974a537915a2c90af3cf-C001-119", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_c9d9997b61974a537915a2c90af3cf_38", "text": "This is because in previous studies (e.g., Zadeh et al. (2018a) ) the verbal modality was shown to be the most effective for unimodal sentiment analysis, while the vocal modality was shown to be the least effective."}
{"sent_id": "c9d9997b61974a537915a2c90af3cf-C001-125", "intents": ["@USE@"], "paper_id": "ABC_c9d9997b61974a537915a2c90af3cf_38", "text": "To evaluate the performance of sentiment score prediction, following previous work (Zadeh et al., 2018a) , we Tables 2 and 3 , the numbers in bold are the best performance for each modality or fusion strategy."}
{"sent_id": "78a7ca27c5ca032116db12205af939-C001-31", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_78a7ca27c5ca032116db12205af939_38", "text": "TieNet is capable of extracting an informative embedding to represent the paired medical image and report, which significantly improves the disease classification performance compared to [16] ."}
{"sent_id": "78a7ca27c5ca032116db12205af939-C001-36", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_78a7ca27c5ca032116db12205af939_38", "text": "Our contributions are in four-fold: (1) we describe an integrated image interpretation framework for disease annotation and medical report generation, (2) we transfer knowledge from large image data sets (ChestX-ray8 [16] and ImageNet) to enhance medical image interpretation using a small number of reports for training (IU X-ray [2] ), (3) we evaluate suitability of the NLP evaluation metrics for medical report generation, and (4) we demonstrate the functionality of localizing the key finding in an X-ray with a heatmap."}
{"sent_id": "78a7ca27c5ca032116db12205af939-C001-79", "intents": ["@USE@"], "paper_id": "ABC_78a7ca27c5ca032116db12205af939_38", "text": "We filter out images and reports that are non-relevant to the eight common thoracic diseases included in both ChestX-ray8 [16] and IU X-ray datasets [2] , resulting in a dataset with 2225 pairs of X-ray image and report."}
{"sent_id": "78a7ca27c5ca032116db12205af939-C001-86", "intents": ["@UNSURE@"], "paper_id": "ABC_78a7ca27c5ca032116db12205af939_38", "text": "We do not fine-tune the DenseNet pretrained with ChestX-ray8 [16] and ResNet pretrained with ImageNet due to the small sample size of IU X-ray dataset [2] ."}
{"sent_id": "f861e6590ff57225395e7d480c66e8-C001-23", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_f861e6590ff57225395e7d480c66e8_38", "text": "Gupta et al. (2016) propose the use of various manually extracted features along with RNNs."}
{"sent_id": "023a954d97b5d761b01f09bb242d19-C001-39", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_023a954d97b5d761b01f09bb242d19_38", "text": "Although recent studies have utilized Long Short-Term Memory (LSTM) in AMR parsing [10, 1] , there are several disadvantages of employing LSTM compared to CNN."}
{"sent_id": "b9a748ac201b2d8f5d52abd60aa018-C001-34", "intents": ["@USE@"], "paper_id": "ABC_b9a748ac201b2d8f5d52abd60aa018_38", "text": "The linguistic (Chall 1948; Fang 1966; Kiyokawa 1990; Messerklinger 2006; Kotani et al. 2014; Kotani & Yoshimi 2016; Yoon et al. 2016) , and learner features (Kotani et al. 2014; Kotani & Yoshimi 2016) used in this study were originally described elsewhere."}
{"sent_id": "4d25b647a261a415d769532386265a-C001-18", "intents": ["@USE@"], "paper_id": "ABC_4d25b647a261a415d769532386265a_38", "text": "To model word order, sinusoidal positional embeddings are used [11] ."}
{"sent_id": "93a1f611592ce6aa5cde7538486f97-C001-15", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_93a1f611592ce6aa5cde7538486f97_38", "text": "Both techniques were already combined in prior work to show, e.g., the increasing association of the lexical item \"gay\" with the meaning dimension of \"homosexuality\" (Kim et al., 2014; Kulkarni et al., 2015) ."}
{"sent_id": "93a1f611592ce6aa5cde7538486f97-C001-39", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_93a1f611592ce6aa5cde7538486f97_38", "text": "We thus concentrate on two experimental protocols-the one described by Kim et al. (2014) (referred to as Kim protocol) and the one from Kulkarni et al. (2015) (referred to as Kulkarni protocol), including close variations thereof."}
{"sent_id": "93a1f611592ce6aa5cde7538486f97-C001-35", "intents": ["@USE@"], "paper_id": "ABC_93a1f611592ce6aa5cde7538486f97_38", "text": "For comparability with earlier studies (Kim et al., 2014; Kulkarni et al., 2015) , we use the fiction part of the GOOGLE BOOKS NGRAM corpus (Michel et al., 2011; Lin et al., 2012) ."}
{"sent_id": "a01a6ab7cf13c7916b1b3823a4b4de-C001-60", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_a01a6ab7cf13c7916b1b3823a4b4de_38", "text": "Our principal base system that generates the n-best output lists is DIRECTL+, which has produced excellent results in the NEWS 2010 Shared Task on Transliteration (Jiampojamarn et al., 2010b) ."}
{"sent_id": "604807137ee5d9a6775821496c6af5-C001-90", "intents": ["@USE@"], "paper_id": "ABC_604807137ee5d9a6775821496c6af5_39", "text": "We also evaluated the performance of domain adaptation (DA) module of SODA on the Amazon review dataset (Blitzer et al., 2007) which is a benchmark dataset for sentiment categorization."}
{"sent_id": "f6a35ed1ec0c01d3e9faa1ec3d8478-C001-20", "intents": ["@USE@"], "paper_id": "ABC_f6a35ed1ec0c01d3e9faa1ec3d8478_39", "text": "We build on previous work in our lab on disagreement detection, classifying stance, identifying high quality arguments, measuring the properties and the persuasive effects of factual vs. emotional arguments, and clustering arguments into their facets or frames related to a particular topic [9, 1, 13, 18, 16, 12, 15] ."}
{"sent_id": "aebac57baf260be18945feba38d6a1-C001-34", "intents": ["@USE@"], "paper_id": "ABC_aebac57baf260be18945feba38d6a1_39", "text": "The evaluation is done using the metrics precision, recall and accuracy, defined in the following way (Koehn and Knight, 2003 ):"}
{"sent_id": "aebac57baf260be18945feba38d6a1-C001-46", "intents": ["@USE@"], "paper_id": "ABC_aebac57baf260be18945feba38d6a1_39", "text": "Table 2 lists the ones used in our system (Langer, 1998; Marek, 2006; Krott, 1999 Concerning the second step, there is some work that uses, for scoring, additional information such as rules for cognate recognition (Brown, 2002) or sentence-aligned parallel corpora and a translation model, as in the full system described by Koehn and Knight (2003) ."}
{"sent_id": "9bab4741e6b9f132c2851bae3a3cf4-C001-85", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_9bab4741e6b9f132c2851bae3a3cf4_39", "text": "In both Niu [11] and Dang's [10] work, topical features as well as the so called collocational features were used."}
{"sent_id": "4498072885df2a126e2db553cf3aca-C001-61", "intents": ["@USE@"], "paper_id": "ABC_4498072885df2a126e2db553cf3aca_39", "text": "Although most unsupervised distributional approaches are based on the DIH, we also consider the distributional SLQS model based on on an alternative informativeness hypothesis (Santus et al., 2014; Shwartz et al., 2017) ."}
{"sent_id": "e59bd02bb560d80ce08dfcd6b35317-C001-23", "intents": ["@USE@"], "paper_id": "ABC_e59bd02bb560d80ce08dfcd6b35317_39", "text": "We then test the robustness of a popular topic modelling algorithm, Latent Dirichlet Allocation (LDA) using a topic stability measure introduced by Greene et al. (2014) over a variety of corpora."}
{"sent_id": "e59bd02bb560d80ce08dfcd6b35317-C001-28", "intents": ["@USE@"], "paper_id": "ABC_e59bd02bb560d80ce08dfcd6b35317_39", "text": "For the evaluation of topic models, we follow the approach by Greene et al. (2014) for measuring topic model agreement ."}
{"sent_id": "653327ecbc925624d509c679fbe0ba-C001-61", "intents": ["@USE@"], "paper_id": "ABC_653327ecbc925624d509c679fbe0ba_39", "text": "Here we utilize BERT [4] as the pretrained encoder for its superior performance in a range of natural language understanding tasks."}
{"sent_id": "7202fd7fe7e776362b126f7cbf0bf3-C001-17", "intents": ["@BACK@", "@MOT@"], "paper_id": "ABC_7202fd7fe7e776362b126f7cbf0bf3_39", "text": "End-to-end speech models often have millions of parameters [7, 8, 9, 10, 11] ."}
{"sent_id": "a6b450d1113e0e6d3d2813c09d12a8-C001-61", "intents": ["@USE@"], "paper_id": "ABC_a6b450d1113e0e6d3d2813c09d12a8_39", "text": "We use the P G measure in optimal configuration as reported by Gurevych (2005) ."}
{"sent_id": "65fbdd0397473763bca35376d581be-C001-45", "intents": ["@USE@"], "paper_id": "ABC_65fbdd0397473763bca35376d581be_39", "text": "Following Horn et al. (2014) , our system simplifies neither proper nouns, as identified by the Natural Language Toolkit (Bird et al., 2009) , nor words in our stoplist, which are already simple."}
{"sent_id": "6cc36fef99fb1f25370175452f30b0-C001-54", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_6cc36fef99fb1f25370175452f30b0_39", "text": "Even an upper bound of around 98%, which is achieved by Matsuzaki and Tsujii (2008) , is not sufficient, since this guarantees a loss of at least 2%."}
{"sent_id": "65f7546e2abfd74c0daa43c25ca63f-C001-26", "intents": ["@USE@"], "paper_id": "ABC_65f7546e2abfd74c0daa43c25ca63f_39", "text": "This approach is the unification of all components in the work of [6] into a single computational model."}
{"sent_id": "4235dbd05a848d934f17f35894c051-C001-41", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_4235dbd05a848d934f17f35894c051_39", "text": "PredPatt extracts predicates and arguments in four stages (White et al., 2016) : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing."}
{"sent_id": "f7255360eacc4e2a4e8bea2f6ab1b0-C001-68", "intents": ["@USE@"], "paper_id": "ABC_f7255360eacc4e2a4e8bea2f6ab1b0_39", "text": "As in (Hearst, 1997) , boundaries found by the method are weighted and sorted in decreasing order."}
{"sent_id": "f7255360eacc4e2a4e8bea2f6ab1b0-C001-72", "intents": ["@USE@"], "paper_id": "ABC_f7255360eacc4e2a4e8bea2f6ab1b0_39", "text": "Each boundary, which is a minimum of the cohesion graph, was weighted by the sum of the differences between its value and the values of the two maxima around it, as in (Hearst, 1997) ."}
{"sent_id": "932a13e179da50c9189bd0c612cb9c-C001-89", "intents": ["@USE@"], "paper_id": "ABC_932a13e179da50c9189bd0c612cb9c_39", "text": "With the help of an efficient implementation, it avoids any translation candidate selection at the document level (Resnik and Smith, 2003; Smith, 2002; Snover et al., 2008; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Fung and Cheung, 2004) ."}
{"sent_id": "5f25b6a3bcaca2e4beb59ce0f3eb5f-C001-17", "intents": ["@UNSURE@"], "paper_id": "ABC_5f25b6a3bcaca2e4beb59ce0f3eb5f_40", "text": "For a more formal distinction between the conventional and iterative approach to WSD, please refer to this paper (Manion and Sainudiin, 2014 Table 1 : Parts of Speech disambiguated (as percentages) for each SemEval Task (denoted by its year)."}
{"sent_id": "0e4ca87c0e2b899bfd1f36dc5974b9-C001-40", "intents": ["@USE@"], "paper_id": "ABC_0e4ca87c0e2b899bfd1f36dc5974b9_40", "text": "For NLI, we use the Vietnamese validation and test sets from the XNLI corpus v1.0 [Conneau et al., 2018] where the Vietnamese training data is machinetranslated from English."}
{"sent_id": "f60796ff05156e81c4b183cdcb05ae-C001-56", "intents": ["@USE@"], "paper_id": "ABC_f60796ff05156e81c4b183cdcb05ae_40", "text": "All digits were replaced with special \"#\" tokens following [9] ."}
{"sent_id": "b4093db328fd6839777a6d34507b34-C001-24", "intents": ["@USE@"], "paper_id": "ABC_b4093db328fd6839777a6d34507b34_40", "text": "The data comes from (Barrett and Søgaard, 2015) and is publicly available 1 ."}
{"sent_id": "7516b533aafa8b41e7e554fa54e39c-C001-12", "intents": ["@USE@"], "paper_id": "ABC_7516b533aafa8b41e7e554fa54e39c_40", "text": "In this note we will focus on the multi-choice MRC tasks, more specifically, the DREAM task [Sun et al., 2019 ]."}
{"sent_id": "5f5a59f8fbf999b9eecfe7c1897b2c-C001-14", "intents": ["@USE@"], "paper_id": "ABC_5f5a59f8fbf999b9eecfe7c1897b2c_40", "text": "The parameters of the Malt models were set to the values reported in (Hall et al., 2007) ."}
{"sent_id": "3c74f66c209335ea33dda38c203199-C001-39", "intents": ["@USE@"], "paper_id": "ABC_3c74f66c209335ea33dda38c203199_40", "text": "The trained models are evaluated on the Italian section of the syntactic benchmark provided by Gulordava et al. (2018) , which includes various non-trivial number agreement constructions."}
{"sent_id": "d6c8b712c8fe3dd87d23886d575098-C001-22", "intents": ["@USE@"], "paper_id": "ABC_d6c8b712c8fe3dd87d23886d575098_40", "text": "In-domain Fine-tuning The Transformer models were finally fine-tuned using the small in-domain parallel data provided for the News task (Luong and Manning, 2015; Schamper et al., 2018) ."}
{"sent_id": "9e8c386b7ea3b5e4e2843fb8382fd8-C001-40", "intents": ["@SIM@", "@DIF@"], "paper_id": "ABC_9e8c386b7ea3b5e4e2843fb8382fd8_41", "text": "In line with previous findings on the Dundee corpus (van Schijndel and Schuler, 2015) , cumulative 5-grams provide a significant improvement over basic n-grams (p < 0.001), but unlike previous work, basic n-grams do not improve over cumulative n-grams on this corpus (p > 0.05)."}
{"sent_id": "f59a8c650583343fe372db42fc109a-C001-11", "intents": ["@USE@"], "paper_id": "ABC_f59a8c650583343fe372db42fc109a_41", "text": "We trained Transformer models (Vaswani et al., 2017) using Sockeye 1 (Hieber et al., 2017) ."}
{"sent_id": "9e491cad55265802275e9aaea9faae-C001-19", "intents": ["@USE@"], "paper_id": "ABC_9e491cad55265802275e9aaea9faae_41", "text": "For the target datasets, we evaluate on two recent QA datasets, WikiQA (Yang et al., 2015) and Se-mEval 2016 (Task 3A) , which possess sufficiently different characteristics from that of SQuAD."}
{"sent_id": "bd64b244756259f18f7c3cb60989a2-C001-44", "intents": ["@USE@"], "paper_id": "ABC_bd64b244756259f18f7c3cb60989a2_41", "text": "Finally, the misalignment detection tool Bicleaner (Sánchez-Cartagena et al. 2018 ) was applied to these aligned sentences (the Bicleaner threshold was set to 0.5 8 )."}
{"sent_id": "e177758a227506bbf9de48f8f35715-C001-42", "intents": ["@USE@"], "paper_id": "ABC_e177758a227506bbf9de48f8f35715_41", "text": "To induce a bilingual dictionary for a pair of languages, we use the projection matrix approach (Mikolov et al., 2013b; Lazaridou et al., 2015) ."}
{"sent_id": "d63acda66b0c17c5c6725c0e20b2d9-C001-8", "intents": ["@USE@"], "paper_id": "ABC_d63acda66b0c17c5c6725c0e20b2d9_41", "text": "We use referential translation machine (RTM) (Biçici, 2018; Biçici and Way, 2015) models for building our prediction models."}
{"sent_id": "d63acda66b0c17c5c6725c0e20b2d9-C001-25", "intents": ["@USE@"], "paper_id": "ABC_d63acda66b0c17c5c6725c0e20b2d9_41", "text": "We use prediction averaging (Biçici, 2018) to obtain a combined prediction from various prediction outputs better than the components, where the performance on the training set is used to obtain weighted average of the top k predictions,ŷ with evaluation metrics indexed by j ∈ J and weights with w:"}
{"sent_id": "0b2e3651610aba4bd7150eee50797f-C001-90", "intents": ["@USE@"], "paper_id": "ABC_0b2e3651610aba4bd7150eee50797f_41", "text": "In this setting, we gradually set the phrase length and the distortion limits of the phrase-based decoder (context size) to 7, 9, 11 and 13, in order to remove the disadvantage of shorter context size of using character as WSR for fair comparison with WordSys as suggested by Duan et al. (2010) ."}
{"sent_id": "021c423c731ecbe3e26b3ce234b390-C001-78", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_021c423c731ecbe3e26b3ce234b390_41", "text": "These methods have been used for fake news detection in previous work (Rashkin et al., 2017; Wang, 2017 fore, we use this model to demonstrate how a classifier trained on data labeled according to publisher's reputation would identify misinformative news articles."}
{"sent_id": "ef6bd5e57196c013d7d0436e5b0ca5-C001-16", "intents": ["@USE@"], "paper_id": "ABC_ef6bd5e57196c013d7d0436e5b0ca5_42", "text": "The architecture of our system is designed by following the official baseline system (Thorne et al., 2018) ."}
{"sent_id": "cd7bb4543828f915bc930841bb8d7c-C001-10", "intents": ["@BACK@", "@EXT@"], "paper_id": "ABC_cd7bb4543828f915bc930841bb8d7c_42", "text": "One exception is (Goyal et al., 2016) , who employed a char-based seq2seq model where the input MR is simply represented as a character sequence, and the output is also generated char-by-char; this approach avoids the rare word problem, as the character vocabulary is very small."}
{"sent_id": "6d8612cfb4bf05322fed1c02f4885a-C001-66", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_6d8612cfb4bf05322fed1c02f4885a_42", "text": "We quantify the number of hedges (Hyland, 2005) and abstract words 1 used, and the ratio of standalone numbers stated per user as these are indicators of specificity (Pennebaker et al., 2003; Pitler and Nenkova, 2008) ."}
{"sent_id": "ee219d599e0e0c2bbebc1849863005-C001-30", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_ee219d599e0e0c2bbebc1849863005_42", "text": "As an alternative that does not necessarily require pre-existing knowledge of the typological features in the language at hand, Littell et al. (2017) have proposed a method for inferring typological features directly from the language's k nearest neighbors (k-NN) according to geodesic distance (distance on the Earth's surface) and genetic distance (distance according to a phylogenetic family tree)."}
{"sent_id": "4705e8c0bac7bf29d8ef3193cf729b-C001-72", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_4705e8c0bac7bf29d8ef3193cf729b_43", "text": "Niu [11] proved in his experiments that Naïve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words)."}
{"sent_id": "4705e8c0bac7bf29d8ef3193cf729b-C001-52", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_4705e8c0bac7bf29d8ef3193cf729b_43", "text": "Another similar study for Chinese [11] is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context."}
{"sent_id": "3fe979e570992b79c8656ab6cb34fb-C001-9", "intents": ["@USE@", "@BACK@"], "paper_id": "ABC_3fe979e570992b79c8656ab6cb34fb_43", "text": "We base these regular extensions on intersective Levin classes, a fine-grained variation on Levin classes, as a source of semantic components associated with specific adjuncts (Dang et al., 1998) ."}
{"sent_id": "1b424cab4d7008997a31be8c2e5198-C001-12", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_1b424cab4d7008997a31be8c2e5198_44", "text": "Most notably, for the natural language inference task, augmenting the hypothesis (u) and premise (v) representations with |u−v| and u · v considerably improves performance in a siamese architecture Mou et al. (2016) ."}
{"sent_id": "3c4c0875593ed0f196f5295fbaeb37-C001-17", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_3c4c0875593ed0f196f5295fbaeb37_44", "text": "Recent advances in corpus-based NLG (Dušek and Jurčíček, 2015; Wen et al., 2015; Mei et al., 2016; Wen et al., 2016; Dušek and Jurčíček, 2016; Lampouras and Vlachos, 2016) require costly training data, consisting of meaning representations (MRs) paired with corresponding NL texts."}
{"sent_id": "5ad1e8b75cc6f5b627f770cced8e0f-C001-75", "intents": ["@BACK@", "@USE@"], "paper_id": "ABC_5ad1e8b75cc6f5b627f770cced8e0f_44", "text": "From the Gold corpus, also the source text (numbered in the repository with 1, see Madnani et al. (2012) ) serves as reference, and the paraphrastic reuse of it (numbered with 2), provides the system output."}
{"sent_id": "fbd028e073459b1b4c2d8d99173e15-C001-31", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_fbd028e073459b1b4c2d8d99173e15_44", "text": "On the FrameNet 1.5 data, presented additional semi-supervised experiments using gold targets, which was recently outperformed by an approach presented by Hermann et al. (2014) that made use of distributed word representations."}
{"sent_id": "ec3702a6b30057fcae65ca297656d2-C001-24", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_ec3702a6b30057fcae65ca297656d2_46", "text": "Examples of this approach are rarer, and we briefly mention two: Enright and Kondrak (2007) use singleton words (hapax legomena) to represent documents in a bilingual collection for the task of detecting document translation pairs, and Krstovski and Smith (2011) construct a vocabulary of overlapping words to represent documents in multilingual collections."}
{"sent_id": "45723171ec398550e687c57d42e7cc-C001-47", "intents": ["@UNSURE@"], "paper_id": "ABC_45723171ec398550e687c57d42e7cc_46", "text": "Further information on the data sets can be found in Weissenbacher et al. (2018) ."}
{"sent_id": "0924035155d4bbac7768c65fbe8f9a-C001-7", "intents": ["@UNSURE@"], "paper_id": "ABC_0924035155d4bbac7768c65fbe8f9a_47", "text": "Our Generation Challenges 2011 shared task system represents an initial attempt to develop a surface realizer for shared task inputs that takes advantage of prior work on broad coverage realization with OpenCCG (White, 2006; Espinosa et al., 2008; Rajkumar and White, 2010) ."}
{"sent_id": "2fdfa1b36fcf0d77826c96101ac428-C001-15", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_2fdfa1b36fcf0d77826c96101ac428_48", "text": "To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017) ."}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-321", "intents": ["@USE@", "@DIF@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "Rationale from (Lei et al., 2016) (Acc: 76.4%):"}
{"sent_id": "8c7722ecab0d6a21e15ce63b8a47f5-C001-69", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_8c7722ecab0d6a21e15ce63b8a47f5_0", "text": "Many previous works (Lei et al., 2016; Chen et al., 2018a) follows the above definition of rationales."}
{"sent_id": "72323bc821355923b8c4444ee37ef9-C001-143", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_72323bc821355923b8c4444ee37ef9_0", "text": "Note that the neighborhood of x corresponds to what we previously termed the \"top generators\" of x [18] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-26", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "Today's status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question [3] , [1] , [5] ."}
{"sent_id": "d8a9f0578e389f8cbe153783dc47b3-C001-109", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_d8a9f0578e389f8cbe153783dc47b3_0", "text": "In practice, prior work deems answers as 100% valid using blind trust (i.e., m = 1 person) [22] as well as more conservative answer validation schemes (i.e., m = 3 people) [3] ."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-232", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "Barrachina et al. [2009] , González-Rubio et al. [2013] and Knowles and Koehn [2016] present an interactive NMT model with the uni-directional interaction protocol (UniDiR), in which users can only interact with the model from left to right."}
{"sent_id": "9b6c02d11028b5bf3813a7d61fed28-C001-243", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_9b6c02d11028b5bf3813a7d61fed28_0", "text": "Our proposed revision memory is inspired by the CopyNet [Gu et al., 2016] and history cache [Tu et al., 2017] , which only focus on better using source or global context in supervised learning, and is different from our model."}
{"sent_id": "2b25e38db7fd20c92d677b73af110c-C001-92", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_2b25e38db7fd20c92d677b73af110c_0", "text": "Indeed, in [2] , the final vectorial representation of a piece of text is defined by the sum of the embeddings Ω with the position encoding E, which would require d model = d input ."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-68", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "(1) Filippova et al. (2015) first encoded the input sentence in its reverse order using the same LSTM before processing the sentence for sequence labeling."}
{"sent_id": "f4becae9cd7eeaa7fd3085ff904aaa-C001-72", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_f4becae9cd7eeaa7fd3085ff904aaa_0", "text": "(4) Filippova et al. (2015) combined the predicted y i−1 with w i to help predict y i ."}
{"sent_id": "52b5ed7a0753402ad4bceb83a2b495-C001-147", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_52b5ed7a0753402ad4bceb83a2b495_1", "text": "Other models such as skipthought vectors (Kiros et al., 2015) and SDAE (Hill et al., 2016) requires building an encoder-decoder model which takes time 3 to learn."}
{"sent_id": "0af8cacc0f85bb557e1943e32450e2-C001-260", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_0af8cacc0f85bb557e1943e32450e2_1", "text": "Most of the top systems build upon either BERT (Devlin et al., 2019) or XLNet , both of which rely on additional external training data."}
{"sent_id": "20b605ec3596ccd204b60cf893b738-C001-49", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_20b605ec3596ccd204b60cf893b738_1", "text": "Zwarts and Dras (2008) explore the Collins et al. (2005) finding by examining whether machine learning techniques can be used to predict, on a sentence-by-sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative."}
{"sent_id": "68b5e39365b153dd2bef32845617f2-C001-45", "intents": ["@DIF@", "@BACK@"], "paper_id": "ABC_68b5e39365b153dd2bef32845617f2_1", "text": "While [Berant and Liang, 2014] is suited for factoid questions with a modest amount of compositionality, [Wang et al., 2015] targets more complicated questions."}
{"sent_id": "50cdfe539f84d793ec50873b5ab066-C001-192", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_50cdfe539f84d793ec50873b5ab066_1", "text": "The models considered in Kottur et al. (2017) were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete."}
{"sent_id": "bead17ef9512f960461b681a78be4c-C001-65", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_bead17ef9512f960461b681a78be4c_2", "text": "Experiments by Bolukbasi et al. (2016) found that debiasing word embeddings using the subspace projection method precludes gender-biased analogies from holding."}
{"sent_id": "64b344bf8ec9b6a113bf6b3f638528-C001-256", "intents": ["@BACK@", "@SIM@"], "paper_id": "ABC_64b344bf8ec9b6a113bf6b3f638528_2", "text": "Recently, Santos et al. (2015) presented their CharWNN network, which augments the neural network of Collobert et al. (2011b) with character level CNNs, and they reported improved performance on Spanish and Portuguese NER."}
{"sent_id": "27be8a173136e48a15f637278fd831-C001-101", "intents": ["@UNSURE@"], "paper_id": "ABC_27be8a173136e48a15f637278fd831_2", "text": "While word pairs and grammatical production rules were the most predictive features in (Stab and Gurevych, 2014b) , we hypothesize that this large and sparse feature space may have negative impact on model robustness (Nguyen and Litman, 2015) ."}
{"sent_id": "4f646eceef2e5fc447a367488b6aaf-C001-271", "intents": ["@UNSURE@"], "paper_id": "ABC_4f646eceef2e5fc447a367488b6aaf_2", "text": "In contrast, a similar hand-crafted random-heading-and-forward model in VLN yields a 16.3% success rate [4] ."}
{"sent_id": "d1dce63d89e8cfc73962413734bf7b-C001-191", "intents": ["@UNSURE@"], "paper_id": "ABC_d1dce63d89e8cfc73962413734bf7b_2", "text": "the previous X-PS method of Ponti et al. (2018) , and 2) the full-vocabulary post-specialization step is again useful as the initial CSLRI-AR model cannot match the performance of CSLRI-PS."}
{"sent_id": "c4cc8d4013b0259eb626d06750e4ab-C001-190", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_c4cc8d4013b0259eb626d06750e4ab_2", "text": "Sequence-to-sequence models (Iyer et al., 2017; Finegan-Dollak et al., 2018) , as shown in the Table 3 , showed poor performance for the query-based split in the zero-shot setting."}
{"sent_id": "7700b6c3c096d5cd7999c34e7614f7-C001-48", "intents": ["@BACK@", "@DIF@"], "paper_id": "ABC_7700b6c3c096d5cd7999c34e7614f7_2", "text": "In (Bansal et al., 2014) , a structural learning model is developed to induce a globally optimal hierarchy."}
{"sent_id": "0e5c3df8309dbaf93d10c94fb292fc-C001-114", "intents": ["@DIF@", "@BACK@"], "paper_id": "ABC_0e5c3df8309dbaf93d10c94fb292fc_3", "text": "TaskSp Iida et al. (2011) used 14 task-specific features, three of which they found to be the most informative in their model."}
{"sent_id": "60b0b54af27a6b04a6708a60834952-C001-213", "intents": ["@DIF@", "@FUT@"], "paper_id": "ABC_60b0b54af27a6b04a6708a60834952_3", "text": "Mean-over-time produced competitive results on many prompts, but contrary to Taghipour and Ng (2016) , bidirectional LSTMS and attention produced some of the best results, which is consistent with results for neural models on other text classification tasks (e.g., Longpre et al. (2016) )."}
{"sent_id": "845c66e6dfafc21ab90e5aa5cbf947-C001-83", "intents": ["@UNSURE@"], "paper_id": "ABC_845c66e6dfafc21ab90e5aa5cbf947_4", "text": "Here we are limited to the subset of the training set that has a sufficient number of segments per word to train a good classifier, and the output dimensionality is equal to the number of words (but see [14] for a study of varying the dimensionality in such a classifier-based embedding model by introducing a bottleneck layer)."}
{"sent_id": "845c66e6dfafc21ab90e5aa5cbf947-C001-159", "intents": ["@UNSURE@"], "paper_id": "ABC_845c66e6dfafc21ab90e5aa5cbf947_4", "text": "We include a comparison with the best prior results on this task from [14] , as well as the result of using standard DTW on the input MFCCs (reproduced from [14] ) and the best prior result using DTW, obtained with frame features learned with correlated autoencoders [22] ."}
{"sent_id": "163770df02c1110edc60e7cac90ad2-C001-185", "intents": ["@UNSURE@"], "paper_id": "ABC_163770df02c1110edc60e7cac90ad2_4", "text": "For both tasks, we compare our models with \"Baseline\" and \"MMI\" [26] ."}
{"sent_id": "a774b918013dbf60eb8cc0ad1de2f9-C001-178", "intents": ["@UNSURE@"], "paper_id": "ABC_a774b918013dbf60eb8cc0ad1de2f9_4", "text": "In the Blitzer et al. (2007) task (top tables), AE-SCL-SR is the best performing model in 9 of 12 setups and on a unified test set consisting of the test sets of all 12 setups (the Test-All column)."}
{"sent_id": "7c8f54479ce1f9d81b49839425f58e-C001-188", "intents": ["@UNSURE@"], "paper_id": "ABC_7c8f54479ce1f9d81b49839425f58e_4", "text": "We compare our model and the type of linguistic reasoning it is capable of to InferSent BiL-STM max pooling model (Conneau et al., 2017) , in order to see what benefits are achieved by adding a hierarchical BiLSTM max pooling structure on top of the basic BiLSTM max pooling architecture."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-187", "intents": ["@UNSURE@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "We compare our model with the feature-based model (MH-F) on hypergraph structure (Lu and Roth, 2015) on both entity mention detection as well as the joint mention and mention heads extraction."}
{"sent_id": "74b8684eaabda30a2d8705adcb19a2-C001-204", "intents": ["@UNSURE@"], "paper_id": "ABC_74b8684eaabda30a2d8705adcb19a2_4", "text": "We show the performance of our approaches in Table 1 compared to the previous state-of-the-art system (Lu and Roth, 2015; Muis and Lu, 2017) on both the ACE2004 and ACE2005 datasets."}
{"sent_id": "fde7f77d4685e1c9ce32a82aed4683-C001-79", "intents": ["@UNSURE@"], "paper_id": "ABC_fde7f77d4685e1c9ce32a82aed4683_4", "text": "To show the effect of α when only relevant (w, S) pairs are used, Figure 3 .2 plots the SCWS score (see Section 4; also (Huang et al., 2012) ) for varying α, for the top-performing embedding from each method."}
{"sent_id": "32e860cdf03df7f6cb58b7f9e85ac0-C001-109", "intents": ["@UNSURE@"], "paper_id": "ABC_32e860cdf03df7f6cb58b7f9e85ac0_4", "text": "In addition, we show a comparison to word-vectors released by (Kiela and Clark, 2015) in the supplementary material."}
{"sent_id": "0a7710557d020087035f4a94b5661c-C001-200", "intents": ["@UNSURE@"], "paper_id": "ABC_0a7710557d020087035f4a94b5661c_5", "text": "Table  4 compares our accuracies with those reported in (Ganchev et al., 2009) for Bulgarian and Spanish."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-64", "intents": ["@MOT@", "@DIF@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "However, as (Barzilay & Lee, 2003) do not propose any evaluation of which clustering algorithm should be used, we experiment a set of clustering algorithms and present the comparative results."}
{"sent_id": "5c13e64d468b8a1c403072f213c992-C001-81", "intents": ["@UNSURE@"], "paper_id": "ABC_5c13e64d468b8a1c403072f213c992_5", "text": "In fact, table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by (Barzilay & Lee, 2003) who only keep the clusters that contain more than 10 sentences."}
{"sent_id": "4cb16f436d910d82c3661052c1fa30-C001-27", "intents": ["@UNSURE@"], "paper_id": "ABC_4cb16f436d910d82c3661052c1fa30_5", "text": "The experiments conducted on MUC and ACE data indicate state-of-the-art results when compared with the methods reported in (Ng and Cardie, 2002) and (Luo et al., 2004) ."}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-113", "intents": ["@MOT@", "@SIM@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "Following Levy et al. (2017) , we distinguish between the traditional RE setting where the aim is to generalize to unseen entities (UnENT) and the zero-shot setting (UnREL) where the aim is to do so for unseen relation types (see Section 2)."}
{"sent_id": "1ac16c74cc5bb4099ae07f89d7f148-C001-202", "intents": ["@UNSURE@"], "paper_id": "ABC_1ac16c74cc5bb4099ae07f89d7f148_5", "text": "We computed the average length of the context in out dataset and Levy et al. (2017) Table 4 : Avarage number of tokens in the context."}
{"sent_id": "b71321a9252376308d627c439e85b7-C001-104", "intents": ["@UNSURE@"], "paper_id": "ABC_b71321a9252376308d627c439e85b7_5", "text": "BiRNN CNN Baseline (Bastings et al., 2017) 14.1 12.1 +Sem (1L) 14.3 12.5 +Sem (2L) 14.4 12.6 +Sem (3L) 14.4 12.7 +Syn (2L) (Bastings et al., 2017) 14.8 13.1 +SelfLoop (1L) 14.1 12.1 +SelfLoop (2L) 14.2 11.5 +SemSyn (1L) 14.1 12.7 +Syn (1L) + Sem (1L) 14.7 12.7 +Syn (1L) + Sem (2L) 14.6 12.8 +Syn (2L) + Sem (1L) 14.9 13.0 +Syn (2L) + Sem (2L) 14.9 13.5"}
{"sent_id": "1baddfeea7d11fc02cc26ff698a601-C001-134", "intents": ["@UNSURE@"], "paper_id": "ABC_1baddfeea7d11fc02cc26ff698a601_5", "text": "Compared to the results in [4] , the improved SA proposed in Section 2.2 gives the same WERs but lower PPLs in using the same feature types."}
{"sent_id": "310272015a781b05c42015c0559b18-C001-168", "intents": ["@UNSURE@"], "paper_id": "ABC_310272015a781b05c42015c0559b18_6", "text": "However, it cannot be blindly assumed that children are selecting low points over as short a window as Brent's (1999a) MI and TP models suggest."}
{"sent_id": "5b17eb75600820a80b3573bf74c427-C001-142", "intents": ["@UNSURE@"], "paper_id": "ABC_5b17eb75600820a80b3573bf74c427_6", "text": "We do not use DA features because we lack an automatic DA tagger, nor do we use prosodic features because (Fernández et al., 2008) was unable to derive any value from them with SVMs."}
{"sent_id": "e831e058f208542af16c1ea236d2c9-C001-36", "intents": ["@SIM@", "@MOT@"], "paper_id": "ABC_e831e058f208542af16c1ea236d2c9_10", "text": "We include the distributions with punctuation is to match with the baseline system reported in [Charniak and Johnson 2001] , where punctuation is included to identify the edited regions."}
{"sent_id": "e831e058f208542af16c1ea236d2c9-C001-59", "intents": ["@UNSURE@"], "paper_id": "ABC_e831e058f208542af16c1ea236d2c9_10", "text": "In the rest of the section, we first briefly introduce the boosting algorithm, then describe the method used in [Charniak and Johnson 2001] , and finally we contrast our improvements with the baseline system."}
{"sent_id": "ffd65a1a02c852a2670b471fb4b110-C001-17", "intents": ["@UNSURE@"], "paper_id": "ABC_ffd65a1a02c852a2670b471fb4b110_10", "text": "In this work, we focus specifically on modeling physical plausibility as presented by Wang et al. (2018) ."}
{"sent_id": "ffd65a1a02c852a2670b471fb4b110-C001-114", "intents": ["@UNSURE@"], "paper_id": "ABC_ffd65a1a02c852a2670b471fb4b110_10", "text": "Performance may benefit from injecting explicit commonsense knowledge into the model, an approach which has previously been used in the supervised setting (Wang et al., 2018) ."}
{"sent_id": "183cf87042a3ad2180ead67555d247-C001-117", "intents": ["@UNSURE@"], "paper_id": "ABC_183cf87042a3ad2180ead67555d247_10", "text": "We compared SentiLR with several state-of-theart pre-trained language representation models: BERT: The pre-trained model based on masked language model and next sentence prediction (Devlin et al., 2019) ."}
{"sent_id": "e3b9c00d792bcddb6eea449179e61e-C001-121", "intents": ["@UNSURE@"], "paper_id": "ABC_e3b9c00d792bcddb6eea449179e61e_10", "text": "The models we compare to are the best models in terms of diversity from [11, 26] , using the single best caption after re-ranking for the latter."}
{"sent_id": "9e0a44722390d0508fbe56785701e6-C001-158", "intents": ["@UNSURE@"], "paper_id": "ABC_9e0a44722390d0508fbe56785701e6_10", "text": "After more analysis, 62% of the false negatives (or 24% of the total examples) were cases were the KB contained the subclass of relation, which we consider a separate relation (although in the data collected by Shwartz et al. (2016) from Yago and Wikidata it is conflated with instance of)."}
{"sent_id": "d9567072d2df6c0010b32e1d1eb676-C001-162", "intents": ["@UNSURE@"], "paper_id": "ABC_d9567072d2df6c0010b32e1d1eb676_10", "text": "Finally, we note that generating sequences larger than 7 characters hurts the BPC of Press et al. (2017) ."}
{"sent_id": "dbe1f1bdf7d94824f6f7cd176a4f6d-C001-163", "intents": ["@UNSURE@"], "paper_id": "ABC_dbe1f1bdf7d94824f6f7cd176a4f6d_10", "text": "Our discussion would not be complete without explaining our results in relation to the recursive neural network model proposed by Ji and Eisenstein (2015) ."}
